We analyze the link between standardization and economic growth by
systematically reviewing leading economics journals, leading economic growth
researchers' articles, and economic growth-related books. We make the following
observations: 1) No article has analyzed the link between standardization and
economic growth in top5 economics journals between 1996 and 2018. 2) A
representative sample of the leading researchers of economic growth has
allocated little attention to the link between standardization and economic
growth. 3) Typically, economic growth textbooks do not contain "standards" or
"standardization" in their word indexes. These findings suggest that the
economic growth theory has neglected the role of standardization.

Accuracy of economic theories and efficiency of economic policy strictly
depend on the choice of the economic variables and processes mostly liable for
description of economic reality. That states the general problem of assessment
of any possible economic variables and processes chargeable for economic
evolution. We show that economic variables and processes described by current
economic theories constitute only a negligible fraction of factors responsible
for economic dynamics. We consider numerous unnoted economic variables and
overlooked economic processes those determine the states and predictions of the
real economics. We regard collective economic variables, collective
transactions and expectations, mean risks of economic variables and
transactions, collective velocities and flows of economic variables,
transactions and expectations as overlooked factors of economic evolution. We
introduce market-based probability of the asset price and consider unnoticed
influence of market stochasticity on randomness of macroeconomic variables. We
introduce economic domain composed by continuous numeric risk grades and
outline that the bounds of the economic domain result in unnoticed inherent
cyclical motion of collective variables, transactions and expectations those
are responsible for observed business cycles. Our treatment of unnoticed and
overlooked factors of theoretical economics and policy decisions preserves a
wide field of studies for many decades for academic researchers, economic
authorities and high-level politicians.

We suggest use continuous numerical risk grades [0,1] of R for a single risk
or the unit cube in Rn for n risks as the economic domain. We consider risk
ratings of economic agents as their coordinates in the economic domain.
Economic activity of agents, economic or other factors change agents risk
ratings and that cause motion of agents in the economic domain. Aggregations of
variables and transactions of individual agents in small volume of economic
domain establish the continuous economic media approximation that describes
collective variables, transactions and their flows in the economic domain as
functions of risk coordinates. Any economic variable A(t,x) defines mean risk
XA(t) as risk weighted by economic variable A(t,x). Collective flows of
economic variables in bounded economic domain fluctuate from secure to risky
area and back. These fluctuations of flows cause time oscillations of
macroeconomic variables A(t) and their mean risks XA(t) in economic domain and
are the origin of any business and credit cycles. We derive equations that
describe evolution of collective variables, transactions and their flows in the
economic domain. As illustration we present simple self-consistent equations of
supply-demand cycles that describe fluctuations of supply, demand and their
mean risks.

We present macroeconomic model that describes evolution of macroeconomic
variables and macroeconomic waves on economic space. Risk ratings of economic
agents play role of their coordinates on economic space. Aggregation of
economic variables like Assets and Investment, Credits and Loans of economic
agents at point x define corresponding macroeconomic variables as functions of
time t and coordinates x on economic space. Evolution of macroeconomic
variables is determined by economic and financial transactions between economic
agents. Such transactions can occur between economic agents with any
coordinates x and y and that reflect non-local "action-at-a-distance" character
of internal macroeconomic interactions. For instance, Buy-Sell transactions
between points x and y on economic space define dynamics of Assets at point x
and Investment at point y. Aggregates of transactions between economic agents
at point x and y on economic space define economic fields as functions of two
coordinates. To describe dynamics of economic fields on economic space we
derive hydrodynamic-like equations. For simple models of interactions between
economic fields we derive hydrodynamic-like equations in a closed form and
obtain wave equations for their perturbations. Economic field waves propagate
on economic space and their amplitudes can grow up as exponent in time and may
disturb economic stability. Diversities of macroeconomic and financial waves on
economic space in simple models uncover importance of wave processes for
macroeconomic modeling and forecasting.

This paper presents macroeconomic model that is based on parallels between
macroeconomic multi-agent systems and multi-particle systems. We use risk
ratings of economic agents as their coordinates on economic space. Aggregates
of economic or financial variables like Investment, Assets, Demand, Credits and
etc. of economic agents near point x define corresponding macroeconomic
variables as functions of time t and coordinates x on economic space. Parallels
between multi-agent and multi-particle systems on economic space allow describe
transition from economic kinetic-like to economic hydrodynamic-like
approximation and derive macroeconomic hydrodynamic-like equations on economic
space. Economic or financial transactions between economic agents determine
evolution of macroeconomic variables This paper describes local macroeconomic
approximation that takes into account transactions between economic agents with
coordinates near same point x on economic space only and describes interaction
between macroeconomic variables by linear differential operators. For simple
model of interaction between macroeconomic variables as Demand on Investment
and Interest Rate we derive hydrodynamic-like equations in a closed form. For
perturbations of these macroeconomic variables we derive macroeconomic wave
equations. Macroeconomic waves on economic space can propagate with exponential
growth of amplitude and cause irregular time fluctuations of macroeconomic
variables or induce economic crises.

The authors of the article have reviewed the scientific literature on the
development of the Russian-Chinese cooperation in the field of combining
economic and logistics projects of the Eurasian Economic Union and the Silk
Road Economic Belt. The opinions of not only Russian, but also Chinese experts
on these projects are indicated, which provides the expansion of the vision of
the concept of the New Silk Road in both countries.

This paper presents a simple model to measure the relative economic growth of
economic systems. The model considers S-Shaped patterns of economic growth
that, represented with a linear model, measure how an economic system grows in
comparison with another one. In particular, this model introduces an approach
which indicates if the economic system has a process of economic growth,
development or under development. The application of the model is provided for
regions and macro regions of the Italian economic system.

This paper presents hydrodynamic-like model of business cycles aggregate
fluctuations of economic and financial variables. We model macroeconomics as
ensemble of economic agents on economic space and agent's risk ratings play
role of their coordinates. Sum of economic variables of agents with coordinate
x define macroeconomic variables as functions of time and coordinates x. We
describe evolution and interactions between macro variables on economic space
by hydrodynamic-like equations. Integral of macro variables over economic space
defines aggregate economic or financial variables as functions of time t only.
Hydrodynamic-like equations define fluctuations of aggregate variables. Motion
of agents from low risk to high risk area and back define the origin for
repeated fluctuations of aggregate variables. Economic or financial variables
on economic space may define statistical moments like mean risk, mean square
risk and higher. Fluctuations of statistical moments describe phases of
financial and economic cycles. As example we present a simple model relations
between Assets and Revenue-on-Assets and derive hydrodynamic-like equations
that describe evolution and interaction between these variables.
Hydrodynamic-like equations permit derive systems of ordinary differential
equations that describe fluctuations of aggregate Assets, Assets mean risks and
Assets mean square risks. Our approach allows describe business cycle aggregate
fluctuations induced by interactions between any number of economic or
financial variables.

This paper aims to study the impact of public and private investments on the
economic growth of developing countries. The study uses the panel data of 39
developing countries covering the periods 1990-2019. The study was based on the
neoclassical growth models or exogenous growth models state in which land,
labor, capital accumulation, etc., and technology proved substantial for
economic growth. The paper finds that public investment has a strong positive
impact on economic growth than private investment. Gross capital formation,
labor growth, and government final consumption expenditure were found
significant in explaining the economic growth. Overall, both public and private
investments are substantial for the economic growth and development of
developing countries.

Any finite conversation can be rationalized.

We use H\"older's inequality to get simple derivations of certain economic
formulas involving CES, Armington, or $n$-stage Armington functions.

At present, the economic measurement of the national statistical offices has
not defined or captured the benefits of the digital economy activity due to the
low quality or inexistence of methodologies. Currently, there is a relevant
debate on the capacity of the digital economy activity to generate
productivity, economic growth, and well-being through innovation and knowledge.
For this reason, this research identified and studied specialized knowledge,
human settlement, and digital economic activity as the factors that influence
regional economic growth. As a result, the impact generated by a new business
operating models based on information technology was measured. Furthermore,
this research used an empirical measurement model that made it possible to
identify certain phenomena such as regional poles of regional economic
development (PRDE) that surround economically flourishing regions. In addition,
it showed that municipalities with high degrees of economic growth were
impacted by digital economic activity and specialized knowledge. This finding
is consistent with economic growth theories that point to technological
evolution as the main factor of modern economic growth. Consequently, this
study contributed beneficial results to the local government to develop
strategies framed in solving industrial cooperation of economically flourishing
regions with their neighbors, facing the problem of agglomeration of resources
and capital reflected in human settlement promote an imbalance in economic
growth and social development.

The coronavirus disease (COVID-19) has caused one of the most serious social
and economic losses to countries around the world since the Spanish influenza
pandemic of 1918 (during World War I). It has resulted in enormous economic as
well as social costs, such as increased deaths from the spread of infection in
a region. This is because public regulations imposed by national and local
governments to deter the spread of infection inevitably involves a deliberate
suppression of the level of economic activity. Given this trade-off between
economic activity and epidemic prevention, governments should execute public
interventions to minimize social and economic losses from the pandemic. A major
problem regarding the resultant economic losses is that it unequally impacts
certain strata of the society. This raises an important question on how such
economic losses should be shared equally across the society. At the same time,
there is some antipathy towards economic compensation by means of public debt,
which is likely to increase economic burden in the future. However, as Paul
Samuelson once argued, much of the burden, whether due to public debt or
otherwise, can only be borne by the present generation, and not by future
generations.

This paper investigates the relationship between economic media sentiment and
individuals' expetations and perceptions about economic conditions. We test if
economic media sentiment Granger-causes individuals' expectations and opinions
concerning economic conditions, controlling for macroeconomic variables. We
develop a measure of economic media sentiment using a supervised machine
learning method on a data set of Swedish economic media during the period
1993-2017. We classify the sentiment of 179,846 media items, stemming from
1,071 unique media outlets, and use the number of news items with positive and
negative sentiment to construct a time series index of economic media
sentiment. Our results show that this index Granger-causes individuals'
perception of macroeconomic conditions. This indicates that the way the
economic media selects and frames macroeconomic news matters for individuals'
aggregate perception of macroeconomic reality.

Existence theory in economics is usually in real domains such as the findings
of chaotic trajectories in models of economic growth, tatonnement, or
overlapping generations models. Computational examples, however, sometimes
converge rapidly to cyclic orbits when in theory they should be nonperiodic
almost surely. We explain this anomaly as the result of digital approximation
and conclude that both theoretical and numerical behavior can still illuminate
essential features of the real data.

The south central ecoregion was a mosiac ecoregion of forest and grassland
continnum which is transiting towards closed canopy forests and losing
ecosystem benefits. We studied role of active management, its economic benefit,
and landonwers atttitdue and behavior towards restoring ecosystem services in
this region. We further studed how the economic benefit varies in this region
with the change in rainfall.

In a series of papers, Ole Peters and his collaborators claim that the
'conceptual basis of mainstream economic theory' is 'flawed' and that the
approach they call 'ergodicity economics' gives 'reason to hope for a future
economic science that is more parsimonious, conceptually clearer and less
subjective' (Peters, 2019). This paper argues that 'ergodicity economics' is
pseudoscience because it has not produced falsifiable implications and should
be taken with skepticism.

Development and growth are complex and tumultuous processes. Modern economic
growth theories identify some key determinants of economic growth. However, the
relative importance of the determinants remains unknown, and additional
variables may help clarify the directions and dimensions of the interactions.
The novel stream of literature on economic complexity goes beyond aggregate
measures of productive inputs, and considers instead a more granular and
structural view of the productive possibilities of countries, i.e. their
capabilities. Different endowments of capabilities are crucial ingredients in
explaining differences in economic performances. In this paper we employ
economic fitness, a measure of productive capabilities obtained through complex
network techniques. Focusing on the combined roles of fitness and some more
traditional drivers of growth, we build a bridge between economic growth
theories and the economic complexity literature. Our findings, in agreement
with other recent empirical studies, show that fitness plays a crucial role in
fostering economic growth and, when it is included in the analysis, can be
either complementary to traditional drivers of growth or can completely
overshadow them.

This paper constructs a global economic policy uncertainty index through the
principal component analysis of the economic policy uncertainty indices for
twenty primary economies around the world. We find that the PCA-based global
economic policy uncertainty index is a good proxy for the economic policy
uncertainty on a global scale, which is quite consistent with the GDP-weighted
global economic policy uncertainty index. The PCA-based economic policy
uncertainty index is found to be positively related with the volatility and
correlation of the global financial market, which indicates that the stocks are
more volatile and correlated when the global economic policy uncertainty is
higher. The PCA-based global economic policy uncertainty index performs
slightly better because the relationship between the PCA-based uncertainty and
market volatility and correlation is more significant.

Since Reform and Opening-up 40 years ago, China has made remarkable
achievements in economic fields. And consumption activities, including
household consumption, have played an important role in it. Consumer activity
is the end of economic activity, because the ultimate aim of other economic
activities is to meet consumer demand; consumer activity is the starting point
of economic activity, because consumption can drive economic and social
development. This paper selects the economic data of more than 40 years since
Reform and Opening-up, and establishes the Vector Autoregressive (VAR) model
and Vector Error Correction (VEC) model, analyzing the influence of consumption
level and total consumption of urban and rural residents on economic growth.
The conclusion is that the increase of urban consumption and rural consumption
can lead to the increase of GDP, and in the long run, urban consumption can
promote economic growth more than rural consumption. According to this
conclusion, we analyze the reasons and puts forward some policy suggestions.

How can econophysics contribute to economics? Since the relation between
basic principles of physics and economics is not established, there is no
reason why physical theories should be of any value for economic theory. While
economic theories leave the physicists largely without orientation in this
field, econo-physicists should orient themselves at concrete problems from
economic practice rather than from economic academics. Thereby physicists
should respect the lead of economists. This then also puts physics closer to
econometrics. Then the natural strength of physics in dealing with empirical
data as the fundamental basis for its research could be valuable for both,
theory as well as for applications in economics.

From positions, attained by modern theoretical physics in understanding of
the universe bases, the methodological and philosophical analysis of
fundamental physical concepts and their formal and informal connections with
the real economic measurings is carried out. Procedures for heterogeneous
economic time determination, normalized economic coordinates and economic mass
are offered, based on the analysis of time series, the concept of economic
Plank's constant has been proposed. The theory has been approved on the real
economic dynamic's time series, including stock indices, Forex and spot prices,
the achieved results are open for discussion.

We consider a number of Artificial Chemistry models for economic activity and
what consequences they have for the formation of economic inequality. We are
particularly interested in what tax measures are effective in dampening
economic inequality. By starting from well-known kinetic exchange models, we
examine different scenarios for reducing the tendency of economic activity
models to form unequal wealth distribution in equilibrium.

The authors of the article analyze the content of the Eurasian integration,
from the initial initiative to the modern Eurasian Economic Union, paying
attention to the factors that led to the transition from the Customs Union and
the Single Economic Space to a stronger integration association. The main
method of research is historical and legal analysis.

The paper discusses the process of social and economic development of
municipalities. A conclusion is made that developing an adequate model of
social and economic development using conventional approaches presents a
considerable challenge. It is proposed to use semantic modeling to represent
the social and economic development of municipalities, and cognitive mapping to
identify the set of connections that occur among indicators and that have a
direct impact on social and economic development.

The article discusses the controversy over infinite growth. It analyzes the
two dominant perspectives on economic growth and finds them based on a limited
and subjective view of reality. An examination of the principal aspects of
economic growth (economic activity, value, and value creation) helps to
understand what fuels economic growth. The article also discusses the
correlations between production, consumption, as well as resources and
population dynamics. The article finds that infinite and exponential economic
growth is essential for the survival of our civilization.

The paper discusses the process of social and economic development of
municipalities. A conclusion is made that developing an adequate model of
social and economic development using conventional approaches presents a
considerable challenge. It is proposed to use semantic modeling to represent
the social and economic development of municipalities, and cognitive mapping to
identify the set of connections that occur among indicators and that have a
direct impact on social and economic development.

This study investigates the long-term effects of temperature variations on
economic growth using a data-driven approach. Leveraging machine learning
techniques, we analyze global land surface temperature data from Berkeley Earth
and economic indicators, including GDP and population data, from the World
Bank. Our analysis reveals a significant relationship between average
temperature and GDP growth, suggesting that climate variations can
substantially impact economic performance. This research underscores the
importance of incorporating climate factors into economic planning and
policymaking, and it demonstrates the utility of machine learning in uncovering
complex relationships in climate-economy studies.

This research paper presents a thorough economic analysis of Bitcoin and its
impact. We delve into fundamental principles, and technological evolution into
a prominent decentralized digital currency. Analysing Bitcoin's economic
dynamics, we explore aspects such as transaction volume, market capitalization,
mining activities, and macro trends. Moreover, we investigate Bitcoin's role in
economy ecosystem, considering its implications on traditional financial
systems, monetary policies, and financial inclusivity. We utilize statistical
and analytical tools to assess equilibrium , market behaviour, and economic .
Insights from this analysis provide a comprehensive understanding of Bitcoin's
economic significance and its transformative potential in shaping the future of
global finance. This research contributes to informed decision-making for
individuals, institutions, and policymakers navigating the evolving landscape
of decentralized finance.

This article presents an analysis of China's economic evolution amidst
demographic changes from 1990 to 2050, offering valuable insights for academia
and policymakers. It uniquely intertwines various economic theories with
empirical data, examining the impact of an aging population, urbanization, and
family dynamics on labor, demand, and productivity. The study's novelty lies in
its integration of Classical, Neoclassical, and Endogenous Growth theories,
alongside models like Barro and Sala-i-Martin, to contextualize China's
economic trajectory. It provides a forward-looking perspective, utilizing
econometric methods to predict future trends, and suggests practical policy
implications. This comprehensive approach sheds light on managing demographic
transitions in a global context, making it a significant contribution to the
field of demographic economics.

Financial inclusion is touted one of the principal drivers for economic
growth for an economy. The study aims to explore the impact of financial
inclusion on economic growth in Bangladesh. In my study, I used the number of
loan accounts as the proxy for financial inclusion. Using time series data from
spans from 2004-2021, the study revealed that there exists a long-run
relationship between GDP, financial inclusion, and other macroeconomic
variables in Bangladesh. The study also found that financial inclusion had a
positive impact on economic growth of Bangladesh during the study period.
Therefore, the policymakers and the central bank of Bangladesh as the apex
authority of financial system should promote financial inclusion activities to
achieve sustainable economic growth.

Recent studies in psychology and neuroscience offer systematic evidence that
fictional works exert a surprisingly strong influence on readers and have the
power to shape their opinions and worldviews. Building on these findings, we
study what we term Potterian economics, the economic ideas, insights, and
structure, found in Harry Potter books, to assess how the books might affect
economic literacy. A conservative estimate suggests that more than 7.3 percent
of the world population has read the Harry Potter books, and millions more have
seen their movie adaptations. These extraordinary figures underscore the
importance of the messages the books convey. We explore the Potterian economic
model and compare it to professional economic models to assess the consistency
of the Potterian economic principles with the existing economic models. We find
that some of the principles of Potterian economics are consistent with
economists models. Many other principles, however, are distorted and contain
numerous inaccuracies, contradicting professional economists views and
insights. We conclude that Potterian economics can teach us about the formation
and dissemination of folk economics, the intuitive notions of naive individuals
who see market transactions as a zero-sum game, who care about distribution but
fail to understand incentives and efficiency, and who think of prices as
allocating wealth but not resources or their efficient use.

This mathematical essay brings together ideas from Economics, Geobiodynamics
and Thermodynamics. Its purpose is to obtain real models of complex
evolutionary systems. More specifically, the essay defines Roegenian Economy
and links Geobiodynamics and Roegenian Economy. In this context, we discuss the
isomorphism between the concepts and techniques of Thermodynamics and
Economics. Then we describe a Roegenian economic system like a Carnot group.
After we analyse the phase equilibrium for two heterogeneous economic systems.
The European Union Economics appears like Cartesian product of Roegenian
economic systems and its Balance is analysed in details. A Section at the end
describes the "economic black holes" as small parts of a a global economic
system in which national income is so great that it causes others poor
enrichment. These ideas can be used to improve our knowledge and understanding
of the nature of development and evolution of thermodynamic-economic systems.

Our computational economic analysis investigates the relationship between
inequality, mobility and the financial accumulation process. Extending the
baseline model by Levy et al., we characterise the economic process through
stylised return structures generating alternative evolutions of income and
wealth through time. First, we explore the limited heuristic contribution of
one and two factors models comprising one single stock (capital wealth) and one
single flow factor (labour) as pure drivers of income and wealth generation and
allocation over time. Second, we introduce heuristic modes of taxation in line
with the baseline approach. Our computational economic analysis corroborates
that the financial accumulation process featuring compound returns plays a
significant role as source of inequality, while institutional arrangements
including taxation play a significant role in framing and shaping the aggregate
economic process that evolves over socioeconomic space and time.

Entrepreneurship is often touted for its ability to generate economic growth.
Through the creative-destructive process, entrepreneurs are often able to
innovate and outperform incumbent organizations, all of which is supposed to
lead to higher employment and economic growth. Although some empirical evidence
supports this logic, it has also been the subject of recent criticisms.
Specifically, entrepreneurship does not lead to growth in developing countries;
it only does in more developed countries with higher income levels. Using
Global Entrepreneurship Monitor data for a panel of 83 countries from 2002 to
2014, we examine the contribution of entrepreneurship towards economic growth.
Our evidence validates earlier studies findings but also exposes previously
undiscovered findings. That is, we find that entrepreneurship encourages
economic growth but not in developing countries. In addition, our evidence
finds that the institutional environment of the country, as measured by GEM
Entrepreneurial Framework Conditions, only contributes to economic growth in
more developed countries but not in developing countries. These findings have
important policy implications. Namely, our evidence contradicts policy
proposals that suggest entrepreneurship and the adoption of pro-market
institutions that support it to encourage economic growth in developing
countries. Our evidence suggests these policy proposals will be unlikely to
generate the economic growth desired.

A thermodynamic approach to the description of economic systems and processes
is developed. It is shown that there is a deep analogy between the parameters
of thermodynamic and economic systems (markets); so each thermodynamic
parameter can be associated with a certain economic parameter or indicator. The
economic meaning of such primordially thermodynamic concepts as internal energy
and temperature has been established. It is shown that many economic laws,
which in economic theory are a generalization of the results of observations,
or are based on the analysis of the psychology of the behavior of market
actors, within the framework of economic thermodynamics can be obtained as the
natural and formal results of the theory. In particular, we show that economic
thermodynamics allows a natural description of such a phenomenon as inflation.
The thermodynamic conditions of market equilibrium stability are derived and
analyzed, as well as the Le Chatelier's principle as applied to economic
systems.

Inferring the uncertainty in economic conditions is significant for both
decision makers as well as market players. In this paper, we propose a novel
approach to measure the economic uncertainties by using the Hidden Markov Model
(HMM). We construct a dimensionless index, Economic Condition Uncertainty (ECU)
index, which ranges from zero to one and is comparable among sectors, regions
and periods. We used the daily electricity consumption data of more than 18,000
firms in Shanghai from 2018 to 2020 to construct the ECU indexes. Results show
that all ECU indexes, whether at sectoral or regional level, successfully
captured the negative impacts of COVID-19 on Shanghai's economic conditions.
Besides, the ECU indexes also presented the heterogeneities in different
districts as well as in different sectors. This reflects the facts that changes
in the uncertainty of economic conditions are mainly related to regional
economic structures and targeted regulatory policies faced by sectors. The ECU
index can also be readily extended to measure the uncertainty of economic
conditions in various realms, which has great potentials in the future.

Economic engineering is a new field wherein economic systems are modelled in
the same manner as traditional mechanical and electrical engineering systems.
In this paper, we use Newton's theory of motion as the basis for the theory of
demand; thereby establishing a theoretical foundation for economic engineering.
We follow Newton's original development, as set forth in the Principia, to
determine economic analogs to his three laws of motion. The pivotal result is
an operational definition for an economic force, i.e. a want or a desire, in
terms of a price adjustment. With this, we model the price effects of scarcity
and trade friction in analogy with the models for the spring and damping force.
In turn, we define economic benefits and surplus as analogous to the
definitions of mechanical work and energy. These are then used to interpret the
various types of economic equilibrium considered by economists from a
mechanical perspective. The effectiveness of the analogy is illustrated by
applying it to modelling the price and inventory dynamics of various economic
agents -- including consumers, dealers, holders, spot and futures traders --
using linear-time invariant systems theory.

This study presents a computational simulation exploring the complex
interactions between population density and economic factors over a 100-year
period. Inspired by the Keller-Segel model, traditionally applied in biological
contexts, my model adapts this framework to analyze urban and economic
dynamics. The simulation employs two coupled partial differential equations to
represent the evolution of population density and money concentration in a
hypothetical region. Population density is initially uniform, while money
concentration begins with a random distribution. The model integrates diffusion
processes for both population and money, coupled with a chemotactic response of
the population towards areas of higher economic activity. Over the course of
the simulation, we observe the emergence of distinct spatial patterns:
population clusters forming around economic hubs and the development of wealth
concentration in certain areas. These patterns highlight the mutual
reinforcement between population density and economic factors. The study
provides insights into the dynamics of urban growth, economic disparities, and
resource distribution, offering a simplified yet powerful lens through which to
view complex socio-economic systems. My findings have implications for urban
planning and policy-making, especially in understanding the long-term evolution
of cities and economic centers.

This study offers an examination of grassroots innovation actors and their
integration within larger economic ecosystems. Through a comparative analysis
in Oaxaca, Mexico; La Plata, Argentina; and Araucania, Chile, this research
sheds light on the vital role that grassroots innovation plays in broader
economic ecosystems. Using Complex Network Analysis and the TE-SER model, the
study unveils how these actors interact, collaborate, and influence major
economic ecosystems in the context of complex social challenges. The findings
highlight that actors from the grassroots innovation ecosystem make up a
significant portion of the larger innovation-driven entrepreneurial economic
ecosystem, accounting for between 20% and 30% in all three cases and are
strategically positioned within the ecosystem's structural network.
Additionally, this study emphasizes the potential for greater integration of
grassroots innovation actors to leverage resources and foster socio-economic
development. The research concludes by advocating for further studies in
similar socio-economic contexts to enhance our understanding of integration
dynamics and mutual benefits between grassroots innovation ecosystems and other
larger economic systems.

Current business cycle theory is an application of the general equilibrium
theory. This paper presents the business cycle model without using general
equilibrium framework. We treat agents risk assessments as their coordinates x
on economic space and establish distribution of all economic agents by their
risk coordinates. We suggest aggregation of agents and their variables by
scales large to compare with risk scales of single agents and small to compare
with economic domain on economic space. Such model is alike to transition from
kinetic description of multi-particle system to hydrodynamic approximation.
Aggregates of agents extensive variables with risk coordinate x determine macro
variables as functions of x alike to hydrodynamic variables. Economic and
financial transactions between agents define evolution of their variables.
Aggregation of transactions between agents with risk coordinates x and y
determine macro transactions as functions of x and y and define evolution of
macro variables at points x and y. We describe evolution and interactions
between macro transactions by hydrodynamic-like system of economic equations.
We show that business cycles are described as consequence of the system of
economic equations on macro transactions. As example we describe Credit
transactions CL(tax,y) that provide Loans from Creditors at point x to
Borrowers at point y and Loan-Repayment transactions LR(t,x,y) that describe
repayments from Borrowers at point y to Creditors at point x. We use
hydrodynamic-like economic equations and derive from them the system of
ordinary differential equations that describe business cycle fluctuations of
macro Credits C(t) and macro Loan-Repayments LR(t) of the entire economics. The
nature of business cycle fluctuations is explained as oscillations of "mean
risk" of economic variables on bounded economic domain of economic space.

The speeches stated by influential politicians can have a decisive impact on
the future of a country. In particular, the economic content of such speeches
affects the economy of countries and their financial markets. For this reason,
we examine a novel dataset containing the economic content of 951 speeches
stated by 45 US Presidents from George Washington (April 1789) to Donald Trump
(February 2017). In doing so, we use an economic glossary carried out by means
of text mining techniques. The goal of our study is to examine the structure of
significant interconnections within a network obtained from the economic
content of presidential speeches. In such a network, nodes are represented by
talks and links by values of cosine similarity, the latter computed using the
occurrences of the economic terms in the speeches. The resulting network
displays a peculiar structure made up of a core (i.e. a set of highly central
and densely connected nodes) and a periphery (i.e. a set of non-central and
sparsely connected nodes). The presence of different economic dictionaries
employed by the Presidents characterize the core-periphery structure. The
Presidents' talks belonging to the network's core share the usage of generic
(non-technical) economic locutions like "interest" or "trade". While the use of
more technical and less frequent terms characterizes the periphery (e.g.
"yield" ). Furthermore, the speeches close in time share a common economic
dictionary. These results together with the economics glossary usages during
the US periods of boom and crisis provide unique insights on the economic
content relationships among Presidents' speeches.

There is growing awareness within the economics profession of the important
role narratives play in the economy. Even though empirical approaches that try
to quantify economic narratives are getting increasingly popular, there is no
theory or even a universally accepted definition of economic narratives
underlying this research. First, we review and categorize the economic
literature concerned with narratives and work out the different paradigms that
are at play. Only a subset of the literature considers narratives to be active
drivers of economic activity. In order to solidify the foundation of narrative
economics, we propose a definition of collective economic narratives, isolating
five important characteristics. We argue that, for a narrative to be
economically relevant, it must be a sense-making story that emerges in a social
context and suggests action to a social group. We also systematize how a
collective economic narrative differs from a topic and from other kinds of
narratives that are likely to have less impact on the economy. With regard to
the popular use of topic modeling as an empirical strategy, we suggest that the
complementary use of other canonical methods from the natural language
processing toolkit and the development of new methods is inevitable to go
beyond identifying topics and be able to move towards true empirical narrative
economics.

Many experimental studies report that economics students tend to act more
selfishly than students of other disciplines, a finding that received
widespread public and professional attention. Two main explanations that the
existing literature offers for the differences found in the behavior between
economists and noneconomists are the selection effect, and the indoctrination
effect. We offer an alternative, novel explanation. We argue that these
differences can be explained by differences in the interpretation of the
context. We test this hypothesis by conducting two social dilemma experiments
in the US and Israel with participants from both economics and non-economics
majors. In the experiments, participants face a tradeoff between profit
maximization, that is the market norm and workers welfare, that is the social
norm. We use priming to manipulate the cues that the participants receive
before they make their decision. We find that when participants receive cues
signaling that the decision has an economic context, both economics and
non-economics students tend to maximize profits. When the participants receive
cues emphasizing social norms, on the other hand, both economics and
non-economics students are less likely to maximize profits. We conclude that
some of the differences found between the decisions of economics and
non-economics students can be explained by contextual cues.

Our study shows that many firms would accumulate at zero output level
(namely, Bankruptcy status) if a perfectly competitive market reaches full
employment (namely, those people who should obtain employment have obtained
employment). As a result, appearance of economic crisis is determined by two
points; that is, (a). Stock market approaches perfect competition; (b). Society
reaches full employment. The empirical research of these two points would lead
to early warning of economic crisis. Moreover, it is a surprise that the state
of economic crisis would be a feasible equilibrium within the framework of the
Arrow-Debreu model. That means that we can not understand the origin of
economic crisis within the framework of modern economics, for example, the
general equilibrium theory.

Data describing historical economic growth are analysed. Included in the
analysis is the world and regional economic growth. The analysis demonstrates
that historical economic growth had a natural tendency to follow hyperbolic
distributions. Parameters describing hyperbolic distributions have been
determined. A search for takeoffs from stagnation to growth produced negative
results. This analysis throws a new light on the interpretation of the
mechanism of the historical economic growth and suggests new lines of research.

This paper investigates the relationships between economic growth, investment
in human capital and income equality in Turkey. The conclusion drawn based on
the data from the OECD and the World Bank suggests that economic growth can
improve income equality depending on the expenditures undertaken by the
government. As opposed to the standard view that economic growth and income
inequality are positively related, the findings of this paper suggest that
other factors such as education and healthcare spending are also driving
factors of income inequality in Turkey. The proven positive impact of
investment in education and health care on income equality could aid
policymakers who aim to achieve fairer income equality and economic growth, in
investment decisions.

The aim of this study is to contribute to the theory of exogenous economic
shocks and their equivalents in an attempt to explain business cycle
fluctuations, which still do not have a clear explanation. To this end the
author has developed an econometric model based on a regression analysis.
Another objective is to tackle the issue of hybrid threats, which have not yet
been subjected to a cross-disciplinary research. These were reviewed in terms
of their economic characteristics in order to complement research in the fields
of defence and security.

Proof-of-work blockchains need to be carefully designed so as to create the
proper incentives for miners to faithfully maintain the network in a
sustainable way. This paper describes how the economic engineering of the
Conflux Network, a high throughput proof-of-work blockchain, leads to sound
economic incentives that support desirable and sustainable mining behavior. In
detail, this paper parameterizes the level of income, and thus network
security, that Conflux can generate, and it describes how this depends on user
behavior and "policy variables'' such as block and interest inflation. It also
discusses how the underlying economic engineering design makes the Conflux
Network resilient against double spending and selfish mining attacks.

We consider the thermodynamic approach to the description of economic systems
and processes. The first and second laws of thermodynamics as applied to
economic systems are derived and analyzed. It is shown that there is a deep
analogy between the parameters of thermodynamic and economic systems (markets);
in particular, each thermodynamic parameter can be associated with a certain
economic parameter or indicator. The economic meaning of such primordially
thermodynamic concepts as pressure, volume, internal energy, heat, etc. has
been established. The thermostatistics of the market is considered. It is shown
that, as in conventional thermostatistics, many market parameters, such as
price of goods, quantity of goods, etc., as well as their fluctuations can be
calculated formally using the partition function of an economic system.

We review economic research regarding the decision making processes of
individuals in economics, with a particular focus on papers which tried
analyzing factors that affect decision making with the evolution of the history
of economic thought. The factors that are discussed here are psychological,
emotional, cognitive systems, and social norms. Apart from analyzing these
factors, it deals with the reasons behind the limitations of rational
decision-making theory in individual decision making and the need for a
behavioral theory of decision making. In this regard, it has also reviewed the
role of situated learning in the decision-making process.

The 2021 Sveriges Riksbank Prize in Economic Sciences in Memory of Alfred
Nobel was awarded to David Card "for his empirical contributions to labour
economics" and to Joshua Angrist and Guido Imbens "for their methodological
contributions to the analysis of causal relationships." We survey these
contributions of the three laureates, and discuss how their empirical and
methodological insights transformed the modern practice of applied
microeconomics. By emphasizing research design and formalizing the causal
content of different econometric procedures, the laureates shed new light on
key questions in labour economics and advanced a robust toolkit for empirical
analyses across many fields.

We study the impact of non-pharmaceutical interventions (NPIs) on mortality
and economic activity across U.S. cities during the 1918 Flu Pandemic. The
combination of fast and stringent NPIs reduced peak mortality by 50% and
cumulative excess mortality by 24% to 34%. However, while the pandemic itself
was associated with short-run economic disruptions, we find that these
disruptions were similar across cities with strict and lenient NPIs. NPIs also
did not worsen medium-run economic outcomes. Our findings indicate that NPIs
can reduce disease transmission without further depressing economic activity, a
finding also reflected in discussions in contemporary newspapers.

Balancing pandemic control and economics is challenging, as the numerical
analysis assuming specific economic conditions complicates obtaining
predictable general findings. In this study, we analytically demonstrate how
adopting timely moderate measures helps reconcile medical effectiveness and
economic impact, and explain it as a consequence of the general finding of
``economic irreversibility" by comparing it with thermodynamics. A general
inequality provides the guiding principles on how such measures should be
implemented. The methodology leading to the exact solution is a novel
theoretical contribution to the econophysics literature.

The article discusses the basic concepts of strategic planning in the Russian
Federation, highlights the legal, financial and resource features that act as
restrictions in decision making in the field of socio-economic development of
municipalities. The analysis concluded that to design an adequate model of
socio-economic development of municipalities is a very difficult task,
particularly when the traditional approaches are applied. To solve the task, we
proposed to use the semantic modeling as well as cognitive maps which are able
to point out the set of dependencies that arise between factors having a direct
impact on socio-economic development.

Economics does not need a scientific revolution. Economics needs accurate
measurements according to high standards of natural sciences and meticulous
work on revealing empirical relationships between measured variables.

Conventional economic analysis of stringent climate change mitigation policy
generally concludes various levels of economic slowdown as a result of
substantial spending on low carbon technology. Equilibrium economics however
could not explain or predict the current economic crisis, which is of financial
nature. Meanwhile the economic impacts of climate policy find their source
through investments for the diffusion of environmental innovations, in parts a
financial problem. Here, we expose how results of economic analysis of climate
change mitigation policy depend entirely on assumptions and theory concerning
the finance of the diffusion of innovations, and that in many cases, results
are simply re-iterations of model assumptions. We show that, while equilibrium
economics always predict economic slowdown, methods using non-equilibrium
approaches suggest the opposite could occur. We show that the solution to
understanding the economic impacts of reducing greenhouse gas emissions lies
with research on the dynamics of the financial sector interacting with
innovation and technology developments, economic history providing powerful
insights through important analogies with previous historical waves of
innovation.

Inspired by Gambetta's theory on the origins of the mafia in Sicily, we
report a geo-concentrating phenomenon of scams in China, and propose a novel
economic explanation. Our analysis has some policy implications.

Structural socioeconomic analysis of Brazil. All basic information about this
South American country is gathered in a comprehensive outlook that includes the
challenges Brazil faces, as well as their causes and posible economic
solutions.

These lecture notes accompany a one-semester graduate course on information
and learning in economic theory. Topics include common knowledge, Bayesian
updating, monotone-likelihood ratio properties, affiliation, the Blackwell
order, cost of information, learning and merging of beliefs, model uncertainty,
model misspecification, and information design.

The research aims to assess the impact of foreign direct investment (FDI) and
domestic investment on Chile's economic growth. By elucidating the relationship
between FDI and domestic investment, the study contributes valuable insights
for economic policy formulation and future investments. The findings hold
significance in shaping Chile's international perception as an investment
destination, potentially influencing its standing in the global economic
landscape. Demonstrating that FDI is a significant driver of economic growth
could enhance confidence among foreign investors. The project's importance lies
in contributing to economic knowledge and guiding strategic decisions for
sustainable economic growth in Chile. Understanding the interplay of FDI and
domestic investment allows for a balanced approach, promoting stable economic
development and mitigating issues like excessive reliance on foreign
investment. The study highlights the theory of internationalization as a
conceptual framework for understanding the motives and strategies of
multinational companies investing abroad. Leveraging data from sources like the
Central Bank of Chile, the research analyzes variables such as Chile's economic
growth (GDP), FDI, and domestic investment. The hypothesis posits a significant
long-term causal relationship between FDI, National Investment (NI), and
Chile's Economic Growth (GDP). Statistical analysis using the Eviews 6 software
tool confirms that attracting foreign investments and promoting internal
investment are imperative for sustainable economic growth in Chile.

We review an emerging body of work by physicists addressing questions of
economic organization and function. We suggest that, beyond simply employing
models familiar from physics to economic observables, remarkable regularities
in economic data may suggest parts of social order that can usefully be
incorporated into, and in turn can broaden, the conceptual structure of
physics.

China has experienced an outstanding economic expansion during the past
decades, however, literature on non-monetary metrics that reveal the status of
China's regional economic development are still lacking. In this paper, we fill
this gap by quantifying the economic complexity of China's provinces through
analyzing 25 years' firm data. First, we estimate the regional economic
complexity index (ECI), and show that the overall time evolution of provinces'
ECI is relatively stable and slow. Then, after linking ECI to the economic
development and the income inequality, we find that the explanatory power of
ECI is positive for the former but negative for the latter. Next, we compare
different measures of economic diversity and explore their relationships with
monetary macroeconomic indicators. Results show that the ECI index and the
non-linear iteration based Fitness index are comparative, and they both have
stronger explanatory power than other benchmark measures. Further multivariate
regressions suggest the robustness of our results after controlling other
socioeconomic factors. Our work moves forward a step towards better
understanding China's regional economic development and non-monetary
macroeconomic indicators.

We recall the similarities between the concepts and techniques of
Thermodynamics and Roegenian Economics. The Phase Diagram for a Roegenian
economic system highlights a triple point and a critical point, with related
explanations. These ideas can be used to improve our knowledge and
understanding of the nature of development and evolution of Roegenian economic
systems.

An economic model of crime is used to explore the consistent estimation of a
simultaneous linear equation without recourse to instrumental variables. A
maximum-likelihood procedure (NISE) is introduced, and its results are compared
to ordinary least squares and two-stage least squares. The paper is motivated
by previous research on the crime model and by the well-known practical problem
that valid instruments are frequently unavailable.

In this comment we discuss how complexity science and network science are
particularly useful for identifying and describing the hidden traces of
economic misbehaviour such as fraud and corruption.

In the article we made an attempt to reveal the contents and development of
the concept of economic clusters, to characterize the specificity of the
regional cluster as a project. We have identified features of an estimation of
efficiency of state participation in the cluster, where the state is an
institution representing the interests of society.

Can online education enable all students to participate in and benefit from
it equally? Massive online education without addressing the huge access gap and
disparities in digital infrastructure would not only exclude a vast majority of
students from learning opportunities but also exacerbate the existing
socio-economic disparities in educational opportunities.

Quantile regression and quantile treatment effect methods are powerful
econometric tools for considering economic impacts of events or variables of
interest beyond the mean. The use of quantile methods allows for an examination
of impacts of some independent variable over the entire distribution of
continuous dependent variables. Measurement in many quantative settings in
economic history have as a key input continuous outcome variables of interest.
Among many other cases, human height and demographics, economic growth,
earnings and wages, and crop production are generally recorded as continuous
measures, and are collected and studied by economic historians. In this paper
we describe and discuss the broad utility of quantile regression for use in
research in economic history, review recent quantitive literature in the field,
and provide an illustrative example of the use of these methods based on 20,000
records of human height measured across 50-plus years in the 19th and 20th
centuries. We suggest that there is considerably more room in the literature on
economic history to convincingly and productively apply quantile regression
methods.

The economic and financial variables of economic agents determine
macroeconomic variables. Current models consider agents' variables that are
determined by the sums of values and volumes of agents' trades during some time
interval {\Delta}. We call them first-order economic variables. We describe how
the volatilities and correlations of market trade values and volumes determine
price volatility. We argue that such a link requests consideration of agents'
economic variables of the second order that are composed of sums of squares of
agents' transactions during {\Delta}. Almost any variable of the first order
should be complemented by its second-order pair. Respectively, the sums of
agents' second-order variables introduce macroeconomic variables of the second
order. The description of the first- and second-order macroeconomic variables
establishes the subject of second-order economic theory. We highlight that the
complexity of second-order economic theory essentially restricts any hopes for
precise predictions of price probability and, at best, could provide estimates
of price volatility. That limits the predictions of price probability to
Gauss's approximations only.

In this paper, we survey recent econometric contributions to measure the
relationship between economic activity and climate change. Due to the critical
relevance of these effects for the well-being of future generations, there is
an explosion of publications devoted to measuring this relationship and its
main channels. The relation between economic activity and climate change is
complex with the possibility of causality running in both directions. Starting
from economic activity, the channels that relate economic activity and climate
change are energy consumption and the consequent pollution. Hence, we first
describe the main econometric contributions about the interactions between
economic activity and energy consumption, moving then to describing the
contributions on the interactions between economic activity and pollution.
Finally, we look at the main results on the relationship between climate change
and economic activity. An important consequence of climate change is the
increasing occurrence of extreme weather phenomena. Therefore, we also survey
contributions on the economic effects of catastrophic climate phenomena.

Previous studies show that natural disasters decelerate economic growth, and
more so in countries with lower financial development. We confirm these results
with more recent data. We are the first to show that fiscal stability reduces
the negative economic impact of natural disasters in poorer countries, and that
catastrophe bonds have the same effect in richer countries.

This study explores how the COVID-19 pandemic's economic impact has
exacerbated nutritional health disparities among women. It sought to understand
the effects of economic challenges on women's dietary choices and access to
nutritious food across different socioeconomic groups. Using a mixed-methods
approach, the research combined quantitative data from health and economic
records with qualitative insights from interviews with diverse women. The study
analyzed trends in nutritional health and economic factors before and after the
pandemic and gathered personal accounts regarding nutrition and economic
difficulties during this period. Findings showed a clear link between the
economic downturn and deteriorating nutritional health, particularly in
low-income and marginalized groups. These women reported decreased access to
healthy foods and an increased dependence on less nutritious options due to
budget constraints, leading to a decline in dietary quality. This trend was
less evident in higher-income groups, highlighting stark disparities. The
pandemic intensified pre-existing nutritional inequalities, with the most
vulnerable groups facing greater adverse effects. However, community support
and public health measures provided some relief. In summary, the pandemic's
economic repercussions have indirectly impaired women's nutritional health,
especially among the socioeconomically disadvantaged. This highlights the
necessity for tailored nutritional interventions and economic policies focused
on safeguarding women's health.

When analyzing the components influencing the stock prices, it is commonly
believed that economic activities play an important role. More specifically,
asset prices are more sensitive to the systematic economic news that impose a
pervasive effect on the whole market. Moreover, the investors will not be
rewarded for bearing idiosyncratic risks as such risks are diversifiable. In
the paper Economic Forces and the Stock Market 1986, the authors introduced an
attribution model to identify the specific systematic economic forces
influencing the market. They first defined and examined five classic factors
from previous research papers: Industrial Production, Unanticipated Inflation,
Change in Expected Inflation, Risk Premia, and The Term Structure. By adding in
new factors, the Market Indices, Consumptions and Oil Prices, one by one, they
examined the significant contribution of each factor to the stock return. The
paper concluded that the stock returns are exposed to the systematic economic
news, and they are priced with respect to their risk exposure. Also, the
significant factors can be identified by simply adopting their model. Driven by
such motivation, we conduct an attribution analysis based on the general
framework of their model to further prove the importance of the economic
factors and identify the specific identity of significant factors.

Economic inequality is one of the pivotal issues for most of economic and
social policy makers across the world to insure the sustainable economic growth
and justice. In the mainstream school of economics, namely neoclassical
theories, economic issues are dealt with in a mechanistic manner. Such a
mainstream framework is majorly focused on investigating a socio-economic
system based on an axiomatic scheme where reductionism approach plays a vital
role. The major limitations of such theories include unbounded rationality of
economic agents, reducing the economic aggregates to a set of predictable
factors and lack of attention to adaptability and the evolutionary nature of
economic agents. In tackling deficiencies of conventional economic models, in
the past two decades, some new approaches have been recruited. One of those
novel approaches is the Complex adaptive systems (CAS) framework which has
shown a very promising performance in action. In contrast to mainstream school,
under this framework, the economic phenomena are studied in an organic manner
where the economic agents are supposed to be both boundedly rational and
adaptive. According to it, the economic aggregates emerge out of the ways
agents of a system decide and interact. As a powerful way of modeling CASs,
Agent-based models (ABMs) has found a growing application among academicians
and practitioners. ABMs show that how simple behavioral rules of agents and
local interactions among them at micro-scale can generate surprisingly complex
patterns at macro-scale. In this paper, ABMs have been used to show (1) how an
economic inequality emerges in a system and to explain (2) how sadaqah as an
Islamic charity rule can majorly help alleviating the inequality and how
resource allocation strategies taken by charity entities can accelerate this
alleviation.

It is hard to exaggerate the role of economic aggregators -- functions that
summarize numerous and / or heterogeneous data -- in economic models since the
early XX$^{th}$ century. In many cases, as witnessed by the pioneering works of
Cobb and Douglas, these functions were information quantities tailored to
economic theories, i.e. they were built to fit economic phenomena. In this
paper, we look at these functions from the complementary side: information. We
use a recent toolbox built on top of a vast class of distortions coined by
Bregman, whose application field rivals metrics' in various subfields of
mathematics. This toolbox makes it possible to find the quality of an
aggregator (for consumptions, prices, labor, capital, wages, etc.), from the
standpoint of the information it carries. We prove a rather striking result.
  From the informational standpoint, well-known economic aggregators do belong
to the \textit{optimal} set. As common economic assumptions enter the analysis,
this large set shrinks, and it essentially ends up \textit{exactly fitting}
either CES, or Cobb-Douglas, or both. To summarize, in the relevant economic
contexts, one could not have crafted better some aggregator from the
information standpoint. We also discuss global economic behaviors of optimal
information aggregators in general, and present a brief panorama of the links
between economic and information aggregators.
  Keywords: Economic Aggregators, CES, Cobb-Douglas, Bregman divergences

The prevalent view in the economics literature is that a high level of
infrastructure investment is a precursor to economic growth. China is
especially held up as a model to emulate. Based on the largest dataset of its
kind, this paper punctures the twin myths that, first, infrastructure creates
economic value, and, second, China has a distinct advantage in its delivery.
Far from being an engine of economic growth, the typical infrastructure
investment fails to deliver a positive risk adjusted return. Moreover, China's
track record in delivering infrastructure is no better than that of rich
democracies. Where investments are debt-financed, overinvesting in unproductive
projects results in the buildup of debt, monetary expansion, instability in
financial markets, and economic fragility, exactly as we see in China today. We
conclude that poorly managed infrastructure investments are a main explanation
of surfacing economic and financial problems in China. We predict that, unless
China shifts to a lower level of higher-quality infrastructure investments, the
country is headed for an infrastructure-led national financial and economic
crisis, which is likely also to be a crisis for the international economy.
China's infrastructure investment model is not one to follow for other
countries but one to avoid.

While deep neural networks (DNNs) have been increasingly applied to choice
analysis showing high predictive power, it is unclear to what extent
researchers can interpret economic information from DNNs. This paper
demonstrates that DNNs can provide economic information as complete as
classical discrete choice models (DCMs). The economic information includes
choice predictions, choice probabilities, market shares, substitution patterns
of alternatives, social welfare, probability derivatives, elasticities,
marginal rates of substitution (MRS), and heterogeneous values of time (VOT).
Unlike DCMs, DNNs can automatically learn the utility function and reveal
behavioral patterns that are not prespecified by domain experts. However, the
economic information obtained from DNNs can be unreliable because of the three
challenges associated with the automatic learning capacity: high sensitivity to
hyperparameters, model non-identification, and local irregularity. To
demonstrate the strength and challenges of DNNs, we estimated the DNNs using a
stated preference survey, extracted the full list of economic information from
the DNNs, and compared them with those from the DCMs. We found that the
economic information either aggregated over trainings or population is more
reliable than the disaggregate information of the individual observations or
trainings, and that even simple hyperparameter searching can significantly
improve the reliability of the economic information extracted from the DNNs.
Future studies should investigate other regularizations and DNN architectures,
better optimization algorithms, and robust DNN training methods to address
DNNs' three challenges, to provide more reliable economic information from
DNN-based choice models.

AI and reinforcement learning (RL) have improved many areas, but are not yet
widely adopted in economic policy design, mechanism design, or economics at
large. At the same time, current economic methodology is limited by a lack of
counterfactual data, simplistic behavioral models, and limited opportunities to
experiment with policies and evaluate behavioral responses. Here we show that
machine-learning-based economic simulation is a powerful policy and mechanism
design framework to overcome these limitations. The AI Economist is a
two-level, deep RL framework that trains both agents and a social planner who
co-adapt, providing a tractable solution to the highly unstable and novel
two-level RL challenge. From a simple specification of an economy, we learn
rational agent behaviors that adapt to learned planner policies and vice versa.
We demonstrate the efficacy of the AI Economist on the problem of optimal
taxation. In simple one-step economies, the AI Economist recovers the optimal
tax policy of economic theory. In complex, dynamic economies, the AI Economist
substantially improves both utilitarian social welfare and the trade-off
between equality and productivity over baselines. It does so despite emergent
tax-gaming strategies, while accounting for agent interactions and behavioral
change more accurately than economic theory. These results demonstrate for the
first time that two-level, deep RL can be used for understanding and as a
complement to theory for economic design, unlocking a new computational
learning-based approach to understanding economic policy.

The study and measurement of economic resilience is ruled by high level of
complexity related to the diverse structure, functionality, spatiality, and
dynamics describing economic systems. Towards serving the demand of
integration, this paper develops a three-dimensional index, capturing
engineering, ecological, and evolutionary aspects of economic resilience that
are considered separately in the current literature. The proposed index is
computed on GDP data of worldwide countries, for the period 1960-2020,
concerning 14 crises considered as shocks, and was found well defined in a
conceptual context of its components. Its application on real-world data allows
introducing a novel classification of countries in terms of economic
resilience, and reveals geographical patterns and structural determinants of
this attribute. Impressively enough, economic resilience appears positively
related to major productivity coefficients, gravitationally driven, and
depended on agricultural specialization, with high structural heterogeneity in
the low class. Also, the analysis fills the literature gap by shaping the
worldwide map of economic resilience, revealing geographical duality and
centrifugal patterns in its geographical distribution, a relationship between
diachronically good performance in economic resilience and geographical
distance from the shocks origin, and a continent differentiation expressed by
the specialization of America in engineering resilience, Africa and Asia in
ecological and evolutionary resilience, and a relative lag of Europe and
Oceania. Finally, the analysis provides insights into the effect of the 2008 on
the globe and supports a further research hypothesis that political instability
is a main determinant of low economic resilience, addressing avenues of further
research.

Economic systems play pivotal roles in the metaverse. However, we have not
yet found an overview that systematically introduces economic systems for the
metaverse. Therefore, we review the state-of-the-art solutions, architectures,
and systems related to economic systems. When investigating those
state-of-the-art studies, we keep two questions in our mind: (1) what is the
framework of economic systems in the context of the metaverse, and (2) what
activities would economic systems engage in the metaverse? This article aims to
disclose insights into the economic systems that work for both the current and
the future metaverse. To have a clear overview of the economic-system
framework, we mainly discuss the connections among three fundamental elements
in the metaverse, i.e., digital creation, digital assets, and the digital
trading market. After that, we elaborate on each topic of the proposed
economic-system framework. Those topics include incentive mechanisms, monetary
systems, digital wallets, decentralized finance (DeFi) activities, and
cross-platform interoperability for the metaverse. For each topic, we mainly
discuss three questions: a) the rationale of this topic, b) why the metaverse
needs this topic, and c) how this topic will evolve in the metaverse. Through
this overview, we wish readers can better understand what economic systems the
metaverse needs, and the insights behind the economic activities in the
metaverse.

The processes of ecological interactions, dispersal and mutations shape the
dynamics of biological communities, and analogous eco-evolutionary processes
acting upon economic entities have been proposed to explain economic change.
This hypothesis is compelling because it explains economic change through
endogenous mechanisms, but it has not been quantitatively tested at the global
economy level. Here, we use an inverse modelling technique and 59 years of
economic data covering 77 countries to test whether the collective dynamics of
national economic activities can be characterised by eco-evolutionary
processes. We estimate the statistical support of dynamic community models in
which the dynamics of economic activities are coupled with positive and
negative interactions between the activities, the spatial dispersal of the
activities, and their transformations into other economic activities. We find
strong support for the models capturing positive interactions between economic
activities and spatial dispersal of the activities across countries. These
results suggest that processes akin to those occurring in ecosystems play a
significant role in the dynamics of economic systems. The strength-of-evidence
obtained for each model varies across countries and may be caused by
differences in the distance between countries, specific institutional contexts,
and historical contingencies. Overall, our study provides a new quantitative,
biologically inspired framework to study the forces shaping economic change.

Energy supply is mandatory for the production of economic value.
Nevertheless, tradition dictates that an enigmatic 'invisible hand' governs
economic valuation. Physical scientists have long proposed alternative but
testable energy cost theories of economic valuation, and have shown the gross
correlation between energy consumption and economic output at the national
level through input-output energy analysis. However, due to the difficulty of
precise energy analysis and highly complicated real markets, no decisive
evidence directly linking energy costs to the selling prices of individual
commodities has yet been found. Over the past century, the US metal market has
accumulated a huge body of price data, which for the first time ever provides
us the opportunity to quantitatively examine the direct energy-value
correlation. Here, by analyzing the market price data of 65 purified chemical
elements (mainly metals) relative to the total energy consumption for refining
them from naturally occurring geochemical conditions, we found a clear
correlation between the energy cost and their market prices. The underlying
physics we proposed has compatibility with conventional economic concepts such
as the ratio between supply and demand or scarcity's role in economic
valuation. It demonstrates how energy cost serves as the 'invisible hand'
governing economic valuation. Thorough understanding of this energy connection
between the human economic and the Earth's biogeochemical metabolism is
essential for improving the overall energy efficiency and furthermore the
sustainability of the human society.

This paper uses nighttime light(NTL) data to measure the nexus of the
non-banking sector, particularly insurance, and economic growth in South
Africa. We hypothesize that insurance sector growth positively propels economic
growth due to its economic growth-supportive traits like investment protection
and optimal risk mitigation. We also claim that Nighttime light data is a good
economic measure than Gross domestic product (GDP). We used weighted
regressions to measure the relationships between nighttime light data, GDP, and
insurance sector development. We used time series South African GDP data
collected from the World Bank for the period running from 2000 to 2018, and the
nighttime lights data from the National Geophysical Data Centre (NGDC) in
partnership with the National Oceanic and Atmospheric Administration (NOAA).
From the models fitted and the reported BIC, AIC, and likelihood ratios, the
insurance sector proved to have more predictive power on economic development
in South Africa, and radiance light explained economic growth better than GDP
and GDP/Capita. We concluded that nighttime data is a good proxy for economic
growth than GDP/Capita in emerging economies like South Africa, where secondary
data needs to be more robust and sometimes inflated. The findings will guide
researchers and policymakers on what drives economic development and what
policies to put in place. It would be interesting to extend the current study
to other sectors such as micro-finances, mutual and hedge funds.

A few considerations on the nature of Economics and its relationship to human
communities through the prism of Self-Organizing-Systems.

A relation between interest rates and inflation is presented using a two
component economic model and a simple general principle. Preliminary results
indicate a remarkable similarity to classical economic theories, in particular
that of Wicksell.

Economic systems are similar with physic systems for their large number of
individuals and the exist of equilibrium. In this paper, we present a model
applying the equilibrium statistical model in economic systems. Consistent with
statistical physics, we define a series of concepts, such as economic
temperature, economic pressure, economic potential, wealth and population.
Moreover, we suggest that these parameters show pretty close relationship with
the concepts in economy. This paper presents related concepts in the
equilibrium economic model and constructs significant theorems and corollaries,
which is derived from the priori possibility postulate, getting theorems
including the equilibrium theorem between open systems, the distribution
theorem of wealth and population along with related corollaries. More
importantly, we demonstrate a method constructing optimal density of states,
optimizing a macroscopic parameter depending on need to get the distribution of
density of states utilizing the variation method, which is significant for the
development of a society. In addition, we calculate a simple economic system as
an example, indicating that the system occupied mostly by the middle class
could develop stably and soundly, explaining the reason for resulting
distributions of macroscopic parameters.

The algebra of transactions as fundamental measurements is constructed on the
basis of the analysis of their properties and represents an expansion of the
Boolean algebra. The notion of the generalized economic measurements of the
economic quantity and quality of objects of transactions is introduced. It has
been shown that the vector space of economic states constructed on the basis of
these measurements is relativistic. The laws of kinematics of economic objects
in this space have been analyzed and the stages of constructing the dynamics
have been formulated. In particular, the principle of maximum benefit, which
represents an economic analog of the principle of least action in the classical
mechanics, and the principle of relativity as the principle of equality of all
possible consumer preferences have been formulated. The notion of economic
interval between two economic objects invariant to the selection of the vector
of consumer preferences has been introduced. Methods of experimental
verification of the principle of relativity in the space of economic states
have been proposed.

To characterize economic development and diagnose the economic health
condition, several popular indices such as gross domestic product (GDP),
industrial structure and income growth are widely applied. However, computing
these indices based on traditional economic census is usually costly and
resources consuming, and more importantly, following a long time delay. In this
paper, we analyzed nearly 200 million users' activities for four consecutive
years in the largest social network (Sina Microblog) in China, aiming at
exploring latent relationships between the online social activities and local
economic status. Results indicate that online social activity has a strong
correlation with local economic development and industrial structure, and more
interestingly, allows revealing the macro-economic structure instantaneously
with nearly no cost. Beyond, this work also provides a new venue to identify
risky signal in local economic structure.

Historical economic growth in Asia (excluding Japan) is analysed. It is shown
that Unified Growth Theory is contradicted by the data, which were used (but
not analysed) during the formulation of this theory. Unified Growth Theory does
not explain the mechanism of economic growth. It explains the mechanism of
Malthusian stagnation, which did not exist and it explains the mechanism of the
transition from stagnation to growth that did not happen. The data show that
the economic growth in Asia was never stagnant but hyperbolic. The alleged
dramatic takeoff around 1900 or around any other time did not happen. However,
the theory contains also a dangerous and strongly-misleading concept that after
a long epoch of stagnation we have now entered the epoch of sustained economic
growth, the concept creating the sense of security. The opposite is true. After
the epoch of sustained and secure economic growth we have now entered the epoch
of a fast-increasing and insecure economic growth.

Economic evaluation is a dynamically advancing knowledge area of health
economics. It has been conceived to provide evidence for allocating scarce
resources to gain the best value for money. The problem of efficiency of
investments becomes even more crucial with advances in modern medicine and
public health which bring about both improved patient outcomes and higher
costs. Despite the abundance of literature on the economic evaluation concepts,
some key notions including the definition of the health economic evaluation
remain open for discussion. Academic literature offers a large number and
growing variety of economic evaluation definitions. It testifies to the fact
that existing definitions do not meet requirements of economists. The aim of
this study was to examine existing definitions and reveal their common
features.

An economic interpretation of the Caputo derivatives of non-integer orders is
proposed. The suggested economic interpretation of the fractional derivatives
is based on a generalization of average and marginal values of economic
indicators. We formulate an economic interpretation by using the concept of the
T-indicator that allows us to describe economic processes with memory. The
standard average and marginal values of indicator are special cases of the
proposed T-indicator, when the order is equal to zero and one, respectively.
The fractional derivatives are interpreted as economic characteristics
(indicators) that are intermediate between the standard average and marginal
values of indicators.

Metaheuristic particle swarm optimization (PSO) algorithm has emerged as one
of the most promising optimization techniques in solving highly constrained
non-linear and non-convex optimization problems in different areas of
electrical engineering. Economic operation of the power system is one of the
most important areas of electrical engineering where PSO has been used
efficiently in solving various issues of practical systems. In this paper, a
comprehensive survey of research works in solving various aspects of economic
load dispatch (ELD) problems of power system engineering using different types
of PSO algorithms is presented. Five important areas of ELD problems have been
identified, and the papers published in the general area of ELD using PSO have
been classified into these five sections. These five areas are (i) single
objective economic load dispatch, (ii) dynamic economic load dispatch, (iii)
economic load dispatch with non-conventional sources, (iv) multi-objective
environmental/economic dispatch, and (v) economic load dispatch of microgrids.
At the end of each category, a table is provided which describes the main
features of the papers in brief. The promising future works are given at the
conclusion of the review.

Regulation is commonly viewed as a hindrance to entrepreneurship, but
heterogeneity in the effects of regulation is rarely explored. We focus on
regional variation in the effects of national-level regulations by developing a
theory of hierarchical institutional interdependence. Using the political
science theory of market-preserving federalism, we argue that regional economic
freedom attenuates the negative influence of national regulation on net job
creation. Using U.S. data, we find that regulation destroys jobs on net, but
regional economic freedom moderates this effect. In regions with average
economic freedom, a one percent increase in regulation results in 14 fewer jobs
created on net. However, a standard deviation increase in economic freedom
attenuates this relationship by four fewer jobs. Interestingly, this moderation
accrues strictly to older firms; regulation usually harms young firm job
creation, and economic freedom does not attenuate this relationship.

The Economy Watcher Survey, which is a market survey published by the
Japanese government, contains \emph{assessments of current and future economic
conditions} by people from various fields. Although this survey provides
insights regarding economic policy for policymakers, a clear definition of the
word "future" in future economic conditions is not provided. Hence, the
assessments respondents provide in the survey are simply based on their
interpretations of the meaning of "future." This motivated us to reveal the
different interpretations of the future in their judgments of future economic
conditions by applying weakly supervised learning and text mining. In our
research, we separate the assessments of future economic conditions into
economic conditions of the near and distant future using learning from positive
and unlabeled data (PU learning). Because the dataset includes data from
several periods, we devised new architecture to enable neural networks to
conduct PU learning based on the idea of multi-task learning to efficiently
learn a classifier. Our empirical analysis confirmed that the proposed method
could separate the future economic conditions, and we interpreted the
classification results to obtain intuitions for policymaking.

The popularity of deep reinforcement learning (DRL) methods in economics have
been exponentially increased. DRL through a wide range of capabilities from
reinforcement learning (RL) and deep learning (DL) for handling sophisticated
dynamic business environments offers vast opportunities. DRL is characterized
by scalability with the potential to be applied to high-dimensional problems in
conjunction with noisy and nonlinear patterns of economic data. In this work,
we first consider a brief review of DL, RL, and deep RL methods in diverse
applications in economics providing an in-depth insight into the state of the
art. Furthermore, the architecture of DRL applied to economic applications is
investigated in order to highlight the complexity, robustness, accuracy,
performance, computational tasks, risk constraints, and profitability. The
survey results indicate that DRL can provide better performance and higher
accuracy as compared to the traditional algorithms while facing real economic
problems at the presence of risk parameters and the ever-increasing
uncertainties.

The European Union Emission Trading Scheme is a carbon emission allowance
trading system designed by Europe to achieve emission reduction targets. The
amount of carbon emission caused by production activities is closely related to
the socio-economic environment. Therefore, from the perspective of economic
policy uncertainty, this article constructs the GARCH-MIDAS-EUEPU and
GARCH-MIDAS-GEPU models for investigating the impact of European and global
economic policy uncertainty on carbon price fluctuations. The results show that
both European and global economic policy uncertainty will exacerbate the
long-term volatility of European carbon spot return, with the latter having a
stronger impact when the change is the same. Moreover, the volatility of the
European carbon spot return can be forecasted better by the predictor, global
economic policy uncertainty. This research can provide some implications for
market managers in grasping carbon market trends and helping participants
control the risk of fluctuations in carbon allowances.

This paper aims to estimate the impact of economic and financial crises on
the unemployment rate in the European Union, taking also into consideration the
institutional specificities, since unemployment was the main channel through
which the economic and financial crisis influenced the social developments.. In
this context, I performed two institutional clusters depending on their
inclusive or extractive institutional features and, in each cases, I computed
the crisis effect on unemployment rate over the 2003-2017 period. Both models
were estimated by using Panel Estimated Generalized Least Squares method, and
are weighted by Period SUR option in order to remove, in advance the possible
inconveniences of the models. The institutions proved to be a relevant
criterion that drives the impact of economic and financial crises on the
unemployment rate, highlighting that countries with inclusive institutions are
less vulnerable to economic shocks and are more resilient than countries with
extractive institutions. The quality of institutions was also found to have a
significant effect on the response of unemployment rate to the dynamic of its
drivers.

To prevent the spread of COVID-19, many cities, states, and countries have
`locked down', restricting economic activities in non-essential sectors. Such
lockdowns have substantially shrunk production in most countries. This study
examines how the economic effects of lockdowns in different regions interact
through supply chains, a network of firms for production, simulating an
agent-based model of production on supply-chain data for 1.6 million firms in
Japan. We further investigate how the complex network structure affects the
interactions of lockdowns, emphasising the role of upstreamness and loops by
decomposing supply-chain flows into potential and circular flow components. We
find that a region's upstreamness, intensity of loops, and supplier
substitutability in supply chains with other regions largely determine the
economic effect of the lockdown in the region. In particular, when a region
lifts its lockdown, its economic recovery substantially varies depending on
whether it lifts lockdown alone or together with another region closely linked
through supply chains. These results propose the need for inter-region policy
coordination to reduce the economic loss from lockdowns.

Urbanization plays a crucial role in the economic development of every
country. The mutual relationship between the urbanization of any country and
its economic productive structure is far from being understood. We analyzed the
historical evolution of product exports for all countries using the World Trade
Web (WTW) with respect to patterns of urbanization from 1995-2010. Using the
evolving framework of economic complexity, we reveal that a country's economic
development in terms of its production and export of goods, is interwoven with
the urbanization process during the early stages of its economic development
and growth. Meanwhile in urbanized countries, the reciprocal relation between
economic growth and urbanization fades away with respect to its later stages,
becoming negligible for countries highly dependent on the export of resources
where urbanization is not linked to any structural economic transformation.

This article presents the results of a cluster analysis of the regions of the
Russian Federation in terms of the main parameters of socio-economic
development according to the data presented in the official data sources of the
Federal State Statistics Service (Rosstat). Studied and analyzed the domestic
and foreign (Eurostat) methodology for assessing the socio-economic development
of territories. The aim of the study is to determine the main parameters of
territorial differentiation and to identify key indicators that affect the
socio-economic development of Russian regions. The authors have carried out a
classification of the constituent entities of the Russian Federation not in
terms of territorial location and geographical features, but in terms of the
specifics and key parameters of the socio-economic situation.

Predicting Residential Property Value in Catonsville, Maryland: A Comparison
of Multiple Regression Techniques

This paper surveys recent applications of methods from the theory of optimal
transport to econometric problems.

This paper studies whether and how differently projected information about
the impact of the Covid-19 pandemic affects individuals' prosocial behavior and
expectations on future outcomes. We conducted an online experiment with British
participants (N=961) when the UK introduced its first lockdown and the outbreak
was on its growing stage. Participants were primed with either the
environmental or economic consequences (i.e., negative primes), or the
environmental or economic benefits (i.e., positive primes) of the pandemic, or
with neutral information. We measured priming effects on an incentivized
take-and-give dictator game and on participants' expectations about future
environmental quality and economic growth. Our results show that primes affect
participants' expectations, but not their prosociality. In particular,
participants primed with environmental consequences hold a more pessimistic
view on future environmental quality, while those primed with economic benefits
are more optimistic about future economic growth. Instead, the positive
environmental prime and the negative economic prime do not influence
expectations. Our results offer insights into how information affects behavior
and expectations during the Covid-19 pandemic.

This paper proposes the characteristics a techno-economic model for 5G should
have considering both mobile network operators perspective and end users needs.
It also presents a review and classification of models in the literature based
on the characteristics of such theoretical techno-economic reference model. The
performed analysis identifies current gaps in the techno-economic modeling
literature for 5G architectures and shows it can be enhanced using agile
techno-economic models like the Universal Techno-Economic Model (UTEM) created
and developed by the author to industrialize assessment of different
technological solutions, considering all market players perspectives and
applicable to decision-making in multiple domains. This model can be used for
an effective and agile 5G techno-economic assessment, including not only
network deployment perspective but also customers and end users requirements as
well as other stakeholders to select the most adequate 5G architectural
solution considering both technical and economic feasibility. UTEM model is
currently available for all industry stakeholders under specific license of
use.

We show the role that an important equation first studied by Fritz John plays
in mechanism design.

It has been found that human mobility exhibits random patterns following the
Levy flight, where human movement contains many short flights and some long
flights, and these flights follow a power-law distribution. In this paper, we
study the social-economical development trajectories of urban cities. We
observe that social-economical movement of cities also exhibit the Levy flight
characteristics. We collect the social and economical data such as the
population, the number of students, GDP and personal income, etc. from several
cities. Then we map these urban data into the social and economical factors
through a deep-learning embedding method Auto-Encoder. We find that the
social-economical factors of these cities can be fitted approximately as a
movement pattern of a power-law distribution. We use the Stochastic
Multiplicative Processes (SMP) to explain such movement, where in the presence
of a boundary constraint, the SMP leads to a power law distribution. It means
that the social-economical trajectories of cities also follow a Levy flight
pattern, where some years have large changes in terms of social-economical
development, and many years have little changes.

I offer reflections on adaptation to climate change, with emphasis on
developing areas.

Botanical pandemics cause enormous economic damage and food shortages around
the globe. However, since botanical pandemics are here to stay in the
short-medium term, domesticated field owners can strategically seed their
fields to optimize each session's economic profit. In this work, we propose a
novel epidemiological-economic mathematical model that describes the economic
profit from a field of plants during a botanical pandemic. We describe the
epidemiological dynamics using a spatio-temporal extended
Susceptible-Infected-Recovered epidemiological model with a non-linear output
economic model. We provide an algorithm to obtain an optimal grid-formed
seeding strategy to maximize economic profit, given field and pathogen
properties. We show that the recovery and basic infection rates have a similar
economic influence. Unintuitively, we show that a larger farm does not promise
higher economic profit. Our results demonstrate a significant benefit of using
the proposed seeding strategy and shed more light on the dynamics of the
botanical pandemic.

Urgently needed carbon emissions reductions might lead to strict
command-and-control decarbonization strategies with potentially negative
economic consequences. Analysing the entire firm-level production network of a
European economy, we have explored how the worst outcomes of such approaches
can be avoided. We compared the systemic relevance of every firm in Hungary
with its annual CO2 emissions to identify optimal emission-reducing strategies
with a minimum of additional unemployment and economic losses. Setting specific
reduction targets, we studied various decarbonization scenarios and quantified
their economic consequences. We determined that for an emissions reduction of
20%, the most effective strategy leads to losses of about 2% of jobs and 2% of
economic output. In contrast, a naive scenario targeting the largest emitters
first results in 28% job losses and 33% output reduction for the same target.
This demonstrates that it is possible to use firm-level production networks to
design highly effective decarbonization strategies that practically preserve
employment and economic output.

As the United States is witnessing elevated racial differences pertaining to
economic disparities, we have found a unique example contrary to the
traditional narrative. Idaho is the only US state where Blacks earn more than
Whites and all other races. In this paper, we examine how Idaho Blacks might
have achieved economic success and, more importantly, what factors might have
led to this achievement in reducing racial and economic disparities.
Preliminary research suggests that fewer barriers to land ownership, smaller
populations, well-knit communities, men's involvement in the family, and a
relatively less hostile environment have played a significant role. Further
research by historians can help the nation uncover the underlying factors to
see if some factors are transportable to other parts of the country.

This study aims to examine the relationship between Foreign Direct Investment
(FDI), personal remittances received, and official development assistance (ODA)
in the economic growth of Bangladesh. The study utilizes time series data on
Bangladesh from 1976 to 2021. Additionally, this research contributes to the
existing literature by introducing the Foreign Capital Depthless Index (FCDI)
and exploring its impact on Bangladesh's economic growth. The results of the
Vector Error Correction Model (VECM) suggest that the economic growth of
Bangladesh depends on FDI, remittances, and aid in the long run. However, these
variables do not exhibit a causal relationship with GDP in the short run. The
relationship between FCDI and economic growth is positive in the long run.
Nevertheless, the presence of these three variables has a more significant
impact on the economic growth of Bangladesh

We introduce a multi-agent simulator for economic systems comprised of
heterogeneous Households, heterogeneous Firms, Central Bank and Government
agents, that could be subjected to exogenous, stochastic shocks. The
interaction between agents defines the production and consumption of goods in
the economy alongside the flow of money. Each agent can be designed to act
according to fixed, rule-based strategies or learn their strategies using
interactions with others in the simulator. We ground our simulator by choosing
agent heterogeneity parameters based on economic literature, while designing
their action spaces in accordance with real data in the United States. Our
simulator facilitates the use of reinforcement learning strategies for the
agents via an OpenAI Gym style environment definition for the economic system.
We demonstrate the utility of our simulator by simulating and analyzing two
hypothetical (yet interesting) economic scenarios. The first scenario
investigates the impact of heterogeneous household skills on their learned
preferences to work at different firms. The second scenario examines the impact
of a positive production shock to one of two firms on its pricing strategy in
comparison to the second firm. We aspire that our platform sets a stage for
subsequent research at the intersection of artificial intelligence and
economics.

This study examines the effects of de-globalization trends on international
trade networks and their role in improving forecasts for economic growth. Using
section-level trade data from nearly 200 countries from 2010 to 2022, we
identify significant shifts in the network topology driven by rising trade
policy uncertainty. Our analysis highlights key global players through
centrality rankings, with the United States, China, and Germany maintaining
consistent dominance. Using a horse race of supervised regressors, we find that
network topology descriptors evaluated from section-specific trade networks
substantially enhance the quality of a country's GDP growth forecast. We also
find that non-linear models, such as Random Forest, XGBoost, and LightGBM,
outperform traditional linear models used in the economics literature. Using
SHAP values to interpret these non-linear model's predictions, we find that
about half of most important features originate from the network descriptors,
underscoring their vital role in refining forecasts. Moreover, this study
emphasizes the significance of recent economic performance, population growth,
and the primary sector's influence in shaping economic growth predictions,
offering novel insights into the intricacies of economic growth forecasting.

This paper analyzes sustainable regional economic development and land use
employing a case study of Russia. The economics of land management in Russia
which is shaped by both historical legacies and contemporary policies
represents an interesting conundrum. Following the dissolution of the Soviet
Union, Russia embarked on a thorny and complex path towards the economic
reforms and transformation characterized, among all, by the privatization and
decentralization of land ownership. This transition was aimed at improving
agricultural productivity and fostering sustainable regional economic
development but also led to new challenges such as uneven distribution of land
resources, unclear property rights, and underinvestment in rural
infrastructure. However, managing all of that effectively poses significant
challenges and opportunities. With the help of the comprehensive bibliographic
network analysis, this study sheds some light on the current state of
sustainable regional economic development and land use management in Russia.
Its results and outcomes might be helpful for the researchers and stakeholders
alike in devising effective strategies aimed at maximizing resources for
sustainable land use, particularly within their respective regional economies.

With globalization's rise, economic interdependence's impacts have become a
prominent factor affecting personal lives, as well as national and
international dynamics. This study examines RT's public diplomacy efforts on
its non-Russian Facebook accounts over the past five years to identify the
prominence of economic topics across language accounts. Computational analysis,
including word embeddings and statistical methods, investigates how offline
economic indicators, like currency values and oil prices, correspond to RT's
online economic content changes. The results demonstrate that RT uses message
reinforcement associated economic topics as an audience targeting strategy and
differentiates their use with changing currency and oil values.

Initially designed to predict and explain the economic trajectories of
countries, cities, and regions, economic complexity has been found applicable
in diverse contexts such as ecology and chess openings. The success of economic
complexity stems from its capacity to assess hidden capabilities within a
system indirectly. The existing algorithms for economic complexity operate only
when the underlying interaction topology conforms to a bipartite graph. A
single link disrupting the bipartite structure renders these algorithms
inapplicable, even if the weight of that link is tiny compared to others. This
paper presents a novel extension of economic complexity to encompass any graph,
overcoming the constraints of bipartite structures. Additionally, it introduces
fitness centrality and orthofitness centrality as new centrality measures in
graphs. Fitness Centrality emerges as a promising metric for assessing node
vulnerability, akin to node betweenness centrality. Furthermore, we unveil the
cost functions that drive the minimization procedures underlying the economic
complexity index and fitness centrality algorithms. This extension broadens the
scope of economic complexity analysis, enabling its application in diverse
network structures beyond bipartite graphs.

International trade has been in the forefront of economic development and
growth debates. Trade openness, its definition, scope, and impacts have also
been studied numerously. Tariff has been dubbed as negative influencer of
economic growth as per conventional wisdom and most empirical studies. This
paper empirically examines relationships among trade openness as trade share to
GDP, import tariff rate and economic growth. Panel dataset of 11 G-20 member
countries were selected for the study. Results found a positively significant
correlation between trade openness and economic growth. Tariff has negatively
significant correlation with economic growth in lagged model. OLS and panel
data fixed-effects regression were employed to carry out the regression
analysis. To deal with endogeneity in trade openness variable, a 1-year lag
regression technique was conducted. Results are robust and significant. Policy
recommendation suggests country specific trade opening and tariff relaxation.

To implement the previously formulated principles of sustainable economic
development, all non-negative solutions of the linear system of equations and
inequalities, which are satisfied by the vector of real consumption, are
completely described. It is established that the vector of real consumption
with the minimum level of excess supply is determined by the solution of some
quadratic programming problem. The necessary and sufficient conditions are
established under which the economic system, described by the "input-output"
production model, functions in the mode of sustainable development. A complete
description of the equilibrium states for which markets are partially cleared
in the economy model of production "input-output" is given, on the basis that
all solutions of system of linear equations and inequalities are completely
described. The existence of a family of taxation vectors in the "input-output"
model of production, under which the economic system is able to function in the
mode of sustainable development, is proved. Restrictions were found for the
vector of taxation in the economic system, under which the economic system is
able to function in the mode of sustainable development. The axioms of the
aggregated description of the economy is proposed.

Most national economies are linked by international trade. Consequently,
economic globalization forms a massive and complex economic network with strong
links, that is, interactions arising from increasing trade. Various interesting
collective motions are expected to emerge from strong economic interactions in
a global economy under trade liberalization. Among the various economic
collective motions, economic crises are our most intriguing problem. In our
previous studies, we have revealed that the Kuramoto's coupled limit-cycle
oscillator model and the Ising-like spin model on networks are invaluable tools
for characterizing the economic crises. In this study, we develop a
mathematical theory to describe an interacting agent model that derives the
Kuramoto model and the Ising-like spin model by using appropriate
approximations. Our interacting agent model suggests phase synchronization and
spin ordering during economic crises. We confirm the emergence of the phase
synchronization and spin ordering during economic crises by analyzing various
economic time series data. We also develop a network reconstruction model based
on entropy maximization that considers the sparsity of the network. Here
network reconstruction means estimating a network's adjacency matrix from a
node's local information. The interbank network is reconstructed using the
developed model, and a comparison is made of the reconstructed network with the
actual data. We successfully reproduce the interbank network and the known
stylized facts. In addition, the exogenous shock acting on an industry
community in a supply chain network and financial sector are estimated.
Estimation of exogenous shocks acting on communities of in the real economy in
the supply chain network provide evidence of the channels of distress
propagating from the financial sector to the real economy through the supply
chain network.

This paper provides an attempt to formalize Hayek's notion of spontaneous
order within the framework of the Arrow-Debreu economy. Our study shows that if
a competitive economy is enough fair and free, then a spontaneous economic
order shall emerge in long-run competitive equilibria so that social members
together occupy an optimal distribution of income. Despite this, the
spontaneous order might degenerate in the form of economic crises whenever an
equilibrium economy approaches the extreme competition. Remarkably, such a
theoretical framework of spontaneous order provides a bridge linking Austrian
economics and Neoclassical economics, where we shall comprehend a truth:
"Freedom promotes technological progress".

We study association between macroeconomic news and stock market returns
using the statistical theory of copulas, and a new comprehensive measure of
news based on the indexing of news wires. We find the impact of economic news
on equity returns to be nonlinear and asymmetric. In particular, controlling
for economic conditions and surprises associated with releases of economic
data, we find that the market reacts strongly and negatively to the most
unfavourable macroeconomic news, but appears to largely discount the good news.
This relationship persists throughout the different stages of the business
cycle.

This article presents an exciting finding for the power industry: the
parameters of secondary frequency control based on integral or proportional
integral control can be tuned to achieve economic operation and frequency
regulation simultaneously. We show that if the power imbalance is represented
by frequency deviation, an iterative dual decomposition based economic dispatch
solving is equivalent to integral control. An iterative method of multipliers
based economic dispatch is equivalent to proportional integral control.
Similarly, if the controller parameters of the secondary frequency controls are
chosen based on generator cost functions, these secondary frequency controllers
achieve both economic operation and frequency regulation simultaneously.

The goal of this study is to analyze the dynamics underlying Algiers urban
area formation with reference to The New Economic Geography (NEG) theories and
more precisely to the paper of Paul Krugman (1991), "Increasing returns and
economic geography" which explains the mechanisms of economic activities
concentration through two types of forces: centripetal forces enhancing the
economic activities concentration and centrifugal forces hindering the
agglomeration process. In fact, these mechanisms are translated into a system
of nonlinear equations which is very hard to solve analytically. As a
consequence, the use of numerical methods is highly advocated. We present some
numerical simulations using real Algerian data.

This study uses Vector Autoregression (VAR) Methodology as well as Vector
Error Correction (VEC) Methodology to examine the existence and direction of
causality between economic growth and IMF lending for Ukraine. The paper
examines the IMF lending data for the period of 1991-2010. Robust empirical
analysis indicates that IMF lending has a negative effect of on Ukraine's
economic growth in the short term. Policy implications of this finding are
that, despite short-run decline in economic growth, IMF lending can result in a
long-run sustainable growth for Ukraine. For this, policymakers need to ensure
that fund's money are used not only to cover budget's deficit, but also to
finance institutional reforms.

The 'Discussion & Debate' issue of this Euro. Phys. J. Special Topic volume
is: 'Can economics be a Physical Science?' I would rather address a more
general question: 'Can economics or sociology avoid joining Natural Science?'
and argue that mainstream or core economics or sociology can not escape joining
natural science.

How technology affects growth or employment has long been debated. With a
hiatus, the debate revived once again in the form of how Information and
Communications Technology, as a form of new technology, exerts on productivity
and employment. Information and Communications Technology perceived as General
Purpose Technology like steam engine or electricity in the past, ushered the
world into a new techno-economic paradigm, given its deep social, economic and
cultural implications. For instance, within economic implication, it is hard to
imagine an economic activity that does not it, directly or indirectly.
Eventually, Information and Communications Technology intensity, measure as the
ratio of Information and Communications Technology investment to total
investment, increased phenomenally in industries across sectors.

The purpose of this paper is to focus on similarity and/or heterogeneity of
taxonomies of innovation present in the economic fields to show as the economic
literature uses different names to indicate the same type of technical change
and innovation, and the same name for different types of innovation. This
ambiguity of classification makes it impossible to compare the various studies;
moreover the numerous typologies existing in the economics of innovation,
technometrics, economics of technical change, management of technology, etc.,
have hindered the development of knowledge in these fields. The research
presents also new directions on the classification of innovation that try to
overcome these problems.

The fundamental purpose of the present research article is to introduce the
basic principles of Dimensional Analysis in the context of the neoclassical
economic theory, in order to apply such principles to the fundamental relations
that underlay most models of economic growth. In particular, basic instruments
from Dimensional Analysis are used to evaluate the analytical consistency of
the Neoclassical economic growth model. The analysis shows that an adjustment
to the model is required in such a way that the principle of dimensional
homogeneity is satisfied.

Traditional centralized energy systems have the disadvantages of difficult
management and insufficient incentives. Blockchain is an emerging technology,
which can be utilized in energy systems to enhance their management and
control. Integrating token economy and blockchain technology, token economic
systems in energy possess the characteristics of strong incentives and low
cost, facilitating integrating renewable energy and demand side management, and
providing guarantees for improving energy efficiency and reducing emission.
This article describes the concept and functionality of token economics, and
then analyzes the feasibility of applying token economics in the energy
systems, and finally discuss the applications of token economics with an
example in integrated energy systems.

The current article traces back the scientific interest to cultural levels
across the organization at the University of National and World Economy, and
especially in the series of Economic Alternatives - an official scientific
magazine, issued by this Institution. Further, a wider and critical review of
international achievements in this field is performed, revealing diverse
analysis perspectives with respect to cultural levels. Also, a useful model of
exploring and teaching the cultural levels beyond the organization is proposed.
  Keywords: globalization, national culture, organization culture, cultural
levels, cultural economics. JEL: M14, Z10.

We propose a combinatorial model of economic development. An economy develops
by acquiring new capabilities allowing for the production of an ever greater
variety of products of increasingly complex products. Taking into account that
economies abandon the least complex products as they develop over time, we show
that variety first increases and then decreases in the course of economic
development. This is consistent with the empirical pattern known as 'the hump'.
Our results question the common association of variety with complexity. We
further discuss the implications of our model for future research.

I propose a novel method, the Wasserstein Index Generation model (WIG), to
generate a public sentiment index automatically. To test the model`s
effectiveness, an application to generate Economic Policy Uncertainty (EPU)
index is showcased.

Colleting the data through a survey in the Northern region of Malaysia;
Kedah, Perlis, Penang and Perak, this study investigates intergenerational
social mobility in Malaysia. We measure and analyzed the factors that influence
social-economic mobility by using binary choice model (logit model). Social
mobility can be measured in several ways, by income, education, occupation or
social class. More often, economic research has focused on some measure of
income. Social mobility variable is measured using the difference between
educational achievement between a father and son. If there is a change of at
least of two educational levels between a father and son, then this study will
assign the value one which means that social mobility has occurred.

How does economics research help in solving societal challenges? This brief
note sheds additional light on this question by providing ways to connect
Journal of Economic Literature (JEL) codes and Sustainable Development Goals
(SDGs) of the United Nations. These simple linkages illustrate that the themes
of SDGs have corresponding JEL classification codes. As the mappings presented
here are necessarily imperfect and incomplete, there is plenty of room for
improvements. In an ideal world, there would be a JEL classification system for
SDGs, a separate JEL code for each of the 17 SDGs.

This paper develops a Nash-equilibrium extension of the classic SIR model of
infectious-disease epidemiology ("Nash SIR"), endogenizing people's decisions
whether to engage in economic activity during a viral epidemic and allowing for
complementarity in social-economic activity. An equilibrium epidemic is one in
which Nash equilibrium behavior during the epidemic generates the epidemic.
There may be multiple equilibrium epidemics, in which case the epidemic
trajectory can be shaped through the coordination of expectations, in addition
to other sorts of interventions such as stay-at-home orders and accelerated
vaccine development. An algorithm is provided to compute all equilibrium
epidemics.

This research aims to provide an explanatory analyses of the business cycles
divergence between Euro Area and Romania, respectively its drivers, since the
synchronisation of output-gaps is one of the most important topic in the
context of a potential EMU accession. According to the estimates, output-gaps
synchronisation entered on a downward path in the subperiod 2010-2017, compared
to 2002-2009. The paper demonstrates there is a negative relationship between
business cycles divergence and three factors (economic structure convergence,
wage structure convergence and economic openness), but also a positive
relationship between it and its autoregressive term, respectively the GDP per
capita convergence.

This paper examines the impact of financial risks on economic growth in the
first 15 Member States of the European Union, considering 1995-2014 period and
aims to lay down a new explanatory model of economic growth, based mainly on
the behavioral reactivity of the financial disruptions mentioned above. The
model was estimated through the panel estimated generalized least squares
method and included additional control variables in order to strengthen the
research conducted. Our goal consists in the examination of the financial risks
in the European Union and in the estimation of their impact on economic growth.

In this study we investigated impact of crop diversification on
socio-economic life of tribal people from eastern ghats of India. We have
adopted linear regression formalism to check impact of cross diversification.
We observe a positive intercept for almost all factors. Coefficient of
correlation is calculated to examine the inter dependence of CDI and our
various individually measured dependent variables. A positive correlation is
observed in almost all factors. This study shows that a positive change
occurred in their social, economic life in the post diversification era.

Optimal transport has become part of the standard quantitative economics
toolbox. It is the framework of choice to describe models of matching with
transfers, but beyond that, it allows to: extend quantile regression; identify
discrete choice models; provide new algorithms for computing the random
coefficient logit model; and generalize the gravity model in trade. This paper
offer a brief review of the basics of the theory, its applications to
economics, and some extensions.

We investigate the effectiveness of different machine learning methodologies
in predicting economic cycles. We identify the deep learning methodology of
Bi-LSTM with Autoencoder as the most accurate model to forecast the beginning
and end of economic recessions in the U.S. We adopt commonly-available macro
and market-condition features to compare the ability of different machine
learning models to generate good predictions both in-sample and out-of-sample.
The proposed model is flexible and dynamic when both predictive variables and
model coefficients vary over time. It provided good out-of-sample predictions
for the past two recessions and early warning about the COVID-19 recession.

This textbook is an introduction to economic networks, intended for students
and researchers in the fields of economics and applied mathematics. The
textbook emphasizes quantitative modeling, with the main underlying tools being
graph theory, linear algebra, fixed point theory and programming. The text is
suitable for a one-semester course, taught either to advanced undergraduate
students who are comfortable with linear algebra or to beginning graduate
students.

This study provides new evidence regarding the extent to which medical care
mitigates the economic consequences of various health shocks for the individual
and a wider family. To obtain causal effects, I focus on the role of medical
scientific discoveries and leverage the longitudinal dimension of unique
administrative data for Sweden. The results indicate that medical innovations
strongly mitigate the negative economic consequences of a health shock for the
individual and create spillovers to relatives. Such mitigating effects are
highly heterogeneous across prognoses. These results suggest that medical
innovation substantially reduces the burden of welfare costs yet produces
income inequalities.

Agent-based computational economics is a field with a rich academic history,
yet one which has struggled to enter mainstream policy design toolboxes,
plagued by the challenges associated with representing a complex and dynamic
reality. The field of Reinforcement Learning (RL), too, has a rich history, and
has recently been at the centre of several exponential developments. Modern RL
implementations have been able to achieve unprecedented levels of
sophistication, handling previously unthinkable degrees of complexity. This
review surveys the historical barriers of classical agent-based techniques in
economic modelling, and contemplates whether recent developments in RL can
overcome any of them.

Nobel laureates cluster together. 696 of the 727 winners of the Nobel Prize
in physics, chemistry, medicine, and economics belong to one single academic
family tree. 668 trace their ancestry to Emmanuel Stupanus, 228 to Lord
Rayleigh (physics, 1904). Craig Mello (medicine, 2006) counts 51 Nobelists
among his ancestors. Chemistry laureates have the most Nobel ancestors and
descendants, economics laureates the fewest. Chemistry is the central
discipline. Its Nobelists have trained and are trained by Nobelists in other
fields. Nobelists in physics (medicine) have trained (by) others. Economics
stands apart. Openness to other disciplines is the same in recent and earlier
times. The familial concentration of Nobelists is lower now than it used to be.

This paper presents a model for smoothly varying heterogeneous persistence of
economic data. We argue that such dynamics arise naturally from the dynamic
nature of economic shocks with various degree of persistence. The
identification of such dynamics from data is done using localised regressions.
Empirically, we identify rich persistence structures that change smoothly over
time in two important data sets: inflation, which plays a key role in policy
formulation, and stock volatility, which is crucial for risk and market
analysis.

We study the dynamic effects of fires on county labor markets in the US using
a novel geophysical measure of fire exposure based on satellite imagery. We
find increased fire exposure causes lower employment growth in the short and
medium run, with medium-run effects being linked to migration. We also document
heterogeneous effects across counties by education and industrial concentration
levels, states of the business cycle, and fire size. By overcoming challenges
in measuring fire impacts, we identify vulnerable places and economic states,
offering guidance on tailoring relief efforts and contributing to a broader
understanding of natural disasters' economic impacts.

In this study, we analyze the relationship between human population growth
and economic dynamics. To do so, we present a modified version of the Verhulst
model and the Solow model, which together simulate population dynamics and the
role of economic variables in capital accumulation. The model incorporates
support and foraging functions, which participate in the dynamic relationship
between population growth and the creation and destruction of carrying
capacity. The validity of the model is demonstrated using empirical data.

Packing peanuts, as defined by Wikipedia, is a common loose-fill packaging
and cushioning material that helps prevent damage to fragile items. In this
paper, I propose that synthetic data, akin to packing peanuts, can serve as a
valuable asset for economic prediction models, enhancing their performance and
robustness when integrated with real data. This hybrid approach proves
particularly beneficial in scenarios where data is either missing or limited in
availability. Through the utilization of Affinity credit card spending and
Womply small business datasets, this study demonstrates the substantial
performance improvements achieved by employing a hybrid data approach,
surpassing the capabilities of traditional economic modeling techniques.

The paper presents instructive interdisciplinary applications of constrained
mechanics calculus in economics on a level appropriate for the undergraduate
physics education. The aim of the paper is: 1. to meet the demand for
illustrative examples suitable for presenting the background of the highly
expanding research field of econophysics even on the undergraduate level and 2.
to enable the students to understand deeper the principles and methods
routinely used in mechanics by looking at the well known methodology from the
different perspective of economics. Two constrained dynamic economic problems
are presented using the economic terminology in an intuitive way. First, the
Phillips model of business cycle is presented as a system of forced
oscillations and the general problem of two interacting economies is solved by
the nonholonomic dynamics approach. Second, the Cass-Koopmans-Ramsey model of
economical growth is solved as a variational problem with a velocity dependent
constraint using the vakonomic approach. The specifics of the solution
interpretation in economics compared to mechanics is discussed in detail, a
discussion of the nonholonomic and vakonomic approaches to constrained problems
in mechanics and economics is provided and an economic interpretation of the
Lagrange multipliers (possibly surprising for the students of physics) is
carefully explained. The paper can be used by the undergraduate students of
physics interested in interdisciplinary physics applications to get in touch
with current scientific approach to economics based on a physical background or
by university teachers as an attractive supplement to the classical mechanics
lessons.

This paper extends Xing's (2023abcd) optimal growth models of catching-up
economies from the case of production function switching to that of economic
structure switching and argues how a country develops its economy by endogenous
structural transformation and efficient resource allocation in a market
mechanism. To achieve this goal, the paper first summarizes three attributes of
economic structures from the literature, namely, structurality, durationality,
and transformality, and discuss their implications for methods of economic
modeling. Then, with the common knowledge assumption, the paper extends Xing's
(2023a) optimal growth model that is based on production function switching and
considers an extended Ramsey model with endogenous structural transformation in
which the social planner chooses the optimal industrial structure, recource
allocation with the chosen structure, and consumption to maximize the
representative household's total utility subject to the resource constraint.
The paper next establishes the mathematical underpinning of the static,
dynamic, and switching equilibria. The Ramsey growth model and its equilibria
are then extended to economies with complicated economic structures consisting
of hierarchical production, technology adoption and innovation, infrastructure,
and economic and political institutions. The paper concludes with a brief
discussion of applications of the proposed methodology to economic development
problems in other scenarios.

Economists have predicted that damages from global warming will be as low as
2.1% of global economic production for a 3$^\circ$C rise in global average
surface temperature, and 7.9% for a 6$^\circ$C rise. Such relatively trivial
estimates of economic damages -- when these economists otherwise assume that
human economic productivity will be an order of magnitude higher than today --
contrast strongly with predictions made by scientists of significantly reduced
human habitability from climate change. Nonetheless, the coupled economic and
climate models used to make such predictions have been influential in the
international climate change debate and policy prescriptions. Here we review
the empirical work done by economists and show that it severely underestimates
damages from climate change by committing several methodological errors,
including neglecting tipping points, and assuming that economic sectors not
exposed to the weather are insulated from climate change. Most fundamentally,
the influential Integrated Assessment Model DICE is shown to be incapable of
generating an economic collapse, regardless of the level of damages. Given
these flaws, economists' empirical estimates of economic damages from global
warming should be rejected as unscientific, and models that have been
calibrated to them, such as DICE, should not be used to evaluate economic risks
from climate change, or in the development of policy to attenuate damages.

The development of trajectory-based operations and the rolling network
operations plan in European air traffic management network implies a move
towards more collaborative, strategic flight planning. This opens up the
possibility for inclusion of additional information in the collaborative
decision-making process. With that in mind, we define the indicator for the
economic risk of network elements (e.g., sectors or airports) as the expected
costs that the elements impose on airspace users due to Air Traffic Flow
Management (ATFM) regulations. The definition of the indicator is based on the
analysis of historical ATFM regulations data, that provides an indication of
the risk of accruing delay. This risk of delay is translated into a monetary
risk for the airspace users, creating the new metric of the economic risk of a
given airspace element. We then use some machine learning techniques to find
the parameters leading to this economic risk. The metric is accompanied by an
indication of the accuracy of the delay cost prediction model. Lastly, the
economic risk is transformed into a qualitative economic severity
classification. The economic risks and consequently economic severity can be
estimated for different temporal horizons and time periods providing an
indicator which can be used by Air Navigation Service Providers to identify
areas which might need the implementation of strategic measures (e.g.,
resectorisation or capacity provision change), and by Airspace Users to
consider operation of routes which use specific airspace regions.

Cities host diverse people and their mixing is the engine of prosperity. In
turn, segregation and inequalities are common features of most cities and
locations that enable the meeting of people with different socio-economic
status are key for urban inclusion. In this study, we adopt the concept of
economic complexity to quantify the sophistication of amenity supply at urban
locations. We propose that neighborhood complexity and amenity complexity are
connected to the ability of locations to attract diverse visitors from various
socio-economic backgrounds across the city. We construct the measures of
amenity complexity based on the local portfolio of diverse and non-ubiquitous
amenities in Budapest, Hungary. Socio-economic mixing at visited third places
is investigated by tracing the daily mobility of individuals and by
characterizing their status by the real-estate price of their home locations.
Results suggest that measures of ubiquity and diversity of amenities do not,
but neighborhood complexity and amenity complexity are correlated with the
urban centrality of locations. Urban centrality is a strong predictor of
socio-economic mixing, but both neighborhood complexity and amenity complexity
add further explanatory power to our models. Our work combines urban mobility
data with economic complexity thinking to show that the diversity of
non-ubiquitous amenities, central locations, and the potentials for
socio-economic mixing are interrelated.

Evolutionary economics has developed into an academic field of its own,
institutionalized around, amongst others, the Journal of Evolutionary Economics
(JEE). This paper analyzes the way and extent to which evolutionary economics
has become an interdisciplinary journal, as its aim was: a journal that is
indispensable in the exchange of expert knowledge on topics and using
approaches that relate naturally with it. Analyzing citation data for the
relevant academic field for the Journal of Evolutionary Economics, we use
insights from scientometrics and social network analysis to find that, indeed,
the JEE is a central player in this interdisciplinary field aiming mostly at
understanding technological and regional dynamics. It does not, however, link
firmly with the natural sciences (including biology) nor to management
sciences, entrepreneurship, and organization studies. Another journal that
could be perceived to have evolutionary acumen, the Journal of Economic Issues,
does relate to heterodox economics journals and is relatively more involved in
discussing issues of firm and industry organization. The JEE seems most keen to
develop theoretical insights.

The dynamic network of relationships among corporations underlies cascading
economic failures including the current economic crisis, and can be inferred
from correlations in market value fluctuations. We analyze the time dependence
of the network of correlations to reveal the changing relationships among the
financial, technology, and basic materials sectors with rising and falling
markets and resource constraints. The financial sector links otherwise weakly
coupled economic sectors, particularly during economic declines. Such links
increase economic risk and the extent of cascading failures. Our results
suggest that firewalls between financial services for different sectors would
reduce systemic risk without hampering economic growth.

We address the problem of banking system resilience by applying
off-equilibrium statistical physics to a system of particles, representing the
economic agents, modelled according to the theoretical foundation of the
current banking regulation, the so called Merton-Vasicek model. Economic agents
are attracted to each other to exchange `economic energy', forming a network of
trades. When the capital level of one economic agent drops below a minimum, the
economic agent becomes insolvent. The insolvency of one single economic agent
affects the economic energy of all its neighbours which thus become susceptible
to insolvency, being able to trigger a chain of insolvencies (avalanche). We
show that the distribution of avalanche sizes follows a power-law whose
exponent depends on the minimum capital level. Furthermore, we present evidence
that under an increase in the minimum capital level, large crashes will be
avoided only if one assumes that agents will accept a drop in business levels,
while keeping their trading attitudes and policies unchanged. The alternative
assumption, that agents will try to restore their business levels, may lead to
the unexpected consequence that large crises occur with higher probability.

The seriousness of the current crisis urgently demands new economic thinking
that breaks the austerity vs. deficit spending circle in economic policy. The
core tenet of the paper is that the most important problems that natural and
social science are facing today are inverse problems, and that a new approach
that goes beyond optimization is necessary. The approach presented here is
radical in the sense that identifies the roots in key assumptions in economic
theory such as optimal behavior and stability to provide an inverse thinking
perspective to economic modeling of use in economic and financial stability
policy. The inverse problem provides a truly multidisciplinary platform where
related problems from different disciplines can be studied under a common
approach with comparable results.

Historical economic growth in countries of the former USSR is analysed. It is
shown that Unified Growth Theory is contradicted by the data, which were used,
but not analysed, during the formulation of this theory. Unified Growth Theory
does not explain the mechanism of economic growth. It explains the mechanism of
Malthusian stagnation, which did not exist and it explains the mechanism of the
transition from stagnation to growth that did not happen. Unified Growth Theory
is full of stories but it is hard to decide which of them are reliable because
they are based on unprofessional examination of data. The data show that the
economic growth in the former USSR was never stagnant but hyperbolic.
Industrial Revolution did not boost the economic growth in the former USSR.
Unified Growth Theory needs to be revised or replaced by a reliable theory to
reconcile it with data and to avoid creating the unwarranted sense of security
about the current economic growth.

Contrary to conventional economic growth theory, which reduces a country's
output to one aggregate variable (GDP), product diversity is central to
economic development, as recent 'economic complexity' research suggests. A
country's product diversity reflects its diversity of knowhow or
'capabilities'. Researchers proposed the Economic Complexity Index (ECI) and
the country Fitness index to estimate a country's number of capabilities from
international export data; these measures predict economic growth better than
conventional variables such as human capital. This paper offers a simpler
measure of a country's knowhow, Log Product Diversity (or LPD, the logarithm of
a country's number of products), which can be derived from a one-parameter
combinatorial model of production in which a set of knowhows combine with some
probability to turn raw materials into a product. ECI and log-fitness can be
interpreted theoretically (using the combinatorial model) and empirically as
potentially noisy estimates of LPD; moreover, controlling for natural
resources, the simple measure better explains the cross-country differences in
GDP and in GDP per capita.

Economic growth in Western Europe, Eastern Europe, Asia, countries of the
former USSR, Africa and Latin America were analysed. It is demonstrated that
the fundamental postulate of the Unified Growth Theory about the existence of
the three regimes of growth (Malthusian regime, post-Malthusian regime and
sustained-growth regime) is contradicted by data. These regimes did not exist.
In particular, there was no escape from the Malthusian trap because there was
no trap. Economic growth in all these regions was not stagnant but hyperbolic.
Unified Growth Theory is fundamentally incorrect. However, this theory is also
dangerously misleading because it claims a transition from the endless epoch of
stagnation to the new era of sustained economic growth, the interpretation
creating the sense of security and a promise of prosperity. The data show that
the opposite is true. Economic growth in the past was sustained and secure.
Now, it is supported by the increasing ecological deficit. The long-term
sustained and secure economic growth has yet to be created. It did not happen
automatically, as suggested incorrectly by the Unified Growth Theory.

A four-pronged approach to dealing with Social Science Phenomenon is
outlined. This methodology is applied to Financial Services, Economic Growth
and Well-Being. The four prongs are like the four directions for an army
general looking for victory. Just like the four directions, we need to be aware
that there is a degree of interconnectedness in the below four prongs.
-Uncertainty Principle of the Social Sciences -Responsibilities of Fiscal
Janitors -Need for Smaller Organizations -Redirecting Growth that Generates
Garbage The importance of gaining a more profound comprehension of welfare and
delineating its components into those that result from an increase in goods and
services, and hence can be attributed to economic growth, and into those that
are not related to economic growth but lead to a better quality of life, is
highlighted. The reasoning being that economic growth alone is an inadequate
indicator of well-being. Hand in hand with a better understanding of the
characteristics of welfare, comes the need to consider the metrics we currently
have that gauge economic growth and supplement those with measures that capture
well-being more holistically.

Economic complexity reflects the amount of knowledge that is embedded in the
productive structure of an economy. By combining tools from network science and
econometrics, a robust and stable relationship between a country's productive
structure and its economic growth has been established. Here we report that not
only goods but also services are important for predicting the rate at which
countries will grow. By adopting a terminology which classifies manufactured
goods and delivered services as products, we investigate the influence of
services on the country's productive structure. In particular, we provide
evidence that complexity indices for services are in general higher than those
for goods, which is reflected in a general tendency to rank countries with
developed service sector higher than countries with economy centred on
manufacturing of goods. By focusing on country dynamics based on experimental
data, we investigate the impact of services on the economic complexity of
countries measured in the product space (consisting of both goods and
services). Importantly, we show that diversification of service exports and its
sophistication can provide an additional route for economic growth in both
developing and developed countries.

Based on the assumption that economic complexity is characterised by the
interactions of economic agents (who) constantly change their actions and
strategies in response to the outcome they mutually create, this paper presents
how network models can be used a proxies for the mapping, quantification and
analysis of Roman economic complexity. Network analysis provides tools to
visualise and analyse the inherent complexity of various types of data and
their combination (archaeological, geographical, textual) or even of a single
piece of evidence. Equally, the relational approach invites to a structural and
quantitative comparison between periods, regions and the economic systems of
polities and empires. An increasing number of proxies of this kind may allow us
to capture the trajectories of economic complexity beyond metaphors.

The memory-type control charts, such as EWMA and CUSUM, are powerful tools
for detecting small quality changes in univariate and multivariate processes.
Many papers on economic design of these control charts use the formula proposed
by Lorenzen and Vance (1986) [Lorenzen, T. J., & Vance, L. C. (1986). The
economic design of control charts: A unified approach. Technometrics, 28(1),
3-10, DOI: 10.2307/1269598]. This paper shows that this formula is not correct
for memory-type control charts and its values can significantly deviate from
the original values even if the ARL values used in this formula are accurately
computed. Consequently, the use of this formula can result in charts that are
not economically optimal. The formula is corrected for memory-type control
charts, but unfortunately the modified formula is not a helpful tool from a
computational perspective. We show that simulation-based optimization is a
possible alternative method.

Two network measures known as the Economic Complexity Index (ECI) and Product
Complexity Index (PCI) have provided important insights into patterns of
economic development. We show that the ECI and PCI are equivalent to a spectral
clustering algorithm that partitions a similarity graph into two parts. The
measures are also related to various dimensionality reduction methods and can
be interpreted as vectors that determine distances between nodes based on their
similarity. Our results shed a new light on the ECI's empirical success in
explaining cross-country differences in GDP/capita and economic growth, which
is often linked to the diversity of country export baskets. In fact, countries
with high (low) ECI tend to specialize in high (low) PCI products. We also find
that the ECI and PCI uncover economically informative specialization patterns
across US states and UK regions.

A generalization of the economic model of logistic growth, which takes into
account the effects of memory and crises, is suggested. Memory effect means
that the economic factors and parameters at any given time depend not only on
their values at that time, but also on their values at previous times. For the
mathematical description of the memory effects, we use the theory of
derivatives of non-integer order. Crises are considered as sharp splashes
(bursts) of the price, which are mathematically described by the
delta-functions. Using the equivalence of fractional differential equations and
the Volterra integral equations, we obtain discrete maps with memory that are
exact discrete analogs of fractional differential equations of economic
processes. We derive logistic map with memory, its generalizations, and
"economic" discrete maps with memory from the fractional differential
equations, which describe the economic natural growth with competition,
power-law memory and crises.

Asteroid mining has been proposed as an approach to complement Earth-based
supplies of rare earth metals and supplying resources in space, such as water.
However, existing studies on the economic viability of asteroid mining have
remained rather simplistic and do not provide much guidance on which
technological improvements would be needed for increasing its economic
viability. This paper develops a techno-economic analysis of asteroid mining
with the objective of providing recommendations for future technology
development and performance improvements. Both, in-space resource provision
such as water and return of platinum to Earth are considered. Starting from
first principles of techno-economic analysis, gradually additional economic and
technological factors are added to the analysis model. Applied to mining
missions involving spacecraft reuse, learning curve effect, and multiple
spacecraft, their economic viability is assessed. A sensitivity analysis with
respect to throughput rate, spacecraft mass, and resource price is performed.
Furthermore, a sample asteroid volatile mining architecture based on small
CubeSat-class spacecraft is presented. It is concluded that key technological
drivers for asteroid mining missions are throughput rate, number of spacecraft
per mission, and the rate in which successive missions are conducted.

We explore economic competitiveness of Indian states based on the economic
complexity algorithm, using a pair of coupled non-linear maps to characterize
the Fitness of states and Complexity of products exported by them. We find that
states produce almost all products within their productive capabilities -
diversifying rather than specializing, and that the probability of coexistence
of any pair of productive capabilities is maximized when capabilities are of
similar complexity. Therefore, states require long time horizons to build
complex capabilities and diverse products. We contextualize Fitness using human
development, and find an emergent typology of states. Of most concern are the
states of Odisha, Uttar Pradesh, and Bihar, stuck in vicious feedback cycles of
poor economic complexity and low human development. Economic complexity also
reveals significant concerns with the economic trajectories of Punjab, Gujarat,
and West Bengal. We discuss these emergent trends within the framework of
India's modern economic history.

We calculate measures of economic complexity for US metropolitan areas for
the years 2007-2015 based on industry employment data. We show that the concept
of economic complexity translates well from the cross-country to the regional
setting, and is able to incorporate local as well as traded industries. The
largest cities and the Northeast of the US have the highest average complexity,
while traded industries are more complex than local-serving ones on average,
but with some exceptions. On average, regions with higher complexity have a
higher income per capita, but those regions also were more affected by the
financial crisis. Finally, economic complexity is a significant predictor of
within-decreases in income per capita and population. Our findings highlight
the importance of subnational regions, and particularly metropolitan areas, as
units of economic geography.

In this book, we outline a model of a non-capitalist market economy based on
not-for-profit forms of business. This work presents both a critique of the
current economic system and a vision of a more socially, economically, and
ecologically sustainable economy. The point of departure is the purpose and
profit-orientation embedded in the legal forms used by businesses (e.g.,
for-profit or not-for-profit) and the ramifications of this for global
sustainability challenges such as environmental pollution, resource use,
climate change, and economic inequality. We document the rapid rise of
not-for-profit forms of business in the global economy and offer a conceptual
framework and an analytical lens through which to view these relatively new
economic actors and their potential for transforming the economy. The book
explores how a market consisting of only or mostly not-for-profit forms of
business might lead to better financial circulation, economic equality, social
well-being, and environmental regeneration as compared to for-profit markets.

The Cooperation Council for the Arab States of the Gulf (GCC) is generally
regarded as a success story for economic integration in Arab countries. The
idea of regional integration gained ground by signing the GCC Charter. It
envisioned a closer economic relationship between member states.Although
economic integration among GCC member states is an ambitious step in the right
direction, there are gaps and challenges ahead. The best way to address the
gaps and challenges that exist in formulating integration processes in the GCC
is to start with a clear set of rules and put the necessary mechanisms in
place. Integration attempts must also exhibit a high level of commitment in
order to deflect dynamics of disintegration that have all too often frustrated
meaningful integration in Arab countries. If the GCC can address these issues,
it could become an economic powerhouse within Arab countries and even Asia.

We provide one of the first systematic assessments of the development and
determinants of economic anxiety at the onset of the coronavirus pandemic.
Using a global dataset on internet searches and two representative surveys from
the US, we document a substantial increase in economic anxiety during and after
the arrival of the coronavirus. We also document a large dispersion in beliefs
about the pandemic risk factors of the coronavirus, and demonstrate that these
beliefs causally affect individuals' economic anxieties. Finally, we show that
individuals' mental models of infectious disease spread understate non-linear
growth and shape the extent of economic anxiety.

The concept of shared value was introduced by Porter and Kramer as a new
conception of capitalism. Shared value describes the strategy of organizations
that simultaneously enhance their competitiveness and the social conditions of
related stakeholders such as employees, suppliers and the natural environment.
The idea has generated strong interest, but also some controversy due to a lack
of a precise definition, measurement techniques and difficulties to connect
theory to practice. We overcome these drawbacks by proposing an economic
framework based on three key aspects: coalition formation, sustainability and
consistency, meaning that conclusions can be tested by means of logical
deductions and empirical applications. The presence of multiple agents to
create shared value and the optimization of both social and economic criteria
in decision making represent the core of our quantitative definition of shared
value. We also show how economic models can be characterized as shared value
models by means of logical deductions. Summarizing, our proposal builds on the
foundations of shared value to improve its understanding and to facilitate the
suggestion of economic hypotheses, hence accommodating the concept of shared
value within modern economic theory.

As of July, 2020, acute respiratory syndrome caused by coronavirus COVID-19
is spreading over the world and causing severe economic damages. While
minimizing human contact is effective in managing the outbreak, it causes
severe economic losses. Strategies solving this dilemma by considering
interrelation between the spread of the virus and economic activities are in
urgent needs for mitigating the health and economic damage. Here we propose an
abstract agent-based model for the outbreak of COVID-19 in which economic
activities are taken into account. The computational simulation of the model
recapitulated the trade-off between health and economic damage associated with
lockdown measures. Based on the simulation results, we discuss how macroscopic
dynamics of infection and economy emerge from the individuals' behaviours. We
believe our model can serve as a platform for discussing solutions to the
abovementioned dilemma.

In economic literature, economic complexity is typically approximated on the
basis of an economy's gross export structure. However, in times of ever
increasingly integrated global value chains, gross exports may convey an
inaccurate image of a country's economic performance since they also
incorporate foreign value-added and double-counted exports. Thus, I introduce a
new empirical approach approximating economic complexity based on a country's
value-added export structure. This approach leads to substantially different
complexity rankings compared to established metrics. Moreover, the explanatory
power of GDP per capita growth rates for a sample of 40 lower-middle- to
high-income countries is considerably higher, even if controlling for typical
growth regression covariates.

The aim of this paper is twofold:1)contribute to a better understanding of
the place of women in Economics and Management disciplines by characterizing
the difference in levels of scientific collaboration between men and women at
the specialties level;2) Investigate the relationship between gender diversity
and citation impact in Economics and Management. Our data, extracted from the
Web of Science database, cover global production as indexed in 302 journals in
Economics and 370 journals in Management, with respectively 153 667 and 163 567
articles published between 2008 and 2018. Results show that collaborative
practices between men and women are quite different in Economics and
Management. We also find that there is a positive and significant effect of
gender diversity on the academic impact of publications. Mixed-gender
publications (co-authored by men and women) receive more citations than
non-mixed papers (written by same-gender author teams) or single-author
publications. The effect is slightly stronger in Management. The regression
analysis also indicates that there is, for both disciplines, a small negative
effect on citations received if the corresponding author is a woman.

The economic impacts of climate change are highly uncertain. Two of the most
important uncertainties are the sensitivity of the climate system and the
so-called damage functions, which relate climate change to economic damages and
benefits. Despite broad awareness of these uncertainties, it is unclear which
of them is most important, both on the global as well as the regional level.
Here we apply different damage functions to data from climate models with
vastly different climate sensitivities, and find that uncertainty in both
climate sensitivity and economic damage per degree of warming are of similar
importance for the global economic impact. Increasing the climate sensitivity
or the sensitivity of the damage function both increases the economic damages
globally. Yet, at the country-level the effect varies depending on the initial
temperature as well as how much the country warms. Our findings emphasise the
importance of including these uncertainties in estimates of future economic
impacts, as they both are vital for the resulting impacts and thus policy
implications.

This paper aims to present empirical analysis of Iranian economic growth from
1950 to 2018 using data from the World Bank, Madison Data Bank, Statistical
Center of Iran, and Central Bank of Iran. The results show that Gross Domestic
Product (GDP) per capital increased by 2 percent annually during this time,
however this indicator has had a huge fluctuation over time. In addition, the
economic growth of Iran and oil revenue have close relationship with each
other. In fact, whenever oil crises happen, great fluctuation in growth rate
and other indicators happened subsequently. Even though the shares of other
sectors like industry and services in GDP have increased over time, the oil
sector still plays a key role in the economic growth of Iran. Moreover, growth
accounting analysis shows contribution of capital plays a significant role in
economic growth of Iran. Furthermore, based on growth accounting framework the
steady state of effective capital is 4.27 for Iran's economy.

We present an integrated database suitable for the investigations of the
Economic development of countries by using the Economic Fitness and Complexity
framework. Firstly, we implement machine learning techniques to reconstruct the
database of Trade of Services and we integrate it with the database of the
Trade of the physical Goods, generating a complete view of the International
Trade and denoted the Universal database. Using this data, we derive a
statistically significant network of interaction of the Economic activities,
where preferred paths of development and clusters of High-Tech industries
naturally emerge. Finally, we compute the Economic Fitness, an algorithmic
assessment of the competitiveness of countries, removing the unexpected
misbehaviour of Economies under-represented by the sole consideration of the
Trade of the physical Goods.

This paper examines the economic consequences of the COVID-19 pandemic to
sub-Saharan Africa (SSA) using the historical approach by analyzing the policy
responses of the region to past crises and their economic consequences. The
study employs the manufacturing-value-added share of GDP as a performance
indicator. The analysis shows that wrong policy intervention to past crises,
lead the African sub-region into the deplorable economic situation. The study
observed that the region leapfrogged prematurely to import substitution, export
promotion, and global value chains. Based on these past experiences, the region
should adopt a gradual approach in responding to the COVID-19 economic
consequences. The sub-region should first address relevant areas of
sustainability, including proactive investment in research and development to
develop home-grown technology, upgrade essential infrastructural facilities,
develop security infrastructures, and strengthen the financial sector.

Macroeconomic data on the Spanish economy during the Second Republic is not
accurate, the interpretation of historical events from the figures obtained is
divergent and misleading. Hasty laws were enacted in attempts to resolve social
problems arising mainly from deep economic inequalities, but they were often
nothing more than declarations of good intentions. Spain suffered in the
aftermath of the international economic downturn as it began to be felt at the
end of the dictatorship of General Primo de Rivera. Economic policy was
developed under the Constitution,but, despite the differences between the first
and second biennium, there was a tendency to maintain the guidelines from the
previous stage and in general, sometimes unfairly, it aimed at least to avoid
the destabilization of the financial system. Nonetheless, it ultimately failed
to achieve its goals, mainly because of the frequent changes of government
mediated by a social crisis of greater significance that had relegated economic
issues into the background.

Economists and social scientists have debated the relative importance of
nature (one's genes) and nurture (one's environment) for decades, if not
centuries. This debate can now be informed by the ready availability of genetic
data in a growing number of social science datasets. This paper explores the
potential uses of genetic data in economics, with a focus on estimating the
interplay between nature (genes) and nurture (environment). We discuss how
economists can benefit from incorporating genetic data into their analyses even
when they do not have a direct interest in estimating genetic effects. We argue
that gene--environment (GxE) studies can be instrumental for (i) testing
economic theory, (ii) uncovering economic or behavioral mechanisms, and (iii)
analyzing treatment effect heterogeneity, thereby improving the understanding
of how (policy) interventions affect population subgroups. We introduce the
reader to essential genetic terminology, develop a conceptual economic model to
interpret gene-environment interplay, and provide practical guidance to
empirical researchers.

In recent years economic complexity has grown into an active field of
fundamental and applied research. Yet, despite important advances, the policy
implications of economic complexity remain unclear or misunderstood. Here I
organize the policy implications of economic complexity in a framework grounded
on 4 Ws: what approaches, focused on identifying target activities and/or
locations; when approaches, focused on timing support for related and unrelated
activities; where approaches, focused on the geographic diffusion of knowledge;
and who approaches, focused on the role played by agents of structural change.
The goal of this paper is to provide a framework that groups, organizes, and
clarifies the policy implications of economic complexity to facilitate its
continued use in regional and international development.

Recent studies have found evidence of a negative association between economic
complexity and inequality at the country level. Moreover, evidence suggests
that sophisticated economies tend to outsource products that are less desirable
(e.g. in terms of wage and inequality effects), and instead focus on complex
products requiring networks of skilled labor and more inclusive institutions.
Yet the negative association between economic complexity and inequality on a
coarse scale could hide important dynamics at a fine-grained level. Complex
economic activities are difficult to develop and tend to concentrate spatially,
leading to 'winner-take-most' effects that spur regional inequality in
countries. Large, complex cities tend to attract both high- and low-skills
activities and workers, and are also associated with higher levels of
hierarchies, competition, and skill premiums. As a result, the association
between complexity and inequality reverses at regional scales; in other words,
more complex regions tend to be more unequal. Ideas from polarization theories,
institutional changes, and urban scaling literature can help to understand this
paradox, while new methods from economic complexity and relatedness can help
identify inclusive growth constraints and opportunities.

This paper examined the economic consequences of the COVID-19 pandemic on
sub-Saharan Africa (SSA) using the historical approach and analysing the policy
responses of the region to past crises and their economic consequences. The
study employed the manufacturing-value-added share of GDP as a performance
indicator. The analysis shows that the wrong policy interventions to past
crises led the sub-Saharan African sub-region into its deplorable economic
situation. The study observed that the region leapfrogged prematurely to import
substitution, export promotion, and global value chains. Based on these
experiences, the region should adopt a gradual approach in responding to the
COVID-19 economic consequences. The sub-region should first address relevant
areas of sustainability, including proactive investment in research and
development to develop homegrown technology, upgrade essential infrastructural
facilities, develop security infrastructure, and strengthen the financial
sector.

The Journal of Economic Literature codes classification system (JEL)
published by the American Economic Association (AEA) is the de facto standard
classification system for research literature in economics. The JEL
classification system is used to classify articles, dissertations, books, book
reviews, and working papers in EconLit, a database maintained by the AEA. Over
time, it has evolved and extended to a system with over 850 subclasses. This
paper reviews the history and development of the JEL classification system,
describes the current version, and provides a selective overview of its uses
and applications in research. The JEL codes classification system has been
adopted by several publishers, and their instructions are reviewed. There are
interesting avenues for future research as the JEL classification system has
been surprisingly little used in existing bibliometric and scientometric
research as well as in library classification systems.

Networks have always played a special role for human beings in shaping social
relations, forming public opinion, and driving economic equilibria. Nowadays,
online networked platforms dominate digital markets and capitalization
leader-boards, while social networks drive public discussion. Despite the
importance of networks in many economic and social domains (economics,
sociology, anthropology, psychology,...), the knowledge about the laws that
dominate their dynamics is still scarce and fragmented. Here, we analyse a wide
set of online networks (those financed by advertising) by investigating their
value dynamics from several perspectives: the type of service, the geographic
scope, the merging between networks, and the relationship between economic and
financial value. The results show that the networks are dominated by strongly
nonlinear dynamics. The existence of non-linearity is often underestimated in
social sciences because it involves contexts that are difficult to deal with,
such as the presence of multiple equilibria -- some of which are unstable. Yet,
these dynamics must be fully understood and addressed if we aim to understand
the recent evolution in the economic, political and social milieus, which are
precisely characterised by corner equilibria (e.g., polarization,
winner-take-all solutions, increasing inequality) and nonlinear patterns.

To achieve inclusive green growth, countries need to consider a multiplicity
of economic, social, and environmental factors. These are often captured by
metrics of economic complexity derived from the geography of trade, thus
missing key information on innovative activities. To bridge this gap, we
combine trade data with data on patent applications and research publications
to build models that significantly and robustly improve the ability of economic
complexity metrics to explain international variations in inclusive green
growth. We show that measures of complexity built on trade and patent data
combine to explain future economic growth and income inequality and that
countries that score high in all three metrics tend to exhibit lower emission
intensities. These findings illustrate how the geography of trade, technology,
and research combine to explain inclusive green growth.

This research is devoted to assessing regional economic disparities in
Ukraine, where regional economic inequality is a crucial issue the country
faces in its medium and long-term development, recently, even in the short
term. We analyze the determinants of regional economic growth, mainly
industrial and agricultural productions, population, human capital, fertility,
migration, and regional government expenditures. Using panel data estimations
from 2004 to 2020 for 27 regions of Ukraine, our results show that the gaps
between regions in Ukraine have widened last two decades. Natural resource
distribution, agricultural and industrial productions, government spending, and
migration can explain the disparities. We show that regional government
spending is highly concentrated in Kyiv, and the potential of the other
regions, especially the Western ones, needs to be used sufficiently. Moreover,
despite its historical and economic opportunity, the East region performed
little development during the last two decades. The inefficient and
inconsistent regional policies played a crucial role in these disparities.

This article conducts a literature review on the topic of monetary policy in
developing countries and focuses on the effectiveness of monetary policy in
promoting economic growth and the relationship between monetary policy and
economic growth. The literature review finds that the activities of central
banks in developing countries are often overlooked by economic models, but
recent studies have shown that there are many factors that can affect the
effectiveness of monetary policy in these countries. These factors include the
profitability of central banks and monetary unions, the independence of central
banks in their operations, and lags, rigidities, and disequilibrium analysis.
The literature review also finds that studies on the topic have produced mixed
results, with some studies finding that monetary policy has a limited or
non-existent impact on economic growth and others finding that it plays a
crucial role. The article aims to provide a comprehensive understanding of the
current state of research in this field and to identify areas for future study.

This article provides a curated review of selected papers published in
prominent economics journals that use machine learning (ML) tools for research
and policy analysis. The review focuses on three key questions: (1) when ML is
used in economics, (2) what ML models are commonly preferred, and (3) how they
are used for economic applications. The review highlights that ML is
particularly used to process nontraditional and unstructured data, capture
strong nonlinearity, and improve prediction accuracy. Deep learning models are
suitable for nontraditional data, whereas ensemble learning models are
preferred for traditional datasets. While traditional econometric models may
suffice for analyzing low-complexity data, the increasing complexity of
economic data due to rapid digitalization and the growing literature suggests
that ML is becoming an essential addition to the econometrician's toolbox.

This chapter develops a feedback economic model that explains the rise of the
Sicilian mafia in the 19th century. Grounded in economic theory, the model
incorporates causal relationships between the mafia activities, predation, law
enforcement, and the profitability of local businesses. Using computational
experiments with the model, we explore how different factors and feedback
effects impact the mafia activity levels. The model explains important
historical observations such as the emergence of the mafia in wealthier regions
and its absence in the poorer districts despite the greater levels of banditry.

In this paper, we explore the economic, institutional, and
political/governmental factors in attracting Foreign Direct Investment (FDI)
inflows in the emerging twenty-four Asian economies. To examine the significant
determinants of FDI, the study uses panel data for a period of seventeen years
(2002-2018). The panel methodology enables us to deal with endogeneity and
other issues. Multiple regression models are done for empirical evidence. The
study focuses on a holistic approach and considers different variables under
three broad areas: economic, institutional, and political aspects. The
variables include Market Size, Trade Openness, Inflation, Natural Resource,
Lending Rate, Capital Formation as economic factors and Business Regulatory
Environment and Business Disclosure Index as institutional factors and
Political Stability, Government Effectiveness, and Rule of Law as political
factors. The empirical findings show most of the economic factors significantly
affect FDI inflows whereas Business Disclosure is the only important
institutional variable. Moreover, political stability has a significant
positive impact in attracting foreign capital flow though the impact of
government effectiveness is found insignificant. Overall, the economic factors
prevail strongly compared to institutional and political factors.

The origin of economic crises is a key problem for economics. We present a
model of long-run competitive markets to show that the multiplicity of
behaviors in an economic system, over a long time scale, emerge as statistical
regularities (perfectly competitive markets obey Bose-Einstein statistics and
purely monopolistic-competitive markets obey Boltzmann statistics) and that how
interaction among firms influences the evolutionary of competitive markets. It
has been widely accepted that perfect competition is most efficient. Our study
shows that the perfectly competitive system, as an extreme case of competitive
markets, is most efficient but not stable, and gives rise to economic crises as
society reaches full employment. In the economic crisis revealed by our model,
many firms condense (collapse) into the lowest supply level (zero supply,
namely bankruptcy status), in analogy to Bose-Einstein condensation. This
curious phenomenon arises because perfect competition (homogeneous
competitions) equals symmetric (indistinguishable) investment direction, a fact
abhorred by nature. Therefore, we urge the promotion of monopolistic
competition (heterogeneous competitions) rather than perfect competition. To
provide early warning of economic crises, we introduce a resolving index of
investment, which approaches zero in the run-up to an economic crisis. On the
other hand, our model discloses, as a profound conclusion, that the
technological level for a long-run social or economic system is proportional to
the freedom (disorder) of this system; in other words, technology equals the
entropy of system. As an application of this new concept, we give a possible
answer to the Needham question: "Why was it that despite the immense
achievements of traditional China it had been in Europe and not in China that
the scientific and industrial revolutions occurred?"

Metabolic fluxes in cells are governed by physical, physiological, and
economic principles. Here I assume an optimal allocation of enzyme resources
and postulate a general principle for metabolism: each enzyme must convert less
valuable into more valuable metabolites to justify its own cost. The "values",
called economic potentials, describe the individual contributions of
metabolites to cell fitness. Local value production implies that the cost of an
enzyme must be balanced by a benefit, given by the economic potential
difference the catalysed reaction multiplied by the flux. Flux profiles that
satisfy this principle - i.e. for which consistent potentials can be found -
are called economical. Economical fluxes must lead from lower to higher
economic potentials, so certain flux cycles are incompatible with any choice of
economic potentials and can be excluded. To obtain economical flux profiles,
non-beneficial local patterns, called futile motifs, can be systematically
removed from a given flux distribution. The principle of local value production
resembles thermodynamic principles and complements them in models. Here I
describe a modelling framework called Value Balance Analysis (VBA) that uses
the two principles and yields the same solution as enzyme cost minimisation (in
kinetic models) and flux cost minimisation (in FBA). Given an economical flux
distribution, kinetic models in enzyme-optimal states and with these fluxes can
be constructed systematically. VBA justifies the principle of minimal fluxes
and the exclusion of futile cycles, predicts enzymes that could be plausible
targets for regulation, provides criteria for the usage of enzymes and
pathways, and explains the choice between high-yield and low-yield flux modes.

Money is a technology for promoting economic prosperity. Over history money
has become increasingly abstract, it used to be hardware, gold coins and the
like, now it is mostly software, data structures located in banks. Here I
propose the logical conclusion of the abstraction of money: to use as money the
most general form of information - computer programs. The key advantage that
using programs for money (program-money) adds to the technology of money is
agency. Program-money is active and thereby can fully participate in economics
as economic agents. I describe the three basic technologies required to
implement program-money: computational languages/logics to unambiguously
describe the actions and interactions of program-money; computational
cryptography to ensure that only the correct actions and interactions are
performed; and a distributed computational environment in which the money can
execute. I demonstrate that most of the technology for program-money has
already been developed. The adoption of program-money transfers responsibility
from human economic agents to money itself and has great potential economic
advantages over the current passive form of money. For example in
microeconomics, adding agency to money will simplify the exchange of ownership,
ensure money is only used legally, automate the negotiation and forming of
contracts, etc. Similar advantages occur in macroeconomics, where for example
the control of the money supply could be transferred from central banks to
money. It is also possible to envisage money that is not owned by any external
human agent or corporation. One motivation for this is to force economic
systems to behave more rationally and/or more like a specific economic theory,
thereby increasing the success of economic forecasting.

We undertake an exploration of the economic income (Gross Domestic Product,
GDP) of Indian districts and cities based on scaling analyses of the dependence
of these quantities on associated population size. Scaling analysis provides a
straightforward method for the identification of network effects in
socioeconomic organization, which are the tell-tale of cities and urbanization.
For districts, a sub-state regional administrative division in India, we find
almost linear scaling of GDP with population, a result quite different from
urban functional units in other national contexts. Using deviations from
scaling, we explore the behavior of these regional units to find strong
distinct geographic patterns of economic behavior. We characterize these
patterns in detail and connect them to the literature on regional economic
development for a diverse subcontinental nation such as India. Given the
paucity of economic data for Urban Agglomerations in India, we use a set of
assumptions to create a new dataset of GDP based on districts, for large
cities. This reveals superlinear scaling of income with city size, as expected
from theory, while displaying similar underlying patterns of economic geography
observed for district economic performance. This analysis of the economic
performance of Indian cities is severely limited by the absence of
higher-fidelity, direct city level economic data. We discuss the need for
standardized and consistent estimates of the size and change in urban economies
in India, and point to a number of proxies that can be explored to develop such
indicators.

Several systematic studies have suggested that a large fraction of published
research is not reproducible. One probable reason for low reproducibility is
insufficient sample size, resulting in low power and low positive predictive
value. It has been suggested that insufficient sample-size choice is driven by
a combination of scientific competition and 'positive publication bias'. Here
we formalize this intuition in a simple model, in which scientists choose
economically rational sample sizes, balancing the cost of experimentation with
income from publication. Specifically, assuming that a scientist's income
derives only from 'positive' findings (positive publication bias) and that
individual samples cost a fixed amount, allows to leverage basic statistical
formulas into an economic optimality prediction. We find that if effects have
i) low base probability, ii) small effect size or iii) low grant income per
publication, then the rational (economically optimal) sample size is small.
Furthermore, for plausible distributions of these parameters we find a robust
emergence of a bimodal distribution of obtained statistical power and low
overall reproducibility rates, both matching empirical findings. Finally, we
explore conditional equivalence testing as a means to align economic incentives
with adequate sample sizes. Overall, the model describes a simple mechanism
explaining both the prevalence and the persistence of small sample sizes, and
is well suited for empirical validation. It proposes economic rationality, or
economic pressures, as a principal driver of irreproducibility and suggests
strategies to change this.

In economics literature, it is accepted that all people are rational and they
try to maximize their utilities as possible as they can. In addition, economic
theories are formed with the assumptions not suitable to real life. For
instance, indifference curves are drawn with the assumptions that there are two
goods, people are rational, more is preferred to less and so on. Hence, the
consumer behaviors are guessed according to this analysis. Nevertheless, these
are invalid in real life. And this inconsistencey are examined by behavioral
economics and neuroeconomics. Behavioral economics claims that people can
behave what they are not expected since people can be irrational, their
willpower is limited and altruistic behaviors can be seen and they can give
more value to what they own. As a result of these, consumer behaviors become
more different than that of economic theory. In addition to behavioral
economics, neuroeconomics also examines consumer behaviors more differently
than mainstream economic theory. It emphasizes the people using prefrontial
cortex of the brain are more rational than the people using hippocampus of the
brain. Therefore, people can make illogical choices compared to economic
theory. In these cases, levying taxes such as personal income tax or value
added tax can be ineffective or effective. In other words, the effect becomes
ambigious. Hence,the hypothesis that if government desires to levy personal
income tax or value added tax, it makes a detailed research in terms of
productivity of taxes forms the fundamental of this study.

Every nation prioritizes the inclusive economic growth and development of all
regions. However, we observe that economic activities are clustered in space,
which results in a disparity in per-capita income among different regions. A
complexity-based method was proposed by Hidalgo and Hausmann [PNAS 106,
10570-10575 (2009)] to explain the large gaps in per-capita income across
countries. Although there have been extensive studies on countries' economic
complexity using international export data, studies on economic complexity at
the regional level are relatively less studied. Here, we study the industrial
sector complexity of prefectures in Japan based on the basic information of
more than one million firms. We aggregate the data as a bipartite network of
prefectures and industrial sectors. We decompose the bipartite network as a
prefecture-prefecture network and sector-sector network, which reveals the
relationships among them. Similarities among the prefectures and among the
sectors are measured using a metric. From these similarity matrices, we cluster
the prefectures and sectors using the minimal spanning tree technique.The
computed economic complexity index from the structure of the bipartite network
shows a high correlation with macroeconomic indicators, such as per-capita
gross prefectural product and prefectural income per person. We argue that this
index reflects the present economic performance and hidden potential of the
prefectures for future growth.

During its history, the ultimate goal of economics has been to develop
similar frameworks for modeling economic behavior as invented in physics. This
has not been successful, however, and current state of the process is the
neoclassical framework that bases on static optimization. By using a static
framework, however, we cannot model and forecast the time paths of economic
quantities because for a growing firm or a firm going into bankruptcy, a
positive profit maximizing flow of production does not exist. Due to these
problems, we present a dynamic theory for the production of a profit-seeking
firm where the adjustment may be stable or unstable. This is important,
currently, because we should be able to forecast the possible future
bankruptcies of firms due to the Covid-19 pandemic. By using the model, we can
solve the time moment of bankruptcy of a firm as a function of several
parameters. The proposed model is mathematically identical with Newtonian model
of a particle moving in a resisting medium, and so the model explains the
reasons that stop the motion too. The frameworks for modeling dynamic events in
physics are thus applicable in economics, and we give reasons why physics is
more important for the development of economics than pure mathematics. (JEL
D21, O12)
  Keywords: Limitations of neoclassical framework, Dynamics of production,
Economic force, Connections between economics and physics.

Calcium (Ca) requirement increases tenfold upon parturition in dairy cows &
buffaloes and its deficiency leads to a condition called milk fever (MF).
Estimation of losses is necessary to understand the depth of the problem and
design preventive measures. How much is the economic loss due to MF? What will
be the efficiency gain if MF is prevented at the advent of a technology? We
answer these questions using survey data and official statistics employing
economic surplus model. MF incidence in sample buffaloes and cows was 19% and
28%, respectively. Total economic losses were calculated as a sum total of
losses from milk production, mortality of animals and treatment costs. Yearly
economic loss due to MF was estimated to be INR 1000 crores (US$ 137 million)
in Haryana. Value of milk lost had the highest share in total economic losses
(58%), followed by losses due to mortality (29%) and treatment costs (13%).
Despite lower MF incidence, losses were higher in buffaloes due to higher milk
prices and market value of animals. The efficiency gain accruing to producers
if MF is prevented, resulting from increased milk production at decreased costs
was estimated at INR 10990 crores (US$ 1.5 billion). As the potential gain if
prevented is around 10 times the economic losses, this study calls for the use
of preventive technology against MF.

The analogies between economics and classical mechanics can be extended from
constrained optimization to constrained dynamics by formalizing economic
(constraint) forces and economic power in analogy to physical (constraint)
forces in Lagrangian mechanics. In the differential-algebraic equation
framework of General Constrained Dynamics (GCD), households, firms, banks, and
the government employ forces to change economic variables according to their
desire and their power to assert their interest. These ex-ante forces are
completed by constraint forces from unanticipated system constraints to yield
the ex-post dynamics. The flexible out-of-equilibrium model can combine
Keynesian concepts such as the balance sheet approach and slow adaptation of
prices and quantities with bounded rationality (gradient climbing) and
interacting agents discussed in behavioral economics and agent-based models.
The framework integrates some elements of different schools of thought and
overcomes some restrictions inherent to optimization approaches, such as the
assumption of markets operating in or close to equilibrium. Depending on the
parameter choice for power relations and adaptation speeds, the model
nevertheless can converge to a neoclassical equilibrium, and reacts to an
austerity shock in a neoclassical or post-Keynesian way.

The existing theorization of development economics and transition economics
is probably inadequate and perhaps even flawed to accurately explain and
analyze a dual economic system such as that in China. China is a country in the
transition of dual structure and system. The reform of its economic system has
brought off a long period of transformation. The allocation of factors is
subjected to the dualistic regulation of planning or administration and market
due to the dualistic system, and thus the signal distortion will be a commonly
seen existence. From the perspective of balanced and safe growth, the
institutional distortions of population birth, population flow, land
transaction and housing supply, with the changing of export, may cause great
influences on the production demand, which includes the iterative contraction
of consumption, the increase of export competitive cost, the widening of
urban-rural income gap, the transferring of residents' income and the crowding
out of consumption. In view of the worldwide shift from a conservative model
with more income than expenditure to the debt-based model with more expenditure
than income and the need for loose monetary policy, we must explore a basic
model that includes variables of debt and land assets that affecting money
supply and price changes, especially in China, where the current debt ratio is
high and is likely to rise continuously. Based on such a logical framework of
dualistic system economics and its analysis method, a preliminary calculation
system is formed through the establishment of models.

Growth in the global human population this century will have momentous
consequences for societies and the environment. Population growth has come with
higher aggregate human welfare, but also climate change and biodiversity loss.
Based on the well-established empirical association and plausible causal
relationship between economic and population growth, we devised a novel method
for forecasting population based on Gross Domestic Product (GDP) per capita.
Although not mechanistically causal, our model is intuitive, transparent,
replicable, and grounded on historical data. Our central finding is that a
richer world is likely to be associated with a lower population, an effect
especially pronounced in rapidly developing countries. In our baseline
scenario, where GDP per capita follows a business-as-usual trajectory, global
population is projected to reach 9.2 billion in 2050 and peak in 2062. With 50%
higher annual economic growth, population peaks even earlier, in 2056, and
declines to below 8 billion by the end of the century. Without any economic
growth after 2020, however, the global population will grow to 9.9 billion in
2050 continue rising thereafter. Economic growth has the largest effect on
low-income countries. The gap between the highest and lowest GDP scenarios
reaches almost 4 billion by 2100. Education and family planning are important
determinants of population growth, but economic growth is also likely to be a
driver of slowing population growth by changing incentives for childbearing.
Since economic growth could slow population growth, it will offset
environmental impacts stemming from higher per-capita consumption of food,
water, and energy, and work in tandem with technological innovation.

Background The COVID-19 pandemic has increased mental distress globally. The
proportion of people reporting anxiety is 26%, and depression is 34% points.
Disentangling associational and causal contributions of behavior, COVID-19
cases, and economic distress on mental distress will dictate different
mitigation strategies to reduce long-term pandemic-related mental distress.
Methods We use the Household Pulse Survey (HPS) April 2020 to February 2021
data to examine mental distress among U.S. citizens attributable to COVID-19.
We combined HPS survey data with publicly available state-level weekly:
COVID-19 case and death data from the Centers for Disease Control, public
policies, and Apple and Google mobility data. Finally, we constructed economic
and mental distress measures to estimate structural models with lag dependent
variables to tease out public health policies' associational and causal path
coefficients on economic and mental distress. Findings From April 2020 to
February 2021, we found that anxiety and depression had steadily climbed in the
U.S. By design, mobility restrictions primarily affected public health policies
where businesses and restaurants absorbed the biggest hit. Period t-1 COVID-19
cases increased job loss by 4.1% and economic distress by 6.3% points in the
same period. Job-loss and housing insecurity in t-1 increased period t mental
distress by 29.1% and 32.7%, respectively. However, t-1 food insecurity
decreased mental distress by 4.9% in time t. The pandemic-related potential
causal path coefficient of period t-1 economic distress on period t depression
is 57.8%, and anxiety is 55.9%. Thus, we show that period t-1 COVID-19 case
information, behavior, and economic distress may be causally associated with
pandemic related period t mental distress.

Motivated by the current fears of a potentially stagflationary global
economic environment, this paper uses new and recently introduced mathematical
techniques to study multivariate time series pertaining to country inflation
(CPI), economic growth (GDP) and equity index behaviours. We begin by assessing
the temporal evolution among various economic phenomena, and complement this
analysis with `economic driver analysis,' where we decouple country economic
trajectories and determine what is most important in their association. Next,
we study the temporal self-similarity of global inflation, growth and equity
index returns to identify the most anomalous historic periods, and windows in
the past that are most similar to current market dynamics. We then introduce a
new algorithm to construct economic state classifications and compute an
economic state integral, where countries are determined to belong in one of
four candidate states based on their inflation and growth behaviours. Finally,
we implement a decade-by-decade portfolio optimisation to determine which
equity indices and portfolio assets have been most beneficial in maximising
portfolio risk-adjusted returns in various market conditions. This could be of
great interest to those looking for asset allocation guidance in the current
period of high economic uncertainty.

Earlier meta-analyses of the economic impact of climate change are updated
with more data, with three new results: (1) The central estimate of the
economic impact of global warming is always negative. (2) The confidence
interval about the estimates is much wider. (3) Elicitation methods are most
pessimistic, econometric studies most optimistic. Two previous results remain:
(4) The uncertainty about the impact is skewed towards negative surprises. (5)
Poorer countries are much more vulnerable than richer ones. A meta-analysis of
the impact of weather shocks reveals that studies, which relate economic growth
to temperature levels, cannot agree on the sign of the impact whereas studies,
which make economic growth a function of temperature change do agree on the
sign but differ an order of magnitude in effect size. The former studies posit
that climate change has a permanent effect on economic growth, the latter that
the effect is transient. The impact on economic growth implied by studies of
the impact of climate change is close to the growth impact estimated as a
function of weather shocks. The social cost of carbon shows a similar pattern
to the total impact estimates, but with more emphasis on the impacts of
moderate warming in the near and medium term.

The need for timely data analysis for economic decisions has prompted most
economists and policy makers to search for non-traditional supplementary
sources of data. In that context, text data is being explored to enrich
traditional data sources because it is easy to collect and highly abundant. Our
work focuses on studying the potential of textual data, in particular news
pieces, for measuring economic policy uncertainty (EPU). Economic policy
uncertainty is defined as the public's inability to predict the outcomes of
their decisions under new policies and future economic fundamentals.
Quantifying EPU is of great importance to policy makers, economists, and
investors since it influences their expectations about the future economic
fundamentals with an impact on their policy, investment and saving decisions.
Most of the previous work using news articles for measuring EPU are either
manual or based on a simple keyword search. Our work proposes a machine
learning based solution involving weak supervision to classify news articles
with regards to economic policy uncertainty. Weak supervision is shown to be an
efficient machine learning paradigm for applying machine learning models in low
resource settings with no or scarce training sets, leveraging domain knowledge
and heuristics. We further generated a weak supervision based EPU index that we
used to conduct extensive econometric analysis along with the Irish
macroeconomic indicators to validate whether our generated index foreshadows
weaker macroeconomic performance

The potential tradeoff between health outcomes and economic impact has been a
major challenge in the policy making process during the COVID-19 pandemic.
Epidemic-economic models designed to address this issue are either too
aggregate to consider heterogeneous outcomes across socio-economic groups, or,
when sufficiently fine-grained, not well grounded by empirical data. To fill
this gap, we introduce a data-driven, granular, agent-based model that
simulates epidemic and economic outcomes across industries, occupations, and
income levels with geographic realism. The key mechanism coupling the epidemic
and economic modules is the reduction in consumption demand due to fear of
infection. We calibrate the model to the first wave of COVID-19 in the New York
metropolitan area, showing that it reproduces key epidemic and economic
statistics, and then examine counterfactual scenarios. We find that: (a) both
high fear of infection and strict restrictions similarly harm the economy but
reduce infections; (b) low-income workers bear the brunt of both the economic
and epidemic harm; (c) closing non-customer-facing industries such as
manufacturing and construction only marginally reduces the death toll while
considerably increasing unemployment; and (d) delaying the start of protective
measures does little to help the economy and worsens epidemic outcomes in all
scenarios. We anticipate that our model will help designing effective and
equitable non-pharmaceutical interventions that minimize disruptions in the
face of a novel pandemic.

The last decades have seen a resurgence of armed conflict around the world,
renewing the need for durable peace agreements. In this paper, I evaluate the
economic effects of the peace agreement between the Colombian government and
the largest guerrilla group in the country, the FARC, putting an end to one of
the longest and most violent armed conflicts in recent history. Using a
difference-in-difference strategy comparing municipalities that historically
had FARC presence and those with presence of a similar, smaller guerrilla
group, the ELN, before and after the start of a unilateral ceasefire by the
FARC, I establish three sets of results. First, violence indicators
significantly and sizably decreased in historically FARC municipalities.
Second, despite this large reduction in violence, I find precisely-estimated
null effects across a variety of economic indicators, suggesting no effect of
the peace agreement on economic activity. Furthermore, I use a sharp
discontinuity in eligibility to the government's flagship business and job
creation program for conflict-affected areas to evaluate the policy's impact,
also finding precisely-estimated null effects on the same economic indicators.
Third, I present evidence that suggests the reason why historically FARC
municipalities could not reap the economic benefits from the reduction in
violence is a lack of state capacity, caused both by their low initial levels
of state capacity and the lack of state entry post-ceasefire. These results
indicate that peace agreements require complementary investments in state
capacity to yield an economic dividend.

Research on productive structures has shown that economic complexity
conditions economic growth. However, little is known about which type of
complexity, e.g., export or industrial complexity, matters more for regional
economic growth in a large emerging country like Brazil. Brazil exports natural
resources and agricultural goods, but a large share of the employment derives
from services, non-tradables, and within-country manufacturing trade. Here, we
use a large dataset on Brazil's formal labor market, including approximately
100 million workers and 581 industries, to reveal the patterns of export
complexity, industrial complexity, and economic growth of 558 micro-regions
between 2003 and 2019. Our results show that export complexity is more evenly
spread than industrial complexity. Only a few -- mainly developed urban places
-- have comparative advantages in sophisticated services. Regressions show that
a region's industrial complexity is a significant predictor for 3-year growth
prospects, but export complexity is not. Moreover, economic complexity in
neighboring regions is significantly associated with economic growth. The
results show export complexity does not appropriately depict Brazil's knowledge
base and growth opportunities. Instead, promoting the sophistication of the
heterogeneous regional industrial structures and development spillovers is a
key to growth.

This study investigates the long-term economic impact of sea-level rise (SLR)
on coastal regions in Europe, focusing on Gross Domestic Product (GDP). Using a
novel dataset covering regional SLR and economic growth from 1900 to 2020, we
quantify the relationships between SLR and regional GDP per capita across 79
coastal EU & UK regions. Our results reveal that the current SLR has already
negatively influenced GDP of coastal regions, leading to a cumulative 4.7% loss
at 39 cm of SLR. Over the 120 year period studied, the actualised impact of SLR
on the annual growth rate is between -0.02% and 0.04%. Extrapolating these
findings to future climate and socio-economic scenarios, we show that in the
absence of additional adaptation measures, GDP losses by 2100 could range
between -6.3% and -20.8% under the most extreme SLR scenario (SSP5-RCP8.5
High-end Ice, or -4.0% to -14.1% in SSP5-RCP8.5 High Ice). This statistical
analysis utilising a century-long dataset, provides an empirical foundation for
designing region-specific climate adaptation strategies to mitigate economic
damages caused by SLR. Our evidence supports the argument for strategically
relocating assets and establishing coastal setback zones when it is
economically preferable and socially agreeable, given that protection
investments have an economic impact.

Small-world networks (SWN) are found to be closer to the real social systems
than both regular and random lattices. Then, a model for the evolution of
economic systems is generalized to SWN. The Sznajd model for the two-state
opinion formation problem is applied to SWN. Then a simple definition of
leaders is included. These models explain some socio-economic aspects.

The methods of statistical physics of open systems are used for describing
the time dependence of economic characteristics (income, profit, cost, supply,
currency etc.) and their correlations with each other. Nonlinear equations
(analogies of known reaction-diffusion, kinetic, Langevin equation) describing
appearance of bifurcations, self-sustained oscillational processes,
self-organizations in economic phenomena are offered.

The author solves two problems: formation of object of econophysics, creation
of the general theory of financial-economic monitoring. In the first problem he
studied two fundamental tasks: a choice of conceptual model and creation of
axiomatic base. It is accepted, that the conceptual model of econophysics is a
concrete definition of entropy conceptual model. Financial and economic
monitoring is considered as monitoring of flows on entropy manifold of phase
space - on a Diffusion field.

The paper present one of attempts to apply the thermodynamics laws to
economics. Introducing common thermodynamic parameters and considering world
economics as a one macrosystem, authors demonstrate the possible consequences
of entropy increasing due to irreversible economics activities. "Entropy"
advices to leaders of different business units are presented.

This paper proposes the Potluck Problem as a model for the behavior of
independent producers and consumers under standard economic assumptions, as a
problem of resource allocation in a multi-agent system in which there is no
explicit communication among the agents.

The paper discusses various practical consequences of treating economics and
finance as an inherently dynamic and chaotic system. On the theoretical side
this looks at the general applicability of the market-making pricing approach
to economics in general. The paper also discuses the consequences of the
endogenous creation of liquidity and the role of liquidity as a state variable.
On the practical side, proposals are made for reducing chaotic behaviour in
both housing markets and stock markets.

Article about objective laws of formation of social and economic institutes
in system of electronic commerce. Rapid development of Internet technologies
became the reason of deep institutional transformation of economic relations.
The author analyzes value transaction costs as motive power of formation of new
economic institutes in network economy.

This contribution discusses in what respect Econophysics may be able to
contribute to the rebuilding of economics theory. It focuses on aggregation,
individual vs collective learning and functional wisdom of the crowds.

Clearly, socio-economic freedom requires some extent of transparency
regarding the implications of choices. In this paper, we review some
established mechanisms for achieving such transparency, without any claim to
completeness, and briefly discuss potential future directions. Our
investigation is structured by four "challenges" under which we subsume the
various requirements on, and approaches to, socio-economic transparency
mechanisms. One main focus is on the inference, i.e., statistical, aspect of
such mechanisms.

How much knowledge is there in an economy? In recent years, data on the mix
of products that countries export has been used to construct measures of
economic complexity that estimate the knowledge available in an economy and
predict future economic growth. Here we introduce a new and simpler metric of
economic complexity (ECI+) that measures the total exports of an economy
corrected by how difficult it is to export each product. We use data from 1973
to 2013 to compare the ability of ECI+, the Economic Complexity Index (ECI),
and Fitness complexity, to predict future economic growth using 5, 10, and
20-year panels in a pooled OLS, a random effects model, and a fixed effects
model. We find that ECI+ outperforms ECI and Fitness in its ability to predict
economic growth and in the consistency of its estimators across most
econometric specifications. On average, one standard deviation increase in ECI+
is associated with an increase in annualized growth of about 4% to 5%. We then
combine ECI+ with measures of physical capital, human capital, and
institutions, to find a robust model of economic growth. The ability of ECI+ to
predict growth, and the value of its coefficient, is robust to these controls.
Also, we find that human capital, political stability, and control of
corruption; are positively associated with future economic growth, and that
income is negatively associated with growth, in agreement with the traditional
growth literature. Finally, we use ECI+ to generate economic growth predictions
for the next 20 years and compare these predictions with the ones obtained
using ECI and Fitness. These findings improve the methods available to estimate
the knowledge intensity of economies and predict future economic growth.

Despite the importance of CAP-related agricultural market regulation
mechanisms within Europe, the agricultural sectors in European countries retain
a degree of sensitivity to macroeconomic activity and policies. This reality
now raises the question of the effects to be expected from the implementation
of the single monetary policy on these agricultural sectors within the Monetary
Union.

We introduce a general Hamiltonian framework that appears to be a natural
setting for the derivation of various production functions in economic growth
theory, starting with the celebrated Cobb-Douglas function. Employing our
method, we investigate some existing models and propose a new one as special
cases of the general $n$-dimensional Lotka-Volterra system of eco-dynamics.

We provide an exact analytical solution of the Nash equilibrium for the $k$th
price auction by using inverse of distribution functions. As applications, we
identify the unique symmetric equilibrium where the valuations have polynomial
distribution, fat tail distribution and exponential distributions.

In the context of the Beckerian theory of marriage, when men and women match
on a single-dimensional index that is the weighted sum of their respective
multivariate attributes, many papers in the literature have used linear
canonical correlation, and related techniques, in order to estimate these
weights. We argue that this estimation technique is inconsistent and suggest
some solutions.

We propose and develop an algebraic approach to revealed preference. Our
approach dispenses with non algebraic structure, such as topological
assumptions. We provide algebraic axioms of revealed preference that subsume
previous, classical revealed preference axioms, as well as generate new axioms
for behavioral theories, and show that a data set is rationalizable if and only
if it is consistent with an algebraic axiom.

The article considers the practice of applying the cluster approach in Russia
as a tool for overcoming economic inequality between Russian regions. The
authors of the study analyze the legal framework of cluster policy in Russia,
noting that its successful implementation requires a little more time than
originally planned. Special attention is paid to the experience of
benchmarking.

The article analyzes the essence of the phenomenon of corruption, highlights
its main varieties and characteristics. The authors of the study apply
historical analysis, emphasizing the long-term nature of corruption and its
historical roots. The paper uses legal analysis to characterize the legal
interpretation of corruption as an economic crime.

We study the impacts of incomplete information on centralized one-to-one
matching markets. We focus on the commonly used Deferred Acceptance mechanism
(Gale and Shapley, 1962). We show that many complete-information results are
fragile to a small infusion of uncertainty about others' preferences.

This paper investigates stochastic continuous time contests with a twist: the
designer requires that contest participants incur some cost to submit their
entries. When the designer wishes to maximize the (expected) performance of the
top performer, a strictly positive submission fee is optimal. When the designer
wishes to maximize total (expected) performance, either the highest submission
fee or the lowest submission fee is optimal.

Network economics is the study of a rich class of equilibrium problems that
occur in the real world, from traffic management to supply chains and two-sided
online marketplaces. In this paper we explore causal inference in network
economics, building on the mathematical framework of variational inequalities,
which is a generalization of classical optimization. Our framework can be
viewed as a synthesis of the well-known variational inequality formalism with
the broad principles of causal inference

I construct the professor-student network for laureates of and candidates for
the Nobel Prize in Economics. I study the effect of proximity to previous
Nobelists on winning the Nobel Prize. Conditional on being Nobel-worthy,
students and grandstudents of Nobel laureates are significantly less likely to
win. Professors and fellow students of Nobel Prize winners, however, are
significantly more likely to win.

Macroeconomics essentially discusses macroeconomic phenomena from the
perspectives of various schools of economic thought, each of which takes
different views on how macroeconomic agents make decisions and how the
corresponding markets operate. Therefore, developing a clear, comprehensive
understanding of how and in what ways these schools of economic thought differ
is a key and a prerequisite for economics students to prosper academically and
professionally in the discipline. This becomes even more crucial as economics
students pursue their studies toward higher levels of education and graduate
school, during which students are expected to attain higher levels of Bloom's
taxonomy, including analysis, synthesis, evaluation, and creation. Teaching the
distinctions and similarities of the two major schools of economic thought has
never been an easy task to undertake in the classroom. Although the reason for
such a hardship can be multi-fold, one reason has undoubtedly been students'
lack of a holistic view on how the two mainstream economic schools of thought
differ. There is strong evidence that students make smoother transition to
higher levels of education after building up such groundwork, on which they can
build further later on (e.g. Didia and Hasnat, 1998; Marcal and Roberts, 2001;
Islam, et al., 2008; Green, et al., 2009; White, 2016). The paper starts with a
visual spectrum of various schools of economic thought, and then narrows down
the scope to the classical and Keynesian schools, i.e. the backbone of modern
macroeconomics. Afterwards, a holistic table contrasts the two schools in terms
of 50 aspects. Not only does this table help economics students enhance their
comprehension, retention, and critical-thinking capability, it also benefits
macroeconomic instructors to ...

I provide necessary and sufficient conditions for an agent's preferences to
be represented by a unique ergodic transformation. Put differently, if an agent
seeks to maximize the time average growth of their wealth, what axioms must
their preferences obey? By answering this, I provide economic theorists a clear
view of where "Ergodicity Economics" deviates from established models.

This paper surveys the recent advances in machine learning method for
economic forecasting. The survey covers the following topics: nowcasting,
textual data, panel and tensor data, high-dimensional Granger causality tests,
time series cross-validation, classification with economic losses.

"Economists miss the boat when they act as if Arrow and Debreu's general
equilibrium model accurately describes markets in the real world of constant
change. In contrast, the classical view on the market mechanism offers a
helpful foundation on which to add modern insights about how markets create and
coordinate information."

If Economics is understood as the study of the interactions among intentional
agents, being rationality the main source of intentional behavior, the
mathematical tools that it requires must be extended to capture systemic
effects. Here we choose an alternative toolbox based on Category Theory. We
examine potential {\em level-agnostic} formalisms, presenting three categories,
$\mathcal{PR}$, $\mathcal{G}$ and an encompassing one, $\mathcal{PR-G}$. The
latter allows for representing dynamic rearrangements of the interactions among
different agents.

In this paper, we study economic dynamics in a standard overlapping
generations model without production. In particular, using numerical methods,
we obtain a necessary and sufficient condition for the existence of a
topological chaos. This is a new application of a recent result characterising
the existence of a topological chaos for a unimodal interval map by Deng, Khan,
Mitra (2022).

This paper shows the universal representations of symmetric functions with
multidimensional variable-size variables, which help assessing the
justification of approximation methods aggregating the information of each
variable by moments. It then discusses how the results give insights into
economic applications, including two-step policy function estimation,
moment-based Markov equilibrium, and aggregative games.

This book is about dynamic programming and its applications in economics,
finance, and adjacent fields. It brings together recent innovations in the
theory of dynamic programming and provides applications and code that can help
readers approach the research frontier. The book is aimed at graduate students
and researchers, although most chapters are accessible to undergraduate
students with solid quantitative backgrounds.

Two fundamental issues are incorporated in the present monograph: the issue
related to the quantification of the social costs and the issue, related to the
defining of the circular economy concept as a theoretical model. The analysis
is based on the methodology of the new institutional economics, which fact
distinguishes it from the many other circular economy analysis based on the
neo-classical methodological apparatus.

In this paper, we examine matching models with imperfectly transferable
utility (ITU). We provide motivating examples, discuss the theoretical
foundations of ITU matching models and present methods for estimating them. We
also explore connected topics and provide an overview of the related
literature. This paper has been submitted as a draft chapter for the Handbook
of the Economics of Matching, edited by Che, Chiappori and Salani\'e.

The application of principles of thermodynamics and statistical mechanics to
economic systems is considered in a broad historical perspective, extending
from prehistoric times to the present day. The hypothesis of maximum entropy
production (MEP), which has been used to model complex physical systems such as
fluid turbulence and the climate of the Earth and other planets, may be applied
to human economic activity, subject to constraints such as the availability of
suitable technology, and the nature of political control. Applied to the
current abundance of available energy from fossil fuel reserves, MEP is shown
to have significant policy implications.

This paper proposes a simple procedure to decide whether the
empirically-observed adjacency or weights matrix, which characterizes the graph
underlying a socio-economic network, is sufficiently symmetric (respectively,
asymmetric) to justify an undirected (respectively, directed) network analysis.
We introduce a new index that satisfies two main properties. First, it can be
applied to both binary or weighted graphs. Second, once suitably standardized,
it distributes as a standard normal over all possible adjacency/weights
matrices. To test the index in practice, we present an application that employs
a set of well-known empirically-observed social and economic networks.

I want to write about what I know and remember about the activities of Leonid
Vital'evich Kantorovich, an outstanding scientist of the 20th century; about
his dramatic struggle for recognition of his mathematical economic theories;
about the initial stage of the history of linear programming; about beautuful
Kantorovich metric, about the creation of a new area of mathematical activity
related to economic applications, which is called sometimes operation research,
sometimes mathematical economics, sometimes linear and convex programming, or
economic cybernetics, etc.; about its place in the modern mathematical
landscape; and, finally, about several personal impressions of this
distinguished scientist. The notes in no way pretend to exhaust these topics.

Most of the econometric and econophysics models have been borrowed from the
statistical physics, and as a cosequence, a new interdisciplinary science
called econophysics has emerged. In this paper we planned to extend the analogy
between different economic processes or phenomena and processes and phenomena
from different fields of physics, other than statistical physics. On the basis
of the economic development process and amplification phenomenon analogy, a new
econophysics model, named economic amplifier, on the electronic amplification
principle from applied physics was proposed und largely analyzed.

A dangerously brief history of the developments of the main ideas in
economics, as observed by a physicist, is given. This was published in
'Econophysics of Stock and Other Markets', Eds. A. Chatterjee, B. K.
Chakrabarti, New Economic Windows Series, Springer, Milan, 2006, pp~219-224.

We formulate thermodynamics of economic systems in terms of an arbitrary
probability distribution for a conserved economic quantity. As in statistical
physics, thermodynamic macroeconomic variables emerge as the mean value of
microeconomic variables and their determination is reduced to the computation
of the partition function, starting from an arbitrary function. Explicit
hypothetical examples are given which include linear and nonlinear economic
systems, as well as multiplicative systems such as those dominated by a Pareto
law distribution. We propose to use the formalism of phase transitions to study
severe changes of macroeconomic variables.

Intertemporal choice has drawn attention in behavioral economics,
econophysics, and neuroeconomics. Recent studies in mainstream economics have
mainly focused on inconsistency in intertemporal choice (dynamic
inconsistency); while impulsivity/impatience in intertemporal choice has been
extensively studied in behavioral economics of addiction. However, recent
advances in neuroeconomic and econophysical studies on intertemporal choice
have made it possible to study both impulsivity and inconsistency in
intertemporal choice within a unified framework. In this paper I propose the
new frameworks for investigations into neuroeconomics of intertemporal choice.
The importance of studying neurochemical and neuroendocrinological modulations
of intertemporal choice and time-perception (e.g. serotonin, dopamine,
cortisol, testosterone, and epinephrine) is emphasized.

Despite spatial econometrics is now considered a consolidated discipline,
only in recent years we have experienced an increasing attention to the
possibility of applying it to the field of discrete choices (e.g. Smirnov, 2010
for a recent review) and limited dependent variable models. In particular, only
a small number of papers introduced the above-mentioned models in Health
Economics. The main purpose of the present paper is to review the different
methodological solutions in spatial discrete choice models as they appeared in
several applied fields by placing an emphasis on the health economics
applications.

This paper argues that the fundamental principle of contemporary financial
economics is balanced reciprocity, not the principle of utility maximisation
that is important in economics more generally. The argument is developed by
analysing the mathematical Fundamental Theory of Asset Pricing with reference
to the emergence of mathematical probability in the seventeenth century in the
context of the ethical assessment of commercial contracts. This analysis is
undertaken within a framework of Pragmatic philosophy and Virtue Ethics. The
purpose of the paper is to mitigate future financial crises by reorienting
financial economics to emphasise the objectives of market stability and social
cohesion rather than individual utility maximisation.

This essay discusses the advantages of a probabilistic agent-based approach
to questions in theoretical economics, from the nature of economic agents, to
the nature of the equilibria supported by their interactions. One idea we
propose is that "agents" are meta-individual, hierarchically structured
objects, that include as irreducible components groupings of different
dimensions. We also explore the effects of non-ergodicity, by constructing a
simple stochastic model for the contingent nature of economic interactions.

A general information equilibrium model in the case of ideal information
transfer is defined and then used to derive the relationship between supply
(information destination) and demand (information source) with the price as the
detector of information exchange between demand and supply. We recover the
properties of the traditional economic supply-demand diagram. Information
equilibrium is then applied to macroeconomic problems, recovering some common
macroeconomic models in particular limits like the AD-AS model, IS-LM model (in
a low inflation limit), the quantity theory of money (in a high inflation
limit) and the Solow-Swan growth model. Information equilibrium results in
empirically accurate models of inflation and interest rates, and can be used to
motivate a 'statistical economics', analogous to statistical mechanics for
thermodynamics.

One of the fundamental postulates of the Unified Growth Theory is the claimed
existence of three distinctly different regimes of economic growth governed by
three distinctly different mechanisms of growth. However, Galor also proposed
that the timing of these regimes is different for developed countries and for
less-developed countries. Africa is the perfect example of economic growth in
less-developed countries. The data used by Galor, but never properly
investigated, are now analysed. They turn out to be in dramatic contradiction
of this theory.

We are in the middle of a complex debate as to whether Economics is really a
proper natural science. The 'Discussion & Debate' issue of this Euro. Phys. J.
Special Topic volume is: 'Can economics be a Physical Science?' I discuss some
aspects here.

The purpose of this paper is to provide a confession of sorts from an
economist to political science and philosophy. A confession of the weaknesses
of the political position of the economist. It is intended as a guide for
political scientists and philosophers to the ostensible policy criteria of
economics, and an illustration of an argument that demonstrates
logico-mathematically, therefore incontrovertibly, that any policy statement by
an economist contains, or is, a political statement. It develops an inescapable
compulsion that the absolute primacy and priority of political theory and
philosophy in the development of policy criteria must be recognised. Economic
policy cannot be divorced from politics as a matter of mathematical fact, and
rather, as Amartya Sen has done, it ought embrace political theory and
philosophy.

This paper presents an approach to assess the economics of customer-sited
energy storage systems (ESSs) which are owned and operated by a customer. The
ESSs can participate in frequency regulation and spinning reserve markets, and
are used to help the customer consume available renewable energy and reduce
electricity bill. A rolling-horizon approach is developed to optimize the
service schedule, and the resulting costs and revenues are used to assess
economics of the ESSs. The economic assessment approach is illustrated with
case studies, from which we obtain some new observations on profitability of
the customer- sited multi-use ESSs.

A very timely issue for economic agent-based models (ABMs) is their empirical
estimation. This paper describes a line of research that could resolve the
issue by using machine learning techniques, using multi-layer artificial neural
networks (ANNs), or so called Deep Nets. The seminal contribution by Hinton et
al. (2006) introduced a fast and efficient training algorithm called Deep
Learning, and there have been major breakthroughs in machine learning ever
since. Economics has not yet benefited from these developments, and therefore
we believe that now is the right time to apply Deep Learning and multi-layered
neural networks to agent-based models in economics.

This paper proposes a Lyapunov-based economic MPC scheme for nonlinear sytems
with non-monotonic Lyapunov functions. Relaxed Lyapunov-based constraints are
used in the MPC formulation to improve the economic performance. These
constraints will enforce a Lyapunov decrease after every few steps. Recursive
feasibility and asymptotical convergence to the steady state can be achieved
using Lyapunov-like stability analysis. The proposed economic MPC can be
applied to minimize energy consumption in HVAC control of commercial buildings.
The Lyapunov-based constraints in the online MPC problem enable the tracking of
the desired set-point temperature. The performance is demonstrated by a virtual
building composed of two adjacent zones.

Recently a measure for Economic Complexity named ECI+ has been proposed by
Albeaik et al. We like the ECI+ algorithm because it is mathematically
identical to the Fitness algorithm, the measure for Economic Complexity we
introduced in 2012. We demonstrate that the mathematical structure of ECI+ is
strictly equivalent to that of Fitness (up to normalization and rescaling). We
then show how the claims of Albeaik et al. about the ability of Fitness to
describe the Economic Complexity of a country are incorrect. Finally, we
hypothesize how the wrong results reported by these authors could have been
obtained by not iterating the algorithm.

In order to address the economical dispatch problem in islanded microgrid,
this letter proposes an optimal criterion and two decentralized
economical-sharing schemes. The criterion is to judge whether global optimal
economical-sharing can be realized via a decentralized manner. On the one hand,
if the system cost functions meet this criterion, the corresponding
decentralized droop method is proposed to achieve the global optimal dispatch.
Otherwise, if the system does not meet this criterion, a modified method to
achieve suboptimal dispatch is presented. The advantages of these methods are
convenient,effective and communication-less.

The business cycles are generated by the oscillating macro-/micro-/nano-
economic output variables in the economy of the scale and the scope in the
amplitude/frequency/phase/time domains in the economics. The accurate forward
looking assumptions on the business cycles oscillation dynamics can optimize
the financial capital investing and/or borrowing by the economic agents in the
capital markets. The book's main objective is to study the business cycles in
the economy of the scale and the scope, formulating the Ledenyov unified
business cycles theory in the Ledenyov classic and quantum econodynamics.

A theoretical self-sustainable economic model is established based on the
fundamental factors of production, consumption, reservation and reinvestment,
where currency is set as a unconditional credit symbol serving as transaction
equivalent and stock means. Principle properties of currency are explored in
this ideal economic system. Physical analysis reveals some facts that were not
addressed by traditional monetary theory, and several basic principles of ideal
currency are concluded: 1. The saving-replacement is a more primary function of
currency than the transaction equivalents; 2. The ideal efficiency of currency
corresponds to the least practical value; 3. The contradiction between constant
face value of currency and depreciable goods leads to intrinsic inflation.

Foundations of equilibrium thermodynamics are the equation of state (EoS) and
four postulated laws of thermodynamics. We use equilibrium thermodynamics
paradigms in constructing the EoS for microeconomics system that is a market.
This speculation is hoped to be first step towards whole pictures of
thermodynamical paradigm of economics.

Bangladesh is the 2nd largest growing country in the world in 2016 with 7.1%
GDP growth. This study undertakes an econometric analysis to examine the
relationship between population growth and economic development. This result
indicates population growth adversely related to per capita GDP growth, which
means rapid population growth is a real problem for the development of
Bangladesh.

The main purpose of the study is to develop the model for transaction costs
measurement in the Collective Waste Recovery Systems. The methodology of New
Institutional Economics is used in the research. The impact of the study is
related both to the enlargement of the limits of the theory about the
interaction between transaction costs and social costs and to the
identification of institutional failures of the European concept for circular
economy. A new model for social costs measurement is developed. Keywords:
circular economy, transaction costs, extended producer responsibility JEL: A13,
C51, D23, L22, Q53

In this essay I discuss potential outcome and graphical approaches to
causality, and their relevance for empirical work in economics. I review some
of the work on directed acyclic graphs, including the recent "The Book of Why,"
by Pearl and MacKenzie. I also discuss the potential outcome framework
developed by Rubin and coauthors, building on work by Neyman. I then discuss
the relative merits of these approaches for empirical work in economics,
focusing on the questions each answer well, and why much of the the work in
economics is closer in spirit to the potential outcome framework.

This paper brings together divergent approaches to time inconsistency from
macroeconomic policy and behavioural economics. Behavioural discount functions
from behavioural microeconomics are embedded into a game-theoretic analysis of
temptation versus enforcement to construct an encompassing model, nesting
combinations of time consistent and time inconsistent preferences. The analysis
presented in this paper shows that, with hyperbolic/quasihyperbolic
discounting, the enforceable range of inflation targets is narrowed. This
suggests limits to the effectiveness of monetary targets, under certain
conditions. The paper concludes with a discussion of monetary policy
implications, explored specifically in the light of current macroeconomic
policy debates.

Models of social learning feature either binary signals or abstract signal
structures often deprived of micro-foundations. Both models are limited when
analyzing interim results or performing empirical analysis. We present a method
of generating signal structures which are richer than the binary model, yet are
tractable enough to perform simulations and empirical analysis. We demonstrate
the method's usability by revisiting two classical papers: (1) we discuss the
economic significance of unbounded signals Smith and Sorensen (2000); (2) we
use experimental data from Anderson and Holt (1997) to perform econometric
analysis. Additionally, we provide a necessary and sufficient condition for the
occurrence of action cascades.

We develop a simple theoretic game a model to analyze the relationship
between electoral sys tems and governments' choice in trade policies. We show
that existence of international pressure or foreign lobby changes a
government's final decision on trade policy, and trade policy in countries with
proportional electoral system is more protectionist than in countries with
majoritarian electoral system. Moreover, lobbies pay more to affect the trade
policy outcomes in countries with proportional representation systems.

We propose a highly schematic economic model in which, in some cases, wage
inequalities lead to higher overall social welfare. This is due to the fact
that high earners can consume low productivity, non essential products, which
allows everybody to remain employed even when the productivity of essential
goods is high and producing them does not require everybody to work. We derive
a relation between heterogeneities in technologies and the minimum Gini
coefficient required to maximize global welfare. Stronger inequalities appear
to be economically unjustified. Our model may shed light on the role of
non-essential goods in the economy, a topical issue when thinking about the
post-Covid-19 world.

This paper considers a simple model where a social planner can influence the
spread-intensity of an infection wave, and, consequently, also the economic
activity and population health, through a single parameter. Population health
is assumed to only be negatively affected when the number of simultaneously
infected exceeds health care capacity. The main finding is that if (i) the
planner attaches a positive weight on economic activity and (ii) it is more
harmful for the economy to be locked down for longer than shorter time periods,
then the optimal policy is to (weakly) exceed health care capacity at some
time.

Interactive simulation toolkits come in handy when teaching macroeconomic
models by facilitating an easy understanding of underlying economic concepts
and offering an intuitive approach to the models' comparative statics. Based on
the example of the IS-LM model, this paper demonstrates innovative
browser-based features well-suited for the shift in education to online
platforms accelerated by COVID-19. The free and open-source code can be found
alongside the standalone HTML files for the AD-AS and the Solow growth model at
https://gitlab.tu-berlin.de/chair-of-macroeconomics/.

We study the effects on economic activity of a pure temporary change in
government debt and the relationship between the debt multiplier and the level
of debt in an overlapping generations framework. The debt multiplier is
positive but quite small during normal times while it is much larger during
crises. Moreover, it increases with the steady state level of debt. Hence, the
call for fiscal consolidation during recessions seems ill-advised. Finally, a
rise in the steady state debt-to-GDP level increases the steady state real
interest rate providing more room for manoeuvre to monetary policy to fight
deflationary shocks.

As more tech companies engage in rigorous economic analyses, we are
confronted with a data problem: in-house papers cannot be replicated due to use
of sensitive, proprietary, or private data. Readers are left to assume that the
obscured true data (e.g., internal Google information) indeed produced the
results given, or they must seek out comparable public-facing data (e.g.,
Google Trends) that yield similar results. One way to ameliorate this
reproducibility issue is to have researchers release synthetic datasets based
on their true data; this allows external parties to replicate an internal
researcher's methodology. In this brief overview, we explore synthetic data
generation at a high level for economic analyses.

The paper presents the professor-student network of Nobel laureates in
economics. 74 of the 79 Nobelists belong to one family tree. The remaining 5
belong to 3 separate trees. There are 350 men in the graph, and 4 women. Karl
Knies is the central-most professor, followed by Wassily Leontief. No classical
and few neo-classical economists have left notable descendants. Harvard is the
central-most university, followed by Chicago and Berlin. Most candidates for
the Nobel prize belong to the main family tree, but new trees may arise for the
students of Terence Gorman and Denis Sargan.

This article deals with the process of managing the social and economic
development of municipal formations. It highlights characteristics and key
issues that arise during management at the municipal level. In order to
minimize the impact of the described issues, it is suggested to consider
municipal social and economic development as a semistructured system which is
modelled using a semantic network. As a result, it is concluded that a rating
of indicators for assessing social and economic development needs to be created
in order to determine the effectiveness and correlation with the targeted
indicators.

Preferences often change -- even in short time intervals -- due to either the
mere passage of time (present-biased preferences) or changes in environmental
conditions (state-dependent preferences). On the basis of the empirical
findings in the context of state-dependent preferences, we critically discuss
the Aristotelian view of unitary decision makers in economics and urge a more
Heraclitean perspective on human decision-making. We illustrate that the
conceptualization of preferences as present-biased or state-dependent has very
different normative implications under the Aristotelian view, although both
concepts are empirically hard to distinguish. This is highly problematic, as it
renders almost any paternalistic intervention justifiable.

The purpose of this dissertation is to present an overview of the operational
and financial performance of airports in Europe. In benchmarking studies,
airports are assessed and compared with other airports based on key indicators
from a technical and an economic point of view. The interest lies primarily in
the question, which key figures can best measure the perception of quality of
service from the point of view of the passenger for the services at an airport.

A general framework is given to analyze the falsifiability of economic models
based on a sample of their observable components. It is shown that, when the
restrictions implied by the economic theory are insufficient to identify the
unknown quantities of the structure, the duality of optimal transportation with
zero-one cost function delivers interpretable and operational formulations of
the hypothesis of specification correctness from which tests can be constructed
to falsify the model.

I propose a new conceptual framework to disentangle the impacts of weather
and climate on economic activity and growth: A stochastic frontier model with
climate in the production frontier and weather shocks as a source of
inefficiency. I test it on a sample of 160 countries over the period 1950-2014.
Temperature and rainfall determine production possibilities in both rich and
poor countries; positively in cold countries and negatively in hot ones.
Weather anomalies reduce inefficiency in rich countries but increase
inefficiency in poor and hot countries; and more so in countries with low
weather variability. The climate effect is larger that the weather effect.

This paper explores a decentralisation initiative in the United Kingdom - the
Northern Powerhouse strategy (NPS) - in terms of its main goal: strengthening
connectivity between Northern cities of England. It focuses on economic
interactions of these cities, defined by ownership linkages between firms,
since the NPS's launch in 2010. The analysis reveals a relatively weak increase
in the intensity of economic regional patterns in the North, in spite of a
shift away from NPS cities' traditional manufacturing base. These results
suggest potential directions for policy-makers in terms of the future
implementation of the NPS.

The purpose of this study is to measure the benefits and costs of using
biochar, a carbon sequestration technology, to reduce the B.C Wine Industry's
carbon emissions. An economic model was developed to calculate the value-added
for each of the three sectors that comprise the BC Wine industry. Results
indicate that each sector of the wine value chain is potentially profitable,
with 9,000 tonnes of CO2 sequestered each year. The study is unique in that it
demonstrates that using biochar, produced from wine industry waste, to
sequester atmospheric CO2 can be both profitable and environmentally
sustainable.

A novel token-distance-based triple approach is proposed for identifying EPU
mentions in textual documents. The method is applied to a corpus of
French-language news to construct a century-long historical EPU index for the
Canadian province of Quebec. The relevance of the index is shown in a
macroeconomic nowcasting experiment.

By investigating the exam scores of introductory economics in a business
school in Taiwan between 2008 and 2019, we find three sets of results: First,
we find no significant difference between genders in the exam scores. Second,
students' majors are significantly associated with their exam scores, which
likely reflects their academic ability measured at college admission. Third,
the exam scores are strong predictors of students' future academic performance.

Social scientists have become increasingly interested in how narratives --
the stories in fiction, politics, and life -- shape beliefs, behavior, and
government policies. This paper provides an unsupervised method to quantify
latent narrative structures in text documents. Our new software package RELATIO
identifies coherent entity groups and maps explicit relations between them in
the text. We provide an application to the United States Congressional Record
to analyze political and economic narratives in recent decades. Our analysis
highlights the dynamics, sentiment, polarization, and interconnectedness of
narratives in political discourse.

We design a novel framework to examine market efficiency through
out-of-sample (OOS) predictability. We frame the asset pricing problem as a
machine learning classification problem and construct classification models to
predict return states. The prediction-based portfolios beat the market with
significant OOS economic gains. We measure prediction accuracies directly. For
each model, we introduce a novel application of binomial test to test the
accuracy of 3.34 million return state predictions. The tests show that our
models can extract useful contents from historical information to predict
future return states. We provide unique economic insights about OOS
predictability and machine learning models.

The distribution of household income is a central concern of modern economic
policy due to its strong influence on life quality. Yet, non-expert audiences
are unaware of the relationship between these two factors. To effectively
communicate the effect of income inequality on the quality of life and among
the strata, we have designed a novel technique for visualizing income
distribution and inequality over time by using the U.S. household income
microdata from the Current Population Survey. The result is a striking dynamic
animation of income distribution over time, drawing public attention and
further investigating economic inequality. Detailed implementation of this
project is available at https://github.com/sangttruong/incomevis.

In this paper, we estimate the causal impact (i.e. Average Treatment Effect,
ATT) of the EU ETS on GHG emissions and firm competitiveness (primarily
measured by employment, turnover, and exports levels) by combining a
difference-in-differences approach with semi-parametric matching techniques and
estimators an to investigate the effect of the EU ETS on the economic
performance of these German manufacturing firms using a Stochastic Production
Frontier model.

An important but understudied question in economics is how people choose when
facing uncertainty in the timing of events. Here we study preferences over time
lotteries, in which the payment amount is certain but the payment time is
uncertain. Expected discounted utility theory (EDUT) predicts decision makers
to be risk-seeking over time lotteries. We explore a normative model of
growth-optimality, in which decision makers maximise the long-term growth rate
of their wealth. Revisiting experimental evidence on time lotteries, we find
that growth-optimality accords better with the evidence than EDUT. We outline
future experiments to scrutinise further the plausibility of growth-optimality.

We study the relationship between foreign debt and GDP growth using a panel
dataset of 50 countries from 1997 to 2015. We find that economic growth
correlates positively with foreign debt and that the relationship is causal in
nature by using the sovereign credit default swap spread as an instrumental
variable. Furthermore, we find that foreign debt increases investment and then
GDP growth in subsequent years. Our findings suggest that lower sovereign
default risks lead to higher foreign debt contributing to GDP growth more in
OECD than non-OECD countries.

We address the question how sensitive international knowledge flows respond
to geo-political conflicts taking the politico-economic tensions between
EU-Russia since the Ukraine crisis 2014 as case study. We base our econometric
analysis on comprehensive data covering more than 500 million scientific
publications and 8 million international co-publications between 1995 and 2018.
Our findings indicate that the imposition of EU sanctions and Russian
counter-sanctions from 2014 onwards has significant negative effects on
bilateral international scientific co-publication rates between EU countries
and Russia. Depending on the chosen control group and sectors considered,
effect size ranges from 15% to 70%. Effects are also observed to grow over
time.

This paper discusses serious drawbacks of existing knowledge in
macroeconomics and finance in explaining and predicting economic and financial
phenomena. Complexity science is proposed as an alternative approach to be used
in order to better understand how economy and financial market work. This paper
argues that understanding characteristics of complex system could greatly
benefit financial analysts, financial regulators, as well as macroeconomic
policy makers.

This article is an edited transcript of the session of the same name at the
38th Annual NABE Economic Policy Conference: Policy Options for Sustainable and
Inclusive Growth. The panelists are experts from government and private
research organizations.

This synthetic control study quantifies the economic costs of the
Russo-Ukrainian war in terms of foregone entrepreneurial activity in both
countries since the invasion of Crimea in 2014. Relative to its synthetic
counterfactual, Ukraine's number of self-employed dropped by 675,000,
corresponding to a relative loss of 20%. The number of Ukrainian SMEs
temporarily dropped by 71,000 (14%) and recovered within five years of the
conflict. In contrast, Russia had lost more than 1.4 million SMEs (42%) five
years into the conflict. The disappearance of Russian SMEs is driven by both
fewer new businesses created and more existing business closures.

Venezuela has suffered three economic catastrophes since independence: one
each in the nineteenth, twentieth, and twenty-first centuries. Prominent
explanations for this trilogy point to the interaction of class conflict and
resource dependence. We turn attention to intra-class conflict, arguing that
the most destructive policy choices stemmed not from the rich defending
themselves against the masses but rather from pitched battles among elites.
Others posit that Venezuelan political institutions failed to sustain growth
because they were insufficiently inclusive; we suggest in addition that they
inadequately mediated intra-elite conflict.

This paper explores the economics of Augmented Reality (AR) and Virtual
Reality (VR) technologies within decision-making contexts. Two metrics are
proposed: Context Entropy, the informational complexity of an environment, and
Context Immersivity, the value from full immersion. The analysis suggests that
AR technologies assist in understanding complex contexts, while VR technologies
provide access to distant, risky, or expensive environments. The paper provides
a framework for assessing the value of AR and VR applications in various
business sectors by evaluating the pre-existing context entropy and context
immersivity. The goal is to identify areas where immersive technologies can
significantly impact and distinguish those that may be overhyped.

We introduce a novel approach to solving dynamic programming problems, such
as those in many economic models, on a quantum annealer, a specialized device
that performs combinatorial optimization. Quantum annealers attempt to solve an
NP-hard problem by starting in a quantum superposition of all states and
generating candidate global solutions in milliseconds, irrespective of problem
size. Using existing quantum hardware, we achieve an order-of-magnitude
speed-up in solving the real business cycle model over benchmarks in the
literature. We also provide a detailed introduction to quantum annealing and
discuss its potential use for more challenging economic problems.

We demonstrate the effectiveness of the logistic function to model the
evolution of two economic systems. The first is the GDP and trade growth of the
USA, and the second is the revenue and human resource growth of IBM. Our
modelling is based on the World Bank data in the case of the USA, and on the
company data in the case of IBM. The coupled dynamics of the two relevant
variables in both systems - GDP and trade for the USA, and revenue and human
resource for IBM - follows a power-law behaviour.

In order to cope with the task allocation in national economic mobilization,
a task allocation planning method based on Hierarchical Task Network (HTN) for
national economic mobilization is proposed. An HTN planning algorithm is
designed to solve and optimize task allocation, and a method is explored to
deal with the resource shortage. Finally, based on a real task allocation case
in national economic mobilization, an experimental study verifies the
effectiveness of the proposed method.

The Bitcoin Lightning Network (LN) is designed to improve the scalability of
blockchain systems by using off-chain payment paths to settle transactions in a
faster, cheaper, and more private manner. This work aims to empirically study
LN's fee revenue for network participants. Under realistic assumptions on
payment amounts, routing algorithms and traffic distribution, we analyze the
economic returns of the network's largest routing nodes which currently hold
the network together, and assess whether the centralizing tendency is
incentive-compatible from an economic viewpoint. Moreover, since recent
literature has proved that participation is economically irrational for the
majority of large nodes, we evaluate the long-term impact on the network
topology when participants start behaving rationally.

This paper analyzes the hypothesis that returns play a risk-compensating role
in the market for corporate revolving lines of credit. Specifically, we test
whether borrower risk and the expected return on these debt instruments are
positively related. Our main findings support this prediction, in contrast to
the only previous work that examined this problem two decades ago.
Nevertheless, we find evidence of mispricing regarding the risk of
deteriorating firms using their facilities more intensively and during the
subprime crisis.

We present a model predictive control (MPC) formulation to directly optimize
economic criteria for linear constrained systems subject to disturbances and
uncertain model parameters. The proposed formulation combines a certainty
equivalent economic MPC with a simple least-squares parameter adaptation. For
the resulting adaptive economic MPC scheme, we derive strong asymptotic and
transient performance guarantees. We provide a numerical example involving
building temperature control and demonstrate performance benefits of online
parameter adaptation.

Decentralized Autonomous Organizations (DAOs), utilizing blockchain
technology to enable collective governance, are a promising innovation. This
research addresses the ongoing query in blockchain governance: How can DAOs
optimize human cooperation? Focusing on the Network Nervous System (NNS), a
comprehensive on-chain governance framework underpinned by the Internet
Computer Protocol (ICP) and liquid democracy principles, we employ theoretical
abstraction and simulations to evaluate its potential impact on cooperation and
economic growth within DAOs. Our findings emphasize the significance of the
NNS's staking mechanism, particularly the reward multiplier, in aligning
individual short-term interests with the DAO's long-term prosperity. This study
contributes to the understanding and effective design of blockchain-based
governance systems.

We introduce the simplest model which relates the emergence of collective
social/economic phenomena to the existence of a (possibly self-organized)
percolation transition. We suggest a series of extensions to financial,
economic, political systems.

We consider the concept of equilibrium in economic systems from statistical
mechanics viewpoint. A new method is suggested for computing the premium on
this basis. The B\"{u}hlmann economic premium principle is derived as a special
case of our method.

We use the maximum entropy principle for pricing the non-life insurance and
recover the B\"{u}hlmann results for the economic premium principle. The
concept of economic equilibrium is revised in this respect.

This paper discusses dynamic evolutionary economics, and introduces a model
of such.

In this paper, we briefly discuss a mathematical concept that can be used in
economics.

A result about projections of Gibbs measures from a particular class arising
in economic modeling is proved.

We present a subtle idea to economically improve
message-unilaterally-transmitted quantum secure direct communication (QSDC)
protocols to realize two-way secure direct communication.

I review three socio-economic models of economic opinions, urban segregation,
and language change and show that the well known two-dimensional Ising model
gives about the same results in each case.

This is a short commentary piece that discusses how the methods used in the
natural sciences can apply to economics in general and financial markets
specifically.

Comment on "Event Excess in the MiniBooNE Search for $\bar
\nu_{\mu}\rightarrow \bar \nu_e$ Oscillations"

Throughout economic history, the global economy has experienced recurring
crises. The persistent recurrence of such economic crises calls for an
understanding of their generic features rather than treating them as singular
events. The global economic system is a highly complex system and can best be
viewed in terms of a network of interacting macroeconomic agents. In this
regard, from the perspective of collective network dynamics, here we explore
how the topology of global macroeconomic network affects the patterns of
spreading of economic crises. Using a simple toy model of crisis spreading, we
demonstrate that an individual country's role in crisis spreading is not only
dependent on its gross macroeconomic capacities, but also on its local and
global connectivity profile in the context of the world economic network. We
find that on one hand clustering of weak links at the regional scale can
significantly aggravate the spread of crises, but on the other hand the current
network structure at the global scale harbors a higher tolerance of extreme
crises compared to more "globalized" random networks. These results suggest
that there can be a potential hidden cost in the ongoing globalization movement
towards establishing less-constrained, trans-regional economic links between
countries, by increasing the vulnerability of global economic system to extreme
crises.

This is a short overview of the contribution of Leonid Kantorovich into the
formation of the modern outlook on the interaction between mathematics and
economics.

Ecological and economic networks have many similarities and are often
compared. However, the comparison is often more apt as metaphor than a direct
equivalence. In this paper, five key differences are explained which should
inform any analysis which compares the two.

Artificial intelligence has impacted many aspects of human life. This paper
studies the impact of artificial intelligence on economic theory. In particular
we study the impact of artificial intelligence on the theory of bounded
rationality, efficient market hypothesis and prospect theory.

In this paper, we completely classify the homothetical hypersurfaces having
null Gauss-Kronocker curvature in a Euclidean (n+1)-space. Several applications
to the production functions in economics are also given.

Historical economic growth in Western and Eastern Europe is analysed. These
regions should have produced the best and the most convincing confirmation of
the Unified Growth Theory because they, and in particular Western Europe, were
the centre of the Industrial Revolution, which according to Galor was the prime
engine of economic growth. However, the data for Western and Eastern Europe
show a remarkable disagreement with the Unified Growth Theory. There is no
connection, whatever, between the data and the Unified Growth Theory. The data
show that there was never a transition from stagnation to growth because there
was no stagnation. Industrial Revolution, which should have the strongest
influence in these regions, had absolutely no impact on changing the economic
growth trajectories. The alleged remarkable or stunning escape from Malthusian
trap did not happen because there was no trap. Unified Growth Theory does not
explain the mechanism of the economic growth because its explanations are based
on mythical features, which did not exist, the features contradicted by data.
This theory needs to be either thoroughly revised or most likely replaced by a
theory supported by a professional analysis of economic growth data.

Historical economic growth in Latin America is analysed using the data of
Maddison. Unified Growth Theory is found to be contradicted by these data in
the same way as it is contradicted by the economic growth in Africa, Asia,
former USSR, Western Europe, Eastern Europe and by the world economic growth.
Paradoxically, Unified Growth Theory is repeatedly and consistently
contradicted by the same data, which were used, but never properly analysed,
during the formulation of this theory. Unified Growth Theory does not explain
the mechanism of the economic growth because it explains features contradicted
by data. This theory is based fundamentally on the unfortunate lack of
understanding of the properties of hyperbolic distribution and on the
unscientific analysis of data. There was no transition from stagnation to
growth at the end of the alleged Malthusian regime because the economic growth
was hyperbolic. There was no escape from Malthusian trap because there was no
trap. There was no takeoff. On the contrary, at the time of the alleged takeoff
economic growth started to be diverted to a slower trajectory. Unified Growth
Theory is dissociated from the reality. This theory needs to be revised or
replaced. In its present form, it is a collection of irrelevant stories based
on impressions and on the unscientific use of data.

This paper describes surface-like waves of macroeconomic Credits-Loans
transactions on economic space. We use agent's risk ratings as their
coordinates and describe evolution of macro variables by transactions between
agents. Aggregations of agent's variables with risk coordinates x on economic
space define macro variables as function of x. Aggregations of transactions
between agents at point x and y determine functions of two variables (x,y) on
economic space. As example we study Credits transactions provided from agents
at point x to agents at point y and thus amount of Loans received by agents at
point y from agents at point x at moment t during time term dt. We model
evolution of macro transactions by hydrodynamic-like equations. Agents fill
macro domain on economic space that is bounded by minimum risk ratings of most
secure and maximum risk ratings of most risky agents. Economic and financial
shocks can disturb steady borders of macro domain and cause perturbations of
transactions. Such disturbances can generate waves that can propagate along
risk borders alike to surface waves in fluids. As example, we describe simple
model interactions between two transactions by hydrodynamic like equations in a
closed form. We introduce notions of "macro accelerations" and their potentials
that establish steady state distributions of transactions on economic space.
For this model in linear approximation we describe surface-like waves and show
that perturbations induced by surface-like waves can exponentially grow up
inside macro domain and induce macro instabilities in a low risk area.
Description of possible steady state distributions of transactions and
surface-like waves on economic space might be important for macro modeling and
policy-making.

A simple but useful method of reciprocal values is introduced, explained and
illustrated. This method simplifies the analysis of hyperbolic distributions,
which are causing serious problems in the demographic and economic research. It
allows for a unique identification of hyperbolic distributions and for
unravelling components of more complicated trajectories. This method is
illustrated by a few examples. They show that fundamental postulates of the
demographic and economic research are contradicted by data, even by precisely
the same data, which are used in this research. The generally accepted
postulates are based on the incorrect understanding of hyperbolic
distributions, which characterise the historical growth of population and the
historical economic growth. In particular, data used, but never analysed,
during the formulation of the Unified Growth Theory show that this theory is
based on fundamentally incorrect premises and thus is fundamentally defective.
Application of this simple method of analysis points to new directions in the
demographic and economic research. It suggests simpler interpretations of the
mechanism of growth. The concept or the evidence of the past primitive and
difficult living conditions, which might be perhaps described as some kind of
stagnation, is not questioned or disputed. It is only demonstrated that
trajectories of the past economic growth and of the growth of population were
not reflecting any form of stagnation and thus that they were not shaped by
these primitive and difficult living conditions. The concept or evidence of an
explosion in technology, medicine, education and in the improved living
conditions is not questioned or disputed. It is only demonstrated that this
possible explosion is not reflected in the trajectories of the economic growth
and of the growth of population.

This paper analyzes the impact of air transport connectivity and
accessibility on scientific collaboration.

This paper proposes the governance framework of a gamified social network for
charity crowdfunding fueled by public computing. It introduces optimal scarce
resource allocation model, technological configuration of the FIRE consensus
protocol, and multi-layer incentivization structure that maximizes value
creation within the network.

The famous Saint Petersburg Paradox (St. Petersburg Paradox) shows that the
theory of expected value does not capture the real-world economics of
decision-making problems. Over the years, many economic theories were developed
to resolve the paradox and explain gaps in the economic value theory in the
evaluation of economic decisions, the subjective utility of the expected
outcomes, and risk aversion as observed in the game of the St. Petersburg
Paradox. In this paper, we use the concept of the relative net utility to
resolve the St. Petersburg Paradox. Because the net utility concept is able to
explain both behavioral economics and the St. Petersburg Paradox, it is deemed
to be a universal approach to handling utility. This paper shows how the
information content of the notion of net utility value allows us to capture a
broader context of the impact of a decision's possible achievements. It
discusses the necessary conditions that the utility function has to conform to
avoid the paradox. Combining these necessary conditions allows us to define the
theorem of indifference in the evaluation of economic decisions and to present
the role of the relative net utility and net utility polarity in a value
rational decision-making process.

This chapter provides a concise survey on different dissipativity conditions
that have appeared in the literature on economic model predictive control and
discusses their decisive role in this context.

Economic interdependencies have become increasingly present in globalized
production, financial and trade systems. While establishing interdependencies
among economic agents is crucial for the production of complex products, they
may also increase systemic risks due to failure propagation. It is crucial to
identify how network connectivity impacts both the emergent production and risk
of collapse of economic systems. In this paper we propose a model to study the
effects of network structure on the behavior of economic systems by varying the
density and centralization of connections among agents. The complexity of
production increases with connectivity given the combinatorial explosion of
parts and products. Emergent systemic risks arise when interconnections
increase vulnerabilities. Our results suggest a universal description of
economic collapse given in the emergence of tipping points and phase
transitions in the relationship between network structure and risk of
individual failure. This relationship seems to follow a sigmoidal form in the
case of increasingly denser or centralized networks. The model sheds new light
on the relevance of policies for the growth of economic complexity, and
highlights the trade-off between increasing the potential production of the
system and its robustness to collapse. We discuss the policy implications of
intervening in the organization of interconnections and system features, and
stress how different network structures and node characteristics suggest
different directions in order to promote complex and robust economic systems.

This study was conducted to examine and understand the spending behavior of
low income households (B40), namely households with income of RM3800 and below.
The study focused on the area Kubang Pasu District, Kedah.

Mapping pathways to achieving the sustainable development goals requires
understanding and predicting how social, economic and political factors impact
biodiversity. Trends in demography, economic growth, regional alliances and
consumption behaviours can have profound effects on the environment by driving
resource use and production. While these distant socio-economic drivers impact
species and ecosystems at global scales, for example by driving greenhouse gas
emissions and climate change, the most prevalent human impacts on biodiversity
manifest through habitat loss and land use change decisions at finer scales. We
provide the first integrated ecological-economic analysis pathway capable of
supporting both national policy design challenges and global scale assessment
of biodiversity risks posed by socio-economic drivers such as population
growth, consumption and trade. To achieve this, we provide state-of-the-art
integration of economic, land use, and biodiversity modelling, and illustrate
its application using two case studies. We evaluate the national-level
implications of change in trading conditions under a multi-lateral free trade
agreement for the bird biodiversity of Vietnam. We review the implications for
land-use and biodiversity under coupled socio-economic (Shared Socioeconomic
Pathways) and climate (Resource Concentration Pathways) scenarios for
Australia. Our study provides a roadmap for setting up high dimensional
integrated analyses foe evaluating global priorities for protecting nature and
livelihoods in vulnerable areas with the greatest conflicts for economic,
social and environmental opportunities.

Modern economies evolved from simpler human exchanges into very convoluted
systems. Today, a multitude of aspects can be regulated, tampered with, or left
to chance; these are economic {\em degrees of freedom} which together shape the
flow of wealth. Economic actors can exploit them, at a cost, and bend that flow
in their favor. If intervention becomes widespread, microeconomic strategies of
different actors can collide or resonate, building into macroeconomic effects.
How viable is a `rigged' economy, and how is this viability affected by growing
economic complexity and wealth? Here we capture essential elements of `rigged'
economies with a toy model. Nash equilibria of payoff matrices in simple cases
show how increased intervention turns economic degrees of freedom from minority
into majority games through a dynamical phase. These stages are reproduced by
agent-based simulations of our model, which allow us to explore scenarios out
of reach for payoff matrices. Increasing economic complexity is then revealed
as a mechanism that spontaneously defuses cartels or consensus situations. But
excessive complexity enters abruptly into a regime of large fluctuations that
threaten the system's viability. This regime results from non-competitive
efforts to intervene the economy coupled across degrees of freedom, becoming
unpredictable. Thus non-competitive actions can result in negative spillover
due to sheer economic complexity. Simulations suggest that wealth must grow
faster than linearly with economic complexity to avoid this regime and keep
economies viable in the long run. Our work provides testable conclusions and
phenomenological charts to guide policing of `rigged' economic systems.

We introduce the price probability measure {\eta}(p;t) that defines the mean
price p(1;t), mean square price p(2;t), price volatility {\sigma}p2(t)and all
price n-th statistical moments p(n;t) as ratio of sums of n-th degree values
C(n;t) and volumes U(n;t) of market trades aggregated during certain time
interval {\Delta}. The definition of the mean price p(1;t) coincides with
definition of the volume weighted average price (VWAP) introduced at least 30
years ago. We show that price volatility {\sigma}p2(t) forecasting requires
modeling evolution of the sums of second-degree values C(2;t) and volumes
U(2;t). We call this model as second-order economic theory. We use numerical
continuous risk ratings as ground for risk assessment of economic agents and
distribute agents by risk ratings as coordinates. We introduce continuous
economic media approximation of squares of values and volumes of agents trades
and their flows aggregated during time interval {\Delta}. We take into account
expectations that govern agents trades and introduce aggregated expectations
alike to aggregated trades. We derive equations for continuous economic media
approximation on the second-degree trades. In the linear approximation we
derive mean square price p(2;t) and volatility {\sigma}p2(t) disturbances as
functions of the first and second-degree trades disturbances. Description of
each next n-th price statistical moment p(n;t) with respect to the unit price
measure {\eta}(p;t) depends on sums of n-th degree values C(n;t) and volumes
U(n;t) of market trades and hence requires development of the corresponding
n-th order economic theory.

The article pays close attention to obtaining gender assessments in the world
of work, which made it possible to characterize the effectiveness of social
policy aimed at achieving gender equality.

This paper evaluates the dynamic response of economic activity to shocks in
uncertainty as percieved by agents.The study focuses on the comparison between
the perception of economic uncertainty by manufacturers and consumers.Since
uncertainty is not directly observable, we approximate it using the geometric
discrepancy indicator of Claveria et al.(2019).This approach allows us
quantifying the proportion of disagreement in business and consumer
expectations of eleven European countries and the Euro Area.First, we compute
three independent indices of discrepancy corresponding to three dimensions of
uncertainty (economic, inflation and employment) and we average them to obtain
aggregate disagreement measures for businesses and for consumers.Next, we use a
bivariate Bayesian vector autoregressive framework to estimate the impulse
response functions to innovations in disagreement in every country.We find that
the effect on economic activity of shocks to the perception of uncertainty
differ markedly between manufacturers and consumers.On the one hand, shocks to
consumer discrepancy tend to be of greater magnitude and duration than those to
manufacturer discrepancy.On the other hand, innovations in disagreement between
the two collectives have an opposite effect on economic activity:shocks to
manufacturer discrepancy lead to a decrease in economic activity, as opposed to
shocks to consumer discrepancy.This finding is of particular relevance to
researchers when using cross-sectional dispersion of survey-based expectations,
since the effect on economic growth of shocks to disagreement depend on the
type of agent.

I show that the structure of the firm is not neutral in respect to regulatory
capital budgeted under rules which are based on the Value-at-Risk.

Global value chains (GVCs) are formed through value-added trade, and some
regions promote economic integration by concluding regional trade agreements to
promote these chains. However, there is no way to quantitatively assess the
scope and extent of economic integration involving various sectors in multiple
countries. In this study, we used the World Input--Output Database to create a
cross-border sector-wise trade in value-added network (international
value-added network (IVAN)) covering the period of 2000--2014 and evaluated
them using network science methods. By applying Infomap to the IVAN, we
confirmed for the first time the existence of two regional communities: Europe
and the Pacific Rim. Helmholtz--Hodge decomposition was used to decompose the
value flows within the region into potential and circular flows, and the annual
evolution of the potential and circular relationships between countries and
sectors was clarified. The circular flow component of the decomposition was
used to define an economic integration index, and findings confirmed that the
degree of economic integration in Europe declined sharply after the economic
crisis in 2009 to a level lower than that in the Pacific Rim. The European
economic integration index recovered in 2011 but again fell below that of the
Pacific Rim in 2013. Moreover, sectoral analysis showed that the economic
integration index captured the effect of Russian mineral resources, free
movement of labor in Europe, and international division of labor in the Pacific
Rim, especially in GVCs for the manufacture of motor vehicles and high-tech
products.

Growth rate of real GDP per capita, GDPpc, is represented as a sum of two
components, a monotonically decreasing economic trend and fluctuations related
to population change. The economic trend is modelled by an inverse function of
GDPpc with a constant numerator which varies for the largest developed
economies. In 2006, a statistical analysis conducted for 19 selected OECD
countries for the period between 1950 and 2003 showed a very weak linear trend
in the annual GDPpc increment for the largest economies: the USA, Japan,
France, Italy, and Spain. The UK, Australia, and Canada showed a slightly
steeper positive linear trend. The 2012 revision showed that the positive
trends became much lower and some of them fell below zero due to the Great
Recession. The fluctuations around the trend values are characterized by a
quasi-normal distribution with heavy and asymmetric tails. This research
revises the previous estimates and extends the set of studied countries by
economies in East Europe, Latin America, BRICS, Africa, and Asia including
several positive outliers with extremely fast growth. The change in GDP
definitions and measuring procedures with time and economic source is discussed
in relation to the statistical significance of the trend estimates and data
quality requirements for a consistent economic model. The relative performance
of all counties since 1960 is compared according to the predicted total GDPpc
growth as a function of the initial value. The performance in the 21st century
is analyzed separately as revealing potential and actual shifts in the global
economic powers.

The article reveals the main theoretical approaches to the analysis and study
of the phenomenon of corruption. Special attention is paid to the consideration
of the index approach to the analysis of corruption.

The article analyzes the legal framework regulating the legal provision of
transport security in Russia. Special attention is paid to the role of
prosecutor's supervision in the field of prevention of crimes in transport.

This note discusses some aspects of interpretations of the theory of optimal
taxation presented in recent works on the Brazilian tax system.

I extend the concept of absorptive capacity, used in the analysis of firms,
to a framework applicable to the national level. First, employing confirmatory
factor analyses on 47 variables, I build 13 composite factors crucial to
measuring six national level capacities: technological capacity, financial
capacity, human capacity, infrastructural capacity, public policy capacity, and
social capacity. My data cover most low- and middle-income- economies (LMICs),
eligible for the World Bank's International Development Association (IDA)
support between 2005 and 2019. Second, I analyze the relationship between the
estimated capacity factors and economic growth while controlling for some of
the incoming flows from abroad and other confounders that might influence the
relationship. Lastly, I conduct K-means cluster analysis and then analyze the
results alongside regression estimates to glean patterns and classifications
within the LMICs. Results indicate that enhancing infrastructure (ICT, energy,
trade, and transport), financial (apparatus and environment), and public policy
capacities is a prerequisite for attaining economic growth. Similarly, I find
improving human capital with specialized skills positively impacts economic
growth. Finally, by providing a ranking of which capacity is empirically more
important for economic growth, I offer suggestions to governments with limited
budgets to make wise investments. Likewise, my findings inform international
policy and monetary bodies on how they could better channel their funding in
LMICs to achieve sustainable development goals and boost shared prosperity.

This paper investigates the impact of information and communication
technology (ICT) adoption on individual well-being.

The purpose of this study is to examine the long-run relationship between
gold prices and Nepal Stock Exchange (NEPSE).

Cuierzhuang Phenomenon (or Cuierzhuang Model) is a regional development
phenomenon or rural revitalization model driven by ICT in the information era,
characterized by the storage and transportation, processing, packaging and
online sales of agricultural products, as well as online and offline
coordination, long-distance and cross-regional economic cooperation, ethnic
blending, equality, and mutual benefit. Unlike the Wenzhou Model, South Jiangsu
Model, and Pearl River Model in the 1980s and 1990s, the Cuierzhuang Model is
not only a rural revitalization brought about by the industrialization and
modernization of northern rural areas with the characteristics of industrial
development in the information age, but also an innovative regional economic
cooperation and development model with folk nature, spontaneous formation,
equality, and mutual benefit. Taking southern Xinjiang as the production base,
Xinjiang jujubes from Hotan and Ruoqiang are continuously transported to
Cuierzhuang, Cangzhou City, Hebei Province, where they are transferred,
cleaned, dried and packaged, and finally sold all over the country. With red
dates as a link, the eastern town of Cuierzhuang, which is more than 4,000
kilometers apart, connected with Xinjiang in the western region. Along the
ancient Silk Road, the farthest route can reach as far as Kashgar through the
southern Xinjiang route. Then, how did this long-distance and cross-regional
economic cooperation channel form, what are the regional economics or economic
geography principles of Cuierzhuang attracting Xinjiang jujube, and the
challenges and opportunities faced by Cuierzhuang phenomenon, etc. A
preliminary economic analysis has been carried out in this paper.

To estimate the reaction of economies to political interventions or external
disturbances, input-output (IO) tables -- constructed by aggregating data into
industrial sectors -- are extensively used. However, economic growth,
robustness, and resilience crucially depend on the detailed structure of
non-aggregated firm-level production networks (FPNs). Due to non-availability
of data little is known about how much aggregated sector-based and detailed
firm-level-based model-predictions differ. Using a nearly complete nationwide
FPN, containing 243,399 Hungarian firms with 1,104,141 supplier-buyer-relations
we self-consistently compare production losses on the aggregated industry-level
production network (IPN) and the granular FPN. For this we model the
propagation of shocks of the same size on both, the IPN and FPN, where the
latter captures relevant heterogeneities within industries. In a COVID-19
inspired scenario we model the shock based on detailed firm-level data during
the early pandemic. We find that using IPNs instead of FPNs leads to errors up
to 37% in the estimation of economic losses, demonstrating a natural limitation
of industry-level IO-models in predicting economic outcomes. We ascribe the
large discrepancy to the significant heterogeneity of firms within industries:
we find that firms within one sector only sell 23.5% to and buy 19.3% from the
same industries on average, emphasizing the strong limitations of industrial
sectors for representing the firms they include. Similar error-levels are
expected when estimating economic growth, CO2 emissions, and the impact of
policy interventions with industry-level IO models. Granular data is key for
reasonable predictions of dynamical economic systems.

We discuss the economic reasons why the predictions of price and return
statistical moments in the coming decades, in the best case, will be limited by
their averages and volatilities. That limits the accuracy of the forecasts of
price and return probabilities by Gaussian distributions. The economic origin
of these restrictions lies in the fact that the predictions of the market-based
n-th statistical moments of price and return for n=1,2,.., require the
description of the economic variables of the n-th order that are determined by
sums of the n-th degrees of values or volumes of market trades. The lack of
existing models that describe the evolution of the economic variables
determined by the sums of the 2nd degrees of market trades results in the fact
that even predictions of the volatilities of price and return are very
uncertain. One can ignore existing economic barriers that we highlight but
cannot overcome or resolve them. The accuracy of predictions of price and
return probabilities substantially determines the reliability of asset pricing
models and portfolio theories. The restrictions on the accuracy of predictions
of price and return statistical moments reduce the reliability and veracity of
modern asset pricing and portfolio theories.

This paper casts within a unified economic framework some key challenges for
the global economic order: de-globalization; the rising impracticability of
global cooperation; and the increasingly confrontational nature of Great Power
competition. In these, economics has been weaponised in the service of national
interest. This need be no bad thing. History provides examples where greater
openness and freer trade emerge from nations seeking only to advance their own
self-interests. But the cases described in the paper provide mixed signals. We
find that some developments do draw on a growing zero-sum perception to
economic and political engagement. That zero-sum explanation alone, however, is
crucially inadequate. Self-serving nations, even when believing the world
zero-sum, have under certain circumstances produced outcomes that have
benefited all. In other circumstances, perfectly-predicted losses have instead
resulted on all sides. Such lose-lose outcomes -- epic fail equilibria --
generalize the Prisoner's Dilemma game and are strictly worse than zero-sum. In
our analysis, Third Nations -- those not frontline in Great Power rivalry --
can serve an essential role in averting epic fail outcomes. The policy
implication is that Third Nations need to provide platforms that will gently
and unobtrusively nudge Great Powers away from epic-fail equilibria and towards
inadvertent cooperation.

Recommender systems have gained increasing attention to personalise consumer
preferences. While these systems have primarily focused on applications such as
advertisement recommendations (e.g., Google), personalized suggestions (e.g.,
Netflix and Spotify), and retail selection (e.g., Amazon), there is potential
for these systems to benefit from a more global, socio-economic, and culturally
aware approach, particularly as companies seek to expand into diverse markets.
This paper aims to investigate the potential of a recommender system that
considers cultural identity and socio-economic factors. We review the most
recent developments in recommender systems and explore the impact of cultural
identity and socio-economic factors on consumer preferences. We then propose an
ontology and approach for incorporating these factors into recommender systems.
To illustrate the potential of our approach, we present a scenario in consumer
subscription plan selection within the entertainment industry. We argue that
existing recommender systems have limited ability to precisely understand user
preferences due to a lack of awareness of socio-economic factors and cultural
identity. They also fail to update recommendations in response to changing
socio-economic conditions. We explore various machine learning models and
develop a final artificial neural network model (ANN) that addresses this gap.
We evaluate the effectiveness of socio-economic and culturally aware
recommender systems across four dimensions: Precision, Accuracy, F1, and
Recall. We find that a highly tuned ANN model incorporating domain-specific
data, select cultural indices and relevant socio-economic factors predicts user
preference in subscriptions with an accuracy of 95%, a precision of 94%, a F1
Score of 92\%, and a Recall of 90\%.

This article reviews the recent advances in the uniqueness and multiplicity
of competitive equilibria in models arising in mathematical economics, finance,
macroeconomics, and trade.

Net profit is sometimes found from data for net operating surplus. We propose
a way to find it from data for consumption, pay and market-value capital, and
concomitantly to reveal the factor shares in consumption.

We propose a stochastic dynamic model of migration and economic aggregation
in a system of employed (immobile) and unemployed (mobile) agents which respond
to local wage gradients. Dependent on the local economic situation, described
by a production function which includes cooperative effects, employed agents
can become unemployed and vice versa. The spatio-temporal distribution of
employed and unemployed agents is investigated both analytically and by means
of stochastic computer simulations. We find the establishment of distinct
economic centers out of a random initial distribution. The evolution of these
centers occurs in two different stages: (i) small economic centers are formed
based on the positive feedback of mutual stimulation/cooperation among the
agents, (ii) some of the small centers grow at the expense of others, which
finally leads to the concentration of the labor force in different extended
economic regions. This crossover to large-scale production is accompanied by an
increase in the unemployment rate. We observe a stable coexistence between
these regions, although they exist in an internal quasistationary
non-equilibrium state and still follow a stochastic eigendynamics.

The possibility of cloning a d-dimensional quantum system without an ancilla
is explored, extending on the economical phase-covariant cloning machine found
in [Phys. Rev. A {\bf 60}, 2764 (1999)] for qubits. We prove the impossibility
of constructing an economical version of the optimal universal cloning machine
in any dimension. We also show, using an ansatz on the generic form of cloning
machines, that the d-dimensional phase-covariant cloner, which optimally clones
all uniform superpositions, can be realized economically only in dimension d=2.
The used ansatz is supported by numerical evidence up to d=7. An economical
phase-covariant cloner can nevertheless be constructed for d>2, albeit with a
lower fidelity than that of the optimal cloner requiring an ancilla. Finally,
using again an ansatz on cloning machines, we show that an economical version
of the Fourier-covariant cloner, which optimally clones the computational basis
and its Fourier transform, is also possible only in dimension d=2.

The stock market has been known to form homogeneous stock groups with a
higher correlation among different stocks according to common economic factors
that influence individual stocks. We investigate the role of common economic
factors in the market in the formation of stock networks, using the arbitrage
pricing model reflecting essential properties of common economic factors. We
find that the degree of consistency between real and model stock networks
increases as additional common economic factors are incorporated into our
model. Furthermore, we find that individual stocks with a large number of links
to other stocks in a network are more highly correlated with common economic
factors than those with a small number of links. This suggests that common
economic factors in the stock market can be understood in terms of
deterministic factors.

The use of equilibrium models in economics springs from the desire for
parsimonious models of economic phenomena that take human reasoning into
account. This approach has been the cornerstone of modern economic theory. We
explain why this is so, extolling the virtues of equilibrium theory; then we
present a critique and describe why this approach is inherently limited, and
why economics needs to move in new directions if it is to continue to make
progress. We stress that this shouldn't be a question of dogma, but should be
resolved empirically. There are situations where equilibrium models provide
useful predictions and there are situations where they can never provide useful
predictions. There are also many situations where the jury is still out, i.e.,
where so far they fail to provide a good description of the world, but where
proper extensions might change this. Our goal is to convince the skeptics that
equilibrium models can be useful, but also to make traditional economists more
aware of the limitations of equilibrium models. We sketch some alternative
approaches and discuss why they should play an important role in future
research in economics.

Purpose: to provide a view and analysis of the immediate field of journals
which surround a number of key heterodox economics journals.
Design/methodology/approach: Using citation data from the Science and Social
Science Citation Index, the individual and collective networks of a number of
journals in this field are analyzed. Findings: The size and shape of the
citation networks of journals can differ substantially, even if in a broadly
similar category. Heterodox economics cannot (yet) be considered as an
integrated specialty: authors in several journals in heterodox economics cite
more from mainstream economics than from other heterodox journals. There are
also strong links with other disciplinary fields such as geography, development
studies, women studies, etc.

In the same way as the Hilbert Program was a response to the foundational
crisis of mathematics, this article tries to formulate a research program for
the socio-economic sciences. The aim of this contribution is to stimulate
research in order to close serious knowledge gaps in mainstream economics that
the recent financial and economic crisis has revealed. By identifying weak
points of conventional approaches in economics, we identify the scientific
problems which need to be addressed. We expect that solving these questions
will bring scientists in a position to give better decision support and policy
advice. We also indicate, what kinds of insights can be contributed by
scientists from other research fields such as physics, biology, computer and
social science. In order to make a quick progress and gain a systemic
understanding of the whole interconnected socio-economic-environmental system,
using the data, information and computer systems available today and in the
near future, we suggest a multi-disciplinary collaboration as most promising
research approach.

In this editorial guide for the special issue on econophysics, we give a
unique review of this young but quickly growing discipline. A suggestive
taxonomy of the development is proposed by making a distinction between
classical econophysics and modern econophysics. For each of these two stages of
development, we identify the key economic issues whose formulations and/or
treatments have been affected by physics or physicists, which includes value,
business fluctuations, economic growth, economic and financial time series, the
distribution of economic entities, interactions of economic agents, and
economic and social networks. The recent advancements in these issues of modern
econophysics are demonstrated by nine articles selected from the papers
presented at the Econophysics Colloquium 2010 held at Academia Sinica in
Taipei.

Economies are complex man-made systems where organisms and markets interact
according to motivations and principles not entirely understood yet. The
increasing dissatisfaction with the postulates of traditional economics i.e.
perfectly rational agents, interacting through efficient markets in the search
of equilibrium, has created new incentives for different approaches in
economics. The science of complexity may provide the platform to cross
disciplinary boundaries in seemingly disparate fields such as brain science and
economics. In this paper we take an integrative stance, fostering new insights
into the economic character of neural activity. The objective here is to
precisely delineate common topics in both neural and economic science, within a
systemic outlook grounded in empirical basis that jolts the unification across
the science of complex systems. It is argued that this mainly relies on the
study of the inverse problem in complex system with a truly Bayesian approach.

Macro-economic models describe the dynamics of economic quantities. The
estimations and forecasts produced by such models play a substantial role for
financial and political decisions. In this contribution we describe an approach
based on genetic programming and symbolic regression to identify variable
interactions in large datasets. In the proposed approach multiple symbolic
regression runs are executed for each variable of the dataset to find
potentially interesting models. The result is a variable interaction network
that describes which variables are most relevant for the approximation of each
variable of the dataset. This approach is applied to a macro-economic dataset
with monthly observations of important economic indicators in order to identify
potentially interesting dependencies of these indicators. The resulting
interaction network of macro-economic indicators is briefly discussed and two
of the identified models are presented in detail. The two models approximate
the help wanted index and the CPI inflation in the US.

The ForMaRE project applies formal mathematical reasoning to economics. We
seek to increase confidence in economics' theoretical results, to aid in
discovering new results, and to foster interest in formal methods, i.e.
computer-aided reasoning, within economics. To formal methods, we seek to
contribute user experience feedback from new audiences, as well as new
challenge problems. In the first project year, we continued earlier game theory
studies but then focused on auctions, where we are building a toolbox of
formalisations, and have started to study matching and financial risk.
  In parallel to conducting research that connects economics and formal
methods, we organise events and provide infrastructure to connect both
communities, from fostering mutual awareness to targeted matchmaking. These
efforts extend beyond economics, towards generally enabling domain experts to
use mechanised reasoning.

The traditional time series methodology requires at least a preliminary
transformation of the data to get stationarity. On the other hand, Robust
Bayesian Dynamic Models (RBDMs) do not assume a regular pattern or stability of
the underlying system but can include points of statement breaks. In this paper
we use RBDMs in order to account possible outliers and structural breaks in
Latin-American economic time series. We work with important economic time
series from Puerto Rico and Mexico. We show by using a random walk model how
RBDMs can be applied for detecting historic changes in the economic inflation
of Mexico. Also, we model the Consumer Price Index (CPI), the Economic Activity
Index (EAI) and the total number of employments (TNE) economic time series in
Puerto Rico using local linear trend and seasonal RBDMs with observational and
states variances. The results illustrate how the model accounts the structural
breaks for the historic recession periods in Puerto Rico.

We investigate the hierarchical structures of countries based on electricity
consumption and economic growth by using the real amounts of their consumption
over a certain time period. We use of electricity consumption data to detect
the topological properties of 60 countries from 1971 to 2008. These countries
are divided into three subgroups: low income group, middle income group and
high income group countries. Firstly, a relationship between electricity
consumption and economic growth is investigated by using the concept of
hierarchical structure methods (minimal spanning tree (MST) and hierarchical
tree (HT)). Secondly, we perform bootstrap techniques to investigate a value of
the statistical reliability to the links of the MST. Finally, we use a
clustering linkage procedure in order to observe the cluster structure more
clearly. The results of the structural topologies of these trees are as
follows: i) we identified different clusters of countries according to their
geographical location and economic growth, ii) we found a strong relation
between energy consumption and economic growth for all the income groups
considered in this study and iii) the results are in good agreement with the
causal relationship between electricity consumption and economic growth.

In complex systems, many different parts interact in non-obvious ways.
Traditional research focuses on a few or a single aspect of the problem so as
to analyze it with the tools available. To get a better insight of phenomena
that emerge from complex interactions, we need instruments that can analyze
simultaneously complex interactions between many parts. Here, a simulator
modeling different types of economies, is used to visualize complex
quantitative aspects that affect economic dynamics. The main conclusions are:
1- Relatively simple economic settings produce complex non-linear dynamics and
therefore linear regressions are often unsuitable to capture complex economic
dynamics; 2- Flexible pricing of goods by individual agents according to their
micro-environment increases the health and wealth of the society, but
asymmetries in price sensitivity between buyers and sellers increase price
inflation; 3- Prices for goods conferring risky long term benefits are not
tracked efficiently by simple market forces. 4- Division of labor creates
synergies that improve enormously the health and wealth of the society by
increasing the efficiency of economic activity. 5- Stochastic modeling improves
our understanding of real economies, and didactic games based on them might
help policy makers and non specialists in grasping the complex dynamics
underlying even simple economic settings.

Internet of things (IoT) has been proposed to be a new paradigm of connecting
devices and providing services to various applications, e.g., transportation,
energy, smart city, and healthcare. In this paper, we focus on an important
issue, i.e., economics of IoT, that can have a great impact to the success of
IoT applications. In particular, we adopt and present the information economics
approach with its applications in IoT. We first review existing economic models
developed for IoT services. Then, we outline two important topics of
information economics which are pertinent to IoT, i.e., the value of
information and information good pricing. Finally, we propose a game theoretic
model to study the price competition of IoT sensing services. Some outlooks on
future research directions of applying information economics to IoT are
discussed.

How are economic activities linked to geographic locations? To answer this
question, we use a data-driven approach that builds on the information about
location, ownership and economic activities of the world's 3,000 largest firms
and their almost one million subsidiaries. From this information we generate a
bipartite network of cities linked to economic activities. Analysing the
structure of this network, we find striking similarities with nested networks
observed in ecology, where links represent mutualistic interactions between
species. This motivates us to apply ecological indicators to identify the
unbalanced deployment of economic activities. Such deployment can lead to an
over-representation of specific economic sectors in a given city, and poses a
significant thread for the city's future especially in times when the
over-represented activities face economic uncertainties. If we compare our
analysis with external rankings about the quality of life in a city, we find
that the nested structure of the city-firm network also reflects such
information about the quality of life, which can usually be assessed only via
dedicated survey-based indicators.

The goal of this paper is to investigate the importance of providing visual
"big pictures" in the teaching of economics. The plurality and variety of
concepts, variables, diagrams, and models involved in economics can be a source
of confusion for many economics students. However, reviewing the existing
literature on the importance of providing visual "big pictures" in the process
of learning suggests that furnishing students with a visual "big picture" that
illustrates the ways through which those numerous, diverse concepts are
connected to each other could be an effective solution to clear up the
mentioned mental chaos. As a practical example, this paper introduces a "big
picture" that can be used as a good resource in intermediate macroeconomics
classes. This figure presents twenty-seven commonly-discussed macroeconomic
diagrams in the intermediate macroeconomics course, and gives little detail on
some of these diagrams, aiming at helping students to get the whole picture at
once on a single piece of paper. This macroeconomics big picture mostly focuses
on the routes through which common diagrams in macroeconomics are connected to
each other, and finally introduces the general macroeconomic equilibrium that
is graphically derived through those connections.

An intriguing open question is whether measurements made on Big Data
recording human activities can yield us high-fidelity proxies of socio-economic
development and well-being. Can we monitor and predict the socio-economic
development of a territory just by observing the behavior of its inhabitants
through the lens of Big Data? In this paper, we design a data-driven analytical
framework that uses mobility measures and social measures extracted from mobile
phone data to estimate indicators for socio-economic development and
well-being. We discover that the diversity of mobility, defined in terms of
entropy of the individual users' trajectories, exhibits (i) significant
correlation with two different socio-economic indicators and (ii) the highest
importance in predictive models built to predict the socio-economic indicators.
Our analytical framework opens an interesting perspective to study human
behavior through the lens of Big Data by means of new statistical indicators
that quantify and possibly "nowcast" the well-being and the socio-economic
development of a territory.

This is an invited article for the Discussion and Debate special issue of The
European Physical Journal Special Topics on the subject "Can Economics Be a
Physical Science?" The first part of the paper traces the personal path of the
author from theoretical physics to economics. It briefly summarizes
applications of statistical physics to monetary transactions in an ensemble of
economic agents. It shows how a highly unequal probability distribution of
money emerges due to irreversible increase of entropy in the system. The second
part examines deep conceptual and controversial issues and fallacies in
monetary economics from econophysics perspective. These issues include the
nature of money, conservation (or not) of money, distinctions between money vs.
wealth and money vs. debt, creation of money by the state and debt by the
banks, the origins of monetary crises and capitalist profit. Presentation uses
plain language understandable to laypeople and may be of interest to both
specialists and general public.

Economic competition between humans leads to income inequality, but, so far,
there has been little understanding of underlying quantitative mechanisms
governing such a collective behavior. We analyze datasets of household income
from 67 countries, ranging from Europe to Latin America, North America and
Asia. For all of the countries, we find a surprisingly uniform rule: Income
distribution for the great majority of populations (low and middle income
classes) follows an exponential law. To explain this empirical observation, we
propose a theoretical model within the standard framework of modern economics
and show that free competition and Rawls' fairness are the underlying
mechanisms producing the exponential pattern. The free parameters of the
exponential distribution in our model have an explicit economic interpretation
and direct relevance to policy measures intended to alleviate income
inequality.

The advent of artificial intelligence has changed many disciplines such as
engineering, social science and economics. Artificial intelligence is a
computational technique which is inspired by natural intelligence such as the
swarming of birds, the working of the brain and the pathfinding of the ants.
These techniques have impact on economic theories. This book studies the impact
of artificial intelligence on economic theories, a subject that has not been
extensively studied. The theories that are considered are: demand and supply,
asymmetrical information, pricing, rational choice, rational expectation, game
theory, efficient market hypotheses, mechanism design, prospect, bounded
rationality, portfolio theory, rational counterfactual and causality. The
benefit of this book is that it evaluates existing theories of economics and
update them based on the developments in artificial intelligence field.

It is commonly believed that patterns of social ties affect individuals'
economic status. Here, we translate this concept into an operational definition
at the network level, which allows us to infer the economic wellbeing of
individuals through a measure of their location and influence in the social
network. We analyze two large-scale sources: telecommunications and financial
data of a whole country's population. Our results show that an individual's
location, measured as the optimal collective influence to the structural
integrity of the social network, is highly correlated with personal economic
status. The observed social network patterns of influence mimics the patterns
of economic inequality. For pragmatic use and validation, we carry out a
marketing campaign that shows a three-fold increase in response rate by
targeting individuals identified by our social network metrics as compared to
random targeting. Our strategy can also be useful in maximizing the effects of
large-scale economic stimulus policies.

The security-constrained economic dispatch (SCED) problem tries to maintain
the reliability of a power network by ensuring that a single failure does not
lead to a global outage. The previous research has mainly investigated SCED by
formulating the problem in different modalities, e.g. preventive or corrective,
and devising efficient solutions for SCED. In this paper, we tackle a novel and
important direction, and analyze the economic cost of incorporating security
constraints in economic dispatch. Inspired by existing inefficiency metrics in
game theory and computer science, we introduce notion of price of security as a
metric that formally characterizes the economic inefficiency of
security-constrained economic dispatch as compared to the original problem
without security constraints. Then, we focus on the preventive approach in a
simple topology comprising two buses and two lines, and investigate the impact
of generation availability and demand distribution on the price of security.
Moreover, we explicitly derive the worst-case input instance that leads to the
maximum price of security. By extensive experimental study on two test-cases,
we verify the analytical results and provide insights for characterizing the
price of security in general networks.

This paper provides a comprehensive literature review on applications of
economic and pricing theory to security issues in wireless networks. Unlike
wireline networks, the broadcast nature and the highly dynamic change of
network environments pose a number of nontrivial challenges to security design
in wireless networks. While the security issues have not been completely solved
by traditional or system-based solutions, economic and pricing models recently
were employed as one efficient solution to discourage attackers and prevent
attacks to be performed. In this paper, we review economic and pricing
approaches proposed to address major security issues in wireless networks
including eavesdropping attack, Denial-of-Service (DoS) attack such as jamming
and Distributed DoS (DDoS), and illegitimate behaviors of malicious users.
Additionally, we discuss integrating economic and pricing models with
cryptography methods to reduce information privacy leakage as well as to
guarantee the confidentiality and integrity of information in wireless
networks. Finally, we highlight important challenges, open issues and future
research directions of applying economic and pricing models to wireless
security issues.

The method of model averaging has become an important tool to deal with model
uncertainty, for example in situations where a large amount of different
theories exist, as are common in economics. Model averaging is a natural and
formal response to model uncertainty in a Bayesian framework, and most of the
paper deals with Bayesian model averaging. The important role of the prior
assumptions in these Bayesian procedures is highlighted. In addition,
frequentist model averaging methods are also discussed. Numerical methods to
implement these methods are explained, and I point the reader to some freely
available computational resources. The main focus is on uncertainty regarding
the choice of covariates in normal linear regression models, but the paper also
covers other, more challenging, settings, with particular emphasis on sampling
models commonly used in economics. Applications of model averaging in economics
are reviewed and discussed in a wide range of areas, among which growth
economics, production modelling, finance and forecasting macroeconomic
quantities.

We analyze social and economic systems with a hierarchical structure and show
that for such systems, it is possible to construct thermostatistics, based on
the intermediate Gentile statistics. We show that in social and economic
hierarchical systems there are elements that obey the Fermi-Dirac statistics
and can be called fermions, as well as elements that are approximately subject
to Bose-Einstein statistics and can be called bosons. We derive the first and
second laws of thermodynamics for the considered economic system and show that
such concepts as temperature, pressure and financial potential (which is an
analogue of the chemical potential in thermodynamics) that characterize the
state of the economic system as a whole, can be introduced for economic
systems.

Monetary integration has both costs and benefits. Europeans have a strong
aversion to exchange rate instability. From this perspective, the EMS has shown
its limits and full monetary union involving a single currency appears to be a
necessity. This is the goal of the EMU project contained in the Maastricht
Treaty. This paper examines the pertinent choices: independence of the Central
Bank, budgetary discipline and economic policy coordination. Therefore, the
implications of EMU for the economic policy of France will be examined. If the
external force disappears, the public sector still cannot circumvent its
solvency constraint. The instrument of national monetary policy will not be
available so the absorption of asymmetric shocks will require greater wage
flexibility and fiscal policy will play a greater role. The paper includes
three parts. The first concerns the economic foundations of monetary union and
the costs it entails. The second is devoted to the institutional arrangements
under the Treaty of Maastricht. The third examines the consequences of monetary
union for the economy and the economic policy of France.

This study analyzes public debts and deficits between European countries. The
statistical evidence here seems in general to reveal that sovereign debts and
government deficits of countries within European Monetary Unification-in
average- are getting worse than countries outside European Monetary
Unification, in particular after the introduction of Euro currency. This
socioeconomic issue might be due to Maastricht Treaty, the Stability and Growth
Pact, the new Fiscal Compact, strict Balanced-Budget Rules, etc. In fact, this
economic policy of European Union, in phases of economic recession, may
generate delay and rigidity in the application of prompt counter-cycle (or
acyclical) interventions to stimulate the economy when it is in a downturn
within countries. Some implications of economic policy are discussed.

The useful life of electrochemical energy storage (EES) is a critical factor
to EES planning, operation, and economic assessment. Today, systems commonly
assume a physical end-of-life criterion, retiring EES when the remaining
capacity reaches a threshold below which the EES is of little use because of
functionality degradation. Here, we propose an economic end of life criterion,
where EES is retired when it cannot earn positive net economic benefit in its
intended application. This criterion depends on the use case and degradation
characteristics of the EES, but is independent of initial capital cost. Using
an intertemporal operational framework to consider functionality and
profitability degradation, our case study shows that the economic end of life
could occur significantly faster than the physical end of life. We argue that
both criteria should be applied in EES system planning and assessment. We also
analyze how R&D efforts should consider cycling capability and calendar
degradation rate when considering the economic end-of-life of EES.

We propose in this work a kinetic wealth-exchange model of economic growth by
introducing saving as a non consumed fraction of production. In this new model,
which starts also from microeconomic arguments, it is found that economic
transactions between pairs of agents leads the system to a macroscopic behavior
where total wealth is not conserved and it is possible to have an economic
growth which is assumed as the increasing of total production in time. This
last macroeconomic result, that we find both numerically through a Monte Carlo
based simulation method and analytically in the framework of a mean field
approximation, corresponds to the economic growth scenario described by the
well known Solow model developed in the economic neoclassical theory. If
additionally to the income related with production due to return on individual
capital, it is also included the individual labor income in the model, then the
Thomas Piketty's second fundamental law of capitalism is found as a emergent
property of the system. We consider that the results obtained in this paper
shows how Econophysics can help to understand the connection between
macroeconomics and microeconomics.

Modern macroeconomic theories were unable to foresee the last Great Recession
and could neither predict its prolonged duration nor the recovery rate. They
are based on supply-demand equilibria that do not exist during recessionary
shocks. Here we focus on resilience as a nonequilibrium property of networked
production systems and develop a linear response theory for input-output
economics. By calibrating the framework to data from 56 industrial sectors in
43 countries between 2000 and 2014, we find that the susceptibility of
individual industrial sectors to economic shocks varies greatly across
countries, sectors, and time. We show that susceptibility-based predictions
that take sector- and country-specific recovery into account, outperform--by
far--standard econometric growth-models. Our results are analytically rigorous,
empirically testable, and flexible enough to address policy-relevant scenarios.
We illustrate the latter by estimating the impact of recently imposed tariffs
on US imports (steel and aluminum) on specific sectors across European
countries.

We estimated system-wise levelized cost of electricity (LCOE) for a power
grid with a high level of renewable energy using our grid optimization model.
The estimation results of the system-wise LCOE are discussed in terms of the
nexus of energy, environment, and economic growth for Small Island Developing
States (SIDS) economies. While 100% renewable energy is technologically
possible with the usage of electricity storage, the estimated LCOE is as high
as 397 $/MWh which is substantially higher than electricity prices for
residential consumers in the US and Japan. The susceptibility analyses imply
that the estimated LCOE increase of 223% with a 100% renewable power grid
corresponds to an as high as 11% decrease in economic growth. This decrease in
economic growth would have a significant negative impact on SIDS economies.
However, hydrogen production via the electrolysis of water using the excess
energy supply from solar photovoltaics would reduce the LCOE, therefore higher
economic growth would be attained with less CO2 emission.

The central problems of Development Economics are the explanation of the
gross disparities in the global distribution, $\cal{D}$, of economic
performance, $\cal{E}$, and the persistence, $\cal{P}$, of said distribution.
Douglass North argued, epigrammatically, that institutions, $\cal{I}$, are the
rules of the game, meaning that $\cal{I}$ determines or at least constrains
$\cal{E}$. This promised to explain $\cal{D}$. 65,000 citations later, the
central problems remain unsolved. North's institutions are informal, slowly
changing cultural norms as well as roads, guilds, and formal legislation that
may change overnight. This definition, mixing the static and the dynamic, is
unsuited for use in a necessarily time dependent theory of developing
economies. We offer here a suitably precise definition of $\cal{I}$, a
dynamical theory of economic development, a new measure of the economy, an
explanation of $\cal{P}$, a bivariate model that explains half of $\cal{D}$,
and a critical reconsideration of North's epigram.

A crucial goal of funding research and development has always been to advance
economic development. On this basis, a consider-able body of research
undertaken with the purpose of determining what exactly constitutes economic
impact and how to accurately measure that impact has been published. Numerous
indicators have been used to measure economic impact, although no single
indicator has been widely adapted. Based on patent data collected from
Altmetric we predict patent citations through various social media features
using several classification models. Patents citing a research paper implies
the potential it has for direct application inits field. These predictions can
be utilized by researchers in deter-mining the practical applications for their
work when applying for patents.

Behavioral economics changed the way we think about market participants and
revolutionized policy-making by introducing the concept of choice architecture.
However, even though effective on the level of a population, interventions from
behavioral economics, nudges, are often characterized by weak generalisation as
they struggle on the level of individuals. Recent developments in data science,
artificial intelligence (AI) and machine learning (ML) have shown ability to
alleviate some of the problems of weak generalisation by providing tools and
methods that result in models with stronger predictive power. This paper aims
to describe how ML and AI can work with behavioral economics to support and
augment decision-making and inform policy decisions by designing personalized
interventions, assuming that enough personalized traits and psychological
variables can be sampled.

Standard macroeconomic models assume that households are rational in the
sense that they are perfect utility maximizers, and explain economic dynamics
in terms of shocks that drive the economy away from the stead-state. Here we
build on a standard macroeconomic model in which a single rational
representative household makes a savings decision of how much to consume or
invest. In our model households are myopic boundedly rational heterogeneous
agents embedded in a social network. From time to time each household updates
its savings rate by copying the savings rate of its neighbor with the highest
consumption. If the updating time is short, the economy is stuck in a poverty
trap, but for longer updating times economic output approaches its optimal
value, and we observe a critical transition to an economy with irregular
endogenous oscillations in economic output, resembling a business cycle. In
this regime households divide into two groups: Poor households with low savings
rates and rich households with high savings rates. Thus inequality and economic
dynamics both occur spontaneously as a consequence of imperfect household
decision making. Our work here supports an alternative program of research that
substitutes utility maximization for behaviorally grounded decision making.

Although Massive Online Open Courses (MOOCs) have the promise to make
rigorous higher education accessible to everyone, prior research has shown that
registrants tend to come from backgrounds of higher socioeconomic status. In
this work, I study geographically granular economic patterns in registration
for HarvardX and MITx courses, and in the accuracy of identifying users'
locations from their IP addresses. Using ZIP Codes identified by the MaxMind IP
geolocation database, I find that per-capita registration rates correlate with
economic prosperity and population density. Comparing these ZIP Codes with
user-provided mailing addresses, I find evidence of bias in MaxMind
geolocation: it makes greater errors, both geographically and economically, for
users from more economically distressed areas; it disproportionately geolocates
users to prosperous areas; and it underestimates the regressive pattern in MOOC
registration. Similar economic biases may affect IP geolocation in other
academic, commercial, and legal contexts.

The compact city, as a sustainable concept, is intended to augment the
efficiency of urban function. However, previous studies have concentrated more
on morphology than on structure. The present study focuses on urban structural
elements, i.e., urban hotspots consisting of high-density and high-intensity
socioeconomic zones, and explores the economic performance associated with
their spatial structure. We use nighttime luminosity (NTL) data and the Loubar
method to identify and extract the hotspot and ultimately draw two conclusions.
First, with population increasing, the hotspot number scales sublinearly with
an exponent of approximately 0.50~0.55, regardless of the location in China,
the EU or the US, while the intersect values are totally different, which is
mainly due to different economic developmental level. Secondly, we demonstrate
that the compactness of hotspots imposes an inverted U-shaped influence on
economic growth, which implies that an optimal compactness coefficient does
exist. These findings are helpful for urban planning.

The steel industry has great impacts on the economy and the environment of
both developed and underdeveloped countries. The importance of this industry
and these impacts have led many researchers to investigate the relationship
between a country's steel consumption and its economic activity resulting in
the so-called intensity of use model. This paper investigates the validity of
the intensity of use model for the case of Iran's steel consumption and extends
this hypothesis by using the indexes of economic activity to model the steel
consumption. We use the proposed model to train support vector machines and
predict the future values for Iran's steel consumption. The paper provides
detailed correlation tests for the factors used in the model to check for their
relationships with the steel consumption. The results indicate that Iran's
steel consumption is strongly correlated with its economic activity following
the same pattern as the economy has been in the last four decades.

This paper develops a new model of business cycles. The model is economical
in that it is solved with an aggregate demand-aggregate supply diagram, and the
effects of shocks and policies are obtained by comparative statics. The model
builds on two unconventional assumptions. First, producers and consumers meet
through a matching function. Thus, the model features unemployment, which
fluctuates in response to aggregate demand and supply shocks. Second, wealth
enters the utility function, so the model allows for permanent zero-lower-bound
episodes. In the model, the optimal monetary policy is to set the interest rate
at the level that eliminates the unemployment gap. This optimal interest rate
is computed from the prevailing unemployment gap and monetary multiplier (the
effect of the nominal interest rate on the unemployment rate). If the
unemployment gap is exceedingly large, monetary policy cannot eliminate it
before reaching the zero lower bound, but a wealth tax can.

Political systems shape institutions and govern institutional change
supporting economic performance, production and diffusion of technological
innovation. This study shows, using global data of countries, that
institutional change, based on a progressive democratization of countries, is a
driving force of inventions, adoption and diffusion of innovations in society.
The relation between technological innovation and level of democracy can be
explained with following factors: higher economic freedom in society, effective
regulation, higher economic and political stability, higher investments in R&D
and higher education, good economic governance and higher level of education
system for training high-skilled human resources. Overall, then, the positive
associations between institutional change, based on a process of
democratization, and paths of technological innovation can sustain best
practices of political economy for the development of economies in the presence
of globalization and geographical expansion of markets.

China's policy interventions to reduce the spread of the coronavirus disease
2019 have environmental and economic impacts. Tropospheric nitrogen dioxide
indicates economic activities, as nitrogen dioxide is primarily emitted from
fossil fuel consumption. Satellite measurements show a 48% drop in tropospheric
nitrogen dioxide vertical column densities from the 20 days averaged before the
2020 Lunar New Year to the 20 days averaged after. This is 20% larger than that
from recent years. We relate to this reduction to two of the government's
actions: the announcement of the first report in each province and the date of
a province's lockdown. Both actions are associated with nearly the same
magnitude of reductions. Our analysis offers insights into the unintended
environmental and economic consequences through reduced economic activities.

The Hicks induced innovation hypothesis states that a price increase of a
production factor is a spur to invention. We propose an alternative hypothesis
restating that a spur to invention require not only an increase of one factor
but also a decrease of at least one other factor to offset the companies' cost.
We illustrate the need for our alternative hypothesis in a historical example
of the industrial revolution in the United Kingdom. Furthermore, we
econometrically evaluate both hypotheses in a case study of research and
development (R&D) in 29 OECD countries from 2003 to 2017. Specifically, we
investigate dependence of investments to R&D on economic environment
represented by average wages and oil prices using panel regression. We find
that our alternative hypothesis is supported for R&D funded and/or performed by
business enterprises while the original Hicks hypothesis holds for R&D funded
by the government and R&D performed by universities. Our results reflect that
business sector is significantly influenced by market conditions, unlike the
government and higher education sectors.

This article shows State of the Art of techno-economic modeling for access
network technologies, presents the characteristics a universal techno-economic
model should have, and shows a classification and analysis of techno-economic
models in the literature based on such characteristics. As a result of his
research in this direction, the author created and developed a Universal
Techno-Economic Model and the corresponding methodology for techno-economic
assessment in multiple domains, currently available for industry stakeholders
under specific licence of use.

Stablecoins are one of the most widely capitalized type of cryptocurrency.
However, their risks vary significantly according to their design and are often
poorly understood. We seek to provide a sound foundation for stablecoin theory,
with a risk-based functional characterization of the economic structure of
stablecoins. First, we match existing economic models to the disparate set of
custodial systems. Next, we characterize the unique risks that emerge in
non-custodial stablecoins and develop a model framework that unifies existing
models from economics and computer science. We further discuss how this
modeling framework is applicable to a wide array of cryptoeconomic systems,
including cross-chain protocols, collateralized lending, and decentralized
exchanges. These unique risks yield unanswered research questions that will
form the crux of research in decentralized finance going forward.

The COVID-19 pandemic has caused more than 8 million confirmed cases and
500,000 death to date. In response to this emergency, many countries have
introduced a series of social-distancing measures including lockdowns and
businesses' temporary shutdowns, in an attempt to curb the spread of the
infection. Accordingly, the pandemic has been generating unprecedent disruption
on practically every aspect of society. This paper demonstrates that
high-frequency electricity market data can be used to estimate the causal,
short-run impact of COVID-19 on the economy. In the current uncertain economic
conditions, timeliness is essential. Unlike official statistics, which are
published with a delay of a few months, with our approach one can monitor
virtually every day the impact of the containment policies, the extent of the
recession and measure whether the monetary and fiscal stimuli introduced to
address the crisis are being effective. We illustrate our methodology on daily
data for the Italian day-ahead power market. Not surprisingly, we find that the
containment measures caused a significant reduction in economic activities and
that the GDP at the end of in May 2020 is still about 11% lower that what it
would have been without the outbreak.

Uncertainty plays an important role in the global economy. In this paper, the
economic policy uncertainty (EPU) indices of the United States and China are
selected as the proxy variable corresponding to the uncertainty of national
economic policy. By adopting the visibility graph algorithm, the four economic
policy uncertainty indices of the United States and China are mapped into
complex networks, and the topological properties of the corresponding networks
are studied. The Hurst exponents of all the four indices are within
$\left[0.5,1\right]$, which implies that the economic policy uncertainty is
persistent. The degree distributions of the EPU networks have power-law tails
and are thus scale-free. The average clustering coefficients of the four EPU
networks are high and close to each other, while these networks exhibit weak
assortative mixing. We also find that the EPU network in United States based on
daily data shows the small-world feature since the average shortest path length
increases logarithmically with the network size such that
$L\left(N\right)=0.626\ln N+0.405$. Our research highlights the possibility to
study the EPU from the view angle of complex networks.

This doctoral dissertation shows State of the Art of techno-economic modeling
for access network technologies, presents the characteristics a universal
techno-economic model should have, and shows a classification and analysis of
techno-economic models in the literature based on such characteristics. In
order to reduce the gap detected in the literature, the author defines and
develops a Universal Techno-Economic Model called UTEM and the corresponding
methodology to industrialize techno-economic assessment in multiple domains
considering all market players perspectives, also suitable for technological
consulting and currently available for all industry stakeholders under specific
license of use.

This paper examines the dynamic interaction between falling and rising
markets for both the real and the financial sectors of the largest economy in
the world using asymmetric causality tests. These tests require that each
underlying variable in the model be transformed into partial sums of the
positive and negative components. The positive components represent the rising
markets and the negative components embody the falling markets. The sample
period covers some part of the COVID19 pandemic. Since the data is non normal
and the volatility is time varying, the bootstrap simulations with leverage
adjustments are used in order to create reliable critical values when causality
tests are conducted. The results of the asymmetric causality tests disclose
that the bear markets are causing the recessions as well as the bull markets
are causing the economic expansions. The causal effect of bull markets on
economic expansions is higher compared to the causal effect of bear markets on
economic recessions. In addition, it is found that economic expansions cause
bull markets but recessions do not cause bear markets. Thus, the policies that
remedy the falling financial markets can also help the economy when it is in a
recession.

We investigate how protectionist policies influence economic growth. Our
empirical strategy exploits an extraordinary tax scandal that gave rise to an
unexpected change of government in Sweden. A free-trade majority in parliament
was overturned by a protectionist majority in 1887. The protectionist
government increased tariffs. We employ the synthetic control method to select
control countries against which economic growth in Sweden can be compared. We
do not find evidence suggesting that protectionist policies influenced economic
growth and examine channels why. The new tariff laws increased government
revenue. However, the results do not suggest that the protectionist government
stimulated the economy by increasing government expenditure.

This paper documents the representation of women in Economics academia in
India by analyzing the share of women in faculty positions, and their
participation in a prestigious conference held annually. Data from the elite
institutions shows that the presence of women as the Economics faculty members
remains low. Of the authors of the papers which were in the final schedule of
the prestigious research conference, the proportion of women authors is again
found to be disproportionately low. Our findings from further analysis indicate
that women are not under-represented at the post-graduate level. Further, the
proportion of women in doctoral programmes has increased over time, and is now
almost proportionate. Tendency of women who earn a doctorate abroad, to not
return to India, time needed to complete a doctoral program, and
responsibilities towards the family may explain lower presence of women in
Economics academia in India.

The COVID-19 pandemic constitutes one of the largest threats in recent
decades to the health and economic welfare of populations globally. In this
paper, we analyze different types of policy measures designed to fight the
spread of the virus and minimize economic losses. Our analysis builds on a
multi-group SEIR model, which extends the multi-group SIR model introduced by
Acemoglu et al.~(2020). We adjust the underlying social interaction patterns
and consider an extended set of policy measures. The model is calibrated for
Germany. Despite the trade-off between COVID-19 prevention and economic
activity that is inherent to shielding policies, our results show that
efficiency gains can be achieved by targeting such policies towards different
age groups. Alternative policies such as physical distancing can be employed to
reduce the degree of targeting and the intensity and duration of shielding. Our
results show that a comprehensive approach that combines multiple policy
measures simultaneously can effectively mitigate population mortality and
economic harm.

We consider demand-side economy. Using Caratheodory's approach, we define
empirical existence of equation of state (EoS) and coordinates. We found new
insights of thermodynamics EoS, the {\it effect structure}. Rules are proposed
as criteria in promoting and classifying an empirical law to EoS status. Four
laws of thermodynamics are given for economics. We proposed a method to model
the EoS with econometrics. Consumer surplus in economics can not be considered
as utility. Concepts such as total wealth, generalized utility and generalized
surplus are introduced. EoS provides solid foundation in statistical mechanics
modelling of economics and finance.

This is an expanded version of the lecture given at the AMS Short Course on
Mean Field Games, on January 13, 2020 in Denver CO. The assignment was to
discuss applications of Mean Field Games in finance and economics. I need to
admit upfront that several of the examples reviewed in this chapter were
already discussed in book form. Still, they are here accompanied with
discussions of, and references to, works which appeared over the last three
years. Moreover, several completely new sections are added to show how recent
developments in financial engineering and economics can benefit from being
viewed through the lens of the Mean Field Game paradigm. The new financial
engineering applications deal with bitcoin mining and the energy markets, while
the new economic applications concern models offering a smooth transition
between macro-economics and finance, and contract theory.

In this paper, we clarify reconstruction-based discretization schemes for
unstructured grids and discuss their economically high-order versions, which
can achieve high-order accuracy under certain conditions at little extra cost.
The clarification leads to one of the most economical approaches: the
flux-and-solution-reconstruction (FSR) approach, where highly economical
schemes can be constructed based on an extended kappa-scheme combined with
economical flux reconstruction formulas, achieving up to fifth-order accuracy
(sixth-order with zero dissipation) when a grid is regular. Various economical
FSR schemes are presented and their formal orders of accuracy are verified by
numerical experiments.

The publication is one of the first studies of its kind, devoted to the
economic dimension of crimes against cultural and archaeological heritage. Lack
of research in this area is largely due to irregular global prevalence vague
definition of economic value of the damage these crimes cause to the society at
national and global level, to present and future generations. The author uses
classical models of Becker and Freeman, by modifying and complementing them
with the tools of economics of culture based on the values of non-use. The
model tries to determine the opportunity costs of this type of crime in several
scenarios and based on this to determine the extent of their limitation at an
affordable cost to society and raising public benefits of conservation of World
and National Heritage.

We conduct a sensitivity analysis of a new type of integrated
climate-economic model recently proposed in the literature, where the core
economic component is based on the Goodwin-Keen dynamics instead of a
neoclassical growth model. Because these models can exhibit much richer
behaviour, including multiple equilibria, runaway trajectories and unbounded
oscillations, it is crucial to determine how sensitive they are to changes in
underlying parameters. We focus on four economic parameters (markup rate, speed
of price adjustments, coefficient of money illusion, growth rate of
productivity) and two climate parameters (size of upper ocean reservoir,
equilibrium climate sensitivity) and show how their relative effects on the
outcomes of the model can be quantified by methods that can be applied to an
arbitrary number of parameters.

General equilibrium macroeconomic models are a core tool used by policymakers
to understand a nation's economy. They represent the economy as a collection of
forward-looking actors whose behaviours combine, possibly with stochastic
effects, to determine global variables (such as prices) in a dynamic
equilibrium. However, standard semi-analytical techniques for solving these
models make it difficult to include the important effects of heterogeneous
economic actors. The COVID-19 pandemic has further highlighted the importance
of heterogeneity, for example in age and sector of employment, in macroeconomic
outcomes and the need for models that can more easily incorporate it. We use
techniques from reinforcement learning to solve such models incorporating
heterogeneous agents in a way that is simple, extensible, and computationally
efficient. We demonstrate the method's accuracy and stability on a toy problem
for which there is a known analytical solution, its versatility by solving a
general equilibrium problem that includes global stochasticity, and its
flexibility by solving a combined macroeconomic and epidemiological model to
explore the economic and health implications of a pandemic. The latter
successfully captures plausible economic behaviours induced by differential
health risks by age.

Standard economic theory uses mathematics as its main means of understanding,
and this brings clarity of reasoning and logical power. But there is a
drawback: algebraic mathematics restricts economic modeling to what can be
expressed only in quantitative nouns, and this forces theory to leave out
matters to do with process, formation, adjustment, creation and nonequilibrium.
For these we need a different means of understanding, one that allows verbs as
well as nouns. Algorithmic expression is such a means. It allows verbs
(processes) as well as nouns (objects and quantities). It allows fuller
description in economics, and can include heterogeneity of agents, actions as
well as objects, and realistic models of behavior in ill-defined situations.
The world that algorithms reveal is action-based as well as object-based,
organic, possibly ever-changing, and not fully knowable. But it is strangely
and wonderfully alive.

This paper presents an economic model predictive controller, under the
assumption that the only measurable signal of the plant is the economic cost to
be minimized. In order to forecast the evolution of this economic cost for a
given input trajectory, a prediction model with a NARX structure, the so-called
oracle, is proposed. Sufficient conditions to ensure the existence of such
oracle are studied, proving that it can be derived for a general nonlinear
system if the economic cost function is a Morse function. Based on this oracle,
economic model predictive controllers are proposed, and their stability is
demonstrated in nominal conditions under a standard dissipativity assumption.
The viability of these controllers in practical settings (where the oracle may
provide imperfect predictions for generic inputs) is proven by means of
input-to-state stability. These properties have been illustrated in a case
study based on a continuously stirred tank reactor.

This tidal stream energy industry has to date been comprised of small
demonstrator projects made up of one to a four turbines. However, there are
currently plans to expand to commercially sized projects with tens of turbines
or more. As the industry moves to large-scale arrays for the first time, there
has been a push to develop tools to optimise the array design and help bring
down the costs. This review investigates different methods of modelling the
economic performance of tidal-stream arrays, for use within these optimisation
tools. The different cost reduction pathways are discussed from costs falling
as the global installed capacity increases, due to greater experience, improved
power curves through larger-diameter higher-rated turbines, to economic
efficiencies that can be found by moving to large-scale arrays. A literature
review is conducted to establish the most appropriate input values for use in
economic models. This includes finding a best case, worst case and typical
values for costs and other related parameters. The information collated in this
review can provide a useful steering for the many optimisation tools that have
been developed, especially when cost information is commercially sensitive and
a realistic parameter range is difficult to obtain.

Large and acute economic shocks such as the 2007-2009 financial crisis and
the current COVID-19 infections rapidly change the economic environment. In
such a situation, the importance of real-time economic analysis using
alternative datais emerging. Alternative data such as search query and location
data are closer to real-time and richer than official statistics that are
typically released once a month in an aggregated form. We take advantage of
spatio-temporal granularity of alternative data and propose a
mixed-FrequencyAggregate Learning (MF-AGL)model that predicts economic
indicators for the smaller areas in real-time. We apply the model for the
real-world problem; prediction of the number of job applicants which is closely
related to the unemployment rates. We find that the proposed model predicts (i)
the regional heterogeneity of the labor market condition and (ii) the rapidly
changing economic status. The model can be applied to various tasks, especially
economic analysis

Developing ways to affordably deliver broadband connectivity is one of the
major issues of our time. In challenging deployment locations with irregular
terrain, traditional Clear-Line-Of-Sight (CLOS) wireless links can be
uneconomical to deploy, as the number of required towers make infrastructure
investment unviable. With new research focusing on developing wireless
diffractive backhaul technologies to provide Non-Line-Of-Sight (NLOS) links,
this paper evaluates the engineering-economic implications. A Three-Dimensional
(3D) techno-economic assessment framework is developed, utilizing a combination
of remote sensing and viewshed geospatial techniques, in order to quantify the
impact of different wireless backhaul strategies. This framework is applied to
assess both Clear-Line-Of-Sight and diffractive Non-Line-Of-Sight strategies
for deployment in Peru, as well as the islands of Kalimantan and Papua, in
Indonesia. The results find that a hybrid strategy combining the use of
Clear-Line-Of-Sight and diffractive Non-Line-Of-Sight links produces a 9-45
percent cost-efficiency saving, relative to only using traditional
Clear-Line-Of-Sight wireless backhaul links.

This paper examines the relationship between net FDI inflows and real GDP for
Turkey from 1970 to 2019. Although conventional economic growth theories and
most empirical research suggest that there is a bi-directional positive effect
between these macro variables, the results indicate that there is a
uni-directional significant short-run positive effect of real GDP on net FDI
inflows to Turkey by employing the Vector Error Correction Model, Granger
Causality, Impulse Response Functions and Variance Decomposition. Also, there
is no long-run effect has been found. The findings recommend Turkish
authorities optimally benefit from the potential positive effect of net
incoming FDI on the real GDP by allocating it for the productive sectoral
establishments while effectively maintaining the country's real economic growth
to attract further FDI inflows.

Space weather is a collective term for different solar or space phenomena
that can detrimentally affect technology. However, current understanding of
space weather hazards is still relatively embryonic in comparison to
terrestrial natural hazards such as hurricanes or earthquakes. Indeed, certain
types of space weather such as large Coronal Mass Ejections (CMEs) are an
archetypal example of a low probability, high severity hazard. Few major
events, short time-series data and a lack of consensus regarding the potential
impacts on critical infrastructure have hampered the economic impact assessment
of space weather. Yet, space weather has the potential to disrupt a wide range
of Critical National Infrastructure (CNI) systems including electricity
transmission, satellite communications and positioning, aviation and rail
transportation. Recently there has been growing interest in these potential
economic and societal impacts. Estimates range from millions of dollars of
equipment damage from the Quebec 1989 event, to some analysts reporting
billions of lost dollars in the wider economy from potential future disaster
scenarios. Hence, this provides motivation for this article which tracks the
origin and development of the socio-economic evaluation of space weather, from
1989 to 2017, and articulates future research directions for the field.

Most explanations of economic growth are based on knowledge spillovers, where
the development of some technologies facilitates the enhancement of others.
Empirical studies show that these spillovers can have a heterogeneous and
rather complex structure. But, so far, little attention has been paid to the
consequences of different structures of such cross-technology interactions: Is
economic development more easily fostered by homogenous or heterogeneous
interactions, by uni- or bidirectional spillovers? Using a detailed description
of an r&d sector with cross-technology interactions embedded in a simple growth
model, we analyse how the structure of spillovers influences growth prospects
and growth patterns. We show that some type of interactions (e.g., one-way
interactions) cannot induce exponential growth, whereas other structures can.
Furthermore, depending on the structure of interactions, all or only some
technologies will contribute to growth in the long run. Finally, some spillover
structures can lead to complex growth patterns, such as technology transitions,
where, over time, different technology clusters are the main engine of growth.

This paper attempts to analyse policymaking in the field of Intellectual
Property (IP) as an instrument of economic growth across the Global North and
South. It begins by studying the links between economic growth and IP, followed
by an understanding of Intellectual Property Rights (IPR) development in the
US, a leading proponent of robust IPR protection internationally. The next
section compares the IPR in the Global North and South and undertakes an
analysis of the diverse factors that result in these differences. The paper
uses the case study of the Indian Pharmaceutical Industry to understand how IPR
may differentially affect economies and conclude that there may not yet be a
one size fits all policy for the adoption of Intellectual Property Rights.

This paper presents a robust economic model predictive control (EMPC)
formulation with zone tracking for discrete-time uncertain nonlinear systems.
The proposed design ensures that the zone tracking objective is achieved in
finite steps and at the same time optimizes the economic performance. In the
proposed design, instead of tracking the original target zone, a robust control
invariant set within the target zone is determined and is used as the actual
zone tracked in the proposed EMPC. This approach ensures that the zone tracking
objective is achieved within finite steps and once the zone tracking objective
is achieved (the system state enters the robust control invariant set), the
system state does not come out of the target zone anymore. To optimize the
economic performance within the zone in the presence of disturbances, we
introduce the notion of risk factor in the controller design. An algorithm to
determine the economic zone to be tracked is provided. The risk factor
determines the conservativeness of the controller and provides a way to tune
the EMPC for better economic performance. A nonlinear chemical example is
presented to demonstrate the performance of the proposed formulation.

We review different classes of cryptocurrencies with emphasis on their
economic properties. Pure-asset coins such as Bitcoin, Ethereum and Ripple are
characterized by not being a liability of any economic agent and most resemble
commodities such as gold. Central bank digital currencies, at the other end of
the economic spectrum, are liabilities of a Central Bank and most resemble
cash. In between, there exist a range of so-called stable coins, with varying
degrees of economic complexity. We use balance sheet operations to highlight
the properties of each class of cryptocurrency and their potential uses. In
addition, we propose the basic structure for a macroeconomic model
incorporating all the different types of cryptocurrencies under consideration.

I develop a rather simple agent-based model to capture a co-evolution of
opinion formation, political decision making and economic outcomes. I use this
model to study how societies form opinions if their members have opposing
interests. Agents are connected in a social network and exchange opinions, but
differ with regard to their interests and ability to gain information about
them. I show that inequality in information and economic resources can have a
drastic impact on aggregated opinion. In particular, my model illustrates how a
tiny, but well-informed minority can influence group decisions to their favor.
This effect is amplified if these agents are able to command more economic
resources to advertise their views and if they can target their advertisements
efficiently, as made possible by the rise of information technology. My results
contribute to the understanding of pressing questions such as climate change
denial and highlight the dangers that economic and information inequality can
pose for democracies.

Techno-economic assessment is a fundamental technique engineers use for
evaluating new communications technologies. However, despite the
techno-economics of the fifth cellular generation (5G) being an active research
area, it is surprising there are few comprehensive evaluations of this growing
literature. With mobile network operators deploying 5G across their networks,
it is therefore an opportune time to appraise current accomplishments and
review the state-of-the-art. Such insight can inform the flurry of 6G research
papers currently underway and help engineers in their mission to provide
affordable high-capacity, low-latency broadband connectivity, globally. The
survey discusses emerging trends from the 5G techno-economic literature and
makes five key recommendations for the design and standardization of Next
Generation 6G wireless technologies.

A recent approach to automated mechanism design, differentiable economics,
represents auctions by rich function approximators and optimizes their
performance by gradient descent. The ideal auction architecture for
differentiable economics would be perfectly strategyproof, support multiple
bidders and items, and be rich enough to represent the optimal (i.e.
revenue-maximizing) mechanism. So far, such an architecture does not exist.
There are single-bidder approaches (MenuNet, RochetNet) which are always
strategyproof and can represent optimal mechanisms. RegretNet is multi-bidder
and can approximate any mechanism, but is only approximately strategyproof. We
present an architecture that supports multiple bidders and is perfectly
strategyproof, but cannot necessarily represent the optimal mechanism. This
architecture is the classic affine maximizer auction (AMA), modified to offer
lotteries. By using the gradient-based optimization tools of differentiable
economics, we can now train lottery AMAs, competing with or outperforming prior
approaches in revenue.

This chapter provides new evidence on educational inequality and reviews the
literature on the causes and consequences of unequal education. We document
large achievement gaps between children from different socio-economic
backgrounds, show how patterns of educational inequality vary across countries,
time, and generations, and establish a link between educational inequality and
social mobility. We interpret this evidence from the perspective of economic
models of skill acquisition and investment in human capital. The models account
for different channels underlying unequal education and highlight how
endogenous responses in parents' and children's educational investments
generate a close link between economic inequality and educational inequality.
Given concerns over the extended school closures during the Covid-19 pandemic,
we also summarize early evidence on the impact of the pandemic on children's
education and on possible long-run repercussions for educational inequality.

This study examined the relationship between trade facilitation and economic
growth among the middle-income countries from 2010 to 2020 using 94 countries
made up of 48 lower-middle-income countries and 46 upper-middle-income
countries. The study utilized both difference and system Generalised Method of
Moments (GMM) since the cross-sections (N) were greater than the periods (T).
The study found that container port traffic, quality of trade and
transport-related infrastructure have a strong influence on imports and exports
of goods and national income while trade tariff hurts the growth of the
countries. The study also found that most of the trade facilitation indicators
indicated a weak positive influence on trade flows and economic growth. Based
on these findings, the study recommends that reforms aimed at significantly
lowering the costs of trading across borders among middle-income countries
should be highly prioritized in policy formulations, with a focus on the export
side by reducing at-the-border documentation, time, and real costs of trading
across borders while the international organizations should continue to report
the set of Trade Facilitation Indicators (TFIs) that identify areas for action
and enable the potential impact of reforms to be assessed.

The efficient market hypothesis (EMH), based on rational expectations and
market equilibrium, is the dominant perspective for modelling economic markets.
However, the most notable critique of the EMH is the inability to model periods
of out-of-equilibrium behaviour in the absence of any significant external
news. When such dynamics emerge endogenously, the traditional economic
frameworks provide no explanation for such behaviour and the deviation from
equilibrium. This work offers an alternate perspective explaining the
endogenous emergence of punctuated out-of-equilibrium dynamics based on bounded
rational agents. In a concise market entrance game, we show how boundedly
rational strategic reasoning can lead to endogenously emerging crises,
exhibiting fat tails in "returns". We also show how other common stylised facts
of economic markets, such as clustered volatility, can be explained due to
agent diversity (or lack thereof) and the varying learning updates across the
agents. This work explains various stylised facts and crisis emergence in
economic markets, in the absence of any external news, based purely on agent
interactions and bounded rational reasoning.

The COVID-19 pandemic and the mitigation policies implemented in response to
it have resulted in economic losses worldwide. Attempts to understand the
relationship between economics and epidemiology has lead to a new generation of
integrated mathematical models. The data needs for these models transcend those
of the individual fields, especially where human interaction patterns are
closely linked with economic activity. In this article, we reflect upon
modelling efforts to date, discussing the data needs that they have identified,
both for understanding the consequences of the pandemic and policy responses to
it through analysis of historic data and for the further development of this
new and exciting interdisciplinary field.

This paper investigates volumetric grid tariff designs under consideration of
different pricing mechanisms and resulting cost allocation across
socio-techno-economic consumer categories. In a case study of 1.56 million
Danish households divided into 90 socio-techno-economic categories, we compare
three alternative grid tariffs and investigate their impact on annual
electricity bills. The results of our design consisting of a time-dependent
threshold penalizing individual peak consumption and a system peak tariff show
(a) a range of different allocations that distribute the burden of additional
grid costs across both technologies and (b) strong positive outcomes, including
reduced expenses for lower-income groups and smaller households.

Massively multiplayer online role-playing games often contain sophisticated
in-game economies. Many important real-world economic phenomena, such as
inflation, economic growth, and business cycles, are also present in these
virtual economies. One major difference between real-world and virtual
economies is the ease and frequency by which a policymaker, in this case, a
game developer, can introduce economic shocks. These economic shocks, typically
implemented with game updates or signaled through community channels, provide
fertile ground to study the effects of economic interventions on markets. In
this work, we study the effect of in-game economic market interventions,
namely, a transaction tax and an item sink, in Old School RuneScape. Using
causal inference methods, we find that the tax did not meaningfully affect the
trading volume of items at the tax boundaries and that the item sink
contributed to the inflation of luxury good prices, without reducing trade
volume. Furthermore, we find evidence that the illicit gold trading market was
relatively unaffected by the implemented market interventions. Our findings
yield useful insights not only into the effect of market interventions in
virtual economies but also for real-world markets.

There has been a long-running debate in Information Technology (IT) and
economics literature about the contrary arguments of IT concerning
digitalization and the economic growth of nations. While many empirical studies
have shown a significant value of IT, others revealed a detrimental impact.
Given the ambiguous results and anecdotal commentary on the increase in
digitalization attributed to the COVID19 global pandemic, this paper aims to
explore the economic growth-digitalization nexus of 59 countries in 7 regions
by employing correlation and regression analyses over the period 2018-2020. The
findings indicate a positive relationship between economic growth and
digitalization for both HIGH and LOW digitalized country categorization and
regional assessment. Consistent with regional results, except for Northern
Africa and Western Asia, and Sub-Saharan Africa regions, the remaining regions
show a positive correlation and regression results. The findings of this study
can be helpful in future prospective national IT and economic development
policies.

We develop a two-region economic geography model with vertical innovations
that improve the quality of manufactured varieties produced in each region. The
chance of innovation depends on the \emph{related variety}, i.e. the importance
of interaction between researchers within the same region rather than across
different regions. As economic integration increases from a low level, a higher
related variety is associated with more agglomerated spatial configurations.
However, if the interaction with foreign scientists is relatively more
important for innovation, economic activities may (completely) re-disperse
after an initial phase of agglomeration due to the increase in the relative
importance of a higher chance of innovation in the less industrialized region.
This non-monotonic relationship between economic integration and spatial
imbalances may exhibit very diverse qualitative properties, not yet described
in the literature.

The report discusses the emergence of the Socio-Economic Soft Matter (SE-SM)
as the result of interactions between physics and economy. First, demographic
changes since the Industrial Revolution onset are tested using Soft Matter
science tools. Notable in the support of innovative derivative-based and
distortions-sensitive analytic tools. It revealed the Weibull type powered
exponential increase, with a notably lesser rising rate since the crossover
detected near the year 1970. Subsequently, demographic (SE-SM) patterns are
tested for Rapa Nui (Easter) Island model case and for four large 'hallmark
cities' where the rise and decay phases have occurred. They are Detroit and
Cleveland in the USA and Lodz (former textile industry center) and Bytom
(former coal mining center) in Poland. The analysis explicitly revealed scaling
patterns for demographic changes, influenced by the historical and
socio-economic backgrounds and the long-lasting determinism in population
changes. Universalistic features of demographic changes are discussed within
the Socio-Economic Soft Matter concept.

Coffee leaf rust is a prevalent botanical disease that causes a worldwide
reduction in coffee supply and its quality, leading to immense economic losses.
While several pandemic intervention policies (PIPs) for tackling this rust
pandemic are commercially available, they seem to provide only partial
epidemiological relief for farmers. In this work, we develop a high-resolution
economical-epidemiological model that captures the rust pandemic's spread in
coffee tree farms and its associated economic impact. Through extensive
simulations for the case of Colombia, a country that consists mostly of
small-size coffee farms and is the second-largest coffee producer in the world,
our results show that it is economically impractical to sustain any profit
without directly tackling the rust pandemic. Furthermore, even in the
hypothetical case where farmers perfectly know their farm's epidemiological
state and the weather in advance, any rust pandemic-related efforts can only
amount to a limited profit of roughly 4% on investment. In the more realistic
case, any rust pandemic-related efforts are expected to result in economic
losses, indicating that major disturbances in the coffee market are
anticipated.

This paper presents a novel machine learning approach to GDP prediction that
incorporates volatility as a model weight. The proposed method is specifically
designed to identify and select the most relevant macroeconomic variables for
accurate GDP prediction, while taking into account unexpected shocks or events
that may impact the economy. The proposed method's effectiveness is tested on
real-world data and compared to previous techniques used for GDP forecasting,
such as Lasso and Adaptive Lasso. The findings show that the
Volatility-weighted Lasso method outperforms other methods in terms of accuracy
and robustness, providing policymakers and analysts with a valuable tool for
making informed decisions in a rapidly changing economic environment. This
study demonstrates how data-driven approaches can help us better understand
economic fluctuations and support more effective economic policymaking.
  Keywords: GDP prediction, Lasso, Volatility, Regularization, Macroeconomics
Variable Selection, Machine Learning JEL codes: C22, C53, E37.

The Information and Communication sector has undoubtedly played a pivotal
role in changing the way people live nowadays. Almost every area of our lives
is affected by the presence and the use of the new information and
communication technologies. In this regard, many researchers' attention has
been attracted by the influence or the significant impact of these technologies
on economic growth and development. Although the history of South Africa has
had some drawbacks that could constitute a big obstacle to the emergence of a
successful economic environment, the actual status of the country regarding its
economy and the role that it plays in Africa towards the rest of the African
countries is a vital example of an emerging economic force in Africa. This
paper examines the crucial role that ICT has played and is still playing in the
South African economy growth and more specifically the significance of the
economic effects of the software industry. It makes use of the framework used
by Heavin et al. (2003) to investigate the Irish software industry in order to
analyze the impact of endogenous factors -- national, enterprise and individual
-- on the software industry and its implication on the economic growth in South
Africa.

In this paper, we introduce the novel concept of economic ports, allowing
modular and distributed optimal operation of networked microgrids. Firstly, we
design a novel price-based controller for optimal operation of a single
microgrid and show asymptotic stability. Secondly, we define novel physical and
economic interconnection ports for the microgrid and study the dissipativity
properties of these ports. Lastly, we propose an interconnection scheme for
microgrids via the economic ports. This interconnection scheme requires only an
exchange of the local prices and allows a globally economic optimal operation
of networked microgrids at steady state, while guaranteeing asymptotic
stability of the networked microgrids via the passivity properties of economic
ports. The methods are demonstrated through various academic examples.

A representation of economic activity in the form of a law of conservation of
value is presented based on the definition of value as potential to act in an
environment. This allows the encapsulation of the term as a conserved quantity
throughout transactions. Marginal value and speed of marginal value are defined
as derivatives of value and marginal value, respectively. Traditional economic
statements are represented here as cycles of value where value is conserved.
Producer-consumer dyads, shortage and surplus, as well as the role of the value
in representing the market and the economy are explored. The role of the
government in the economy is also explained through the cycles of value the
government is involved in. Traditional economic statements and assumptions
produce existing hypotheses as outcomes of the law of conservation of value.

This paper aims to assess the impact of COVID-19 on the public finance of
Chinese local governments, with a particular focus on the effect of lockdown
measures on startups during the pandemic. The outbreak has placed significant
fiscal pressure on local governments, as containment measures have led to
declines in revenue and increased expenses related to public health and social
welfare. In tandem, startups have faced substantial challenges, including
reduced funding and profitability, due to the negative impact of lockdown
measures on entrepreneurship. Moreover, the pandemic has generated short- and
long-term economic shocks, affecting both employment and economic recovery. To
address these challenges, policymakers must balance health concerns with
economic development. In this regard, the government should consider
implementing more preferential policies that focus on startups to ensure their
survival and growth. Such policies may include financial assistance, tax
incentives, and regulatory flexibility to foster innovation and
entrepreneurship. By and large, the COVID-19 pandemic has had a profound impact
on both the public finance of Chinese local governments and the startup
ecosystem. Addressing the challenges faced by local governments and startups
will require a comprehensive approach that balances health and economic
considerations and includes targeted policies to support entrepreneurship and
innovation.

The paper examined the impact of agricultural credit on economic growth in
Bangladesh. The annual data of agriculture credit were collected from annual
reports of the Bangladesh Bank and other data were collected from the world
development indicator (WDI) of the World Bank. By employing Johansen
cointegration test and vector error correction model (VECM), the study revealed
that there exists a long run relationship between the variables. The results of
the study showed that agriculture credit had a positive impact on GDP growth in
Bangladesh. The study also found that gross capital formation had a positive,
while inflation had a negative association with economic growth in Bangladesh.
Therefore, the government and policymakers should continue their effort to
increase the volume of agriculture credit to achieve sustainable economic
growth.

In economic modeling, there has been an increasing investigation into
multi-agent simulators. Nevertheless, state-of-the-art studies establish the
model based on reinforcement learning (RL) exclusively for specific agent
categories, e.g., households, firms, or the government. It lacks concerns over
the resulting adaptation of other pivotal agents, thereby disregarding the
complex interactions within a real-world economic system. Furthermore, we pay
attention to the vital role of the government policy in distributing tax
credits. Instead of uniform distribution considered in state-of-the-art, it
requires a well-designed strategy to reduce disparities among households and
improve social welfare. To address these limitations, we propose an expansive
multi-agent economic model comprising reinforcement learning agents of numerous
types. Additionally, our research comprehensively explores the impact of tax
credit allocation on household behavior and captures the spectrum of spending
patterns that can be observed across diverse households. Further, we propose an
innovative government policy to distribute tax credits, strategically
leveraging insights from tax credit spending patterns. Simulation results
illustrate the efficacy of the proposed government strategy in ameliorating
inequalities across households.

Although high-resolution gridded climate variables are provided by multiple
sources, the need for country and region-specific climate data weighted by
indicators of economic activity is becoming increasingly common in
environmental and economic research. We process available information from
different climate data sources to provide spatially aggregated data with global
coverage for both countries (GADM0 resolution) and regions (GADM1 resolution)
and for a variety of climate indicators (average precipitations, average
temperatures, average SPEI). We weigh gridded climate data by population
density or by night light intensity -- both proxies of economic activity --
before aggregation. Climate variables are measured daily, monthly, and
annually, covering (depending on the data source) a time window from 1900 (at
the earliest) to 2023. We pipeline all the preprocessing procedures in a
unified framework, which we share in the open-access Weighted Climate Data
Repository web app. Finally, we validate our data through a systematic
comparison with those employed in leading climate impact studies.

As AI adoption accelerates, research on its economic impacts becomes a
salient source to consider for stakeholders of AI policy. Such research is
however still in its infancy, and one in need of review. This paper aims to
accomplish just that and is structured around two main themes. Firstly, the
path towards transformative AI, and secondly the wealth created by it. It is
found that sectors most embedded into global value chains will drive economic
impacts, hence special attention is paid to the international trade
perspective. When it comes to the path towards transformative AI, research is
heterogenous in its predictions, with some predicting rapid, unhindered
adoption, and others taking a more conservative view based on potential
bottlenecks and comparisons to past disruptive technologies. As for wealth
creation, while some agreement is to be found in AI's growth boosting
abilities, predictions on timelines are lacking. Consensus exists however
around the dispersion of AI induced wealth, which is heavily biased towards
developed countries due to phenomena such as anchoring and reduced bargaining
power of developing countries. Finally, a shortcoming of economic growth models
in failing to consider AI risk is discovered. Based on the review, a
calculated, and slower adoption rate of AI technologies is recommended.

Economic choice prediction is an essential challenging task, often
constrained by the difficulties in acquiring human choice data. Indeed,
experimental economics studies had focused mostly on simple choice settings.
The AI community has recently contributed to that effort in two ways:
considering whether LLMs can substitute for humans in the above-mentioned
simple choice prediction settings, and the study through ML lens of more
elaborated but still rigorous experimental economics settings, employing
incomplete information, repetitive play, and natural language communication,
notably language-based persuasion games. This leaves us with a major
inspiration: can LLMs be used to fully simulate the economic environment and
generate data for efficient human choice prediction, substituting for the
elaborated economic lab studies? We pioneer the study of this subject,
demonstrating its feasibility. In particular, we show that a model trained
solely on LLM-generated data can effectively predict human behavior in a
language-based persuasion game, and can even outperform models trained on
actual human data.

The outbreak of COVID-19 has highlighted the intricate interplay between
public health and economic stability on a global scale. This study proposes a
novel reinforcement learning framework designed to optimize health and economic
outcomes during pandemics. The framework leverages the SIR model, integrating
both lockdown measures (via a stringency index) and vaccination strategies to
simulate disease dynamics. The stringency index, indicative of the severity of
lockdown measures, influences both the spread of the disease and the economic
health of a country. Developing nations, which bear a disproportionate economic
burden under stringent lockdowns, are the primary focus of our study. By
implementing reinforcement learning, we aim to optimize governmental responses
and strike a balance between the competing costs associated with public health
and economic stability. This approach also enhances transparency in
governmental decision-making by establishing a well-defined reward function for
the reinforcement learning agent. In essence, this study introduces an
innovative and ethical strategy to navigate the challenge of balancing public
health and economic stability amidst infectious disease outbreaks.

In this paper, we introduce EconLogicQA, a rigorous benchmark designed to
assess the sequential reasoning capabilities of large language models (LLMs)
within the intricate realms of economics, business, and supply chain
management. Diverging from traditional benchmarks that predict subsequent
events individually, EconLogicQA poses a more challenging task: it requires
models to discern and sequence multiple interconnected events, capturing the
complexity of economic logics. EconLogicQA comprises an array of multi-event
scenarios derived from economic articles, which necessitate an insightful
understanding of both temporal and logical event relationships. Through
comprehensive evaluations, we exhibit that EconLogicQA effectively gauges a
LLM's proficiency in navigating the sequential complexities inherent in
economic contexts. We provide a detailed description of EconLogicQA dataset and
shows the outcomes from evaluating the benchmark across various leading-edge
LLMs, thereby offering a thorough perspective on their sequential reasoning
potential in economic contexts. Our benchmark dataset is available at
https://huggingface.co/datasets/yinzhu-quan/econ_logic_qa.

Tackling real-world socio-economic challenges requires designing and testing
economic policies. However, this is hard in practice, due to a lack of
appropriate (micro-level) economic data and limited opportunity to experiment.
In this work, we train social planners that discover tax policies in dynamic
economies that can effectively trade-off economic equality and productivity. We
propose a two-level deep reinforcement learning approach to learn dynamic tax
policies, based on economic simulations in which both agents and a government
learn and adapt. Our data-driven approach does not make use of economic
modeling assumptions, and learns from observational data alone. We make four
main contributions. First, we present an economic simulation environment that
features competitive pressures and market dynamics. We validate the simulation
by showing that baseline tax systems perform in a way that is consistent with
economic theory, including in regard to learned agent behaviors and
specializations. Second, we show that AI-driven tax policies improve the
trade-off between equality and productivity by 16% over baseline policies,
including the prominent Saez tax framework. Third, we showcase several emergent
features: AI-driven tax policies are qualitatively different from baselines,
setting a higher top tax rate and higher net subsidies for low incomes.
Moreover, AI-driven tax policies perform strongly in the face of emergent
tax-gaming strategies learned by AI agents. Lastly, AI-driven tax policies are
also effective when used in experiments with human participants. In experiments
conducted on MTurk, an AI tax policy provides an equality-productivity
trade-off that is similar to that provided by the Saez framework along with
higher inverse-income weighted social welfare.

At what level should government or companies support research? This complex
multi-faceted question encompasses such qualitative bonus as satisfying natural
human curiosity, the quest for knowledge and the impact on education and
culture, but one of its most scrutinized component reduces to the assessment of
economic performance and wealth creation derived from research. In certain
areas such as biotechnology, semi-conductor physics, optical communications,
the impact of basic research is direct while, in other disciplines, the path
from discovery to applications is full of surprises. As a consequence, there
are persistent uncertainties in the quantification of the exact economic
returns of public expenditure on basic research. Here, we suggest that these
uncertainties have a fundamental origin to be found in the interplay between
the intrinsic ``fat tail'' power law nature of the distribution of economic
returns, characterized by a mathematically diverging variance, and the
stochastic character of discovery rates. In the regime where the cumulative
economic wealth derived from research is expected to exhibit a long-term
positive trend, we show that strong fluctuations blur out significantly the
short-time scales: a few major unpredictable innovations may provide a finite
fraction of the total creation of wealth. In such a scenario, any attempt to
assess the economic impact of research over a finite time horizon encompassing
only a small number of major discoveries is bound to be highly unreliable. New
tools, developed in the theory of self-similar and complex systems to tackle
similar extreme fluctuations in Nature can be adapted to measure the economic
benefits of research, which is intimately associated to this large variability.

The metabolic state of a cell, comprising fluxes, metabolite concentrations
and enzyme levels, is shaped by a compromise between metabolic benefit and
enzyme cost. This hypothesis and its consequences can be studied by
computational models and using a theory of metabolic value. In optimal
metabolic states, any increase of an enzyme level must improve the metabolic
performance to justify its own cost, so each active enzyme must contribute to
the cell's benefit by producing valuable products. This principle of value
production leads to variation rules that relate metabolic fluxes and reaction
elasticities to enzyme costs. Metabolic value theory provides a language to
describe this. It postulates a balance of local values, which I derive here
from concepts of metabolic control theory. Economic state variables, called
economic potentials and loads, describe how metabolites, reactions, and enzymes
contribute to metabolic performance. Economic potentials describe the indirect
value of metabolite production, while economic loads describe the indirect
value of metabolite concentrations. These economic variables, and others, are
linked by local balance equations. These laws for optimal metabolic states
define conditions for metabolic fluxes that hold for a wide range of rate laws.
To produce metabolic value, fluxes run from lower to higher economic
potentials, must be free of futile cycles, and satisfy a principle of minimal
weighted fluxes. Given an economical flux mode, one can systematically
construct kinetic models in which all enzymes have positive effects on
metabolic performance.

A country's mix of products predicts its subsequent pattern of
diversification and economic growth. But does this product mix also predict
income inequality? Here we combine methods from econometrics, network science,
and economic complexity to show that countries exporting complex products (as
measured by the Economic Complexity Index) have lower levels of income
inequality than countries exporting simpler products. Using multivariate
regression analysis, we show that economic complexity is a significant and
negative predictor of income inequality and that this relationship is robust to
controlling for aggregate measures of income, institutions, export
concentration, and human capital. Moreover, we introduce a measure that
associates a product to a level of income inequality equal to the average GINI
of the countries exporting that product (weighted by the share the product
represents in that country's export basket). We use this measure together with
the network of related products (or product space) to illustrate how the
development of new products is associated with changes in income inequality.
These findings show that economic complexity captures information about an
economy's level of development that is relevant to the ways an economy
generates and distributes its income. Moreover, these findings suggest that a
country's productive structure may limit its range of income inequality.
Finally, we make our results available through an online resource that allows
for its users to visualize the structural transformation of over 150 countries
and their associated changes in income inequality between 1963 and 2008.

Peru's abundant natural resources and friendly trade policies has made the
country a major economic player in both South America and the global community.
Consequently, exports are playing an increasingly important role in Peru's
national economy. Indeed, growing from 13.1% as of 1994, exports now contribute
approximately 21% of the GDP of Peru as of 2015. Given Peru's growing global
influence, the time is ripe for a thorough analysis of the most important
factors governing its export performance. Thus, within the framework of the
augmented gravity model of trade, this paper examines Peru's export performance
and attempts to identify the dominant economic factors that should be further
developed to increase the value of exports. The analysis was conducted from
three different aspects: (1) general economic parameters' effect on Peru's
export value, (2) more specific analysis into a major specific trade good,
copper, and (3) the impact that regional trade agreements have had on Peru's
export performance. Our panel data analysis results for each dataset revealed
interesting economic trends and were consistent with the theoretical
expectations of the gravity model: namely positive coefficients for economic
size and negative coefficients for distance. This report's results can be a
reference for the proper direction of Peruvian economic policy so as to enhance
economic growth in a sustainable direction.

This paper models macro financial variables alike to financial fluids with
local interactions and describes surface-like waves of Investment and Profits.
We regard macro-finance as ensemble of economic agents and use their risk
ratings as coordinates on economic space. Aggregations of agent's financial
variables with risk coordinates x on economic space define macro financial
variables as function of x. We describe evolution and interactions between
macro financial variables alike to financial fluids by hydrodynamic-like
equations. Minimum and maximum risk grades define most secure and most risky
agents respectively. That determines borders of macro-finance domain that is
filled by economic agents. Perturbations of agent's risk coordinates near risk
borders of macro domain cause disturbances of macro financial variables like
Investment and Profits. Such disturbances can generate waves that propagate
along risk borders. These waves may exponentially amplify perturbations inside
of macro domain and impact financial sustainability. We study simple model
Investment and Profits and describe linear approximation of steady state
distributions of Investment and Profits on macro-finance domain that fulfill
dreams of Investors: "more risks-more Profits". We describe Investment and
Profits waves on risk border of economic space alike to surface waves in
fluids. We present simple examples that specify waves as possible origin of
time fluctuations of macro financial variables. Description of possible steady
state distributions of macro financial variables and financial risk waves on
economic space could help for better policy-making and managing sustainable
macro-finance.

We develop a machine-learning-based method, Principal Smooth-Dynamics
Analysis (PriSDA), to identify patterns in economic development and to automate
the development of new theory of economic dynamics. Traditionally, economic
growth is modeled with a few aggregate quantities derived from simplified
theoretical models. Here, PriSDA identifies important quantities. Applied to 55
years of data on countries' exports, PriSDA finds that what most distinguishes
countries' export baskets is their diversity, with extra weight assigned to
more sophisticated products. The weights are consistent with previous measures
of product complexity in the literature. The second dimension of variation is a
proficiency in machinery relative to agriculture. PriSDA then couples these
quantities with per-capita income and infers the dynamics of the system over
time. According to PriSDA, the pattern of economic development of countries is
dominated by a tendency toward increased diversification. Moreover, economies
appear to become richer after they diversify (i.e., diversity precedes growth).
The model predicts that middle-income countries with diverse export baskets
will grow the fastest in the coming decades, and that countries will converge
onto intermediate levels of income and specialization. PriSDA is generalizable
and may illuminate dynamics of elusive quantities such as diversity and
complexity in other natural and social systems.

Economic theory is a mathematically rich field in which there are
opportunities for the formal analysis of singularities and catastrophes. This
article looks at the historical context of singularities through the work of
two eminent Frenchmen around the late 1960s and 1970s. Ren\'e Thom (1923-2002)
was an acclaimed mathematician having received the Fields Medal in 1958,
whereas G\'erard Debreu (1921-2004) would receive the Nobel Prize in economics
in 1983. Both were highly influential within their fields and given the
fundamental nature of their work, the potential for cross-fertilisation would
seem to be quite promising. This was not to be the case: Debreu knew of Thom's
work and cited it in the analysis of his own work, but despite this and other
applied mathematicians taking catastrophe theory to economics, the theory never
achieved a lasting following and relatively few results were published. This
article reviews Debreu's analysis of the so called ${\it regular}$ and ${\it
crtitical}$ economies in order to draw some insights into the economic
perspective of singularities before moving to how singularities arise naturally
in the Nash equilibria of game theory. Finally a modern treatment of stochastic
game theory is covered through recent work on the quantal response equilibrium.
In this view the Nash equilibrium is to the quantal response equilibrium what
deterministic catastrophe theory is to stochastic catastrophe theory, with some
caveats regarding when this analogy breaks down discussed at the end.

With the booming economy in China, many researches have pointed out that the
improvement of regional transportation infrastructure among other factors had
an important effect on economic growth. Utilizing a large-scale dataset which
includes 3.5 billion entry and exit records of vehicles along highways
generated from toll collection systems, we attempt to establish the relevance
of mid-distance land transport patterns to regional economic status through
transportation network analyses. We apply standard measurements of complex
networks to analyze the highway transportation networks. A set of traffic flow
features are computed and correlated to the regional economic development
indicator. The multi-linear regression models explain about 89% to 96% of the
variation of cities' GDP across three provinces in China. We then fit gravity
models using annual traffic volumes of cars, buses, and freight trucks between
pairs of cities for each province separately as well as for the whole dataset.
We find the temporal changes of distance-decay effects on spatial interactions
between cities in transportation networks, which link to the economic
development patterns of each province. We conclude that transportation big data
reveal the status of regional economic development and contain valuable
information of human mobility, production linkages, and logistics for regional
management and planning. Our research offers insights into the investigation of
regional economic development status using highway transportation big data.

Social distancing is the primary policy prescription for combating the
COVID-19 pandemic, and has been widely adopted in Europe and North America. We
estimate the value of disease avoidance using an epidemiological model that
projects the spread of COVID-19 across rich and poor countries. Social
distancing measures that "flatten the curve" of the disease to bring demand
within the capacity of healthcare systems are predicted to save many lives in
high-income countries, such that practically any economic cost is worth
bearing. These social distancing policies are estimated to be less effective in
poor countries with younger populations less susceptible to COVID-19, and more
limited healthcare systems, which were overwhelmed before the pandemic.
Moreover, social distancing lowers disease risk by limiting people's economic
opportunities. Poorer people are less willing to make those economic
sacrifices. They place relatively greater value on their livelihood concerns
compared to contracting COVID-19. Not only are the epidemiological and economic
benefits of social distancing much smaller in poorer countries, such policies
may exact a heavy toll on the poorest and most vulnerable. Workers in the
informal sector lack the resources and social protections to isolate themselves
and sacrifice economic opportunities until the virus passes. By limiting their
ability to earn a living, social distancing can lead to an increase in hunger,
deprivation, and related mortality and morbidity. Rather than a blanket
adoption of social distancing measures, we advocate for the exploration of
alternative harm-reduction strategies, including universal mask adoption and
increased hygiene measures.

There has been considerable public debate about whether the economic impact
of the current COVID19 restrictions are worth the costs. Although the potential
impact of COVID19 has been modelled extensively, very few numbers have been
presented in the discussions about potential economic impacts. For a good
answer to the question - will the restrictions cause as much harm as COVID19? -
credible evidence-based estimates are required, rather than simply rhetoric.
Here we provide some preliminary estimates to compare the impact of the current
restrictions against the direct impact of the virus. Since most countries are
currently taking an approach that reduces the number of COVID19 deaths, the
estimates we provide for deaths from COVID19 are deliberately taken from the
low end of the estimates of the infection fatality rate, while estimates for
deaths from an economic recession are deliberately computed from double the
high end of confidence interval for severe economic recessions. This ensures
that an adequate challenge to the status quo of the current restrictions is
provided. Our analysis shows that strict restrictions to eradicate the virus
are likely to lead to at least eight times fewer total deaths than an immediate
return to work scenario.

We analyse the economics and epidemiology of different scenarios for a phased
restart of the UK economy. Our economic model is designed to address the unique
features of the COVID-19 pandemic. Social distancing measures affect both
supply and demand, and input-output constraints play a key role in restricting
economic output. Standard models for production functions are not adequate to
model the short-term effects of lockdown. A survey of industry analysts
conducted by IHS Markit allows us to evaluate which inputs for each industry
are absolutely necessary for production over a two month period. Our model also
includes inventory dynamics and feedback between unemployment and consumption.
We demonstrate that economic outcomes are very sensitive to the choice of
production function, show how supply constraints cause strong network effects,
and find some counter-intuitive effects, such as that reopening only a few
industries can actually lower aggregate output. Occupation-specific data and
contact surveys allow us to estimate how different industries affect the
transmission rate of the disease. We investigate six different re-opening
scenarios, presenting our best estimates for the increase in R0 and the
increase in GDP. Our results suggest that there is a reasonable compromise that
yields a relatively small increase in R0 and delivers a substantial boost in
economic output. This corresponds to a situation in which all non-consumer
facing industries reopen, schools are open only for workers who need childcare,
and everyone who can work from home continues to work from home.

Crises like COVID-19 or the Japanese earthquake in 2011 exposed the fragility
of corporate supply networks. The production of goods and services is a highly
interdependent process and can be severely impacted by the default of critical
suppliers or customers. While knowing the impact of individual companies on
national economies is a prerequisite for efficient risk management, the
quantitative assessment of the involved economic systemic risks (ESR) is
hitherto practically non-existent, mainly because of a lack of fine-grained
data in combination with coherent methods. Based on a unique value added tax
dataset we derive the detailed production network of an entire country and
present a novel approach for computing the ESR of all individual firms. We
demonstrate that a tiny fraction (0.035%) of companies has extraordinarily high
systemic risk impacting about 23% of the national economic production should
any of them default. Firm size alone cannot explain the ESR of individual
companies; their position in the production networks does matter substantially.
If companies are ranked according to their economic systemic risk index (ESRI),
firms with a rank above a characteristic value have very similar ESRI values,
while for the rest the rank distribution of ESRI decays slowly as a power-law;
99.8% of all companies have an impact on less than 1% of the economy. We show
that the assessment of ESR is impossible with aggregate data as used in
traditional Input-Output Economics. We discuss how simple policies of
introducing supply chain redundancies can reduce ESR of some extremely risky
companies.

This is an introductory textbook of the history of economics of inequality
for undergraduates and genreral readers. It begins with Adam Smith's critique
of Rousseau. The first and second chapters focus on Smith and Karl Marx, in the
broad classical tradition of economics, where it is believed that there is an
inseparable relationship between production and distribution, economic growth
and inequality. Chapters 3 and 4 argue that despite the fact that the founders
of the neoclassical school had shown an active interest in social issues,
namely worker poverty, the issues of production and distribution became
discussed separately among neoclassicals. Toward the end of the 20th century,
however, there was a renewed awareness within economics of the problem of the
relationship between production and distribution. The young Piketty's
beginnings as an economist are set against this backdrop. Chapters 5 to 8
explain the circumstances of the restoration of classical concerns within the
neoclassical framework. Then, in chapters 9 and 10, I discuss the fact that
Thomas Piketty's seminal work is a new development in this "inequality
renaissance," and try to gain a perspective on future trends in the debate.
Mathematical appendix presents simple models of growth and distribution.

This exercise proposes a learning mechanism to model economic agent's
decision-making process using an actor-critic structure in the literature of
artificial intelligence. It is motivated by the psychology literature of
learning through reinforcing good or bad decisions. In a model of an
environment, to learn to make decisions, this AI agent needs to interact with
its environment and make explorative actions. Each action in a given state
brings a reward signal to the agent. These interactive experience is saved in
the agent's memory, which is then used to update its subjective belief of the
world. The agent's decision-making strategy is formed and adjusted based on
this evolving subjective belief. This agent does not only take an action that
it knows would bring a high reward, it also explores other possibilities. This
is the process of taking explorative actions, and it ensures that the agent
notices changes in its environment and adapt its subjective belief and
decisions accordingly. Through a model of stochastic optimal growth, I
illustrate that the economic agent under this proposed learning structure is
adaptive to changes in an underlying stochastic process of the economy. AI
agents can differ in their levels of exploration, which leads to different
experience in the same environment. This reflects on to their different
learning behaviours and welfare obtained. The chosen economic structure
possesses the fundamental decision making problems of macroeconomic models,
i.e., how to make consumption-saving decisions in a lifetime, and it can be
generalised to other decision-making processes and economic models.

The Ballast Water Management Convention can decrease the introduction risk of
harmful aquatic organisms and pathogens, yet the Convention increases shipping
costs and causes subsequent economic impacts. This paper examines whether the
Convention generates disproportionate invasion risk reduction results and
economic impacts on Small Island Developing States (SIDS) and Least Developed
Countries (LDCs). Risk reduction is estimated with an invasion risk assessment
model based on a higher-order network, and the effects of the regulation on
national economies and trade are estimated with an integrated shipping cost and
computable general equilibrium modeling framework. Then we use the Lorenz curve
to examine if the regulation generates risk or economic inequality among
regions. Risk reduction ratios of all regions (except Singapore) are above 99%,
which proves the effectiveness of the Convention. The Gini coefficient of 0.66
shows the inequality in risk changes relative to income levels among regions,
but risk reductions across all nations vary without particularly high risks for
SIDS and LDCs than for large economies. Similarly, we reveal inequality in
economic impacts relative to income levels (the Gini coefficient is 0.58), but
there is no evidence that SIDS and LDCs are disproportionately impacted
compared to more developed regions. Most changes in GDP, real exports, and real
imports of studied regions are minor (smaller than 0.1%). However, there are
more noteworthy changes for select sectors and trade partners including Togo,
Bangladesh, and Dominican Republic, whose exports may decrease for textiles and
metal and chemicals. We conclude the Convention decreases biological invasion
risk and does not generate disproportionate negative impacts on SIDS and LDCs.

The empirical evidence suggests that key accumulation decisions and risky
choices associated with economic development depend, at least in part, on
economic preferences such as willingness to take risk and patience. This paper
studies whether temperature could be one of the potential channels that
influences such economic preferences. Using data from the Indonesia Family Life
Survey and NASAs Modern Era Retrospective Analysis for Research and
Applications data we exploit quasi exogenous variations in outdoor temperatures
caused by the random allocation of survey dates. This approach allows us to
estimate the effects of temperature on elicited measures of risk aversion,
rational choice violations, and impatience. We then explore three possible
mechanisms behind this relationship, cognition, sleep, and mood. Our findings
show that higher temperatures lead to significantly increased rational choice
violations and impatience, but do not significantly increase risk aversion.
These effects are mainly driven by night time temperatures on the day prior to
the survey and less so by temperatures on the day of the survey. This impact is
quasi linear and increasing when midnight outdoor temperatures are above 22C.
The evidence shows that night time temperatures significantly deplete cognitive
functioning, mathematical skills in particular. Based on these findings we
posit that heat induced night time disturbances cause stress on critical parts
of the brain, which then manifest in significantly lower cognitive functions
that are critical for individuals to perform economically rational decision
making.

Distributed Energy Resources (DERs) can significantly affect the net social
benefit in power systems, raising concerns pertaining to distributive justice,
equity, and fairness. Electricity tariff and DERs share a symbiotic
relationship whereby the design of the former directly impacts the economic
efficiency and equity in the system. Current tariff design approaches suffer
from opaque efficiency-equity trade-offs and are also agnostic of the
externalities that affect both economic efficiency and equity. Therefore, this
paper develops a justice-cognizant tariff design framework that improves the
economic efficiency of tariff without sacrificing its distributional equity,
and encompasses economic welfare, social costs of environmental and public
health impacts, and socio-economic and demographic characteristics of
electricity consumers. The proposed framework is based on a Single Leader
Single Follower (SLSF) game incorporating a multi-objective optimization
problem, and is evaluated on four different tariff structures. The SLSF game is
reformulated as a Multi-Objective Problem with Equilibrium Constraints (MOPEC)
and is solved by integrating the objective sum method for multi-objective
optimization and Scholtes's relaxation technique for equilibrium constraints.
We compare the economic efficiency and equity of the proposed framework using
the 11-zone New York ISO and 7-bus Manhattan power networks. The results
demonstrate that spatially- and temporally-granular tariffs ensure equity and
economic efficiency at a lower energy burden to consumers.

The spreading of Covid-19 pandemic has highlighted the close link between
economics and health in the context of emergency management. A widespread
vaccination campaign is considered the main tool to contain the economic
consequences. This paper will focus, at the level of wealth distribution
modelling, on the economic improvements induced by the vaccination campaign in
terms of its effectiveness rate. The economic trend during the pandemic is
evaluated resorting to a mathematical model joining a classical compartmental
model including vaccinated individuals with a kinetic model of wealth
distribution based on binary wealth exchanges. The interplay between wealth
exchanges and the progress of the infectious disease is realized by assuming on
the one hand that individuals in different compartments act differently in the
economic process and on the other hand that the epidemic affects risk in
economic transactions. Using the mathematical tools of kinetic theory, it is
possible to identify the equilibrium states of the system and the formation of
inequalities due to the pandemic in the wealth distribution of the population.
Numerical experiments highlight the importance of the vaccination campaign and
its positive effects in reducing economic inequalities in the multi-agent
society.

Economic unions are international agreements oriented to increase economic
efficiency and establishing political and cultural ties between the member
countries. Becoming a member of an existing union usually requires the approval
of both the candidate and members, while leaving it may require only the
unilateral will of the exiting country. There are many examples of accession of
states to previously consolidated economic unions, and a recent example of
leaving is the withdrawal of the United Kingdom from the European Union.
Motivated by the Brexit process, in this paper we propose an agent-based model
to study the determinant factors driving withdrawals from an economic union. We
show that both Union and local taxes promote the exits, whereas customs fees
out of the Union boost cohesion. Furthermore, heterogeneity in both business
conditions and wealth distribution promotes withdrawals, while countries' size
diversity does not have a significant effect on them. We also deep into the
individual causes that lead to dissatisfaction and, ultimately, to exits. We
found that, for low Union taxes, the wealth inequality within the country is
the leading cause of anti-Union opinion spreading. Conversely, for high Union
taxes, the country's performance turns out to be the main driving force,
resulting in a risk of wealthier countries leaving the Union. These findings
will be helpful for the design of economic policies and effective informative
campaigns.

Recent strides in economic complexity have shown that the future economic
development of nations can be predicted with a single "economic fitness"
variable, which captures countries' competitiveness in international trade. The
predictions by this low-dimensional approach could match or even outperform
predictions based on much more sophisticated methods, such as those by the
International Monetary Fund (IMF). However, all prior works in economic
complexity aimed to quantify countries' fitness from World Trade export data,
without considering the possibility to infer countries' potential for growth
from alternative sources of data. Here, motivated by the long-standing
relationship between technological development and economic growth, we aim to
forecast countries' growth from patent data. Specifically, we construct a
citation network between countries from the European Patent Office (EPO)
dataset. Initial results suggest that the H-index centrality in this network is
a potential candidate to gauge national economic performance. To validate this
conjecture, we construct a two-dimensional plane defined by the H-index and GDP
per capita, and use a forecasting method based on dynamical systems to test the
predicting accuracy of the H-index. We find that the predictions based on the
H-index-GDP plane outperform the predictions by IMF by approximately 35%, and
they marginally outperform those by the economic fitness extracted from trade
data. Our results could inspire further attempts to identify predictors of
national growth from different sources of data related to scientific and
technological innovation.

Generally, system operators conduct the economic operation of power systems
in an open-loop predict-then-optimize process: the renewable energy source
(RES) availability and system reserve requirements are first predicted; given
the predictions, system operators solve optimization models such as unit
commitment (UC) to determine the economical operation plans accordingly.
However, such an open-loop process could essentially compromise the operation
economics because its predictors myopically seek to improve the immediate
statistical prediction errors instead of the ultimate operation cost. To this
end, this paper presents a closed-loop predict-and-optimize framework, offering
a prescriptive UC to improve the operation economics. First, a bilevel
mixed-integer programming model is leveraged to train cost-oriented predictors
tailored for optimal system operations: the upper level trains the RES and
reserve predictors based on their induced operation cost; the lower level, with
given predictions, mimics the system operation process and feeds the induced
operation cost back to the upper level. Furthermore, the embeddability of the
trained predictors grants a prescriptive UC model, which simultaneously
provides RES-reserve predictions and UC decisions with enhanced operation
economics. Finally, numerical case studies using real-world data illustrate the
potential economic and practical advantages of prescriptive UC over
deterministic, robust, and stochastic UC models.

Production theory, defined as the study of the economic process of
transforming inputs into outputs, consists of two simultaneous economic forces:
cost minimization and profit maximization. The cost minimization problem
involves deriving conditional factor demand functions and the cost function.
The profit maximization problem involves deriving the output supply function,
the profit function, and unconditional factor demand functions. Nested within
the process are Shephard's lemma, Hotelling's lemmas, direct and indirect
mathematical relations, and other elements contributing to the dynamics of the
process. The intricacies and hidden underlying influences pose difficulties in
presenting the material for an instructor, and inhibit learning by students.
Simply put, the primary aim of this paper is to facilitate the teaching and
learning of the production theory realm of Economics through the use of a
conceptual visual model. This paper proposes a pedagogical tool in the form of
a detailed graphic illustrating t he relationship between profit maximization
and cost minimization under technical constraints, with an emphasis on the
similarities and differences between the perfect competition and monopoly
cases. The potential that such a visual has to enhance learning when
supplementing traditional context is discussed under the context of
contemporary learning literature. Embedded in the discussion is an example of
how we believe our model could be conceptualized and utilized in a real-world
setting to evaluate an industrial project with an economic point of view.

Cooperation is fundamental for human prosperity. Blockchain, as a trust
machine, is a cooperative institution in cyberspace that supports cooperation
through distributed trust with consensus protocols. While studies in computer
science focus on fault tolerance problems with consensus algorithms, economic
research utilizes incentive designs to analyze agent behaviors. To achieve
cooperation on blockchains, emerging interdisciplinary research introduces
rationality and game-theoretical solution concepts to study the equilibrium
outcomes of various consensus protocols. However, existing studies do not
consider the possibility for agents to learn from historical observations.
Therefore, we abstract a general consensus protocol as a dynamic game
environment, apply a solution concept of bounded rationality to model agent
behavior, and resolve the initial conditions for three different stable
equilibria. In our game, agents imitatively learn the global history in an
evolutionary process toward equilibria, for which we evaluate the outcomes from
both computing and economic perspectives in terms of safety, liveness,
validity, and social welfare. Our research contributes to the literature across
disciplines, including distributed consensus in computer science, game theory
in economics on blockchain consensus, evolutionary game theory at the
intersection of biology and economics, bounded rationality at the interplay
between psychology and economics, and cooperative AI with joint insights into
computing and social science. Finally, we discuss that future protocol design
can better achieve the most desired outcomes of our honest stable equilibria by
increasing the reward-punishment ratio and lowering both the cost-punishment
ratio and the pivotality rate.

The subject of this study is inflation, a problem that has plagued America
and the world over the last several decades. Despite a rich trove of scholarly
studies and a wide range of tools developed to deal with inflation, we are
nowhere near a solution of this problem. We are now in the middle of the
inflation that threatens to become a stagflation or even a full recession; and
we have no idea what to prevent this outcome. This investigation explores the
real source of inflation. Tracing the problem of inflation to production, it
finds that inflation is not a phenomenon intrinsic to economy; rather, it is a
result of inefficiencies and waste in our economy. The investigation leads to a
conclusion that the solution of the problem of inflation is in achieving full
efficiency in production. Our economic production is a result of the evolution
that is propelled by the process of creation. In order to end economic
inefficiencies, we should model our economic practice on the process that
preceded production and has led to its emergence. In addition, the study will
outline ways in which our economic theory and practice must be changed to
achieve full efficiency of our production. Finally, the study provides a
critical overview of the current theories of inflation and remedies that are
proposed to deal with it.

We propose a social network-aware approach to studying socio-economic
segregation. The key question that we address is whether patterns of
segregation are more pronounced in social networks than the common spatial
neighborhood-focused manifestations of segregation. We, therefore, conduct a
population-scale social network analysis to study socio-economic segregation at
a comprehensive and highly granular social network level: 17.2 million
registered residents of the Netherlands that are connected through around 1.3
billion ties distributed over four distinct tie types. We take income
assortativity as a measure of socio-economic segregation, compare a social
network and spatial neighborhood approach, and find that the social network
structure exhibits two times as much segregation. As such, this work challenges
the dominance of the spatial perspective on segregation in both literature and
policymaking. While at a particular scale of spatial aggregation (e.g., the
geographical neighborhood), patterns of socio-economic segregation may appear
relatively minimal, they may in fact persist in the underlying social network
structure. Furthermore, we discover higher socio-economic segregation in larger
cities, shedding a different light on the common view of cities as hubs for
diverse socio-economic mixing. A population-scale social network perspective
hence offers a way to uncover hitherto 'hidden' segregation that extends beyond
spatial neighborhoods and infiltrates multiple aspects of human life.

This work validates a dynamic production network model, used to quantify the
impact of economic shocks caused by COVID-19 in the UK, using data for Belgium.
Because the model was published early during the 2020 COVID-19 pandemic, it
relied on several assumptions regarding the magnitude of the observed economic
shocks, for which more accurate data have become available in the meantime. We
refined the propagated shocks to align with observed data collected during the
pandemic and calibrated some less well-informed parameters using 115 economic
time series. The refined model effectively captures the evolution of GDP,
revenue, and employment during the COVID-19 pandemic in Belgium at both
individual economic activity and aggregate levels. However, the reduction in
business-to-business demand is overestimated, revealing structural shortcomings
in accounting for businesses' motivations to sustain trade despite the
pandemic's induced shocks. We confirm that the relaxation of the stringent
Leontief production function by a survey on the criticality of inputs
significantly improved the model's accuracy. However, despite a large dataset,
distinguishing between varying degrees of relaxation proved challenging.
Overall, this work demonstrates the model's validity in assessing the impact of
economic shocks caused by an epidemic in Belgium.

The international community must collaborate to mitigate climate change and
sustain economic growth. However, collaboration is hard to achieve, partly
because no global authority can ensure compliance with international climate
agreements. Combining AI with climate-economic simulations offers a promising
solution to design international frameworks, including negotiation protocols
and climate agreements, that promote and incentivize collaboration. In
addition, these frameworks should also have policy goals fulfillment, and
sustained commitment, taking into account climate-economic dynamics and
strategic behaviors. These challenges require an interdisciplinary approach
across machine learning, economics, climate science, law, policy, ethics, and
other fields.
  Towards this objective, we organized AI for Global Climate Cooperation, a
Mila competition in which teams submitted proposals and analyses of
international frameworks, based on (modifications of) RICE-N, an AI-driven
integrated assessment model (IAM). In particular, RICE-N supports modeling
regional decision-making using AI agents. Furthermore, the IAM then models the
climate-economic impact of those decisions into the future.
  Whereas the first track focused only on performance metrics, the proposals
submitted to the second track were evaluated both quantitatively and
qualitatively. The quantitative evaluation focused on a combination of (i) the
degree of mitigation of global temperature rise and (ii) the increase in
economic productivity. On the other hand, an interdisciplinary panel of human
experts in law, policy, sociology, economics and environmental science,
evaluated the solutions qualitatively. In particular, the panel considered the
effectiveness, simplicity, feasibility, ethics, and notions of climate justice
of the protocols. In the third track, the participants were asked to critique
and improve RICE-N.

Objectives: Lung cancer remains a significant global public health challenge
and is still one of the leading cause of cancer-related death in Argentina.
This study aims to assess the disease and economic burden of lung cancer in the
country.
  Study design: Burden of disease study
  Methods. A mathematical model was developed to estimate the disease burden
and direct medical cost attributable to lung cancer. Epidemiological parameters
were obtained from local statistics, the Global Cancer Observatory, the Global
Burden of Disease databases, and a literature review. Direct medical costs were
estimated through micro-costing. Costs were expressed in US dollars (US$),
April 2023 (1 US$ =216.38 argentine pesos). A second-order Monte Carlo
simulation was performed to estimate the uncertainty.
  Results: Considering approximately 10,000 deaths, 12,000 incident cases, and
14,000 5-year prevalent cases, the economic burden of lung cancer in Argentina
in 2023 was estimated to be US$ 556.20 million (396.96 -718.20), approximately
1.4% of the total healthcare expenditure for the country. The cost increased
with a higher stage of the disease and the main driver was the drug acquisition
(80%). 179,046 Disability-adjusted life years could be attributable to lung
cancer representing the 10% of the total cancer.
  Conclusion: The disease and economic burden of lung cancer in Argentina
implies a high cost for the health system and would represent 19% of the
previously estimated economic burden for 29 cancers in Argentina.

The purpose of this study is to measure the Total Factor Productivity (TFP)
growth and determine the share of each of the economic growth sources in the
mining sector of Iran. The time period of this study is 1355-1385 of the Solar
Hijri calendar (roughly overlaying with the time period of 1976-2006 of the
Gregorian calendar). In this paper, the shares of total factor productivity
growth (TFPG) and factors' accumulations in the economic growth of the mining
sector are estimated using a neoclassical growth accounting approach. Based on
the estimated restricted Cobb-Douglas production function and the results
obtained from the Solow residual equation, the annual growth rates of TFP were
measured for each year. According to the findings, the average annual growth
rate of TFP has been 2.94% during the time period of the present study. The
other findings of this study indicate that the average contributions of TFPG,
labor accumulation and capital accumulation in the economic growth of the
mining sector have been 56%, 23%, and 21%, respectively, during the time period
of the study. As such, it can be concluded that the policy of benefiting from
available factors in the mining sector together with the policy of accumulating
factors have simultaneously caused the value-added growth of this sector.
Therefore, considering the desired performance of the mining sector in terms of
its sizable productivity growth, it can be argued that the mining sector can
aid Iran's economic development plans to achieve their assigned economic
objectives, one of which is to increase the share of total factor productivity
growth in economic growth.

Reliable estimates of indirect economic losses arising from natural disasters
are currently out of scientific reach. To address this problem, we propose a
novel approach that combines a probabilistic physical damage catastrophe model
with a new generation of macroeconomic agent-based models (ABMs). The ABM moves
beyond the state of the art by exploiting large data sets from detailed
national accounts, census data, and business information, etc., to simulate
interactions of millions of agents representing \emph{each} natural person or
legal entity in a national economy. The catastrophe model introduces a copula
approach to assess flood losses, considering spatial dependencies of the flood
hazard. These loss estimates are used in a damage scenario generator that
provides input for the ABM, which then estimates indirect economic losses due
to the event. For the first time, we are able to link environmental and
economic processes in a computer simulation at this level of detail. We show
that moderate disasters induce comparably small but positive short- to
medium-term, and negative long-term economic impacts. Large-scale events,
however, trigger a pronounced negative economic response immediately after the
event and in the long term, while exhibiting a temporary short- to medium-term
economic boost. We identify winners and losers in different economic sectors,
including the fiscal consequences for the government. We quantify the critical
disaster size beyond which the resilience of an economy to rebuild reaches its
limits. Our results might be relevant for the management of the consequences of
systemic events due to climate change and other disasters.

Recent progress in artificial intelligence (AI) marks a pivotal moment in
human history. It presents the opportunity for machines to learn, adapt, and
perform tasks that have the potential to assist people, from everyday
activities to their most creative and ambitious projects. It also has the
potential to help businesses and organizations harness knowledge, increase
productivity, innovate, transform, and power shared prosperity. This tremendous
potential raises two fundamental questions: (1) Will AI actually advance
national and global economic transformation to benefit society at large? and
(2) What issues must we get right to fully realize AI's economic value, expand
prosperity and improve lives everywhere? We explore these questions by
considering the recent history of technology and innovation as a guide for the
likely impact of AI and what we must do to realize its economic potential to
benefit society. While we do not presume the future will be entirely like that
past, for reasons we will discuss, we do believe prior experience with
technological change offers many useful lessons. We conclude that while
progress in AI presents a historic opportunity to advance our economic
prosperity and future wellbeing, its economic benefits will not come
automatically and that AI risks exacerbating existing economic challenges
unless we collectively and purposefully act to enable its potential and address
its challenges. We suggest a collective policy agenda - involving developers,
deployers and users of AI, infrastructure providers, policymakers, and those
involved in workforce training - that may help both realize and harness AI's
economic potential and address its risks to our shared prosperity.

We examine the contribution of Islamic legal institutions to the comparative
economic decline of the Middle East behind Latin Europe, which can be observed
since the late Middle Ages. To this end, we explore whether the sacralization
of Islamic law and its focus on the Sharia as supreme, sacred and unchangeable
legal text, which reached its culmination in the 13th century had an impact on
economic development. We use the population size of 145 cities in Islamic
countries and 648 European cities for the period 800-1800 as proxies for the
level of economic development, and construct novel estimates of the number of
law schools (i.e. madaris) and estimate their contribution to the
pre-industrial economic development. Our triple-differences estimates show that
a higher density of madrasas before the sacralization of Islamic law predicts a
more vibrant urban economy characterized by higher urban growth. After the
consolidation of the sharia sacralization of law in the 13th century, greater
density of law schools is associated with stagnating population size. We show
that the economic decline of the Middle East can be partly explained by the
absence of legal innovations or substitutes of them, which paved the way for
the economic rise of Latin Europe, where ground-breaking legal reforms
introduced a series of legal innovations conducive for economic growth. We find
that the number of learned lawyers trained in universities with law schools is
highly and positively correlated with the western European city population. Our
counterfactual estimates show that almost all Islamic cities under
consideration would have had much larger size by the year 1700 if legal
innovations comparable to those in Western Europe were introduced. By making
use of a series of synthetic control and difference-in-differences estimators
our findings are robust against a large number of model specification checks.

From some observations on economic behaviors, in particular changing economic
conditions with time and space, we develop a very simple model for the
evolution of economic entities within a geographical type of framework. We
raise a few questions and attempt to investigate whether some of them can be
tackled by our model. Several cases of interest are reported. It is found that
the model even in its simple forms can lead to a large variety of situations,
including: delocalization and cycles, but also pre-chaotic behavior.

We study the effect of the social stratification on the wealth distribution
on a system of interacting economic agents that are constrained to interact
only within their own economic class. The economical mobility of the agents is
related to its success in exchange transactions. Different wealth distributions
are obtained as a function of the width of the economic class. We find a range
of widths in which the society is divided in two classes separated by a deep
gap that prevents further exchange between poor and rich agents. As a
consequence, the middle wealth class is eliminated. The high values of the Gini
indices obtained in these cases indicate a highly unequal society. On the other
hand, lower and higher widths induce lower Gini indices and a fairer wealth
distribution.

We study a credit risk model which captures effects of economic interactions
on a firm's default probability. Economic interactions are represented as a
functionally defined graph, and the existence of both cooperative, and
competitive, business relations is taken into account. We provide an analytic
solution of the model in a limit where the number of business relations of each
company is large, but the overall fraction of the economy with which a given
company interacts may be small. While the effects of economic interactions are
relatively weak in typical (most probable) scenarios, they are pronounced in
situations of economic stress, and thus lead to a substantial fattening of the
tails of loss distributions in large loan portfolios. This manifests itself in
a pronounced enhancement of the Value at Risk computed for interacting
economies in comparison with their non-interacting counterparts.

We review some methods recently used in the literature to detect the
existence of a certain degree of common behavior of stock returns belonging to
the same economic sector. Specifically, we discuss methods based on random
matrix theory and hierarchical clustering techniques. We apply these methods to
a set of stocks traded at the New York Stock Exchange. The investigated time
series are recorded at a daily time horizon.
  All the considered methods are able to detect economic information and the
presence of clusters characterized by the economic sector of stocks. However,
different methodologies provide different information about the considered set.
Our comparative analysis suggests that the application of just a single method
could not be able to extract all the economic information present in the
correlation coefficient matrix of a set of stocks.

Using the similar formulas of the preference relation and the utility
function, we propose the confidence relations and the corresponding influence
functions that represent various interacting strengths of different families,
cliques and systems of organization. Since they can affect products, profit and
prices, etc., in an economic system, and are usually independent of economic
results, therefore, the system can produce a multiply connected topological
economics. If the political economy is an economy chaperoned polity, it will
produce consequentially a binary economy. When the changes of the product and
the influence are independent one another, they may be a node or saddle point.
When the influence function large enough achieves a certain threshold value, it
will form a wormhole with loss of capital. Various powers produce usually the
economic wormhole and various corruptions.

More than thirty years ago, Charnes, Cooper and Schinnar (1976) established
an enlightening contact between economic production functions (EPFs) -- a
cornerstone of neoclassical economics -- and information theory, showing how a
generalization of the Cobb-Douglas production function encodes homogeneous
functions.
  As expected by Charnes \textit{et al.}, the contact turns out to be much
broader: we show how information geometry as pioneered by Amari and others
underpins static and dynamic descriptions of microeconomic cornerstones.
  We show that the most popular EPFs are fundamentally grounded in a very weak
axiomatization of economic transition costs between inputs. The strength of
this characterization is surprising, as it geometrically bonds altogether a
wealth of collateral economic notions
  -- advocating for applications in various economic fields --: among all, it
characterizes (i) Marshallian and Hicksian demands and their geometric duality,
(ii) Slutsky-type properties for the transformation paths, (iii) Roy-type
properties for their elementary variations.

These notes discuss several topics in neoclassical economics and
alternatives, with an aim of reviewing fundamental issues in modeling economic
markets. I start with a brief, non-rigorous summary of the basic Arrow-Debreu
model of general equilibrium, as well as its extensions to include time and
contingency. I then argue that symmetries due to similarly endowed individuals
and similar products are generically broken by the constraints of scarcity,
leading to the existence of multiple equilibria.
  This is followed by an evaluation of the strengths and weaknesses of the
model generally. Several of the weaknesses are concerned with the treatments of
time and contingency. To address these we discuss a class of agent based
models.
  Another set of issues has to do with the fundamental meaning of prices and
the related question of what the observables of a non-equilibrium, dynamic
model of an economic market should be. We argue that these issues are addressed
by formulating economics in the language of a gauge theory, as proposed
originally by Malaney and Weinstein. We review some of their work and provide a
sketch of how gauge invariance can be incorporated into the formulation of
agent based models.

Although classical economic theory is based on the concept of stable
equilibrium, real economic systems appear to be always out of equilibrium.
Indeed, they share many of the dynamical features of other complex systems,
e.g., ecological food-webs. We focus on the relation between increasing
complexity of the economic network and its stability with respect to small
perturbations in the dynamical variables associated with the constituent nodes.
Inherent delays and multiple time-scales suggest that economic systems will be
more likely to exhibit instabilities as their complexity is increased even
though the speed at which transactions are conducted has increased many-fold
through technological developments. Analogous to the birth of nonlinear
dynamics from Poincare's work on the question of whether the solar system is
stable, we suggest that similar theoretical developments may arise from efforts
by econophysicists to understand the mechanisms by which instabilities arise in
the economy.

A new constructivist approach to modeling in economics and theory of
consciousness is proposed. The state of elementary object is defined as a set
of its measurable consumer properties. A proprietor's refusal or consent for
the offered transaction is considered as a result of elementary economic
measurement. Elementary (indivisible) technology, in which the object's
consumer values are variable, in this case can be formalized as a generalized
economic measurement. The algebra of such measurements has been constructed. It
has been shown that in the general case the quantum-mechanical formalism of the
theory of selective measurements is required for description of such
conditions. The economic analogs of the elementary slit experiments in physics
have been created. The proposed approach can be also used for consciousness
modeling.

A new constructivist approach to modeling in economics and theory of
consciousness is proposed. The state of elementary object is defined as a set
of its measurable consumer properties. A proprietor's refusal or consent for
the offered transaction is considered as a result of elementary economic
measurement. We were also able to obtain the classical interpretation of the
quantum-mechanical law of addition of probabilities by introducing a number of
new notions. The principle of "local equity" assumes the transaction completed
(regardless of the result) of the states of transaction partners are not
changed in connection with the reception of new information on proposed offers
or adopted decisions (consent or refusal of the transaction). However it has no
relation to the paradoxes of quantum theory connected with non-local
interaction of entangled states. In the economic systems the mechanism of
entangling has a classical interpretation, while the quantum-mechanical
formalism of the description of states appears as a result of idealization of
the selection mechanism in the proprietor's consciousness.

Despite the recent availability of large data sets on human movements, a full
understanding of the rules governing motion within social systems is still
missing, due to incomplete information on the socio-economic factors and to
often limited spatio-temporal resolutions. Here we study an entire society of
individuals, the players of an online-game, with complete information on their
movements in a network-shaped universe and on their social and economic
interactions. Such a "socio-economic laboratory" allows to unveil the intricate
interplay of spatial constraints, social and economic factors, and patterns of
mobility. We find that the motion of individuals is not only constrained by
physical distances, but also strongly shaped by the presence of socio-economic
areas. These regions can be recovered perfectly by community detection methods
solely based on the measured human dynamics. Moreover, we uncover that
long-term memory in the time-order of visited locations is the essential
ingredient for modeling the trajectories.

In this article, we address the question of how non-knowledge about future
events that influence economic agents' decisions in choice settings has been
formally represented in economic theory up to date. To position our discussion
within the ongoing debate on uncertainty, we provide a brief review of
historical developments in economic theory and decision theory on the
description of economic agents' choice behaviour under conditions of
uncertainty, understood as either (i) ambiguity, or (ii) unawareness.
Accordingly, we identify and discuss two approaches to the formalisation of
non-knowledge: one based on decision-making in the context of a state space
representing the exogenous world, as in Savage's axiomatisation and some
successor concepts (ambiguity as situations with unknown probabilities), and
one based on decision-making over a set of menus of potential future
opportunities, providing the possibility of derivation of agents' subjective
state spaces (unawareness as situation with imperfect subjective knowledge of
all future events possible). We also discuss impeding challenges of the
formalisation of non-knowledge.

The economic life of an asset is the optimum length of its usefulness, which
is the moment that the asset's expenses are minimum. In this paper, the
economic life of physical assets, such as industry machine and equipment, can
be interpreted as the moment that the minimum is reached by its equivalent
property cost function, defined as the sum of all equivalent capital and
maintenance costs during its life.
  Many authors in classical papers have used principles of engineering economic
to solve the assets replacement problem. However, in the literature, the main
attributes found were proved with intuitive ideas instead mathematical
analysis. Therefore, in this paper the main goal is to study these principles
of engineering economic with mathematical techniques.
  Here, is used non-smooth analysis to classify all the possibilities for the
minimum of a class of equivalent property cost functions of assets. The minimum
of these function gives the optimum moment for the asset to be replaced, i.e.,
its economic life.

The new business paradigms originate a strong necessity to re-think the
theory of the firm with the aim to get a better understanding on the
organizational and functional principles of the firm, operating in the
investment economies in the prosperous societies. In this connection, we make
the innovative research to advance our scientific knowledge on the theory of
firm in the conditions of the nonlinear dynamic financial and economic systems.
We propose that the nonlinearities have to be taken to the consideration and
the nonlinear differential equation have to be used to model the firm in the
modern theories of the firm in the nonlinear dynamic financial and economic
systems. We apply the econophysical approach with the dynamic regimes modeling
on the bifurcation diagram as in the dynamic chaos theory with the purpose to
accurately characterize the nonlinearities in the economic theory of the firm.
We introduce the Ledenyov firm stability theorem, based on the Lyapunov
stability criteria, to precisely characterize the stability of the firm in the
nonlinear dynamic financial and economic systems in the time of globalization.

The current economic crisis has provoked an active response from the
interdisciplinary scientific community. As a result many papers suggesting what
can be improved in understanding of the complex socio-economics systems were
published. Some of the most prominent papers on the topic include (Bouchaud,
2009; Farmer and Foley, 2009; Farmer et al, 2012; Helbing, 2010; Pietronero,
2008). These papers share the idea that agent-based modeling is essential for
the better understanding of the complex socio-economic systems and consequently
better policy making. Yet in order for an agent-based model to be useful it
should also be analytically tractable, possess a macroscopic treatment
(Cristelli et al, 2012). In this work we shed a new light on our research
group's contributions towards understanding of the correspondence between the
inter-individual interactions and collective behavior. We also provide some new
insights into the implications of the global and local interactions, the
leadership and the predator-prey interactions in the complex socio-economic
systems.

TASP (Time Average Shapley Polygon, Bena{\=\i}m, Hofbauer and Hopkins,
\emph{Journal of Economic Theory}, 2009), as a novel evolutionary dynamics
model, predicts that a game could converge to cycles instead of fix points
(Nash equilibria). To verify TASP theory, using the four strategy
Dekel-Scotchmer games (Dekel and Scotchmer, \emph{Journal of Economic Theory},
1992), four experiments were conducted (Cason, Friedman and Hopkins,
\emph{Journal of Economic Theory}, 2010), in which, however, reported no
evidence of cycles (Cason, Friedman and Hopkins, \emph{The Review of Economic
Studies}, 2013). We reanalysis the four experiment data by testing the
stochastic averaging of angular momentum in period-by-period transitions of the
social state. We find, the existence of persistent cycles in Dekel-Scotchmer
game can be confirmed. On the cycles, the predictions from evolutionary models
had been supported by the four experiments.

This paper focuses on botnet economics and design of defensive strategies. It
takes the view that by combining scarce information on the attackers business
models, with rational economic analysis of these business processes, one can
identify design rules for economic defense mechanisms which the target can
implement, often in a cheap way in addition to technical means. A short survey
of game theory in the security area, is followed by a real case of an Internet
casino. This leads to develop a model, applicable to this case and others,
which is presented first qualitatively then quantitatively. This allows
carrying out different analyses based on different equilibrium or termination
principles; the ones studied are reward break-even analysis, and Max-Min
analysis from game theory, for the target and the attackers. On that basis, a
number of specific economic and information led defense strategies are
identified which can be further studied using the model and specific
adaptations to other data or cases.

The relationship between micro-structure and macro-structure of complex
systems using information geometry has been dealt by several authors. From this
perspective, we are going to apply it as a geometrical structure connecting
both microeconomics and macroeconomics . The results lead us to introduce new
modified quantities into both micro-macro economics that enable us to describe
the link between them. The importance of such a scheme is to find out -with
some accuracy- a new method can be introduced for examining the stability of an
economic system. This type of requirement is expressed by examining the
stability of the equations of path deviations for some economic systems as
described in a statistical manifold. Such a geometization scheme of economic
systems is an important step toward identifying risk management factors and so
contributes to the growing literature of econophysics.

There is great uncertainty about future climate conditions and the
appropriate policies for managing interactions between the climate and the
economy. We develop a multidimensional computational model to examine how
uncertainties and risks in the economic and climate systems affect the social
cost of carbon (SCC)---that is, the present value of the marginal damage to
economic output caused by carbon emissions. The SCC is substantially increased
by economic and climate risks at both current and future times. Furthermore,
the SCC is itself a stochastic process with significant variation; for example,
the basic elements of risk incorporated into our model cause the SCC in 2100 to
be, with significant probability, ten times what it would be without those
risks. We have only imprecise information about what parameter values are best
for approximating reality. To deal with this parametric uncertainty we perform
extensive uncertainty quantification and show that these findings are robust
for a wide range of alternative specifications. More generally, this work shows
that large-scale computing can enable economists to examine substantially more
complex and realistic models for the purposes of policy analysis.

Indonesian traditional villagers have a tradition for the sake of their own
social and economic security named 'nyumbang'. There are wide variations of the
traditions across the archipelago, and we revisit an observation to one in
Subang, West Java, Indonesia. The paper discusses and employs the evolutionary
game theoretic insights to see the process of 'gantangan', of the intertwining
social cohesion and economic expectation of the participation within the
traditional activities. The current development of the gantangan tradition is
approached and generalized to propose a view between the economic and social
sphere surrounding modern people. While some explanations due to the current
development of gantangan is drawn, some aspects related to traditional views
complying the modern life with social and economic expectations is outlined.

The decline in extent of wild pollinators in recent years has been partly
associated with changing farm practices and in particular with increasing
pesticide use. In this paper we combine ecological modelling with economic
analysis of a single farm output under the as- sumption that both pollination
and pest control are essential inputs. We show that the drive to increase farm
output can lead to a local decline in the wild bee population. Commercial bees
are often considered an alternative to wild pollinators, but we show that their
intro- duction can lead to further decline and finally local extinction of wild
bees. The transitions between different outcomes are characterised by threshold
behaviour and are potentially difficult to predict and detect in advance. Small
changes in economic parameters (input prices) and ecological parameters (wild
bees carrying capacity and effect of pesticides on bees) can move the
economic-ecological system beyond the extinction threshold. We also show that
increasing the pesticide price or decreasing the commercial bee price might
lead to re-establishment of wild bees following their local extinction. Thus,
we demonstrate the importance of combining ecological modelling with economics
to study the provision of ecosystem services and to inform sustainable
management of ecosystem service providers.

Most models that try to explain economic growth indicate exponential growth
paths. In recent years, however, a lively discussion has emerged considering
the validity of this notion. In the empirical literature dealing with drivers
of economic growth, the majority of articles is based upon an implicit
assumption of exponential growth. Few scholarly articles have addressed this
issue so far. In order to shed light on this issue, we estimate autoregressive
integrated moving average time series models based on Gross Domestic Product
Per Capita data for 18 mature economies from 1960 to 2013. We compare the
adequacy of linear and exponential growth models and conduct several robustness
checks. Our fndings cast doubts on the widespread belief of exponential growth
and suggest a deeper discussion on alternative economic grow theories.

In this paper we present a kinetic model with stochastic game-type
interactions, analyzing the relationship between the level of political
competition in a society and the degree of economic liberalization. The above
issue regards the complex interactions between economy and institutional
policies intended to introduce technological innovations in a society, where
technological innovations are intended in a broad sense comprehending reforms
critical to production. A special focus is placed on the political replacement
effect described in a macroscopic model by Acemoglu and Robinson (AR-model,
henceforth), which can determine the phenomenon of innovation 'blocking',
possibly leading to economic backwardness. One of the goals of our modelization
is to obtain a mesoscopic dynamical model whose macroscopic outputs are
qualitatively comparable with stylized facts of the AR-model. A set of
numerical solutions is presented showing the non monotonous relationship
between economic liberization and political competition, which can be
considered as an emergent phenomenon of the complex socio-economic interaction
dynamic.

Over the last decades, the distribution of income and wealth has been
deteriorating in many countries, leading to increased inequalities within and
between societies. This tendency has revived the interest in the subject
greatly, yet it still receives very little attention within the realm of
mainstream economic thinking. One reason for this is that the basic paradigm of
"standard economics", the representative-agent General Equilibrium framework,
is badly equipped to cope with distributional issues. Here we argue that when
the economy is treated as a complex system composed of many heterogeneous
interacting agents who give rise to emergent phenomena, to address the main
stylized facts of income/wealth distribution requires leaving the toolbox of
mainstream economics in favour of alternative approaches. The "k-generalized"
family of income/wealth distributions, building on the categories of
complexity, is an example of how advances in the field can be achieved within
new interdisciplinary research contexts.

Beyond its obvious macro-economic relevance, fiat money has important
micro-economic implications. They matter for addressing No. 8 in Smale's
"Mathematical Problems for the Next Century": extend the mathematical model of
general equilibrium theory to include price adjustments. In the canonical
Arrow-Debreu framework, equilibrium prices are set by a fictitious auctioneer.
Removing that fiction raises the question of how prices are set and adjusted by
decentralised actors with incomplete information. We investigate this question
through a very basic model where a unique factor of production, labour,
produces a single consumption good, called jelly for brevity. The point of the
model is to study a price dynamics based on the firm's expectations about jelly
demand and labour supply. The system tends towards economic equilibrium,
however, depending on the initial conditions it might not get there. In
different model versions, different kinds of money are introduced. Compared to
the case of no money, the introduction of money as a store of value facilitates
the system reaching economic equilibrium. If money is introduced as a third
commodity, i.e. there is also a demand for money, the system dynamics in
general becomes more complex.

The 2015 Nobel Prize in Economic Sciences was awarded to Eugene Fama, Lars
Peter Hansen and Robert Shiller for their contributions to the empirical
analysis of asset prices. Eugene Fama [1] is an advocate of the efficient
market hypothesis. The efficient market hypothesis assumes that asset price is
determined by using all available information and only reacts to new
information not incorporated into the fundamentals. Thus, the movement of stock
prices is unpredictable. Robert Shiller [2] has been studying the existence of
irrational bubbles, which are defined as the long term deviations of asset
price from the fundamentals. This drives us to the unsettled question of how
the market actually works.
  In this paper, I look back at the development of economics and consider the
direction in which we should move in order to truly understand the workings of
an economic society.

This paper presents a comprehensive literature review on applications of
economic and pricing models for resource management in cloud networking. To
achieve sustainable profit advantage, cost reduction, and flexibility in
provisioning of cloud resources, resource management in cloud networking
requires adaptive and robust designs to address many issues, e.g., resource
allocation, bandwidth reservation, request allocation, and workload allocation.
Economic and pricing models have received a lot of attention as they can lead
to desirable performance in terms of social welfare, fairness, truthfulness,
profit, user satisfaction, and resource utilization. This paper reviews
applications of the economic and pricing models to develop adaptive algorithms
and protocols for resource management in cloud networking. Besides, we survey a
variety of incentive mechanisms using the pricing strategies in sharing
resources in edge computing. In addition, we consider using pricing models in
cloud-based Software Defined Wireless Networking (cloud-based SDWN). Finally,
we highlight important challenges, open issues and future research directions
of applying economic and pricing models to cloud networking

The availability of big data recorded from massively multiplayer online
role-playing games (MMORPGs) allows us to gain a deeper understanding of the
potential connection between individuals' network positions and their economic
outputs. We use a statistical filtering method to construct dependence networks
from weighted friendship networks of individuals. We investigate the 30
distinct motif positions in the 13 directed triadic motifs which represent
microscopic dependences among individuals. Based on the structural similarity
of motif positions, we further classify individuals into different groups. The
node position diversity of individuals is found to be positively correlated
with their economic outputs. We also find that the economic outputs of leaf
nodes are significantly lower than that of the other nodes in the same motif.
Our findings shed light on understanding the influence of network structure on
economic activities and outputs in socioeconomic system.

If Tunisia was hailed as a success story with its high rankings on economic,
educational, and other indicators compared to other Arab countries, the 2011
popular uprisings demonstrate the need for political reforms but also major
economic reforms. The Arab spring highlights the fragility of its main economic
pillars including the tourism and the foreign direct investment. In such
turbulent times, the paper examines the economic impact of migrant'
remittances, expected to have a countercyclical behavior. Our results reveal
that prior to the Arab Spring, the impacts of remittances on growth and
consumption seem negative and positive respectively, while they varyingly
influence local investment. These three relationships held in the short-run. By
considering the period surrounding the 2011 uprisings, the investment effect of
remittances becomes negative and weak in the short-and medium-run, whereas
positive and strong remittances' impacts on growth and consumption are found in
the long term.

Economic complexity reflects the amount of knowledge that is embedded in the
productive structure of an economy. It resides on the premise of hidden
capabilities - fundamental endowments underlying the productive structure. In
general, measuring the capabilities behind economic complexity directly is
difficult, and indirect measures have been suggested which exploit the fact
that the presence of the capabilities is expressed in a country's mix of
products. We complement these studies by introducing a probabilistic framework
which leverages Bayesian non-parametric techniques to extract the dominant
features behind the comparative advantage in exported products. Based on
economic evidence and trade data, we place a restricted Indian Buffet Process
on the distribution of countries' capability endowment, appealing to a culinary
metaphor to model the process of capability acquisition. The approach comes
with a unique level of interpretability, as it produces a concise and
economically plausible description of the instantiated capabilities.

Ergodicity economics is a new branch of economic theory that notes the
conceptual difference between time averages and expectation values, which
coincide only for ergodic observables. It postulates that individual agents
maximise the time average growth rate of wealth, known widely as growth
optimality. This contrasts with the dominant behavioural model in economics,
expected utility theory, in which agents maximise expectation values of changes
in psychologically transformed wealth. Historically, growth optimality was
explored for additive and multiplicative gambles. Here we apply it to a general
class of wealth dynamics, extending the range of economic situations where it
may be used. Moreover, we show a correspondence between growth optimality and
expected utility theory, in which the ergodicity transformation in the former
is identified as the utility function in the latter. This correspondence offers
a theoretical basis for choosing utility functions and predicts that wealth
dynamics are strong determinants of risk preferences.

We consider the use of Quantifier Elimination (QE) technology for automated
reasoning in economics. QE dates back to Tarski's work in the 1940s with
software to perform it dating to the 1970s. There is a great body of work
considering its application in science and engineering but we show here how it
can also find application in the social sciences. We explain how many suggested
theorems in economics could either be proven, or even have their hypotheses
shown to be inconsistent, automatically; and describe the application of this
in both economics education and research. We describe a bank of QE examples
gathered from economics literature and note the structure of these are, on
average, quite different to those occurring in the computer algebra literature.
This leads us to suggest a new incremental QE approach based on result
memorization of commonly occurring generic QE results.

The question about fair income inequality has been an important open question
in economics and in political philosophy for over two centuries with only
qualitative answers such as the ones suggested by Rawls, Nozick, and Dworkin.
We provided a quantitative answer recently, for an ideal free-market society,
by developing a game-theoretic framework that proved that the ideal inequality
is a lognormal distribution of income at equilibrium. In this paper, we develop
another approach, using the Nash Bargaining Solution (NBS) framework, which
also leads to the same conclusion. Even though the conclusion is the same, the
new approach, however, reveals the true nature of NBS, which has been of
considerable interest for several decades. Economists have wondered about the
economic meaning or purpose of the NBS. While some have alluded to its fairness
property, we show more conclusively that it is all about fairness. Since the
essence of entropy is also fairness, we see an interesting connection between
the Nash product and entropy for a large population of rational economic
agents.

Real-time dispatch practices for operating the electric grid in an economic
and reliable manner are evolving to accommodate higher levels of renewable
energy generation. In particular, stochastic optimization is receiving
increased attention as a technique for handling the inherent uncertainty in
wind and solar generation. The typical two-stage stochastic optimization
formulation relies on a sample average approximation with scenarios
representing errors in forecasting renewable energy ramp events. Standard Monte
Carlo sampling approaches can result in prohibitively high-dimensional systems
for optimization, as well as a poor representation of extreme events that
challenge grid reliability. We propose two alternative scenario creation
strategies, importance sampling and Bayesian quadrature, that can reduce the
estimator's variance. Their performance is assessed on a week's worth of 5
minute stochastic economic dispatch decisions for realistic wind and electrical
system data. Both strategies yield more economic solutions and improved
reliability compared to Monte Carlo sampling, with Bayesian quadrature being
less computationally intensive than importance sampling and more economic when
considering at least 20 scenarios.

The accumulation of knowledge required to produce economic value is a process
that often relates to nations economic growth. Such a relationship, however, is
misleading when the proxy of such accumulation is the average years of
education. In this paper, we show that the predictive power of this proxy
started to dwindle in 1990 when nations schooling began to homogenized. We
propose a metric of human capital that is less sensitive than average years of
education and remains as a significant predictor of economic growth when tested
with both cross-section data and panel data. We argue that future research on
economic growth will discard educational variables based on quantity as
predictor given the thresholds that these variables are reaching.

Why do some economic activities agglomerate more than others? And, why does
the agglomeration of some economic activities continue to increase despite
recent developments in communication and transportation technologies? In this
paper, we present evidence that complex economic activities concentrate more in
large cities. We find this to be true for technologies, scientific
publications, industries, and occupations. Using historical patent data, we
show that the urban concentration of complex economic activities has been
continuously increasing since 1850. These findings suggest that the increasing
urban concentration of jobs and innovation might be a consequence of the
growing complexity of the economy.

The ability to uncover preferences from choices is fundamental for both
positive economics and welfare analysis. Overwhelming evidence shows that
choice is stochastic, which has given rise to random utility models as the
dominant paradigm in applied microeconomics. However, as is well known, it is
not possible to infer the structure of preferences in the absence of
assumptions on the structure of noise. This makes it impossible to empirically
test the structure of noise independently from the structure of preferences.
Here, we show that the difficulty can be bypassed if data sets are enlarged to
include response times. A simple condition on response time distributions (a
weaker version of first order stochastic dominance) ensures that choices reveal
preferences without assumptions on the structure of utility noise. Sharper
results are obtained if the analysis is restricted to specific classes of
models. Under symmetric noise, response times allow to uncover preferences for
choice pairs outside the data set, and if noise is Fechnerian, even choice
probabilities can be forecast out of sample. We conclude by showing that
standard random utility models from economics and standard drift-diffusion
models from psychology necessarily generate data sets fulfilling our sufficient
condition on response time distributions.

Standard economic dispatch problems that consider line losses are linear
approximations of a non-convex economic dispatch problem formulated by fixing
voltage magnitudes and assuming the decoupling of real and reactive power. This
paper formulates and analyzes the general non-convex economic dispatch problem,
incorporating and generalizing the Fictitious Nodal Demand (FND) model,
resulting in a slack bus independent formulation that provides insight into
standard formulations by pointing out commonly used but unnecessary assumptions
and by deriving proper choices of "tuning parameters." The proper choice of
loss allocation is derived to assign half of the losses of each transmission
line to adjacent buses, justifying approaches in the literature. Line
constraints are proposed in the form of voltage angle difference limits and are
proven equivalent to various other line limits including current magnitude
limits and mid-line power flow limits. The formulated general economic dispatch
problem with marginal losses consistently models flows and loss approximation,
results in approximately correct outcomes and is proven to be reference bus
independent. Various approximations of this problem are compared using
realistically large transmission network test cases.

Quantifying the improvement in human living standard, as well as the city
growth in developing countries, is a challenging problem due to the lack of
reliable economic data. Therefore, there is a fundamental need for alternate,
largely unsupervised, computational methods that can estimate the economic
conditions in the developing regions. To this end, we propose a new network
science- and representation learning-based approach that can quantify economic
indicators and visualize the growth of various regions. More precisely, we
first create a dynamic network drawn out of high-resolution nightlight
satellite images. We then demonstrate that using representation learning to
mine the resulting network, our proposed approach can accurately predict
spatial gross economic expenditures over large regions. Our method, which
requires only nightlight images and limited survey data, can capture
city-growth, as well as how people's living standard is changing; this can
ultimately facilitate the decision makers' understanding of growth without
heavily relying on expensive and time-consuming surveys.

We study the qualitative and quantitative appearance of stylized facts in
several agent-based computational economic market (ABCEM) models. We perform
our simulations with the SABCEMM (Simulator for Agent-Based Computational
Economic Market Models) tool recently introduced by the authors (Trimborn et
al. 2019). Furthermore, we present novel ABCEM models created by recombining
existing models and study them with respect to stylized facts as well. This can
be efficiently performed by the SABCEMM tool thanks to its object-oriented
software design. The code is available on GitHub (Trimborn et al. 2018), such
that all results can be reproduced by the reader.

Estimating the capabilities, or inputs of production, that drive and
constrain the economic development of urban areas has remained a challenging
goal. We posit that capabilities are instantiated in the complexity and
sophistication of urban activities, the knowhow of individual workers, and the
city-wide collective knowhow. We derive a model that indicates how the value of
these three quantities can be inferred from the probability that an individual
in a city is employed in a given urban activity. We illustrate how to estimate
empirically these variables using data on employment across industries and
metropolitan statistical areas in the US. We then show how the functional form
of the probability function derived from our theory is statistically superior
when compared to competing alternative models, and that it explains well-known
results in the urban scaling and economic complexity literature. Finally, we
show how the quantities are associated with metrics of economic performance,
suggesting our theory can provide testable implications for why some cities are
more prosperous than others.

Originally, the Carnot cycle is a theoretical thermodynamic cycle that
provides an upper limit on the efficiency that any classical thermodynamic
engine can achieve during the conversion of heat into work, or conversely, the
efficiency of a refrigeration system in creating a temperature difference by
the application of work to the system. The aim of this paper is to introduce
and study the economic Carnot cycles into a Roegenian economy, using our
Thermodynamic-Economic Dictionary. Of course, the most difficult questions are:
what is the economic significance of such a cycle? Roegenian economics is
acceptable or not, in terms of practical applications? Our answer is yes for
both questions.

In this feasibility study, the impact of academic research from social
sciences and humanities on technological innovation is explored through a study
of citations patterns of journal articles in patents. Specifically we focus on
citations of journals from the field of environmental economics in patents
included in an American patent database (USPTO). Three decades of patents have
led to a small set of journal articles (85) that are being cited from the field
of environmental economics. While this route of measuring how academic research
is validated through its role in stimulating technological progress may be
rather limited (based on this first exploration), it may still point to a
valuable and interesting topic for further research.

Uncovering the structure of socioeconomic systems and timely estimation of
socioeconomic status are significant for economic development. The
understanding of socioeconomic processes provides foundations to quantify
global economic development, to map regional industrial structure, and to infer
individual socioeconomic status. In this review, we will make a brief manifesto
about a new interdisciplinary research field named Computational
Socioeconomics, followed by detailed introduction about data resources,
computational tools, data-driven methods, theoretical models and novel
applications at multiple resolutions, including the quantification of global
economic inequality and complexity, the map of regional industrial structure
and urban perception, the estimation of individual socioeconomic status and
demographic, and the real-time monitoring of emergent events. This review,
together with pioneering works we have highlighted, will draw increasing
interdisciplinary attentions and induce a methodological shift in future
socioeconomic studies.

Killer technology is a radical innovation, based on new products and/or
processes, that with high technical and/or economic performance destroys the
usage value of established techniques previously sold and used. Killer
technology is a new concept in economics of innovation that may be useful for
bringing a new perspective to explain and generalize the behavior and
characteristics of innovations that generate a destructive creation for
sustaining technical change. To explore the behavior of killer technologies, a
simple model is proposed to analyze and predict how killer technologies destroy
and substitute established technologies. Empirical evidence of this theoretical
framework is based on historical data on the evolution of some example
technologies. Theoretical framework and empirical evidence hint at general
properties of the behavior of killer technologies to explain corporate,
industrial, economic and social change and to support best practices for
technology management of firms and innovation policy of nations. Overall, then,
the proposed theoretical framework can lay a foundation for the development of
more sophisticated concepts to explain the behavior of vital technologies that
generate technological and industrial change in society.

The increasing penetration of renewable energy resources in power systems,
represented as random processes, converts the traditional deterministic
economic dispatch problem into a stochastic one. To solve this stochastic
economic dispatch, the conventional Monte Carlo method is prohibitively time
consuming for medium- and large-scale power systems. To overcome this problem,
we propose in this paper a novel Gaussian-process-emulator-based approach to
quantify the uncertainty in the stochastic economic dispatch considering wind
power penetration. Based on the dimension-reduction results obtained by the
Karhunen-Lo\`eve expansion, a Gaussian-process emulator is constructed. This
surrogate allows us to evaluate the economic dispatch solver at sampled values
with a negligible computational cost while maintaining a desirable accuracy.
Simulation results conducted on the IEEE 118-bus system reveal that the
proposed method has an excellent performance as compared to the traditional
Monte Carlo method.

We analyze the optimal control of disease prevention and treatment in a basic
SIS model. We develop a simple macroeconomic setup in which the social planner
determines how to optimally intervene, through income taxation, in order to
minimize the social cost, inclusive of infection and economic costs, of the
spread of an epidemic disease. The disease lowers economic production and thus
income by reducing the size of the labor force employed in productive
activities, tightening thus the economy's overall resources constraint. We
consider a framework in which the planner uses the collected tax revenue to
intervene in either prevention (aimed at reducing the rate of infection) or
treatment (aimed at increasing the speed of recovery). Both optimal prevention
and treatment policies allow the economy to achieve a disease-free equilibrium
in the long run but their associated costs are substantially different along
the transitional dynamic path. By quantifying the social costs associated with
prevention and treatment we determine which policy is most cost-effective under
different circumstances, showing that prevention (treatment) is desirable
whenever the infectivity rate is low (high).

Challenge Theory (CT), a new approach to decision under risk departs
significantly from expected utility, and is based on firmly psychological,
rather than economic, assumptions. The paper demonstrates that a purely
cognitive-psychological paradigm for decision under risk can yield excellent
predictions, comparable to those attained by more complex economic or
psychological models that remain attached to conventional economic constructs
and assumptions. The study presents a new model for predicting the popularity
of choices made in binary risk problems. A CT-based regression model is tested
on data gathered from 126 respondents who indicated their preferences with
respect to 44 choice problems. Results support CT's central hypothesis,
strongly associating between the Challenge Index (CI) attributable to every
binary risk problem, and the observed popularity of the bold prospect in that
problem (with r=-0.92 and r=-0.93 for gains and for losses, respectively). The
novelty of the CT perspective as a new paradigm is illuminated by its simple,
single-index (CI) representation of psychological effects proposed by Prospect
Theory for describing choice behavior (certainty effect, reflection effect,
overweighting small probabilities and loss aversion).

The informational context is regularly questioned in a transitional economic
regime like the one implemented in China or Vietnam. This article investigates
this issue and the predictive power of fundamental analysis in such context and
more precisely in a Chinese context with an analysis of 3 different industries
(media, power, and steel). Through 3 different kinds of correlation, we examine
25 financial determinants for 60 Chinese listed companies between 2011 and
2015. Our results show that fundamental analysis can effectively be used as an
investment tool in transitional economic context. Contrasting with the EMH for
which the accounting information is instantaneously integrated into the
financial information (stock prices), our study suggests that these two levels
of information are not synchronized in China opening therefore a door for a
fundamental analysis based prediction. Furthermore, our results also indicate
that accounting information illustrates quite well the economic reality since
financial reports in each industry can disclose a part of stock value
information in line with the economic situation of the industry under
consideration.

Forests will have two notable economic roles in the future: providing
renewable raw material and storing carbon to mitigate climate change. The
pricing of forest carbon leads to longer rotation times and consequently larger
carbon stocks, but also exposes landowners to a greater risk of forest damage.
This paper investigates optimal forest rotation under carbon pricing and forest
damage risk. I provide the optimality conditions for this problem and
illustrate the setting with numerical calculations representing boreal forests
under a range of carbon prices and damage probabilities. The relation between
damage probability and carbon price towards the optimal rotation length is
nearly linear, with carbon pricing having far greater impact. As such,
increasing forest carbon stocks by lengthening rotations is an economically
attractive method for climate change mitigation, despite the forest damage
risk. Carbon pricing also increases land expectation value and reduces the
economic risks of the landowner. The production possibility frontier under
optimal rotation suggests that significantly larger forests carbon stocks are
achievable, but imply lower harvests. However, forests' societally optimal role
between these two activities is not yet clear-cut; but rests on the future
development of relative prices between timber, carbon and other commodities
dependent on land-use.

There is an analogy between machine learning systems and economic entities in
that they are both adaptive, and their behaviour is specified in a more-or-less
explicit way. It appears that the area of AI that is most analogous to the
behaviour of economic entities is that of morally good decision-making, but it
is an open question as to how precisely moral behaviour can be achieved in an
AI system. This paper explores the analogy between these two complex systems,
and we suggest that a clearer understanding of this apparent analogy may help
us forward in both the socio-economic domain and the AI domain: known results
in economics may help inform feasible solutions in AI safety, but also known
results in AI may inform economic policy. If this claim is correct, then the
recent successes of deep learning for AI suggest that more implicit
specifications work better than explicit ones for solving such problems.

Using a large dataset of research seminars held at US economics departments
in 2018, I explore the factors that determine who is invited to present at a
research seminar and whether the invitation is accepted. I find that
high-quality scholars have a higher probability of being invited than
low-quality scholars, and researchers are more likely to accept an invitation
if it is issued by a top economics department. The probability of being invited
increases with the size of the host department. Young and low-quality scholars
have a higher probability of accepting an invitation. The distance between the
host department and invited scholar reduces the probability of being invited
and accepting the invitation. Female scholars do not have a lower probability
of being invited to give a research seminar than men.

Climate change threatens biodiversity directly by influencing biophysical
variables that drive species' geographic distributions and indirectly through
socio-economic changes that influence land use patterns, driven by global
consumption, production and climate. To date, no detailed analyses have been
produced that assess the relative importance of, or interaction between, these
direct and indirect climate change impacts on biodiversity at large scales.
Here, we apply a new integrated modelling framework to quantify the relative
influence of biophysical and socio-economically mediated impacts on avian
species in Vietnam and Australia. We find that socio-economically mediated
impacts on suitable ranges are largely outweighed by biophysical impacts, but
global shifts of production are likely to result in adverse impacts on habitats
worldwide. By translating economic futures and shocks into spatially explicit
predictions of biodiversity change, we now have the power to analyse in a
consistent way outcomes for nature and people of any change to policy,
regulation, trading conditions or consumption trend at any scale from
sub-national to global.

We consider the problem of computing equilibria (steady-states) for
droop-controlled, islanded, AC microgrids that are both economic-optimal and
dynamically stable. This work is motivated by the observation that classical
optimal power flow (OPF) formulations used for economic optimization might
provide equilibria that are not reachable by low-level controllers (i.e.,
closed-loop unstable). This arises because OPF problems only enforce
steady-state conditions and do not capture the dynamics. We explain this
behavior by using a port-Hamiltonian microgrid representation. To overcome the
limitations of OPF, the port-Hamiltonian representation can be exploited to
derive a bilevel OPF formulation that seeks to optimize economics while
enforcing stability. Unfortunately, bilevel optimization with a nonconvex inner
problem is difficult to solve in general. As such, we propose an alternative
approach (that we call probing OPF), which identifies an economic-optimal and
stable equilibrium by probing a neighborhood of equilibria using random
perturbations. The probing OPF is advantageous in that it is formulated as a
standard nonlinear program, in that it is compatible with existing OPF
frameworks, and in that it is applicable to diverse microgrid models.
Experiments with the IEEE 118-bus system reveal that few probing points are
required to enforce stability.

The Levy-Levy-Solomon model (A microscopic model of the stock market: cycles,
booms, and crashes, Economic Letters 45 (1))is one of the most influential
agent-based economic market models. In several publications this model has been
discussed and analyzed. Especially Lux and Zschischang (Some new results on the
Levy, Levy and Solomon microscopic stock market model, Physica A, 291(1-4))
have shown that the model exhibits finite-size effects. In this study we extend
existing work in several directions. First, we show simulations which reveal
finite-size effects of the model. Secondly, we shed light on the origin of
these finite-size effects. Furthermore, we demonstrate the sensitivity of the
Levy-Levy-Solomon model with respect to random numbers. Especially, we can
conclude that a low-quality pseudo random number generator has a huge impact on
the simulation results. Finally, we study the impact of the stopping criteria
in the market clearance mechanism of the Levy-Levy-Solomon model.

Economic assessment in environmental science concerns the measurement or
valuation of environmental impacts, adaptation, and vulnerability. Integrated
assessment modeling is a unifying framework of environmental economics, which
attempts to combine key elements of physical, ecological, and socioeconomic
systems. Uncertainty characterization in integrated assessment varies by
component models: uncertainties associated with mechanistic physical models are
often assessed with an ensemble of simulations or Monte Carlo sampling, while
uncertainties associated with impact models are evaluated by conjecture or
econometric analysis. Manifold sampling is a machine learning technique that
constructs a joint probability model of all relevant variables which may be
concentrated on a low-dimensional geometric structure. Compared with
traditional density estimation methods, manifold sampling is more efficient
especially when the data is generated by a few latent variables. The
manifold-constrained joint probability model helps answer policy-making
questions from prediction, to response, and prevention. Manifold sampling is
applied to assess risk of offshore drilling in the Gulf of Mexico.

This paper studies an integrated system of political and economic systems
from a systematic perspective to explore the complex interaction between them,
and specially analyzes the case of the US presidential election forecasting.
Based on the signed association networks of industrial structure constructed by
economic data, our framework simulates the diffusion and evolution of opinions
during the election through a kinetic model called the Potts Model. Remarkably,
we propose a simple and efficient prediction model for the US presidential
election, and meanwhile inspire a new way to model the economic structure.
Findings also highlight the close relationship between economic structure and
political attitude. Furthermore, the case analysis in terms of network and
economy demonstrates the specific features and the interaction between
political tendency and industrial structure in a particular period, which is
consistent with theories in politics and economics.

We interpret multi-product supply chains (SCs) as coordinated markets; under
this interpretation, a SC optimization problem is a market clearing problem
that allocates resources and associated economic values (prices) to different
stakeholders that bid into the market (suppliers, consumers, transportation,
and processing technologies). The market interpretation allows us to establish
fundamental properties that explain how physical resources (primal variables)
and associated economic values (dual variables) flow in the SC. We use duality
theory to explain why incentivizing markets by forcing stakeholder
participation (e.g., by imposing demand satisfaction or service provision
constraints) yields artificial price behavior, inefficient allocations, and
economic losses. To overcome these issues, we explore market incentive
mechanisms that use bids; here, we introduce the concept of a stakeholder graph
(a product-based representation of a supply chain) and show that this
representation allows us to naturally determine minimum bids that activate the
market. These results provide guidelines to design SC formulations that
properly remunerate stakeholders and to design policy that foster market
transactions. The results are illustrated using an urban waste management
problem for a city of 100,000 residents.

Massive open online courses (MOOCs) promise to make rigorous higher education
accessible to everyone, but prior research has shown that registrants tend to
come from backgrounds of higher socioeconomic status. We study geographically
granular economic patterns in about 76,000 U.S. registrations for about 600
HarvardX and MITx courses between 2012 and 2018, identifying registrants'
locations using both IP geolocation and user-reported mailing addresses. By
either metric, we find higher registration rates among postal codes with
greater prosperity or population density. However, we also find evidence of
bias in IP geolocation: it makes greater errors, both geographically and
economically, for users from more economically distressed areas; it
disproportionately places users in prosperous areas; and it underestimates the
regressive pattern in MOOC registration. Researchers should use IP geolocation
in MOOC studies with care, and consider the possibility of similar economic
biases affecting its other academic, commercial, and legal uses.

The purpose of this study is to investigate the effects of the COVID-19
pandemic on economic policy uncertainty in the US and the UK. The impact of the
increase in COVID-19 cases and deaths in the country, and the increase in the
number of cases and deaths outside the country may vary. To examine this, the
study employs bootstrap ARDL cointegration approach from March 8, 2020 to May
24, 2020. According to the bootstrap ARDL results, a long-run equilibrium
relationship is confirmed for five out of the 10 models. The long-term
coefficients obtained from the ARDL models suggest that an increase in COVID-19
cases and deaths outside of the UK and the US has a significant effect on
economic policy uncertainty. The US is more affected by the increase in the
number of COVID-19 cases. The UK, on the other hand, is more negatively
affected by the increase in the number of COVID-19 deaths outside the country
than the increase in the number of cases. Moreover, another important finding
from the study demonstrates that COVID-19 is a factor of great uncertainty for
both countries in the short-term.

In the data-rich environment, using many economic predictors to forecast a
few key variables has become a new trend in econometrics. The commonly used
approach is factor augment (FA) approach. In this paper, we pursue another
direction, variable selection (VS) approach, to handle high-dimensional
predictors. VS is an active topic in statistics and computer science. However,
it does not receive as much attention as FA in economics. This paper introduces
several cutting-edge VS methods to economic forecasting, which includes: (1)
classical greedy procedures; (2) l1 regularization; (3) gradient descent with
sparsification and (4) meta-heuristic algorithms. Comprehensive simulation
studies are conducted to compare their variable selection accuracy and
prediction performance under different scenarios. Among the reviewed methods, a
meta-heuristic algorithm called sequential Monte Carlo algorithm performs the
best. Surprisingly the classical forward selection is comparable to it and
better than other more sophisticated algorithms. In addition, we apply these VS
methods on economic forecasting and compare with the popular FA approach. It
turns out for employment rate and CPI inflation, some VS methods can achieve
considerable improvement over FA, and the selected predictors can be well
explained by economic theories.

In this paper, we propose a spatially constrained clustering problem
belonging to the family of "p-regions" problems. Our formulation is motivated
by the recent developments of economic complexity on the evolution of the
economic output through key interactions among industries within economic
regions. The objective of this model consists in aggregating a set of
geographic areas into a prescribed number of regions (so-called innovation
ecosystems) such that the resulting regions preserve the most relevant
interactions among industries. We formulate the p-Innovation Ecosystems model
as a mixed-integer programming (MIP) problem and propose a heuristic solution
approach. We explore a case involving the municipalities of Colombia to
illustrate how such a model can be applied and used for policy and regional
development.

Global ballast water management regulations aiming to decrease aquatic
species invasion require actions that can increase shipping costs. We employ an
integrated shipping cost and global economic modeling approach to investigate
the impacts of ballast water regulations on bilateral trade, national
economies, and shipping patterns. Given the potential need for more stringent
regulation at regional hotspots of species invasions, this work considers two
ballast water treatment policy scenarios: implementation of current
international regulations, and a possible stricter regional regulation that
targets ships traveling to and from the United States while other vessels
continue to face current standards. We find that ballast water management
compliance costs under both scenarios lead to modest negative impacts on
international trade and national economies overall. However, stricter
regulations applied to U.S. ports are expected to have large negative impacts
on bilateral trade of several specific commodities for a few countries. Trade
diversion causes decreased U.S. imports of some products, leading to minor
economic welfare losses.

The development of Digital Economy sets its own requirements for the
formation and development of so-called digital doubles and digital shadows of
real objects (subjects/regions). An integral element of their development and
application is a multi-level matrix of targets and resource constraints (time,
financial, technological, production, etc.). The volume of statistical
information collected for a digital double must meet several criteria: be
objective, characterize the real state of the managed object as accurately as
possible, contain all the necessary information on all managed parameters, and
at the same time avoid unnecessary and duplicate indicators ("information
garbage"). The relevance of forming the profile of the "digital shadow of the
region" in the context of multitasking and conflict of departmental and Federal
statistics predetermined the goal of the work-to form a system of indicators of
the socio-economic situation of regions based on the harmonization of
information resources. In this study, an inventory of the composition of
indicators of statistical forms for their relevance and relevance was carried
out on the example of assessing the economic health of the subject and the
level of provision of banking services

In a fast-changing technology-driven era, drafting an implementable strategic
roadmap to achieve economic prosperity becomes a real challenge. Although the
national and international strategic development plans may vary, they usually
target the improvement of the quality of living standards through boosting the
national GDP per capita and the creation of decent jobs. There is no doubt that
human capacity building, through higher education, is vital to the availability
of highly qualified workforce supporting the implementation of the
aforementioned strategies. In other words, fulfillment of most strategic
development plan goals becomes dependent on the drafting and implementation of
successful higher education strategies. For MENA region countries, this is
particularly crucial due to many specific challenges, some of which are
different from those facing developed nations. More details on the MENA region
higher education strategic planning challenges as well as the proposed higher
education strategic requirements to support national economic prosperity and
fulfill the 2030 UN SDGs are given in the paper.

Measuring the geographical distribution of economic activity plays a key role
in scientific research and policymaking. However, previous studies and data on
economic activity either have a coarse spatial resolution or cover a limited
time span, and the high-resolution characteristics of socioeconomic dynamics
are largely unknown. Here, we construct a dataset on the economic activity of
mainland China, the gridded establishment dataset (GED), which measures the
volume of establishments at a 0.01$^{\circ}$ latitude by 0.01$^{\circ}$
longitude scale. Specifically, our dataset captures the geographically based
opening and closing of approximately 25.5 million firms that registered in
mainland China over the period 2005-2015. The characteristics of fine
granularity and long-term observability give the GED a high application value.
The dataset not only allows us to quantify the spatiotemporal patterns of the
establishments, urban vibrancy and socioeconomic activity, but also helps us
uncover the fundamental principles underlying the dynamics of industrial and
economic development.

Political orientation polarizes the attitudes of more educated individuals on
controversial issues. A highly controversial issue in Europe is immigration. We
found the same polarizing pattern for opinion toward immigration in a
representative sample of citizens of a southern European middle-size city.
Citizens with higher numeracy, scientific and economic literacy presented a
more polarized view of immigration, depending on their worldview orientation.
Highly knowledgeable individuals endorsing an egalitarian-communitarian
worldview were more in favor of immigration, whereas highly knowledgeable
individuals with a hierarchical-individualist worldview were less in favor of
immigration. Those low in numerical, economic, and scientific literacy did not
show a polarized attitude. Results highlight the central role of
socio-political orientation over information theories in shaping attitudes
toward immigration.

Agent-based computational economics (ACE) - while adopted comparably widely
in other domains of managerial science - is a rather novel paradigm for
management accounting research (MAR). This paper provides an overview of
opportunities and difficulties that ACE may have for research in management
accounting and, in particular, introduces a framework that researchers in
management accounting may employ when considering ACE as a paradigm for their
particular research endeavor. The framework builds on the two interrelated
paradigmatic elements of ACE: a set of theoretical assumptions on economic
agents and the approach of agent-based modeling. Particular focus is put on
contrasting opportunities and difficulties of ACE in comparison to other
research methods employed in MAR.

Social distancing has been the only effective way to contain the spread of an
infectious disease prior to the availability of the pharmaceutical treatment.
It can lower the infection rate of the disease at the economic cost. A pandemic
crisis like COVID-19, however, has posed a dilemma to the policymakers since a
long-term restrictive social distancing or even lockdown will keep economic
cost rising. This paper investigates an efficient social distancing policy to
manage the integrated risk from economic health and public health issues for
COVID-19 using a stochastic epidemic modeling with mobility controls. The
social distancing is to restrict the community mobility, which was recently
accessible with big data analytics. This paper takes advantage of the community
mobility data to model the COVID-19 processes and infer the COVID-19 driven
economic values from major market index price, which allow us to formulate the
search of the efficient social distancing policy as a stochastic control
problem. We propose to solve the problem with a deep-learning approach. By
applying our framework to the US data, we empirically examine the efficiency of
the US social distancing policy and offer recommendations generated from the
algorithm.

We analyse 'stop-and-go' containment policies that produce infection cycles
as periods of tight lockdowns are followed by periods of falling infection
rates. The subsequent relaxation of containment measures allows cases to
increase again until another lockdown is imposed and the cycle repeats. The
policies followed by several European countries during the Covid-19 pandemic
seem to fit this pattern. We show that 'stop-and-go' should lead to lower
medical costs than keeping infections at the midpoint between the highs and
lows produced by 'stop-and-go'. Increasing the upper and reducing the lower
limits of a stop-and-go policy by the same amount would lower the average
medical load. But increasing the upper and lowering the lower limit while
keeping the geometric average constant would have the opposite effect. We also
show that with economic costs proportional to containment, any path that brings
infections back to the original level (technically a closed cycle) has the same
overall economic cost.

Economic activities favor mutual geographical proximity and concentrate
spatially to form cities. In a world of diminishing transport costs, however,
the advantage of physical proximity is fading, and the role of cities in the
economy may be declining. To provide insights into the long-run evolution of
cities, we analyzed Japan's census data over the 1970--2015 period. We found
that fewer and larger cities thrived at the national scale, suggesting an
eventual mono-centric economy with a single megacity; simultaneously, each
larger city flattened out at the local scale, suggesting an eventual extinction
of cities. We interpret this multi-scale phenomenon as an instance of pattern
formation by self-organization, which is widely studied in mathematics and
biology. However, cities' dynamics are distinct from mathematical or biological
mechanisms because they are governed by economic interactions mediated by
transport costs between locations. Our results call for the synthesis of
knowledge in mathematics, biology, and economics to open the door for a general
pattern formation theory that is applicable to socioeconomic phenomena.

Society is undergoing many transformations and faces economic crises,
environmental, social, and public health issues. At the same time, the
Internet, mobile communications, cloud technologies, and social networks are
growing rapidly and fostering the digitalization processes of business and
society. It is in this context that the shared economy has assumed itself as a
new social and economic system based on the sharing of resources and has
allowed the emergence of innovative businesses like Airbnb. However, COVID-19
has challenged this business model in the face of restrictions imposed in the
tourism sector. Its consequences are not exclusively short-term and may also
call into question the sustainability of Airbnb. In this sense, this study aims
to explore the sustainability of the Airbnb business model considering two
theories which advocate that hosts can cover the short-term financial effects,
while another defends a paradigm shift in the demand for long-term
accommodations to ensure greater stability for hosts.

Natural and anthropogenic disasters frequently affect both the supply and
demand side of an economy. A striking recent example is the Covid-19 pandemic
which has created severe disruptions to economic output in most countries.
These direct shocks to supply and demand will propagate downstream and upstream
through production networks. Given the exogenous shocks, we derive a lower
bound on total shock propagation. We find that even in this best case scenario
network effects substantially amplify the initial shocks. To obtain more
realistic model predictions, we study the propagation of shocks bottom-up by
imposing different rationing rules on industries if they are not able to
satisfy incoming demand. Our results show that economic impacts depend strongly
on the emergence of input bottlenecks, making the rationing assumption a key
variable in predicting adverse economic impacts. We further establish that the
magnitude of initial shocks and network density heavily influence model
predictions.

Bitcoin is a peer-to-peer electronic payment system that has rapidly grown in
popularity in recent years. Usually, the complete history of Bitcoin blockchain
data must be queried to acquire variables with economic meaning. This task has
recently become increasingly difficult, as there are over 1.6 billion
historical transactions on the Bitcoin blockchain. It is thus important to
query Bitcoin transaction data in a way that is more efficient and provides
economic insights. We apply cohort analysis that interprets Bitcoin blockchain
data using methods developed for population data in the social sciences.
Specifically, we query and process the Bitcoin transaction input and output
data within each daily cohort. This enables us to create datasets and
visualizations for some key Bitcoin transaction indicators, including the daily
lifespan distributions of spent transaction output (STXO) and the daily age
distributions of the cumulative unspent transaction output (UTXO). We provide a
computationally feasible approach for characterizing Bitcoin transactions that
paves the way for future economic studies of Bitcoin.

The growth in AI is rapidly transforming the structure of economic
production. However, very little is known about how within-AI specialization
may relate to broad-based economic diversification. This paper provides a
data-driven framework to integrate the interconnection between AI-based
specialization with goods and services export specialization to help design
future comparative advantage based on the inherent capabilities of nations.
Using detailed data on private investment in AI and export specialization for
more than 80 countries, we propose a systematic framework to help identify the
connection from AI to goods and service sector specialization. The results are
instructive for nations that aim to harness AI specialization to help guide
sources of future competitive advantage. The operational framework could help
inform the public and private sector to uncover connections with nearby areas
of specialization.

During the global spread of COVID-19, Japan has been among the top countries
to maintain a relatively low number of infections, despite implementing limited
institutional interventions. Using a Tokyo Metropolitan dataset, this study
investigated how these limited intervention policies have affected public
health and economic conditions in the COVID-19 context. A causal loop analysis
suggested that there were risks to prematurely terminating such interventions.
On the basis of this result and subsequent quantitative modelling, we found
that the short-term effectiveness of a short-term pre-emptive stay-at-home
request caused a resurgence in the number of positive cases, whereas an
additional request provided a limited negative add-on effect for economic
measures (e.g. the number of electronic word-of-mouth (eWOM) communications and
restaurant visits). These findings suggest the superiority of a mild and
continuous intervention as a long-term countermeasure under epidemic pressures
when compared to strong intermittent interventions.

The ongoing coronavirus disease 2019 (COVID-19) pandemic has wreaked havoc
worldwide with millions of lives claimed, human travel restricted and economic
development halted. Leveraging city-level mobility and case data, our analysis
shows that the spatial dissemination of COVID-19 can be well explained by a
local diffusion process in the mobility network rather than a global diffusion
process, indicating the effectiveness of the implemented disease prevention and
control measures. Based on the constructed case prediction model, it is
estimated that there could be distinct social consequences if the COVID-19
outbreak happened in different areas. During the epidemic control period, human
mobility experienced substantial reductions and the mobility network underwent
remarkable local and global structural changes toward containing the spread of
COVID-19. Our work has important implications for the mitigation of disease and
the evaluation of the socio-economic consequences of COVID-19 on society.

The study of human mobility patterns is a crucially important research field
for its impact on several socio-economic aspects and, in particular, the
measure of regularity patters of human mobility can provide a across-the-board
view of many social distancing variables in epidemics such as: human movement
trends, physical interpersonal distances and population density. We will show
that the notion of information entropy is also strongly related to demographic
and economic trends by the use and analysis of real-time data. In the present
research paper we address three different problems. First, we provide an
evidence-based analytical approach which relates the human mobility patterns,
social distancing attitudes and population density, with entropic measures
which depict for erraticity of human contact behaviors. Second, we investigate
the correlations between the aggregated mobility and entropic measures versus
five external economic indicators. Finally,we show how entropic measures
represents a useful tool for testing the limitations of typical assumptions in
epidemiological and mobility models.

Economic Model Predictive Control (MPC) dissipativity theory is central to
discussing the stability of policies resulting from minimizing economic stage
costs. In its current form, the dissipativity theory for economic MPC applies
to problems based on deterministic dynamics or to very specific classes of
stochastic problems, and does not readily extend to generic Markov Decision
Processes. In this paper, we clarify the core reason for this difficulty, and
propose a generalization of the economic MPC dissipativity theory that
circumvents it. This generalization focuses on undiscounted infinite-horizon
problems and is based on nonlinear stage cost functionals, allowing one to
discuss the Lyapunov asymptotic stability of policies for Markov Decision
Processes in terms of the probability measures underlying their stochastic
dynamics. This theory is illustrated for the stochastic Linear Quadratic
Regulator with Gaussian process noise, for which a storage functional can be
provided explicitly. For the sake of brevity, we limit our discussion to
undiscounted Markov Decision Processes.

This study examines how foreign aid and institutions affect entrepreneurship
activity following natural disasters. We use insights from the
entrepreneurship, development, and institutions literature to develop a model
of entrepreneurship activity in the aftermath of natural disasters. First, we
hypothesize the effect of natural disasters on entrepreneurship activity
depends on the amount of foreign aid received. Second, we hypothesize that
natural disasters and foreign aid either encourages or discourages
entrepreneurship activity depending on two important institutional conditions:
the quality of government and economic freedom. The findings from our panel of
85 countries from 2006 to 2016 indicate that natural disasters are negatively
associated with entrepreneurship activity, but both foreign aid and economic
freedom attenuate this effect. In addition, we observe that foreign aid is
positively associated with entrepreneurship activity but only in countries with
high quality government. Hence, we conclude that the effect of natural
disasters on entrepreneurship depends crucially on the quality of government,
economic freedom, and foreign aid. Our findings provide new insights into how
natural disasters and foreign aid affect entrepreneurship and highlight the
important role of the institutional context.

This research studies the impact of online news on social and economic
consumer perceptions through semantic network analysis. Using over 1.8 million
online articles on Italian media covering four years, we calculate the semantic
importance of specific economic-related keywords to see if words appearing in
the articles could anticipate consumers' judgments about the economic situation
and the Consumer Confidence Index. We use an innovative approach to analyze big
textual data, combining methods and tools of text mining and social network
analysis. Results show a strong predictive power for the judgments about the
current households and national situation. Our indicator offers a complementary
approach to estimating consumer confidence, lessening the limitations of
traditional survey-based methods.

In order to study the phenomenon of regional economic development and urban
expansion from the perspective of night-light remote sensing images,
researchers use NOAA-provided night-light remote sensing image data (data from
1992 to 2013) along with ArcGIS software to process image information, obtain
the basic pixel information data of specific areas of the image, and analyze
these data from the space-time domain for presentation of the trend of regional
economic development in China in recent years, and tries to explore the
urbanization effect brought by the rapid development of China's economy.
Through the analysis and study of the data, the results show that the
urbanization development speed in China is still at its peak, and has great
development potential and space. But at the same time, people also need to pay
attention to the imbalance of regional development.

We propose a model, which nests a susceptible-infected-recovered-deceased
(SIRD) epidemic model into a dynamic macroeconomic equilibrium framework with
agents' mobility. The latter affect both their income and their probability of
infecting and being infected. Strategic complementarities among individual
mobility choices drive the evolution of aggregate economic activity, while
infection externalities caused by individual mobility affect disease diffusion.
The continuum of rational forward-looking agents coordinates on the Nash
equilibrium of a discrete time, finite-state, infinite-horizon Mean Field Game.
We prove the existence of an equilibrium and provide a recursive construction
method for the search of an equilibrium(a), which also guides our numerical
investigations. We calibrate the model by using Italian experience on COVID-19
epidemic and we discuss policy implications.

The economic consequences of drought episodes are increasingly important,
although they are often difficult to apprehend in part because of the
complexity of the underlying mechanisms. In this article, we will study one of
the consequences of drought, namely the risk of subsidence (or more
specifically clay shrinkage induced subsidence), for which insurance has been
mandatory in France for several decades. Using data obtained from several
insurers, representing about a quarter of the household insurance market, over
the past twenty years, we propose some statistical models to predict the
frequency but also the intensity of these droughts, for insurers, showing that
climate change will have probably major economic consequences on this risk. But
even if we use more advanced models than standard regression-type models (here
random forests to capture non linearity and cross effects), it is still
difficult to predict the economic cost of subsidence claims, even if all
geophysical and climatic information is available.

The results based on the nonparametric nearest neighbor matching suggest a
statistically significant positive effect of the EU ETS on the economic
performance of the regulated firms during Phase I of the EU ETS. A year-by-year
analysis shows that the effect was only significant during the first year of
Phase I. The EU ETS, therefore, had a particularly strong effect when it was
introduced. It is important to note that the EU ETS does not homogeneously
affect firms in the manufacturing sector. We found a significant positive
impact of EU ETS on the economic performance of regulated firms in the paper
industry.

We introduce the notion of Point in Time Economic Scenario Generation (PiT
ESG) with a clear mathematical problem formulation to unify and compare
economic scenario generation approaches conditional on forward looking market
data. Such PiT ESGs should provide quicker and more flexible reactions to
sudden economic changes than traditional ESGs calibrated solely to long periods
of historical data. We specifically take as economic variable the S&P500 Index
with the VIX Index as forward looking market data to compare the nonparametric
filtered historical simulation, GARCH model with joint likelihood estimation
(parametric), Restricted Boltzmann Machine and the conditional Variational
Autoencoder (Generative Networks) for their suitability as PiT ESG. Our
evaluation consists of statistical tests for model fit and benchmarking the out
of sample forecasting quality with a strategy backtest using model output as
stop loss criterion. We find that both Generative Networks outperform the
nonparametric and classic parametric model in our tests, but that the CVAE
seems to be particularly well suited for our purposes: yielding more robust
performance and being computationally lighter.

When making decisions under risk, people often exhibit behaviors that
classical economic theories cannot explain. Newer models that attempt to
account for these irrational behaviors often lack neuroscience bases and
require the introduction of subjective and problem-specific constructs. Here,
we present a decision-making model inspired by the prediction error signals and
introspective neuronal replay reported in the brain. In the model, decisions
are chosen based on anticipated surprise, defined by a nonlinear average of the
differences between individual outcomes and a reference point. The reference
point is determined by the expected value of the possible outcomes, which can
dynamically change during the mental simulation of decision-making problems
involving sequential stages. Our model elucidates the contribution of each
stage to the appeal of available options in a decision-making problem. This
allows us to explain several economic paradoxes and gambling behaviors. Our
work could help bridge the gap between decision-making theories in economics
and neurosciences.

There are different interpretations of the terms "tokens" and "token-based
systems" in the literature around blockchain and digital currencies although
the distinction between token-based and account-based systems is well
entrenched in economics. Despite the wide use of the terminologies of tokens
and tokenisation in the cryptocurrency community, the underlying concept
sometimes does not square well with the economic notions, or is even contrary
to them. The UTXO design of Bitcoin exhibits partially characteristics of a
token-based system and partially characteristics of an account-based system. A
discussion on the difficulty to implement the economic notion of tokens in the
digital domain, along with an exposition of the design of UTXO, is given in
order to discuss why UTXO-based systems should be viewed as account-based
according to the classical economic notion. Besides, a detailed comparison
between UTXO-based systems and account-based systems is presented. Using the
data structure of the system state representation as the defining feature to
distinguish digital token-based and account-based systems is therefore
suggested. This extended definition of token-based systems covers both physical
and digital tokens while neatly distinguishing token-based and account-based
systems.

We build a deep-learning-based SEIR-AIM model integrating the classical
Susceptible-Exposed-Infectious-Removed epidemiology model with forecast modules
of infection, community mobility, and unemployment. Through linking Google's
multi-dimensional mobility index to economic activities, public health status,
and mitigation policies, our AI-assisted model captures the populace's
endogenous response to economic incentives and health risks. In addition to
being an effective predictive tool, our analyses reveal that the long-term
effective reproduction number of COVID-19 equilibrates around one before mass
vaccination using data from the United States. We identify a "policy frontier"
and identify reopening schools and workplaces to be the most effective. We also
quantify protestors' employment-value-equivalence of the Black Lives Matter
movement and find that its public health impact to be negligible.

Upon arrival to a new country, many immigrants face job downgrading, a
phenomenon describing workers being in jobs below the ones they have based on
the skills they possess. Moreover, in the presence of downgrading immigrants
receiving lower wage returns to the same skills compared to natives. The level
of downgrading could depend on the immigrant type and numerous other factors.
This study examines the determinants of skill downgrading among two types of
immigrants - refugees and economic immigrants - in the German labor markets
between 1984 and 2018. We find that refugees downgrade more than economic
immigrants, and this discrepancy between the two groups persists over time. We
show that language skill improvements exert a strong influence on subsequent
labor market outcomes of both groups.

The properties of money commonly referenced in the economics literature were
originally identified by Jevons (1876) and Menger (1892) in the late 1800s and
were intended to describe physical currencies, such as commodity money,
metallic coins, and paper bills. In the digital era, many non-physical
currencies have either entered circulation or are under development, including
demand deposits, cryptocurrencies, stablecoins, central bank digital currencies
(CBDCs), in-game currencies, and quantum money. These forms of money have novel
properties that have not been studied extensively within the economics
literature, but may be important determinants of the monetary equilibrium that
emerges in the forthcoming era of heightened currency competition. This paper
makes the first exhaustive attempt to identify and define the properties of all
physical and digital forms of money. It reviews both the economics and computer
science literatures and categorizes properties within an expanded version of
the original functions-and-properties framework of money that includes societal
and regulatory objectives.

There is growing interest in the role of sentiment in economic
decision-making. However, most research on the subject has focused on positive
and negative valence. Conviction Narrative Theory (CNT) places Approach and
Avoidance sentiment (that which drives action) at the heart of real-world
decision-making, and argues that it better captures emotion in financial
markets. This research, bringing together psychology and machine learning,
introduces new techniques to differentiate Approach and Avoidance from positive
and negative sentiment on a fundamental level of meaning. It does this by
comparing word-lists, previously constructed to capture these concepts in text
data, across a large range of semantic features. The results demonstrate that
Avoidance in particular is well defined as a separate type of emotion, which is
evaluative/cognitive and action-orientated in nature. Refining the Avoidance
word-list according to these features improves macroeconomic models, suggesting
that they capture the essence of Avoidance and that it plays a crucial role in
driving real-world economic decision-making.

Monitoring economic conditions and financial stability with an early warning
system serves as a prevention mechanism for unexpected economic events. In this
paper, we investigate the statistical performance of sequential break-point
detectors for stationary time series regression models with extensive
simulation experiments. We employ an online sequential scheme for monitoring
economic indicators from the European as well as the American financial markets
that span the period during the 2008 financial crisis. Our results show that
the performance of these tests applied to stationary time series regressions
such as the AR(1) as well as the AR(1)-GARCH(1,1) depend on the severity of the
break as well as the location of the break-point within the out-of-sample
period. Consequently, our study provides some useful insights to practitioners
for sequential break-point detection in economic and financial conditions.

Do low corporate taxes always favor multinational production over economic
integration? We propose a two-country model in which multinationals choose the
locations of production plants and foreign distribution affiliates and shift
profits between them through transfer prices. With high trade costs, plants are
concentrated in the low-tax country; surprisingly, this pattern reverses with
low trade costs. Indeed, economic integration has a non-monotonic impact:
falling trade costs first decrease and then increase the plant share in the
high-tax country, which we empirically confirm. Moreover, allowing for transfer
pricing makes tax competition tougher and international coordination on
transfer-pricing regulation can be beneficial.

We propose concept of equation of state (EoS) effect structure in form of
diagrams and rules. This concept helps justifying EoS status of an empirical
relation. We apply the concept to closed system of consumers and we are able to
formulate its EoS. According to the new concept, EoS are classified into three
classes. Manifold space of thermodynamics formulation of demand-side economics
is identified. Formal analogies of thermodynamics and economics consumers'
system are made. New quantities such as total wealth, generalized utility and
generalized consumer surplus are defined. Microeconomics' concept of consumer
surplus is criticized and replaced with generalized consumer surplus. Smith's
law of demand is included in our new paradigm as a specific case resembling
isothermal process. Absolute zero temperature state resembles the nirvana state
in Buddhism philosophy. Econometric modelling of consumers' EoS is proposed at
last.

A key element to understand complex systems is the relationship between the
spatial scale of investigation and the structure of the interrelation among its
elements. When it comes to economic systems, it is now well-known that the
country-product bipartite network exhibits a nested structure, which is the
foundation of different algorithms that have been used to scientifically
investigate countries' development and forecast national economic growth.
Changing the subject from countries to companies, a significantly different
scenario emerges. Through the analysis of a unique dataset of Italian firms'
exports and a worldwide dataset comprising countries' exports, here we find
that, while a globally nested structure is observed at the country level, a
local, in-block nested structure emerges at the level of firms. Remarkably,
this in-block nestedness is statistically significant with respect to suitable
null models and the algorithmic partitions of products into blocks have a high
correspondence with exogenous product classifications. These findings lay a
solid foundation for developing a scientific approach based on the physics of
complex systems to the analysis of companies, which has been lacking until now.

Economists often estimate models using data from a particular domain, e.g.
estimating risk preferences in a particular subject pool or for a specific
class of lotteries. Whether a model's predictions extrapolate well across
domains depends on whether the estimated model has captured generalizable
structure. We provide a tractable formulation for this "out-of-domain"
prediction problem and define the transfer error of a model based on how well
it performs on data from a new domain. We derive finite-sample forecast
intervals that are guaranteed to cover realized transfer errors with a
user-selected probability when domains are iid, and use these intervals to
compare the transferability of economic models and black box algorithms for
predicting certainty equivalents. We find that in this application, the black
box algorithms we consider outperform standard economic models when estimated
and tested on data from the same domain, but the economic models generalize
across domains better than the black-box algorithms do.

We present a dynamic physico-economic model of Earth orbit use with
endogenous satellite collision risk to study conditions under which
debris-producing collisions between orbiting bodies result in debris growth
that may render Earth's orbits unusable, an outcome known as Kessler Syndrome.
We characterize the dynamics of objects in orbit under open access as well as
when external costs -- the impact of an additional satellite launch on the
collision risk faced by all satellites -- are internalized, and we show that
Kessler Syndrome can emerge in both cases. Finally, we show that once the
economic incentives of satellite launching are modeled, for Kessler Syndrome to
emerge, autocatalytic debris growth is essential. In our main calibration,
Kessler Syndrome can emerge anytime between the year 2040 and the year 2184,
with the precise date being very sensitive to the calibration of autocatalytic
debris growth parameters.

Business economics research on digital platforms often overlooks existing
knowledge from other fields of research leading to conceptual ambiguity and
inconsistent findings. To reduce these restrictions and foster the utilization
of the extensive body of literature, we apply a mixed methods design to
summarize the key findings of scientific platform research. Our bibliometric
analysis identifies 14 platform-related research fields. Conducting a
systematic qualitative content analysis, we identify three primary research
objectives related to platform ecosystems: (1) general literature defining and
unifying research on platforms; (2) exploitation of platform and ecosystem
strategies; (3) improvement of platforms and ecosystems. Finally, we discuss
the identified insights from a business economics perspective and present
promising future research directions that could enhance business economics and
management research on digital platforms and platform ecosystems.

This paper uses a large UK cohort to investigate the impact of early-life
pollution exposure on individuals' human capital and health outcomes in older
age. We compare individuals who were exposed to the London smog in December
1952 whilst in utero or in infancy to those born after the smog and those born
at the same time but in unaffected areas. We find that those exposed to the
smog have substantially lower fluid intelligence and worse respiratory health,
with some evidence of a reduction in years of schooling.

Nature (one's genes) and nurture (one's environment) jointly contribute to
the formation and evolution of health and human capital over the life cycle.
This complex interplay between genes and environment can be estimated and
quantified using genetic information readily available in a growing number of
social science data sets. Using genetic data to improve our understanding of
individual decision making, inequality, and to guide public policy is possible
and promising, but requires a grounding in essential genetic terminology,
knowledge of the literature in economics and social-science genetics, and a
careful discussion of the policy implications and prospects of the use of
genetic data in the social sciences and economics.

We examine the spillover effect of neighboring ports on regional industrial
diversification and their economic resilience using the export data of South
Korea from 2006 to 2020. First, we build two distinct product spaces of ports
and port regions, and provide direct estimates of the role of neighboring ports
as spillover channels spatially linked. This is in contrast to the previous
literature that mainly regarded ports as transport infrastructure per se.
Second, we confirm that the knowledge spillover effect from neighboring ports
had a non-negligible role in sustaining regional economies during the recovery
after the economic crisis but its power has weakened recently due to a loosened
global value chain.

This paper presents a sandbox example of how the integration of models
borrowed from Behavioral Economic (specifically Protection-Motivation Theory)
into ML algorithms (specifically Bayesian Networks) can improve the performance
and interpretability of ML algorithms when applied to Behavioral Data. The
integration of Behavioral Economics knowledge to define the architecture of the
Bayesian Network increases the accuracy of the predictions in 11 percentage
points. Moreover, it simplifies the training process, making unnecessary
training computational efforts to identify the optimal structure of the
Bayesian Network. Finally, it improves the explicability of the algorithm,
avoiding illogical relations among variables that are not supported by previous
behavioral cybersecurity literature. Although preliminary and limited to 0ne
simple model trained with a small dataset, our results suggest that the
integration of behavioral economics and complex ML models may open a promising
strategy to improve the predictive power, training costs and explicability of
complex ML models. This integration will contribute to solve the scientific
issue of ML exhaustion problem and to create a new ML technology with relevant
scientific, technological and market implications.

Bayesian models of group learning are studied in Economics since the 1970s.
and more recently in computational linguistics. The models from Economics
postulate that agents maximize utility in their communication and actions. The
Economics models do not explain the ``probability matching" phenomena that are
observed in many experimental studies. To address these observations, Bayesian
models that do not formally fit into the economic utility maximization
framework were introduced. In these models individuals sample from their
posteriors in communication. In this work we study the asymptotic behavior of
such models on connected networks with repeated communication. Perhaps
surprisingly, despite the fact that individual agents are not utility
maximizers in the classical sense, we establish that the individuals ultimately
agree and furthermore show that the limiting posterior is Bayes optimal.
  We explore the interpretation of our results in terms of Large Language
Models (LLMs). In the positive direction our results can be interpreted as
stating that interaction between different LLMs can lead to optimal learning.
However, we provide an example showing how misspecification may lead LLM agents
to be overconfident in their estimates.

Studies have evaluated the economic feasibility of 100% renewable power
systems using the optimization approach, but the mechanisms determining the
results remain poorly understood. Based on a simple but essential model, this
study found that the bottleneck formed by the largest mismatch between demand
and power generation profiles determines the optimal capacities of generation
and storage and their trade-off relationship. Applying microeconomic theory,
particularly the duality of quantity and value, this study comprehensively
quantified the relationships among the factor cost of technologies, their
optimal capacities, and total system cost. Using actual profile data for
multiple years/regions in Japan, this study demonstrated that hybrid systems
comprising cost-competitive multiple renewable energy sources and different
types of storage are critical for the economic feasibility of any profile.

Inbreeding homophily is a prevalent feature of human social networks with
important individual and group-level social, economic, and health consequences.
The literature has proposed an overwhelming number of dimensions along which
human relationships might sort, without proposing a unified
empirically-grounded framework for their categorization. We exploit rich data
on a sample of University freshmen with very similar characteristic - age, race
and education- and contrast the relative importance of observable vs.
unobservables characteristics in their friendship formation. We employ Bayesian
Model Averaging, a methodology explicitly designed to target model uncertainty
and to assess the robustness of each candidate attribute while predicting
friendships. We show that, while observable features such as assignment of
students to sections, gender, and smoking are robust key determinants of
whether two individuals befriend each other, unobservable attributes, such as
personality, cognitive abilities, economic preferences, or socio-economic
aspects, are largely sensible to the model specification, and are not important
predictors of friendships.

The benefits of using complex network analysis (CNA) to study complex
systems, such as an economy, have become increasingly evident in recent years.
However, the lack of a single comparative index that encompasses the overall
wellness of a structure can hinder the simultaneous analysis of multiple
ecosystems. A formula to evaluate the structure of an economic ecosystem is
proposed here, implementing a mathematical approach based on CNA metrics to
construct a comparative measure that reflects the collaboration dynamics and
its resultant structure. This measure provides the relevant actors with an
enhanced sense of the social dynamics of an economic ecosystem, whether related
to business, innovation, or entrepreneurship. Available graph metrics were
analysed, and 14 different formulas were developed. The efficiency of these
formulas was evaluated on real networks from 11 different innovation-driven
entrepreneurial economic ecosystems in six countries from Latin America and
Europe and on 800 random graphs simulating similarly constructed networks.

This paper characterizes equilibrium properties of a broad class of economic
models that allow multiple heterogeneous agents to interact in heterogeneous
manners across several markets. Our key contribution is a new theorem providing
sufficient conditions for uniqueness and stability of equilibria in this class
of models. To illustrate the applicability of our theorem, we characterize the
general equilibrium properties of two commonly used quantitative trade models.
Specifically, our analysis provides a first proof of uniqueness and stability
of the equilibrium in multi-country trade models featuring (i) multiple
sectors, or (ii) heterogeneity across countries in terms of their labor cost
shares. These examples also provide a practical toolkit for future research on
how our theorem can be applied to establish uniqueness and stability of
equilibria in a broad set of economic models.

Contact tracing and quarantine programs have been one of the leading
Non-Pharmaceutical Interventions against COVID-19. Some governments have relied
on mandatory programs, whereas others embrace a voluntary approach. However,
there is limited evidence on the relative effectiveness of these different
approaches. In an interactive online experiment conducted on 731 subjects
representative of the adult US population in terms of sex and region of
residence, we find there is a clear ranking. A fully mandatory program is
better than an optional one, and an optional system is better than no
intervention at all. The ranking is driven by reductions in infections, while
economic activity stays unchanged. We also find that political conservatives
have higher infections and levels of economic activity, and they are less
likely to participate in the contact tracing program.

Collusive practices of firms continue to be a major threat to competition and
consumer welfare. Academic research on this topic aims at understanding the
economic drivers and behavioral patterns of cartels, among others, to guide
competition authorities on how to tackle them. Utilizing topical machine
learning techniques in the domain of natural language processing enables me to
analyze the publications on this issue over more than 20 years in a novel way.
Coming from a stylized oligopoly-game theory focus, researchers recently turned
toward empirical case studies of bygone cartels. Uni- and multivariate time
series analyses reveal that the latter did not supersede the former but filled
a gap the decline in rule-based reasoning has left. Together with a tendency
towards monocultures in topics covered and an endogenous constriction of the
topic variety, the course of cartel research has changed notably: The variety
of subjects included has grown, but the pluralism in economic questions
addressed is in descent. It remains to be seen whether this will benefit or
harm the cartel detection capabilities of authorities in the future.

In this paper we give an elementary analysis of economics of Bitcoin that
combines the transaction demand by the consumers and the supply of hashrate by
miners. We argue that the decreasing block reward will have no significant
effect on the exchange rate (price) of Bitcoin and thus the network will be
transitioning to a regime where transaction fees will play a bigger part of
miners' revenue. We consider a simple model where consumers demand bitcoins for
transactions, but not for hoarding bitcoins, and we analyze market equilibrium
where the demand is matched with the hashrate supplied by miners. Our main
conclusion is that the exchange rate of Bitcoin cannot be determined from the
market equilibrium and so our arguments support the hypothesis that Bitcoin
price has no economic fundamentals and is free to fluctuate according to the
present demand for hoarding and speculation. We point out that increasing fees
bear the risk of Bitcoin being outcompeted by its main rival Ethereum, and that
decreasing revenues to miners depreciate the perception of Bitcoin as a medium
for store value (hoarding demand) which will have effect its exchange rate.

In the present study, for the first time, an effort sharing approach based on
Inertia and Capability principles is proposed to assess European Union (EU27)
carbon budget distribution among the Member States. This is done within the
context of achieving the Green Deal objective and EU27 carbon neutrality by
2050. An in-depth analysis is carried out about the role of Economic Decoupling
embedded in the Capability principle to evaluate the correlation between the
expected increase of economic production and the level of carbon intensity in
the Member States. As decarbonization is a dynamic process, the study proposes
a simple mathematical model as a policy tool to assess and redistribute Member
States carbon budgets as frequently as necessary to encourage progress or
overcome the difficulties each Member State may face during the decarbonization
pathways.

This paper presents discrete convex analysis as a tool for economics and game
theory. Discrete convex analysis is a new framework of discrete mathematics and
optimization, developed during the last two decades. Recently, it is being
recognized as a powerful tool for analyzing economic or game models with
indivisibilities. The main feature of discrete convex analysis is the
distinction of two convexity concepts, M-convexity and L-convexity, for
functions in integer or binary variables, together with their conjugacy
relationship. The crucial fact is that M-concavity, or its variant called
M-natural-concavity, is equivalent to the (gross) substitutes property in
economics. Fundamental theorems in discrete convex analysis such as the M-L
conjugacy theorems, discrete separation theorems and discrete fixed point
theorems yield structural results in economics such as the existence of
equilibria and the lattice structure of equilibrium price vectors. Algorithms
in discrete convex analysis give iterative auction algorithms as well as
computational methods for equilibria.

Since its emergence around 2010, deep learning has rapidly become the most
important technique in Artificial Intelligence (AI), producing an array of
scientific firsts in areas as diverse as protein folding, drug discovery,
integrated chip design, and weather prediction. As more scientists and
engineers adopt deep learning, it is important to consider what effect
widespread deployment would have on scientific progress and, ultimately,
economic growth. We assess this impact by estimating the idea production
function for AI in two computer vision tasks that are considered key test-beds
for deep learning and show that AI idea production is notably more
capital-intensive than traditional R&D. Because increasing the
capital-intensity of R&D accelerates the investments that make scientists and
engineers more productive, our work suggests that AI-augmented R&D has the
potential to speed up technological change and economic growth.

This paper provides a systematic and critical review of the economics
literature on data as an economic good and draws lessons for data governance.
We conclude that focusing on data as an economic good in governance efforts is
hardwired to only result in more data production and cannot deliver other
societal goals contrary to what is often claimed in the literature and policy.
Data governance is often a red herring which distracts from other digital
problems. The governance of digital society cannot rely exclusively on
data-centric economic models. We review the literatures and the underlying
empirical and political claims concerning data commons. While commons thinking
is useful to frame digital problems in terms of ecologies, it has important
limitations. We propose a political-ecological approach to governing the
digital society, defined by ecological thinking about governance problems and
the awareness of the political nature of framing the problems and mapping their
ecological makeup.

Decarbonization of power systems plays a crucial role in achieving carbon
neutral goals across the globe, but there exists a sharp contradiction between
the emission reduction and levelized generation cost. Therefore, it is of great
importance for power system operators to take economic as well as low-carbon
factors into account. This paper establishes a low-carbon economic dispatch
model of bulk power systems based on Nash bargaining game, which derives a Nash
bargaining solution making a reasonable trade-off between economic and
low-carbon objectives. Because the Nash bargaining solution satisfies Pareto
effectiveness, we analyze the computational complexity of Pareto frontiers with
parametric linear programming and interpret the inefficiency of the method.
Instead, we assign a group of dynamic weights in the objective function of the
proposed low-carbon economic dispatch model so as to improve the computational
efficiency by decoupling time periods and avoiding the complete computation of
Pareto frontiers. In the end, we validate the proposed model and the algorithm
by a realistic nationwide simulation in mainland China.

The availability of data on economic uncertainty sparked a lot of interest in
models that can timely quantify episodes of international spillovers of
uncertainty. This challenging task involves trading off estimation accuracy for
more timely quantification. This paper develops a local vector autoregressive
model (VAR) that allows for adaptive estimation of the time-varying
multivariate dependency. Under local, we mean that for each point in time, we
simultaneously estimate the longest interval on which the model is constant
with the model parameters. The simulation study shows that the model can handle
one or multiple sudden breaks as well as a smooth break in the data. The
empirical application is done using monthly Economic Policy Uncertainty data.
The local model highlights that the empirical data primarily consists of long
homogeneous episodes, interrupted by a small number of heterogeneous ones, that
correspond to crises. Based on this observation, we create a crisis index,
which reflects the homogeneity of the sample over time. Furthermore, the local
model shows superiority against the rolling window estimation.

This is a study on the potential widespread usage of alternative fuel
vehicles, linking them with the socio-economic status of the respective
consumers as well as the impact on the resulting air quality index. Research in
this area aims to leverage machine learning techniques in order to promote
appropriate policies for the proliferation of alternative fuel vehicles such as
electric vehicles with due justice to different population groups. Pearson
correlation coefficient is deployed in the modeling the relationships between
socio-economic data, air quality index and data on alternative fuel vehicles.
Linear regression is used to conduct predictive modeling on air quality index
as per the adoption of alternative fuel vehicles, based on socio-economic
factors. This work exemplifies artificial intelligence for social good.

Racialized economic segregation, a key metric that simultaneously accounts
for spatial, social and income polarization, has been linked to adverse health
outcomes, including morbidity and mortality; however, statistical methods for
measuring the association between racialized economic segregation and health
outcomes are not well-developed and are usually studied at the individual
level. In this paper we propose a two-stage Bayesian statistical framework that
provides a broad, flexible approach to studying the spatially varying
association between premature mortality and racialized economic segregation,
while accounting for neighborhood-level latent health factors across US
counties. We apply our method by using data from three sources: (1) the CDC
WONDER, (2) the County Health Rankings, and (3) the Public Health Disparities
Geocoding Project. Findings from our study show that the posterior estimates of
latent health factors clearly demonstrate geographical patterning across US
counties. Additionally, our results highlight the importance of accounting for
the presence of spatial autocorrelation in racialized economic segregation
measures, in health equity focused settings.

One perspective to view the economic development of cities is through the
presence of multinational firms; how subsidiaries of various organizations are
set up throughout the globe and how cities are connected to each other through
these networks of multinational firms. Analysis of these networks can reveal
interesting economical and spatial trends, as well as help us understand the
importance of cities in national and regional economic development. This paper
aims to study networks of cities formed due to the linkages of multinational
firms over a decade (from 2010 to 2019). More specifically we are interested in
analyzing the growth and stability of various cities in terms of the
connections they form with other cities over time. Our results can be
summarized into two key findings: First, we ascertain the central position of
several cities due to their economically stable connections; Second, we
successfully identify cities that have evolved over the past decade as the
presence of multinational firms has increased in these cities.

Cybersecurity planning is challenging for digitized companies that want
adequate protection without overspending money. Currently, the lack of
investments and perverse economic incentives are the root cause of
cyberattacks, which results in several economic impacts on companies worldwide.
Therefore, cybersecurity planning has to consider technical and economic
dimensions to help companies achieve a better cybersecurity strategy. This
article introduces SECAdvisor, a tool to support cybersecurity planning using
economic models. SECAdvisor allows to (a) understand the risks and valuation of
different businesses' information, (b) calculate the optimal investment in
cybersecurity for a company, (c) receive a recommendation of protections based
on the budget available and demands, and (d) compare protection solutions in
terms of cost-efficiency. Furthermore, evaluations on usability and real-world
training activities performed using SECAdvisor are discussed.

We introduce a spatial economic growth model where space is described as a
network of interconnected geographic locations and we study a corresponding
finite-dimensional optimal control problem on a graph with state constraints.
Economic growth models on networks are motivated by the nature of spatial
economic data, which naturally possess a graph-like structure: this fact makes
these models well-suited for numerical implementation and calibration. The
network setting is different from the one adopted in the related literature,
where space is modeled as a subset of a Euclidean space, which gives rise to
infinite dimensional optimal control problems. After introducing the model and
the related control problem, we prove existence and uniqueness of an optimal
control and a regularity result for the value function, which sets up the basis
for a deeper study of the optimal strategies. Then, we focus on specific cases
where it is possible to find, under suitable assumptions, an explicit solution
of the control problem. Finally, we discuss the cases of networks of two and
three geographic locations.

As large language models (LLMs) like GPT become increasingly prevalent, it is
essential that we assess their capabilities beyond language processing. This
paper examines the economic rationality of GPT by instructing it to make
budgetary decisions in four domains: risk, time, social, and food preferences.
We measure economic rationality by assessing the consistency of GPT's decisions
with utility maximization in classic revealed preference theory. We find that
GPT's decisions are largely rational in each domain and demonstrate higher
rationality score than those of human subjects in a parallel experiment and in
the literature. Moreover, the estimated preference parameters of GPT are
slightly different from human subjects and exhibit a lower degree of
heterogeneity. We also find that the rationality scores are robust to the
degree of randomness and demographic settings such as age and gender, but are
sensitive to contexts based on the language frames of the choice situations.
These results suggest the potential of LLMs to make good decisions and the need
to further understand their capabilities, limitations, and underlying
mechanisms.

This paper explores the features of cyclonic disturbances (CDs) in the North
Indian Ocean (NIO) by utilizing data from 1990 to 2022. It investigates the
occurrence rate of these disturbances and their effects on human and economic
losses throughout the mentioned period. The analysis demonstrates a rising
trend in the occurrence of CDs in the NIO. While there has been a slight
decline in CD-related fatalities since 2015, but there has been a considerable
increase in economic losses. These findings can be attributed to enhanced
government initiatives in disaster prevention and mitigation in recent years,
as well as rapid economic growth in regions prone to CDs. The study sheds light
on the significance of addressing the impact of CDs on both human lives and
economic stability in the NIO region.

A central question in economics is whether automation will displace human
labor and diminish standards of living. Whilst prior works typically frame this
question as a competition between human labor and machines, we frame it as a
competition between human consumers and human suppliers. Specifically, we
observe that human needs favor long tail distributions, i.e., a long list of
niche items that are substantial in aggregate demand. In turn, the long tails
are reflected in the goods and services that fulfill those needs. With this
background, we propose a theoretical model of economic activity on a long tail
distribution, where innovation in demand for new niche outputs competes with
innovation in supply automation for mature outputs. Our model yields analytic
expressions and asymptotes for the shares of automation and labor in terms of
just four parameters: the rates of innovation in supply and demand, the
exponent of the long tail distribution and an initial value. We validate the
model via non-linear stochastic regression on historical US economic data with
surprising accuracy.

This paper explores the potential impacts of large language models (LLMs) on
the Chinese labor market. We analyze occupational exposure to LLM capabilities
by incorporating human expertise and LLM classifications, following Eloundou et
al. (2023)'s methodology. We then aggregate occupation exposure to the industry
level to obtain industry exposure scores. The results indicate a positive
correlation between occupation exposure and wage levels/experience premiums,
suggesting higher-paying and experience-intensive jobs may face greater
displacement risks from LLM-powered software. The industry exposure scores
align with expert assessments and economic intuitions. We also develop an
economic growth model incorporating industry exposure to quantify the
productivity-employment trade-off from AI adoption. Overall, this study
provides an analytical basis for understanding the labor market impacts of
increasingly capable AI systems in China. Key innovations include the
occupation-level exposure analysis, industry aggregation approach, and economic
modeling incorporating AI adoption and labor market effects. The findings will
inform policymakers and businesses on strategies for maximizing the benefits of
AI while mitigating adverse disruption risks.

Quantitative models are an important decision-making factor for policy makers
and investors. Predicting an economic recession with high accuracy and
reliability would be very beneficial for the society. This paper assesses
machine learning technics to predict economic recessions in United States using
market sentiment and economic indicators (seventy-five explanatory variables)
from Jan 1986 - June 2022 on a monthly basis frequency. In order to solve the
issue of missing time-series data points, Autoregressive Integrated Moving
Average (ARIMA) method used to backcast explanatory variables. Analysis started
with reduction in high dimensional dataset to only most important characters
using Boruta algorithm, correlation matrix and solving multicollinearity issue.
Afterwards, built various cross-validated models, both probability regression
methods and machine learning technics, to predict recession binary outcome. The
methods considered are Probit, Logit, Elastic Net, Random Forest, Gradient
Boosting, and Neural Network. Lastly, discussed different models performance
based on confusion matrix, accuracy and F1 score with potential reasons for
their weakness and robustness.

We examine whether substantial AI automation could accelerate global economic
growth by about an order of magnitude, akin to the economic growth effects of
the Industrial Revolution. We identify three primary drivers for such growth:
1) the scalability of an AI ``labor force" restoring a regime of increasing
returns to scale, 2) the rapid expansion of an AI labor force, and 3) a massive
increase in output from rapid automation occurring over a brief period of time.
Against this backdrop, we evaluate nine counterarguments, including regulatory
hurdles, production bottlenecks, alignment issues, and the pace of automation.
We tentatively assess these arguments, finding most are unlikely deciders. We
conclude that explosive growth seems plausible with AI capable of broadly
substituting for human labor, but high confidence in this claim seems currently
unwarranted. Key questions remain about the intensity of regulatory responses
to AI, physical bottlenecks in production, the economic value of superhuman
abilities, and the rate at which AI automation could occur.

Water markets represent a policy tool that aims at finding efficient water
allocations among competing users by promoting reallocations from low-value to
high-value uses. In Canada, water markets have been discussed and implemented
at the provincial level; however, at the national level a study about the
economic benefits of its implementation is still lacking. This paper fills this
void by implementing a water market in Canada and examine how water endowment
shocks would affect the economy under the assumptions of general equilibrium
theory. Our results show a water market would damp the economic loss in case of
reductions in water endowment, but it also cuts back on the economic expansion
that would follow from an increase on it. These results provide new insights on
the subject and will provide a novel look and reinvigorate informed discussions
on the use of water markets in Canada as a potential tool to cope with
climate-induced water supply changes.

We study the implications of model uncertainty in a climate-economics
framework with three types of capital: "dirty" capital that produces carbon
emissions when used for production, "clean" capital that generates no emissions
but is initially less productive than dirty capital, and knowledge capital that
increases with R\&D investment and leads to technological innovation in green
sector productivity. To solve our high-dimensional, non-linear model framework
we implement a neural-network-based global solution method. We show there are
first-order impacts of model uncertainty on optimal decisions and social
valuations in our integrated climate-economic-innovation framework. Accounting
for interconnected uncertainty over climate dynamics, economic damages from
climate change, and the arrival of a green technological change leads to
substantial adjustments to investment in the different capital types in
anticipation of technological change and the revelation of climate damage
severity.

This study examines the role of human capital investment in driving
sustainable socio-economic growth within the energy industry. The fuel and
energy sector undeniably forms the backbone of contemporary economies,
supplying vital resources that underpin industrial activities, transportation,
and broader societal operations. In the context of the global shift toward
sustainability, it is crucial to focus not just on technological innovation but
also on cultivating human capital within this sector. This is particularly
relevant considering the recent shift towards green and renewable energy
solutions. In this study, we utilize bibliometric analysis, drawing from a
dataset of 1933 documents (represented by research papers, conference
proceedings, and book chapters) indexed in the Web of Science (WoS) database.
We conduct a network cluster analysis of the textual and bibliometric data
using VOSViewer software. The findings stemming from our analysis indicate that
investments in human capital are perceived as important in achieving long-term
sustainable economic growth in the energy companies both in Russia and
worldwide. In addition, it appears that the role of human capital in the energy
sector is gaining more popularity both among Russian and international
researchers and academics.

The present study aimed to forecast the exports of a select group of
Organization for Economic Co-operation and Development (OECD) countries and
Iran using the neural networks. The data concerning the exports of the above
countries from 1970 to 2019 were collected. The collected data were implemented
to forecast the exports of the investigated countries for 2021 to 2025. The
analysis was performed using the Multi-Layer-Perceptron (MLP) neural network in
Python. Out of the total number, 75 percent were used as training data, and 25
percent were used as the test data. The findings of the study were evaluated
with 99% accuracy, which indicated the reliability of the output of the
network. The Results show that Covid-19 has affected exports over time.
However, long-term export contracts are less affected by tensions and crises,
due to the effect of exports on economic growth, per capita income and it is
better for economic policies of countries to use long-term export contracts.

The co-evolution of economic and ecological activities represents one of the
fundamental challenges in the realm of sustainable development. This study on
the word trends in mainstream newspapers from the UK and China reveals that
both early-industrialised countries and latecomers follow three modes of
economic and ecological co-evolution. First, both economic and ecological words
demonstrate an S-shaped growth trajectory, and the mode underscores the
importance of information propagation, whilst also highlighting the crucial
role of self-organisation in the accept society. Second, the co-occurrence of
these two type words exhibits a Z-shaped relationship: for two-thirds of the
observed period, they display synergistic interactions, while the remaining
time shows trade-offs. Lastly, the words related to ecological degradation
follow M-shaped trajectories in parallel with economic growth, suggesting
periodic disruptions and reconstructions in their interrelationships. Our
findings contribute to a more nuanced understanding of the co-evolutionary
mechanisms that govern collective behaviours in human society.

This paper delves into the intricate relationship between the formation of
air alliances and the shifts in airport and economic gravity centers across
European countries during the period spanning 1990 to 2019. Employing
descriptive analysis and the weighted mean center methodology, it explores the
interplay between air passenger numbers and economic indicators, revealing a
close and interdependent correlation between these factors. The study sheds
light on the dynamic landscape of economic gravity centers, which experienced
discernible shifts over time. However, it observes even more pronounced
transitions in airport gravity centers. Statistical t-tests underscore
significant differences in standard deviations when comparing pre- and post-air
alliance periods for airport gravity centers. These disparities serve as a
testament to the profound impact of airline alliances on the distribution of
air traffic. These findings underscore the pivotal role of air alliances in
reshaping the aviation landscape, and they beckon further investigation into
their influence on broader economic transformations. Notably, this study
pioneers the use of geographical means and standard deviations for rigorous
statistical testing of economic hypotheses, signifying a significant
contribution to the field.

Nigeria's remarkable information and communication technology (ICT) journey
spans decades, playing a pivotal role in economic sustainability, especially as
the nation celebrates its Republic at Sixty. This paper provides an overview of
Nigeria's ICT journey, underscoring its central role in sustainable economic
prosperity. We explore the potential of artificial intelligence, blockchain,
and the Internet of Things (IoT), revealing the remarkable opportunities on the
horizon. We stress the urgency of achieving digital inclusivity, bridging the
urban-rural gap, and reducing the technological divide, all of which are
critical as Nigeria marks its sixtieth year. We intend to prove the invaluable
opportunities of ICT for policymakers, business leaders, and educational
institutes as Nigeria looks towards enduring economic development in this
digital age. Specifically, we envision a dynamic landscape where emerging
technologies are set to redefine industries, supercharge economic growth, and
enhance the quality of life for every Nigerian.

In the era of sustainability, firms grapple with the decision of how much to
invest in green innovation and how it influences their economic trajectory.
This study employs the Crepon, Duguet, and Mairesse (CDM) framework to examine
the conversion of R&D funds into patents and their impact on productivity,
effectively addressing endogeneity by utilizing predicted dependent variables
at each stage to exclude unobservable factors. Extending the classical CDM
model, this study contrasts green and non-green innovations' economic effects.
The results show non-green patents predominantly drive productivity gains,
while green patents have a limited impact in non-heavy polluting firms.
However, in high-pollution and manufacturing sectors, both innovation types
equally enhance productivity. Using unconditional quantile regression, I found
green innovation's productivity impact follows an inverse U-shape, unlike the
U-shaped pattern of non-green innovation. Significantly, in the 50th to 80th
productivity percentiles of manufacturing and high-pollution firms, green
innovation not only contributes to environmental sustainability but also
outperforms non-green innovation economically.

This book, written in Portuguese, presents a comprehensive analysis of the
air transport industry in Brazil, highlighting its vital importance to the
country's economy. It explores the sector's complexity, from economic
characteristics to interaction with the national aeronautical industry, through
the specialization of the workforce and market demand analysis. The book delves
into the economic regulation of air transport, tracing the evolution from
periods of strict regulation to phases of liberalization and deregulation, and
examines market dynamics, focusing on concentration and competitiveness. It
also analyzes demand and supply through case studies, investigating everything
from tourists traveling within Brazil to the coverage of the national territory
and the prices of air tickets. Finally, the book proposes principles for the
regulation and public policies of the air sector, emphasizing the priority of
the passenger, the business environment, access to air transport, and economic
efficiency, culminating in the advocacy for a free market, but with protection
for competition and the consumer.

There is increasing interest in using LLMs as decision-making "agents." Doing
so includes many degrees of freedom: which model should be used; how should it
be prompted; should it be asked to introspect, conduct chain-of-thought
reasoning, etc? Settling these questions -- and more broadly, determining
whether an LLM agent is reliable enough to be trusted -- requires a methodology
for assessing such an agent's economic rationality. In this paper, we provide
one. We begin by surveying the economic literature on rational decision making,
taxonomizing a large set of fine-grained "elements" that an agent should
exhibit, along with dependencies between them. We then propose a benchmark
distribution that quantitatively scores an LLMs performance on these elements
and, combined with a user-provided rubric, produces a "rationality report
card." Finally, we describe the results of a large-scale empirical experiment
with 14 different LLMs, characterizing the both current state of the art and
the impact of different model sizes on models' ability to exhibit rational
behavior.

Artificial Intelligence (AI) holds promise as a technology that can be used
to improve government and economic policy-making. This paper proposes a new
research agenda towards this end by introducing Social Environment Design, a
general framework for the use of AI for automated policy-making that connects
with the Reinforcement Learning, EconCS, and Computational Social Choice
communities. The framework seeks to capture general economic environments,
includes voting on policy objectives, and gives a direction for the systematic
analysis of government and economic policy through AI simulation. We highlight
key open problems for future research in AI-based policy-making. By solving
these challenges, we hope to achieve various social welfare objectives, thereby
promoting more ethical and responsible decision making.

By integrating survival analysis, machine learning algorithms, and economic
interpretation, this research examines the temporal dynamics associated with
attaining a 5 percent rise in purchasing power parity-adjusted GDP per capita
over a period of 120 months (2013-2022). A comparative investigation reveals
that DeepSurv is proficient at capturing non-linear interactions, although
standard models exhibit comparable performance under certain circumstances. The
weight matrix evaluates the economic ramifications of vulnerabilities, risks,
and capacities. In order to meet the GDPpc objective, the findings emphasize
the need of a balanced approach to risk-taking, strategic vulnerability
reduction, and investment in governmental capacities and social cohesiveness.
Policy guidelines promote individualized approaches that take into account the
complex dynamics at play while making decisions.

The digital innovation accompanied by explicit economic incentives have
fundamentally changed the process of innovation diffusion. As a representative
of digital innovation, NFTs provide a decentralized and secure way to
authenticate and trade digital assets, offering the potential for new revenue
streams in the digital space. However, current researches about NFTs mainly
focus on their transaction networks and community culture, leaving the
interplay among diffusion dynamics, economic dynamics, and social constraints
on Twitter. By collecting and analyzing NFTs-related tweet dataset, the
motivations of retweeters, the information mechanisms behind emojis, and the
networked-based diffusion dynamics is systematically investigated. Results
indicate that Retweeting is fueled by Freemint and trading information, with
the higher economic incentives as a major motivation and some potential
organizational tendencies. The diffusion of NFT is primarily driven by a
'Ringed-layered' information mechanism involving individual promoters and
speculators. Both the frequency and presentation of content contribute
positively to the growth of the retweet network. This study contributes to the
innovation diffusion theory with economic incentives embedded.

Integration of various electricity generating technologies (such as natural
gas, wind, nuclear, etc.) with storage systems (such as thermal, battery
electric, hydrogen, etc.) has the potential to improve the economic
competitiveness of modern energy systems. Driven by the need to efficiently
assess the economic feasibility of various energy system configurations in
early system concept development, this work outlines a versatile computational
framework for assessing the net present value of various integrated storage
technologies. The subsystems' fundamental dynamics are defined, with a
particular emphasis on balancing critical physical and economic domains to
enable optimal decision-making in the context of capacity and dispatch
optimization. In its presented form, the framework formulates a linear, convex
optimization problem that can be efficiently solved using a direct
transcription approach in the open-source software DTQP. Three case studies are
considered to demonstrate and validate the capabilities of the framework,
highlighting its value and computational efficiency in facilitating economic
assessment of various configurations of energy systems. In particular, natural
gas with thermal storage and carbon capture, wind energy with battery storage,
and nuclear with hydrogen are demonstrated.

This paper is an original attempt to understand the foundations of economic
reasoning. It endeavors to rigorously define the relationship between
subjective interpretations and objective valuations of such interpretations in
the context of theoretical economics. This analysis is substantially expanded
through a dynamic approach, where the truth of a valuation results in an
updated interpretation or changes in the agent's subjective belief regarding
the effectiveness of the selected action as well as the objective reality of
the effectiveness of all other possible actions (i.e. consequence realization).
Complications arise when the economic agent is presented with a set of actions
that render ambiguous preference, or when the effectiveness of an action cannot
be perceived upon its selection, thereby necessitating a different theory of
choice and consequence realization.

This paper presents a model that studies the impact of credit expansions
arising from increases in collateral values or lower interest rate policies on
long-run productivity and economic growth in a two-sector endogenous growth
economy, with the driver of growth lying in one sector (manufacturing) but not
in the other (real estate). We show that it is not so much aggregate credit
expansion that matters for long-run productivity and economic growth but
sectoral credit expansions. Credit expansions associated mainly with relaxation
of real estate financing (capital investment financing) will be
productivity-and growth-retarding (enhancing). Without financial regulations,
low interest rates and more expansionary monetary policy may so encourage land
speculation using leverage that productive capital investment and economic
growth are decreased. Unlike in standard macroeconomic models, in ours, the
equilibrium price of land will be finite even if the safe rate of interest is
less than the rate of output growth.

Natural resource use and waste production, disposal, and reuse in human
economies are treated in their economic, technological and thermodynamic
aspects. The physical nature of economic production, consumption, saving, and
waste is compared and contrasted with non-human ecologies in terms of entropy
production and evolutionary complexity.

A birth-death lattice gas model about the influence of an environment on the
fitness and concentration evolution of economic entities is analytically
examined. The model can be mapped onto a high order logistic map. The control
parameter is a (scalar) "business plan". Conditions are searched for growth and
decay processes, stable states, upper and lower bounds, bifurcations, periodic
and chaotic solutions. The evolution equation of the economic population for
the best fitted companies indicates "microscopic conditions" for cycling. The
evolution of a dynamic exponent is shown as a function of the business plan
parameters.

Researchers have long proposed using economic approaches to resource
allocation in computer systems. However, few of these proposals became
operational, let alone commercial. Questions persist about the economic
approach regarding its assumptions, value, applicability, and relevance to
system design. The goal of this paper is to answer these questions. We find
that market-based resource allocation is useful, and more importantly, that
mechanism design and system design should be integrated to produce systems that
are both economically and computationally efficient.

In this article one investigates Rugina's Orientation Table and one gives
particular examples for several of its seven models. Leon Walras's Economics of
Stable Equilibrium and Keynes's Economics of Disequilibrium are combined in
Rugina's Orientation Table in systems which are s% stable and 100-s% unstable,
where s may be 100, 95, 65, 50, 35, 5, and 0. The Classical Logic and Modern
Logic are united in Rugina's Integrated Logic, and then generalized in the
Neutrosophic Logic.

A model for economic behavior, under heterogeneous spatial economic
conditions is developed. The role of selection pressure in a Bak-Sneppen-like
dynamics with entity diffusion on a lattice is studied by Monte-Carlo
simulation taking into account business rule(s), like enterprise - enterprise
short range location "interaction"(s), business plan(s) through spin-offs or
merging and enterprise survival evolution law(s). It is numerically found that
the model leads to a sort of phase transition for the fitness gap as a function
of the selection pressure.

In this paper we establish a deep connection between the 3 qubit one-to-two
phase-covariant quantum cloning network of Fuchs et al. [C. Fuchs, N. Gisin,
R.B. Griffiths, C.S. Niu, and A. Peres, Phys. Rev. A 56 no 4, 1163 (1997)], and
its economic 2 qubit counterpart due to Niu and Griffiths [Phys.Rev. A 60 no 4,
2764 (1999)]. A general, necessary and sufficient criterion is derived in order
to characterize the reducibility of 3 qubit cloners to 2 qubit cloners. When
this criterion is fulfilled, economic cloning is possible. We show that the
optimal isotropic or universal 3 qubit cloning machine is not reducible to a 2
qubit cloner.

The monograph is concerned with some key problems of the theory of nonlinear
economic dynamics. The authors' concept consists in analyzing the problem of
structural instability of economic systems within the framework of the
synergetic paradigm. As examples, the classical models of macroeconomics are
considered. The authors present the results of the study of the phenomenon of
self-organization in open and nonequilibrium economic systems. The generation
of limit cycles, as well as of more complex periodic structures, is discussed;
the character of their stability is examined.

Most people agree that human activities are consistent with physical laws.
One may naturally think that sensible economic theories can be derived from
physical laws and evolutionary principles. This is indeed the case. In this
paper, we present a newly developed production theory of economics from
biophysical principles. The theory is a compact analytical model that provides,
in our view, a much more realistic understanding of economic (as well as social
and biological) phenomena than the neoclassical theory of production.

A detailed study of the criteria for stability of the scalar potential and
the proper electroweak symmetry breaking pattern in the economical 3-3-1 model,
is presented. For the analysis we use, and improve, a method previously
developed to study the scalar potential in the two-Higgs-doublet extension of
the standard model. A new theorem related to the stability of the potential is
stated. As a consequence of this study, the consistency of the economical 3-3-1
model emerges.

In this note, we would like to find the laws of electrodynamics in simple
economic systems. In this direction, we identify the chief economic variables
and parameters, scalar and vector, which are amenable to be put directly into
the crouch of the laws of electrodynamics, namely Maxwell's equations.
Moreover, we obtain Phillp's curve, recession and Black-Scholes formula, as
sample applications.

The present paper is based on studying, analyzing and implementing the expert
systems in the financial and accounting domain of the companies, describing the
use method of the informational systems that can be used in the multi-national
companies, public interest institutions, and medium and small dimension
economical entities, in order to optimize the managerial decisions and render
efficient the financial-accounting functionality. The purpose of this paper is
aimed to identifying the economical exigencies of the entities, based on the
already used accounting instruments and the management software that could
consent the control of the economical processes and patrimonial assets.

We argue that Ulam-Hyers stability concept is quite significant in realistic
problems in numerical analysis, biology and economics. A generalization to
nonlinear systems is proposed and applied to the logistic equation (both
differential and difference), SIS epidemic model, Cournot model in economics
and a reaction diffusion equation. To the best of our knowledge this is the
first time Ulam-Hyers stability is considered from the applications point of
view.

There are different Standards of digital multimedia transmission, for example
DVB in Europe and ISDB in Japan and DMB in Korea, with different delivery
system (example MPEG-2, MPEG-4).This paper describe an overview of Digital
Multimedia Transmission (DMT) technologies. The economic aspects of digital
content & software solution industry as a strategic key in the future will be
discussed. The study then focuses on some important policy and technology
issues, such S-DMB, T-DMB, Digital Video Broadcasting Handheld (DVB-H) and
concludes DMT policies for convergence of telecommunications and broadcasting.

Non-symmetric rectangular correlation matrices occur in many problems in
economics. We test the method of extracting statistically meaningful
correlations between input and output variables of large dimensionality and
build a toy model for artificially included correlations in large random time
series.The results are then applied to analysis of polish macroeconomic data
and can be used as an alternative to classical cointegration approach.

We consider a heterogeneous agent-based economic model where economic agents
have strictly bounded rationality and where income allocation strategies evolve
through selective imitation. Income is calculated by a Cobb-Douglas type
production function, and selection of strategies for imitation depends on the
income growth rate they generate. We show that under these conditions, when an
agent adopts a new strategy, the effect on its income growth rate is
immediately visible to other agents, which allows a group of imitating agents
to quickly adapt their strategies when needed.

Some optimization or equilibrium problems involving somehow the concept of
optimal transport are presented in these notes, mainly devoted to applications
to economic and game theory settings. A variant model of transport, taking into
account traffic congestion effects is the first topic, and it shows various
links with Monge-Kantorovich theory and PDEs. Then, two models for urban
planning are introduced. The last section is devoted to two problems from
economics and their translation in the language of optimal transport.

Neuronal mechanisms underlying addiction have been attracting attention in
neurobiology, economics, neuropsychiatry, and neuroeconomics. This paper
proposes a possible link between economic theory of addiction (Becker and
Murphy, 1988) and neurobiological theory of bidirectional synaptic plasticity
(Bienenstock, Cooper, Munro, 1982) based on recent findings in neuroeconomics
and neurobiology of addiction. Furthermore, it is suggested that several
neurobiological substrates such as cortisol (a stress hormone), NMDA and AMPA
receptors/subunits and intracellular calcium in the postsynaptic neurons are
critical factors determining parameters in Becker and Murphy's economic theory
of addiction. Future directions in the application of the theory to studies in
neuroeconomics and neuropsychiatry of addiction and its relation to stress at
the molecular level are discussed.

As we show by using notions of equilibrium in infinite sequential games,
crashes or financial escalations are rational for economic or environmental
agents, who have a vision of an infinite world. This contradicts a picture of a
self-regulating, wise and pacific economic world. In other words, in this
context, equilibrium is not synonymous of stability. We try to draw, from this
statement, methodological consequences and new ways of thinking, especially in
economic game theory. Among those new paths, coinduction is the basis of our
reasoning in infinite games.

Models that explain the economical and political realities of nowadays
societies should help all the world's citizens. Yet, the last four years showed
that the current models are missing. Here we develop a dynamical
society-deciders model showing that the long lasting economical stress can be
solved when increasing fairness in nations. fairness is computed for each
nation using indicators from economy and politics. Rather than austerity versus
spending, the dynamical model suggests that solving crises in western societies
is possible with regulations that reduce the stability of the deciders, while
shifting wealth in the direction of the people. This shall increase the
dynamics among socio-economic classes, further increasing fairness.

In this paper an attempt is made to review technological, economical and
legal aspects of the spam in detail. The technical details will include
different techniques of spam control e.g., filtering techniques, Genetic
Algorithm, Memory Based Classifier, Support Vector Machine Method, etc. The
economic aspect includes Shaping/Rate Throttling Approach/Economic Filtering
and Pricing/Payment based spam control. Finally, the paper discusses the legal
provisions for the control of spam. The scope of the legal options is limited
to USA, European Union, New Zealand, Canada, Britain and Australia.

A stochastic model for pure-jump diffusion (the compound renewal process) can
be used as a zero-order approximation and as a phenomenological description of
tick-by-tick price fluctuations. This leads to an exact and explicit general
formula for the martingale price of a European call option. A complete
derivation of this result is presented by means of elementary probabilistic
tools.

Chaos and nonlinear economic dynamics are addressed for a quantum coupled map
lattice model of an artificial economy, with quantized supply and demand
equilibrium conditions. The measure theoretic properties and the patterns that
emerge in both the economic business volume dynamics' diagrams as well as in
the quantum mean field averages are addressed and conclusions are drawn in
regards to the application of quantum chaos theory to address signatures of
chaotic dynamics in relevant discrete economic state variables.

We propose an extremely simple mathematical model that is shown to be able to
account for more than 99 per cent of all the variation in economic and
demographic macrodynamics of the world for almost two millennia of its history.
This appears to suggest a novel approach to the formation of the general theory
of social macroevolution.

In this paper, simple mathematical models from Control Theory are applied to
three very important economic paradigms, namely (a) minimum wages in
self-regulating markets, (b) market-versus-true values and currency rates, and
(c) government spending and taxation levels. Analytical solutions are provided
in all three paradigms and some useful conclusions are drawn in terms of
variable analysis. This short study can be used as an example of how feedback
models and stability analysis can be applied as a guideline of 'proofs' in the
context of economic policies.

In this article, we review quantile models with endogeneity. We focus on
models that achieve identification through the use of instrumental variables
and discuss conditions under which partial and point identification are
obtained. We discuss key conditions, which include monotonicity and
full-rank-type conditions, in detail. In providing this review, we update the
identification results of Chernozhukov and Hansen (2005, Econometrica). We
illustrate the modeling assumptions through economically motivated examples. We
also briefly review the literature on estimation and inference.
  Key Words: identification, treatment effects, structural models, instrumental
variables

Risk, including economic risk, is increasingly a concern for public policy
and management. The possibility of dealing effectively with risk is hampered,
however, by lack of a sound empirical basis for risk assessment and management.
The paper demonstrates the general point for cost and demand risks in urban
rail projects. The paper presents empirical evidence that allow valid economic
risk assessment and management of urban rail projects, including benchmarking
of individual or groups of projects. Benchmarking of the Copenhagen Metro is
presented as a case in point. The approach developed is proposed as a model for
other types of policies and projects in order to improve economic and financial
risk assessment and management in policy and planning.

As we show using the notion of equilibrium in the theory of infinite
sequential games, bubbles and escalations are rational for economic and
environmental agents, who believe in an infinite world. This goes against a
vision of a self regulating, wise and pacific economy in equilibrium. In other
words, in this context, equilibrium is not a synonymous of stability. We
attempt to draw from this statement methodological consequences and a new
approach to economics. To the mindware of economic agents (a concept due to
cognitive psychology) we propose to add coinduction to properly reason on
infinite games. This way we refine the notion of rationality.

Mathematically, a homothetic function is a function of the form $f({\bf
x})=F(h(x_1,...,x_n))$, where $h$ is a homogeneous function of any degree $d\ne
0$ and $F$ is a monotonically increasing function. In economics homothetic
functions are production functions whose marginal technical rate of
substitution is homogeneous of degree zero.
  In this paper we classify homothetic functions satisfying the homogeneous
Monge-Amp\`ere equation. Several applications to production models in economics
will also be given.

A novel comparative research and analysis method is proposed and applied on
the Hungarian economic sectors. The question of what factors have an effect on
their net income is essential for enterprises. First, the potential indicators
related to economic sectors were studied and then compared to the net income of
the surveyed enterprises. The data resulting from the comparison showed that
the growing penetration of electronic marketpalces contributed to the change of
the net income of enterprises in various economic sectors to the extent of 37%.
Among all the potential indicators, only the indicator of electronic
marketplaces has a direct influence on the net income of enterprises. Two
clusters based on the potential indicators were indicated.

Every production-recycling iteration accumulates an inevitable proportion of
its matter-energy in the environment, lest the production process itself would
be a system in perpetual motion, violating the second law of Thermodynamics.
Such high-entropy matter depletes finite stocks of ecosystem services provided
by the ecosphere, hence are incompatible with the long-term growth in the
material scale of the economic process. Moreover, the complex natural systems
governing such stocks respond to depletion by possibly sudden environmental
transitions, thus hindering markets' very ability to adapt to the new
equilibrium conditions. Consequently, uncertainty of critical resilience
thresholds constrains material economic growth.

Recent research in economic theory attempts to study optimal economic growth
and spatial location of economic activity in a unified framework. So far, the
key result of this literature - asymptotic convergence, even in the absence of
decreasing returns to capital - relies on specific assumptions about the
objective of the social planner. We show that this result does not depend on
such restrictive assumptions and obtains for a broader class of objective
functions. We also generalize this finding, allowing for the time-varying
technology parameter, and provide an explicit solution for the dynamics of
spatial distribution of the capital stock.

The econophysics approach to socio-economic systems is based on the
assumption of their complexity. Such assumption inevitably lead to another
assumption, namely that underlying interconnections within socio-economic
systems, particularly financial markets, are nonlinear, which is shown to be
true even in mainstream economic literature. Thus it is surprising to see that
network analysis of financial markets is based on linear correlation and its
derivatives. An analysis based on partial correlation is of particular interest
as it leading to the vicinity of causality detection in time series analysis.
In this paper we generalise the Planar Maximally Filtered Graphs and Partial
Correlation Planar Graphs to incorporate nonlinearity using partial mutual
information.

This paper discusses the size distribution, - in economic terms - of the
Italian municipalities over the period 2007-2011. Yearly data are rather well
fitted by a modified Lavalette law, while Zipf-Mandelbrot-Pareto law seems to
fail in this doing. The analysis is performed either at a national as well as
at a local (regional and provincial) level. Deviations are discussed as
originating in so called king and vice-roy effects. Results confirm that Italy
is shared among very different regional realities. The case of Lazio is
puzzling.

The present paper analyses the formal parallelism existing between the laws
of thermodynamics and some economic principles. Based on previous works, we
shall show how the existence in Economics of principles analogous to those in
thermodynamics involves the occurrence of economic events that remind of
well-known phenomenological thermodynamic paradigms (i.e., the magnetocaloric
effect and population inversion). We shall also show how the phase transition
and renormalization theory provides a natural framework to understand and
predict trend changes in stock markets. Finally, current negotiation strategies
in financial markets are briefly reviewed.

Society needs to prepare for more severe space weather than it has
experienced in the modern technological era. To enable that, we must both
quantify extreme-event characteristics and analyze impacts of lesser events
that are frequent yet severe enough to be informative. Exploratory studies
suggest that economic impacts of a century-level space hurricane and of a
century of lesser space-weather "gales" may turn out to be of the same order of
magnitude. The economic benefits of effective mitigation of the impacts of
space gales may substantially exceed the required investments, even as these
investments provide valuable information to prepare for the worst possible
storms.

In this paper we relate the Equilibrium Assignment Problem (EAP), which is
underlying in several economics models, to a system of nonlinear equations that
we call the "nonlinear Bernstein-Schr\"odinger system", which is well-known in
the linear case, but whose nonlinear extension does not seem to have been
studied. We apply this connection to derive an existence result for the EAP,
and an efficient computational method.

We analyze the decisive role played by the complexity of economic systems at
the onset of the industrialization process of countries over the past 50 years.
Our analysis of the input growth dynamics, based on a recently introduced
measure of economic complexity, reveals that more differentiated and more
complex economies face a lower barrier (in terms of GDP per capita) when
starting the transition towards industrialization. Moreover, adding the
complexity dimension to the industrialization process description helps to
reconcile current theories with empirical findings.

The theorems we proved describe the structure of economic equilibrium in the
exchange economy model. We have studied the structure of property vectors under
given structure of demand vectors at which given price vector is equilibrium
one. On this ground, we describe the general structure of the equilibrium state
and give characteristic of equilibrium state describing economic recession. The
theory developed is applied to explain the state of the economy in some
European countries.

Empirical analysis in economics often faces the difficulty that the data is
correlated and heterogeneous in some unknown form. Spatial parametric
approaches have been widely used to account for dependence structures, but the
problem of directly deal with spatially varying parameters has been largely
unexplored. The problem can be serious in all those cases in which we have no
prior information justified by the economic theory. In this paper we propose an
algorithm-based procedure which is able to endogenously identify structural
breaks in space. The proposed algorithm is illustrated by using two well known
house price data sets.

Filtering has had a profound impact as a device of perceiving information and
deriving agent expectations in dynamic economic models. For an abstract
economic system, this paper shows that the foundation of applying the filtering
method corresponds to the existence of a conditional expectation as an
equilibrium process. Agent-based rational behavior of looking backward and
looking forward is generalized to a conditional expectation process where the
economic system is approximated by a class of models, which can be represented
and estimated without information loss. The proposed framework elucidates the
range of applications of a general filtering device and is not limited to a
particular model class such as rational expectations.

In the light of contemporary discussions of inter and transdisciplinarity,
this paper approaches econophysics and sociophysics to seek a response to the
question -- whether these interdisciplinary fields could contribute to physics
and economics. Drawing upon the literature on history and philosophy of
science, the paper argues that the two way traffic between physics and
economics has a long history and this is likely to continue in the future.

This research reports on the relationship and significance of social-economic
factors (age, sex, employment status) and modes of HIV/AIDS transmission to the
HIV/AIDS spread. Logistic regression model, a form of probabilistic function
for binary response was used to relate social-economic factors (age, sex,
employment status) to HIV/AIDS spread. The statistical predictive model was
used to project the likelihood response of HIV/AIDS spread with a larger
population using 10,000 Bootstrap re-sampling observations.

The optimization of process economics within the model predictive control
(MPC) formulation has given rise to a new control paradigm known as economic
MPC (EMPC). Several authors have discussed the closed-loop properties of
EMPC-controlled deterministic systems, however, little have uncertain systems
been studied. In this paper we propose EMPC formulations for nonlinear
Markovian switching systems which guarantee recursive feasibility, asymptotic
performance bounds and constrained mean square (MS) stability.

We present functional forms allowing a broader range of analytic solutions to
common economic equilibrium problems. These can increase the realism of
pen-and-paper solutions or speed large-scale numerical solutions as
computational subroutines. We use the latter approach to build a tractable
heterogeneous firm model of international trade accommodating economies of
scale in export and diseconomies of scale in production, providing a natural,
unified solution to several puzzles concerning trade costs. We briefly
highlight applications in a range of other fields. Our method of generating
analytic solutions is a discrete approximation to a logarithmically modified
Laplace transform of equilibrium conditions.

China's new labor law -- Labor Contract Law has been put into practice for
over one year. Since its inception, debates have been whirling around the
nation, if not the world. In this article, we take an economic perspective to
analyze the possible impact of the core item -- open-ended employment contract,
and we find that it deals poorly with adverse selection, with moral hazard
problems arise, which fails to meet the expectations of law-makers and other
parties.

This paper proposes a new objective function and quantile regression (QR)
algorithm for load forecasting (LF). In LF, the positive forecasting errors
often have different economic impact from the negative forecasting errors.
Considering this difference, a new objective function is proposed to put
different prices on the positive and negative forecasting errors. QR is used to
find the optimal solution of the proposed objective function. Using normalized
net energy load of New England network, the proposed method is compared with a
time series method, the artificial neural network method, and the support
vector machine method. The simulation results show that the proposed method is
more effective in reducing the economic cost of the LF errors than the other
three methods.

We develop a complexity measure for large-scale economic systems based on
Shannon's concept of entropy. By adopting Leontief's perspective of the
production process as a circular flow, we formulate the process as a Markov
chain. Then we derive a measure of economic complexity as the average number of
bits required to encode the flow of goods and services in the production
process. We illustrate this measure using data from seven national economies,
spanning several decades.

I show that firms price almost competitively and consumers can infer product
quality from prices in markets where firms differ in quality and production
cost, and learning prices is costly. Bankruptcy risk or regulation links higher
quality to lower cost. If high-quality firms have lower cost, then they can
signal quality by cutting prices. Then the low-quality firms must cut prices to
retain customers. This price-cutting race to the bottom ends in a separating
equilibrium in which the low-quality firms charge their competitive price and
the high-quality firms charge slightly less.

This paper examines the risk-adjusted performance and differential fund flows
for socially responsible mutual funds (SRMF). The results show that SRMF rated
high on ESG, perform better than lower rated ESG funds during the period of
economic crisis. The findings also show that low ESG rated SRMF had higher
differential cash-flows than high rated ESG funds except for the period of
economic down turn. The findings are of interest to financial advisors,
investors, mutual fund managers, and researchers on how SRMF performance
responds to periods of economic downturn and expansion

In this article the problem of reconstructing the pattern of connection
between agents from partial empirical data in a macro-economic model is
addressed, given a set of behavioral equations. This systemic point of view
puts the focus on distributional and network effects, rather than
time-dependence. Using the theory of complex networks we compare several models
to reconstruct both the topology and the flows of money of the different types
of monetary transactions, while imposing a series of constraints related to
national accounts, and to empirical network sparsity. Some properties of
reconstructed networks are compared with their empirical counterpart.

Statistical agencies face a dual mandate to publish accurate statistics while
protecting respondent privacy. Increasing privacy protection requires decreased
accuracy. Recognizing this as a resource allocation problem, we propose an
economic solution: operate where the marginal cost of increasing privacy equals
the marginal benefit. Our model of production, from computer science, assumes
data are published using an efficient differentially private algorithm. Optimal
choice weighs the demand for accurate statistics against the demand for
privacy. Examples from U.S.\ statistical programs show how our framework can
guide decision-making. Further progress requires a better understanding of
willingness-to-pay for privacy and statistical accuracy.

This article outlines different stages in development of the national culture
model, created by Geert Hofstede and his affiliates. This paper reveals and
synthesizes the contemporary review of the application spheres of this
framework. Numerous applications of the dimensions set are used as a source of
identifying significant critiques, concerning different aspects in model's
operation. These critiques are classified and their underlying reasons are also
outlined by means of a fishbone diagram.

We use a simple combinatorial model of technological change to explain the
Industrial Revolution. The Industrial Revolution was a sudden large improvement
in technology, which resulted in significant increases in human wealth and life
spans. In our model, technological change is combining or modifying earlier
goods to produce new goods. The underlying process, which has been the same for
at least 200,000 years, was sure to produce a very long period of relatively
slow change followed with probability one by a combinatorial explosion and
sudden takeoff. Thus, in our model, after many millennia of relative quiescence
in wealth and technology, a combinatorial explosion created the sudden takeoff
of the Industrial Revolution.

Two long-lived senders play a dynamic game of competitive persuasion. Each
period, each provides information to a single short-lived receiver. When the
senders also set prices, we unearth a folk theorem: if they are sufficiently
patient, virtually any vector of feasible and individually rational payoffs can
be sustained in a subgame perfect equilibrium. Without price-setting, there is
a unique subgame perfect equilibrium. In it, patient senders provide less
information--maximally patient ones none.

We explore how institutional quality moderates the effectiveness of
privatization on entrepreneurs sales performance. To do this, we blend agency
theory and entrepreneurial cognition theory with insights from institutional
economics to develop a model of emerging market venture performance. Using data
from the World Banks Enterprise Survey of entrepreneurs in China, our results
suggest that private-owned enterprises (POEs) outperform state-owned
enterprises (SOEs) but only in environments with high-quality market
institutions. In environments with low-quality market institutions, SOEs
outperform POEs. These findings suggest that the effectiveness of privatization
on entrepreneurial performance is context-specific, which reveals more nuance
than previously has been attributed.

In this study, the prevalent methodology for design of the industrial policy
in developing countries was critically assessed, and it was shown that the
mechanism and content of classical method is fundamentally contradictory to the
goals and components of the endogenous growth theories. This study, by
proposing a new approach, along settling Schumpeter's economic growth theory as
a policy framework, designed the process of entering, analyzing and processing
data as the mechanism of the industrial policy in order to provide "theoretical
consistency" and "technical and Statistical requirements" for targeting the
growth stimulant factor effectively.

In almost every election cycle, the validity of the United States Electoral
College is brought into question. The 2016 Presidential Election again brought
up the issue of a candidate winning the popular vote but not winning the
Electoral College, with Hillary Clinton receiving close to three million more
votes than Donald Trump. However, did the popular vote actually determine the
most liked candidate in the election? In this paper, we demonstrate that
different voting policies can alter which candidate is elected. Additionally,
we explore the trade-offs between each of these mechanisms. Finally, we
introduce two novel mechanisms with the intent of electing the least polarizing
candidate.

The standard model of sequential capacity choices is the Stackelberg quantity
leadership model with linear demand. I show that under the standard
assumptions, leaders' actions are informative about market conditions and
independent of leaders' beliefs about the arrivals of followers. However, this
Stackelberg independence property relies on all standard assumptions being
satisfied. It fails to hold whenever the demand function is non-linear,
marginal cost is not constant, goods are differentiated, firms are
non-identical, or there are any externalities. I show that small deviations
from the linear demand assumption may make the leaders' choices completely
uninformative.

Most products are produced and sold by supply chain networks, where an
interconnected network of producers and intermediaries set prices to maximize
their profits. I show that there exists a unique equilibrium in a price-setting
game on a network. The key distortion reducing both total profits and social
welfare is multiple-marginalization, which is magnified by strategic
interactions. Individual profits are proportional to influentiality, which is a
new measure of network centrality defined by the equilibrium characterization.
The results emphasize the importance of the network structure when considering
policy questions such as mergers or trade tariffs.

We examine the implications of consumer privacy when preferences today depend
upon past consumption choices, and consumers shop from different sellers in
each period. Although consumers are ex ante identical, their initial
consumption choices cannot be deterministic. Thus ex post heterogeneity in
preferences arises endogenously. Consumer privacy improves social welfare,
consumer surplus and the profits of the second-period seller, while reducing
the profits of the first period seller, relative to the situation where
consumption choices are observed by the later seller.

I study optimal disclosure policies in sequential contests. A contest
designer chooses at which periods to publicly disclose the efforts of previous
contestants. I provide results for a wide range of possible objectives for the
contest designer. While different objectives involve different trade-offs, I
show that under many circumstances the optimal contest is one of the three
basic contest structures widely studied in the literature: simultaneous,
first-mover, or sequential contest.

In economic development, there are often regions that share similar economic
characteristics, and economic models on such regions tend to have similar
covariate effects. In this paper, we propose a Bayesian clustered regression
for spatially dependent data in order to detect clusters in the covariate
effects. Our proposed method is based on the Dirichlet process which provides a
probabilistic framework for simultaneous inference of the number of clusters
and the clustering configurations. The usage of our method is illustrated both
in simulation studies and an application to a housing cost dataset of Georgia.

Inspired by Labov's seminal work on stylistic variation as a function of
social stratification, we develop and compare neural models that predict a
person's presumed socio-economic status, obtained through distant
supervision,from their writing style on social media. The focus of our work is
on identifying the most important stylistic parameters to predict
socio-economic group. In particular, we show the effectiveness of
morpho-syntactic features as stylistic predictors of socio-economic group,in
contrast to lexical features, which are good predictors of topic.

This survey reviews recent developments in revealed preference theory. It
discusses the testable implications of theories of choice that are germane to
specific economic environments. The focus is on expected utility in risky
environments; subjected expected utility and maxmin expected utility in the
presence of uncertainty; and exponentially discounted utility for intertemporal
choice. The testable implications of these theories for data on choice from
classical linear budget sets are described, and shown to follow a common
thread. The theories all imply an inverse relation between prices and
quantities, with different qualifications depending on the functional forms in
the theory under consideration.

Several studies of the Job Corps tend to nd more positive earnings effects
for males than for females. This effect heterogeneity favouring males contrasts
with the results of the majority of other training programmes' evaluations.
Applying the translated quantile approach of Bitler, Hoynes, and Domina (2014),
I investigate a potential mechanism behind the surprising findings for the Job
Corps. My results provide suggestive evidence that the effect of heterogeneity
by gender operates through existing gender earnings inequality rather than Job
Corps trainability differences.

This paper extends the core results of discrete time infinite horizon dynamic
programming to the case of state-dependent discounting. We obtain a condition
on the discount factor process under which all of the standard optimality
results can be recovered. We also show that the condition cannot be
significantly weakened. Our framework is general enough to handle complications
such as recursive preferences and unbounded rewards. Economic and financial
applications are discussed.

The disclosure of the VW emission manipulation scandal caused a
quasi-experimental market shock to the observable environmental quality of VW
diesel vehicles. To investigate the market reaction to this shock, we collect
data from a used-car online advertisement platform. We find that the supply of
used VW diesel vehicles increases after the VW emission scandal. The positive
supply side effects increase with the probability of manipulation. Furthermore,
we find negative impacts on the asking prices of used cars subject to a high
probability of manipulation. We rationalize these findings with a model for
sorting by the environmental quality of used cars.

We study the relationship between national culture and the disposition effect
by investigating international differences in the degree of investors'
disposition effect. We utilize brokerage data of 387,993 traders from 83
countries and find great variation in the degree of the disposition effect
across the world. We find that the cultural dimensions of long-term orientation
and indulgence help to explain why certain nationalities are more prone to the
disposition effect. We also find support on an international level for the role
of age and gender in explaining the disposition effect.

The link between taxation and justice is a classic debate issue, while also
being very relevant at a time of changing environmental factors and conditions
of the social and economic system. Technologically speaking, there are three
types of taxes: progressive, proportional and regressive. Although justice,
like freedom, is an element and manifestation of the imagined reality in
citizens minds, the state must comply with it. In particular, the tax system
has to adapt to the mass imagined reality in order for it to appear fairer and
more acceptable.

Charles Cobb and Paul Douglas in 1928 used data from the US manufacturing
sector for 1899-1922 to introduce what is known today as the Cobb-Douglas
production function that has been widely used in economic theory for decades.
We employ the R programming language to fit the formulas for the parameters of
the Cobb-Douglas production function generated by the authors recently via the
bi-Hamiltonian approach to the same data set utilized by Cobb and Douglas. We
conclude that the formulas for the output elasticities and total factor
productivity are compatible with the original 1928 data.

Unlike the classical kinetic theory of rarefied gases, where microscopic
interactions among gas molecules are described as binary collisions, the
modelling of socio-economic phenomena in a multi-agent system naturally
requires to consider, in various situations, multiple interactions among the
individuals. In this paper, we collect and discuss some examples related to
economic and gambling activities. In particular, we focus on a linearisation
strategy of the multiple interactions, which greatly simplifies the kinetic
description of such systems while maintaining all their essential aggregate
features, including the equilibrium distributions.

After the fall of the Berlin Wall, Central and Eastern Europe were subject to
strong polarisation processes. This article proposes examines two neglected
aspects regarding the transition period: a comparative static assessment of
foreign trade since 1967 until 2012 and a city-centred analysis of
transnational companies in 2013. Results show a growing economic
differentiation between the North-West and South-East as well as a division
between large metropolises and other cities. These findings may complement the
targeting of specific regional strategies such as those conceived within the
Cohesion policy of the European Union.

Extraction of financial and economic events from text has previously been
done mostly using rule-based methods, with more recent works employing machine
learning techniques. This work is in line with this latter approach, leveraging
relevant Wikipedia sections to extract weak labels for sentences describing
economic events. Whereas previous weakly supervised approaches required a
knowledge-base of such events, or corresponding financial figures, our approach
requires no such additional data, and can be employed to extract economic
events related to companies which are not even mentioned in the training data.

The modeling of financial markets as disequilibrium models by ordinary
differential equations has become a popular modeling tool. One famous example
of such a model is the Beja-Goldman model(The Journal of Finance, 1980) which
we consider in this paper. We study the passage from disequilibrium dynamics to
equilibrium. Mathematically, this limit corresponds to an asymptotic limit also
known as a Tikhonov-Fenichel reduction. Furthermore, we analyze the stability
of the reduced equilibrium model and discuss the economic implications. We
conduct several numerical examples to visualize and support our analysis.

The solution existence of finite horizon optimal economic growth problems is
studied by invoking Filippov's Existence Theorem for optimal control problems
with state constraints of the Bolza type from the monograph of L. Cesari
[Optimization Theory and Applications, Springer-Verlag, New York, 1983]. Our
results are obtained not only for general problems but also for typical ones
with the production function and the utility function being either the AK
function or the Cobb--Douglas one. Some open questions and conjectures about
the regularity of the global solutions of finite horizon optimal economic
growth problems are formulated in this paper.

This paper used a primary data collected through a surveys among farmers in
rural Kedah to examine the effect of non farm income on poverty and income
inequality. This paper employed two method, for the first objective which is to
examine the impact of non farm income to poverty, we used poverty decomposition
techniques - Foster, greer and Thorbecke (FGT) as has been done by Adams
(2004). For the second objective, which is to examine the impact of non farm
income to income inequality, we used Gini decomposition techniques.

Scholars present their new research at seminars and conferences, and send
drafts to peers, hoping to receive comments and suggestions that will improve
the quality of their work. Using a dataset of papers published in economics
journals, this article measures how much peers' individual and collective
comments improve the quality of research. Controlling for the quality of the
research idea and author, I find that a one standard deviation increase in the
number of peers' individual and collective comments increases the quality of
the journal in which the research is published by 47%.

We construct long-term prediction intervals for time-aggregated future values
of univariate economic time series. We propose computational adjustments of the
existing methods to improve coverage probability under a small sample
constraint. A pseudo-out-of-sample evaluation shows that our methods perform at
least as well as selected alternative methods based on model-implied Bayesian
approaches and bootstrapping. Our most successful method yields prediction
intervals for eight macroeconomic indicators over a horizon spanning several
decades.

Lucas and Moll have proposed a system of forward-backward partial
differential equations that model knowledge diffusion and economic growth. It
arises from a microscopic model of learning for a mean-field type interacting
system of individual agents. In this paper, we prove existence of traveling
wave solutions to this system. They correspond to what is known in economics as
balanced growth path solutions. We also study the dependence of the solutions
and their propagation speed on various economic parameters of the system.

Conventional wisdom to improve the effectiveness of economic dispatch is to
design the load forecasting method as accurately as possible. However, this
approach can be problematic due to the temporal and spatial correlations
between system cost and load prediction errors. This motivates us to adopt the
notion of end-to-end machine learning and to propose a task-specific learning
criteria to conduct economic dispatch. Specifically, to maximize the data
utilization, we design an efficient optimization kernel for the learning
process. We provide both theoretical analysis and empirical insights to
highlight the effectiveness and efficiency of the proposed learning framework.

Discretely-constrained Nash-Cournot games have attracted attention as they
arise in various competitive energy production settings in which players must
make one or more discrete decisions. Gabriel et al. ["Solving
discretely-constrained Nash-Cournot games with an application to power
markets." Networks and Spatial Economics 13(3), 2013] claim that the set of
equilibria to a discretely-constrained Nash-Cournot game coincides with the set
of solutions to a corresponding discretely-constrained mixed complementarity
problem. We show that this claim is false.

This article introduces the Hedonic Metric (HM) approach as an original
method to model the demand for differentiated products. Using this approach,
initially, we create an n-dimensional hedonic space based on the characteristic
information available to consumers. Next, we allocate products into this space
and estimate the elasticities using distances. Our model makes it possible to
estimate a large number of differentiated products in a single demand system.
We applied our model to estimate the retail demand for fluid milk products.

In this paper, we study the financial and economic implications of a zombie
epidemic on a major industrialized nation. We begin with a consideration of the
epidemiological modeling of the zombie contagion. The emphasis of this work is
on the computation of direct and indirect financial consequences of this
contagion of the walking dead. A moderate zombie outbreak leaving 1 million
people dead in a major industrialized nation could result in GDP losses of
23.44% over the subsequent year and a drop in financial market of 29.30%. We
conclude by recommending policy actions necessary to prevent this potential
economic collapse.

I estimate the Susceptible-Infected-Recovered (SIR) epidemic model for
Coronavirus Disease 2019 (COVID-19). The transmission rate is heterogeneous
across countries and far exceeds the recovery rate, which enables a fast
spread. In the benchmark model, 28% of the population may be simultaneously
infected at the peak, potentially overwhelming the healthcare system. The peak
reduces to 6.2% under the optimal mitigation policy that controls the timing
and intensity of social distancing. A stylized asset pricing model suggests
that the stock price temporarily decreases by 50% in the benchmark case but
shows a W-shaped, moderate but longer bear market under the optimal policy.

In this paper we aim to assess linear relationships between the non constant
variances of economic variables. The proposed methodology is based on a
bootstrap cumulative sum (CUSUM) test. Simulations suggest a good behavior of
the test for sample sizes commonly encountered in practice. The tool we provide
is intended to highlight relations or draw common patterns between economic
variables through their non constant variances. The outputs of this paper is
illustrated considering U.S. regional data.

In regional economics research, a problem of interest is to detect
similarities between regions, and estimate their shared coefficients in
economics models. In this article, we propose a mixture of finite mixtures
(MFM) clustered regression model with auxiliary covariates that account for
similarities in demographic or economic characteristics over a spatial domain.
Our Bayesian construction provides both inference for number of clusters and
clustering configurations, and estimation for parameters for each cluster.
Empirical performance of the proposed model is illustrated through simulation
experiments, and further applied to a study of influential factors for monthly
housing cost in Georgia.

We study search, evaluation, and selection of candidates of unknown quality
for a position. We examine the effects of "soft" affirmative action policies
increasing the relative percentage of minority candidates in the candidate
pool. We show that, while meant to encourage minority hiring, such policies may
backfire if the evaluation of minority candidates is noisier than that of
non-minorities. This may occur even if minorities are at least as qualified and
as valuable as non-minorities. The results provide a possible explanation for
why certain soft affirmative action policies have proved counterproductive,
even in the absence of (implicit) biases.

This paper identifies the mathematical equivalence between economic networks
of Cobb-Douglas agents and Artificial Neural Networks. It explores two
implications of this equivalence under general conditions. First, a burgeoning
literature has established that network propagation can transform microeconomic
perturbations into large aggregate shocks. Neural network equivalence amplifies
the magnitude and complexity of this phenomenon. Second, if economic agents
adjust their production and utility functions in optimal response to local
conditions, market pricing is a sufficient and robust channel for information
feedback leading to macro learning.

Over sixty percent of employees at a large South African company contribute
the minimum rate of 7.5 percent to a retirement fund, far below the rate of 15
percent recommended by financial advisers. I use a field experiment to
investigate whether providing employees with a retirement calculator, which
shows projections of retirement income, leads to increases in contributions.
The impact is negligible. The lack of response to the calculator suggests many
employees may wish to save less than the minimum. I use a model of asymmetric
information to explain why the employer sets a binding minimum.

This paper provides an overview on stablecoins and introduces a novel
terminology to help better identify stablecoins with truly disruptive
potential. It provides a compact definition for stablecoins, identifying the
unique features that make them distinct from previously known payment systems.
Furthermore, it surveys the different use cases for stablecoins as well as the
underlying economic incentives for creating them. Finally, it outlines critical
regulatory considerations that constrain stablecoins and summarizes key factors
that are driving their rapid development.

Did sovereign default risk affect macroeconomic activity through firms'
access to credit during the European sovereign debt crisis? We investigate this
question by a estimating a structural panel vector autoregressive model for
Italy, Spain, Portugal, and Ireland, where the sovereign risk shock is
identified using sign restrictions. The results suggest that decline in the
creditworthiness of the sovereign contributed to a fall in private lending and
economic activity in several euro-area countries by reducing the value of
banks' assets and crowding out private lending.

We investigate how distorted, yet structured, beliefs can persist in
strategic situations. Specifically, we study two-player games in which each
player is endowed with a biased-belief function that represents the discrepancy
between a player's beliefs about the opponent's strategy and the actual
strategy. Our equilibrium condition requires that (i) each player choose a
best-response strategy to his distorted belief about the opponent's strategy,
and (ii) the distortion functions form best responses to one another. We obtain
sharp predictions and novel insights into the set of stable outcomes and their
supporting stable biases in various classes of games.

I study labor markets in which firms hire via referrals. I develop an
employment model showing that--despite initial equality in ability, employment,
wages, and network structure--minorities receive fewer jobs through referral
and lower expected wages, simply because their social group is smaller. This
disparity, termed "social network discrimination," falls outside the dominant
economics discrimination models--taste-based and statistical. Social network
discrimination can be mitigated by minorities having more social ties or a
"stronger-knit" network. I calibrate the model using a
nationally-representative U.S. sample and estimate the lower-bound welfare gap
caused by social network discrimination at over four percent, disadvantaging
black workers.

This paper aims to provide a comprehensive analysis on the income
inequalities recorded in the EU-15 in the 1995-2014 period and to estimate the
impact of private sector credit on income disparities. In order to estimate the
impact, I used the panel data technique with 15 cross-sections for the first 15
Member States of the European Union, applying generalized error correction
model.

This research aims to provide an overview of the existing inequalities and
their drivers in the member states of the European Union as well as their
developements in the 2002-2008 and 2009- 2015 sub-periods. It also analyses the
impact of health and education government spending on income inequality in the
European Union over the 2002-2015 period. In this context, I applied the
Estimated Generalized Least Squares method using panel data for the 28-member
states of the European Union.

The main goal of the paper is to extract the aggregate demand and aggregate
supply shocks in Greece, Ireland, Italy and Portugal, as well as to examine the
correlation among the two types of shocks. The decomposition of the shocks was
achieved by using a structural vector autoregression that analyses the
relationship between the evolution of the gross domestic product and inflation
in the period 1997-2015. The goal of the paper is to confirm the aggregate
demand - aggregate supply model in the above-mentioned economies.

It is shown that the ratio between the mean and the $L^2$-norm leads to a
particularly parsimonious description of the mean-variance efficient frontier
and the dual pricing kernel restrictions known as the Hansen-Jagannathan (HJ)
bounds. Because this ratio has not appeared in economic theory previously, it
seems appropriate to name it the Hansen ratio. The initial treatment of the
mean-variance theory via the Hansen ratio is extended in two directions, to
monotone mean-variance preferences and to arbitrary Hilbert space setting. A
multiperiod example with IID returns is also discussed.

The outbreak of COVID-19 in March 2020 led to a shutdown of economic
activities in Europe. This included the sports sector, since public gatherings
were prohibited. The German Bundesliga was among the first sport leagues
realising a restart without spectators. Several recent studies suggest that the
home advantage of teams was eroded for the remaining matches. Our paper
analyses the reaction by bookmakers to the disappearance of such home
advantage. We show that bookmakers had problems to adjust the betting odds in
accordance to the disappeared home advantage, opening opportunities for
profitable betting strategies.

In this paper we propose a macro-dynamic age-structured set-up for the
analysis of epidemics/economic dynamics in continuous time. The resulting
optimal control problem is reformulated in an infinite dimensional Hilbert
space framework where we perform the basic steps of dynamic programming
approach. Our main result is a verification theorem which allows to guess the
feedback form of optimal strategies. This will be a departure point to discuss
the behavior of the models of the family we introduce and their policy
implications.

In this paper the dynamics of an economic system with foreign financing, of
integer or fractional order, are analyzed. The symmetry of the system
determines the existence of two pairs of coexisting attractors. The
integer-order version of the system proves to have several combinations of
coexisting hidden attractors with self-excited attractors. Because one of the
system variables represents the foreign capital in ow, the presence of hidden
attractors could be of a real interest in economic models. The fractional-order
variant presents another interesting coexistence of attractors in the
fractional order space.

I characterize the consumer-optimal market segmentation in competitive
markets where multiple firms selling differentiated products to consumers with
unit demand. This segmentation is public---in that each firm observes the same
market segments---and takes a simple form: in each market segment, there is a
dominant firm favored by all consumers in that segment. By segmenting the
market, all but the dominant firm maximally compete to poach the consumer's
business, setting price to equal marginal cost. Information, thus, is being
used to amplify competition. This segmentation simultaneously generates an
efficient allocation and delivers to each firm its minimax profit.

In tis paper we consider approaches for time series forecasting based on deep
neural networks and neuro-fuzzy nets. Also, we make short review of researches
in forecasting based on various models of ANFIS models. Deep Learning has
proven to be an effective method for making highly accurate predictions from
complex data sources. Also, we propose our models of DL and Neuro-Fuzzy
Networks for this task. Finally, we show possibility of using these models for
data science tasks. This paper presents also an overview of approaches for
incorporating rule-based methodology into deep learning neural networks.

This paper explores the link between education and the decision to start
smoking as well as the decision to quit smoking. Data is gathered from IPUMS
CPS and Centers for Disease Control and Prevention. Probit analysis (with the
use of probability weight and robust standard error) indicates that every
additional year of education will reduce the 2.3 percentage point of the
smoking probability and will add 3.53 percentage point in quitting likelihood,
holding home restriction, public restriction, cigarette price, family income,
age, gender, race, and ethnicity constant. I believe that tobacco epidemic is a
serious global issue that may be mitigated by using careful regulations on
smoking restriction and education.

We examine the long-term behavior of a Bayesian agent who has a misspecified
belief about the time lag between actions and feedback, and learns about the
payoff consequences of his actions over time. Misspecified beliefs about time
lags result in attribution errors, which have no long-term effect when the
agent's action converges, but can lead to arbitrarily large long-term
inefficiencies when his action cycles. Our proof uses concentration
inequalities to bound the frequency of action switches, which are useful to
study learning problems with history dependence. We apply our methods to study
a policy choice game between a policy-maker who has a correctly specified
belief about the time lag and the public who has a misspecified belief.

The existence of involuntary unemployment advocated by J. M. Keynes is a very
important problem of the modern economic theory. Using a three-generations
overlapping generations model, we show that the existence of involuntary
unemployment is due to the instability of the economy. Instability of the
economy is the instability of the difference equation about the equilibrium
price around the full-employment equilibrium, which means that a fall in the
nominal wage rate caused by the presence of involuntary unemployment further
reduces employment. This instability is due to the negative real balance effect
that occurs when consumers' net savings (the difference between savings and
pensions) are smaller than their debt multiplied by the marginal propensity to
consume from childhood consumption.

A new and rapidly growing econometric literature is making advances in the
problem of using machine learning methods for causal inference questions. Yet,
the empirical economics literature has not started to fully exploit the
strengths of these modern methods. We revisit influential empirical studies
with causal machine learning methods and identify several advantages of using
these techniques. We show that these advantages and their implications are
empirically relevant and that the use of these methods can improve the
credibility of causal analysis.

Previous studies show that prenatal shocks to embryos could have adverse
impacts on health endowment at birth. Using the universe of birth data and a
difference-in-difference-in-difference strategy, I find that exposure to
Ramadan during prenatal development has negative birth outcomes. Exposure to a
full month of fasting is associated with 96 grams lower birth-weight. These
results are robust across specifications and do not appear to be driven by
mothers selective fertility.

Many important economic outcomes result from cumulative effects of smaller
choices, so the best outcomes require accounting for other choices at each
decision point. We document narrow bracketing -- the neglect of such accounting
-- in work choices in a pre-registered experiment on MTurk: bracketing changes
average willingness to work by 13-28%. In our experiment, broad bracketing is
so simple to implement that narrow bracketing cannot possibly be due to optimal
conservation of cognitive resources, so it must be suboptimal. We jointly
estimate disutility of work and bracketing, finding gender differences in
convexity of disutility, but not in bracketing.

We study strategic interactions between firms with heterogeneous beliefs
about future climate impacts. To that end, we propose a Cournot-type
equilibrium model where firms choose mitigation efforts and production
quantities such as to maximize the expected profits under their subjective
beliefs. It is shown that optimal mitigation efforts are increased by the
presence of uncertainty and act as substitutes; i.e., one firm's lack of
mitigation incentivizes others to act more decidedly, and vice versa.

The worries expressed by Alan Greenspan that the long run economic growth of
the United States will fade away due to increasing burden of entitlements
motivated us to empirically investigate the impact of entitlements of key
macroeconomic variables. To examine this contemporary issue, we estimate a
vector error-correction model is used to analyze the impact of entitlements on
the price level, real output, and the long-term interest rate. The results show
that a shock to entitlements leads to decrease in output and lends support to
the assertion made by Alan Greenspan. Several robustness checks are conducted
and the results of the model qualitatively remains unchanged.

This paper exhibits a duality between the theory of Revealed Preference of
Afriat and the housing allocation problem of Shapley and Scarf. In particular,
it is shown that Afriat's theorem can be interpreted as a second welfare
theorem in the housing problem. Using this duality, the revealed preference
problem is connected to an optimal assignment problem, and a geometrical
characterization of the rationalizability of experiment data is given. This
allows in turn to give new indices of rationalizability of the data, and to
define weaker notions of rationalizability, in the spirit of Afriat's
efficiency index.

We characterize solutions for two-sided matching, both in the transferable
and in the nontransferable-utility frameworks, using a cardinal formulation.
Our approach makes the comparison of the matching models with and without
transfers particularly transparent. We introduce the concept of a no-trade
matching to study the role of transfers in matching. A no-trade matching is one
in which the availability of transfers do not affect the outcome.

In many economic contexts, agents from a same population team up to better
exploit their human capital. In such contexts (often called "roommate matching
problems"), stable matchings may fail to exist even when utility is
transferable. We show that when each individual has a close substitute, a
stable matching can be implemented with minimal policy intervention. Our
results shed light on the stability of partnerships on the labor market.
Moreover, they imply that the tools crafted in empirical studies of the
marriage problem can easily be adapted to many roommate problems.

We present a necessary and sufficient condition for Alt's system to be
represented by a continuous utility function. Moreover, we present a necessary
and sufficient condition for this utility function to be concave. The latter
condition can be seen as an extension of Gossen's first law, and thus has an
economic interpretation. Together with the above results, we provide a
necessary and sufficient condition for Alt's utility to be continuously
differentiable.

In Historical Economics, Persistence studies document the persistence of some
historical phenomenon or leverage this persistence to identify causal
relationships of interest in the present. In this chapter, we analyze the
implications of allowing for heterogeneous treatment effects in these studies.
We delineate their common empirical structure, argue that heterogeneous
treatment effects are likely in their context, and propose minimal abstract
models that help interpret results and guide the development of empirical
strategies to uncover the mechanisms generating the effects.

In this paper, we first revisit the Koenker and Bassett variational approach
to (univariate) quantile regression, emphasizing its link with latent factor
representations and correlation maximization problems. We then review the
multivariate extension due to Carlier et al. (2016, 2017) which relates vector
quantile regression to an optimal transport problem with mean independence
constraints. We introduce an entropic regularization of this problem, implement
a gradient descent numerical method and illustrate its feasibility on
univariate and bivariate examples.

Previous research on two-dimensional extensions of Hotelling's location game
has argued that spatial competition leads to maximum differentiation in one
dimensions and minimum differentiation in the other dimension. We expand on
existing models to allow for endogenous entry into the market. We find that
competition may lead to the min/max finding of previous work but also may lead
to maximum differentiation in both dimensions. The critical issue in
determining the degree of differentiation is if existing firms are seeking to
deter entry of a new firm or to maximizing profits within an existing, stable
market.

We argue for a foundational epistemic claim and a hypothesis about the
production and uses of mathematical epidemiological models, exploring the
consequences for our political and socio-economic lives. First, in order to
make the best use of scientific models, we need to understand why models are
not truly representational of our world, but are already pitched towards
various uses. Second, we need to understand the implicit power relations in
numbers and models in public policy, and, thus, the implications for good
governance if numbers and models are used as the exclusive drivers of decision
making.

Democracy is widely believed to contribute to economic growth and public
health in the 20th and earlier centuries. We find that this conventional wisdom
is reversed in this century, i.e., democracy has persistent negative impacts on
GDP growth during 2001-2020. This finding emerges from five different
instrumental variable strategies. Our analysis suggests that democracies cause
slower growth through less investment and trade. For 2020, democracy is also
found to cause more deaths from Covid-19.

Expansions in the size and scope of public procurement across the Atlantic
have increased calls for accountability of democratic governments. Indefinite
Delivery, Indefinite Quantity contracts by their very nature are less
transparent but serve as major tools of public procurement in both the European
and American economies. This paper utilizes a cross-Atlantic perspective to
discuss common challenges faced by governments and contracting entities while
highlighting the need for balancing transparency with efficiency to avoid
negative economic outcomes. It concludes by discussing and providing potential
solutions to certain common challenges.

We modify the standard model of price competition with horizontally
differentiated products, imperfect information, and search frictions by
allowing consumers to flexibly acquire information about a product's match
value during their visits. We characterize a consumer's optimal search and
information acquisition protocol and analyze the pricing game between firms.
Notably, we establish that in search markets there are fundamental differences
between search frictions and information frictions, which affect market prices,
profits, and consumer welfare in markedly different ways. Although higher
search costs beget higher prices (and profits for firms), higher information
acquisition costs lead to lower prices and may benefit consumers. We discuss
implications of our findings for policies concerning disclosure rules and
hidden fees.

We consider a generalization of rational inattention problems by measuring
costs of information through the information radius (Sibson, 1969; Verd\'u,
2015) of statistical experiments. We introduce a notion of attention elasticity
measuring the sensitivity of attention strategies with respect to changes in
incentives. We show how the introduced class of cost functions controls
attention elasticities while the Shannon model restricts attention elasticity
to be unity. We explore further differences and similarities relative to the
Shannon model in relation to invariance, posterior separability, consideration
sets, and the ability to learn events with certainty. Lastly, we provide an
efficient alternating minimization method -- analogous to the Blahut-Arimoto
algorithm -- to obtain optimal attention strategies.

We revisit the dynamic relationship between domestic economic policy
uncertainty and stock markets using the symmetric thermal optimal path (TOPS)
method. We observe different interaction patterns in emerging and developed
markets. Economic policy uncertainty drives the stock market in China, while
stock markets play a leading role in the UK and the US. Meanwhile, the lead-lag
relationship of the three countries reacts significantly to extreme events. Our
findings have important implications for investors and policy makers.

The authors of the study conduct a legal analysis of the concept of energy
security. Energy is vital for sustainable development, and sustainability is
not only at the heart of development, but also economic, environmental, social
and military policies. To ensure the sustainability of the policy, 'security'
seems to be a mandatory goal to achieve. The article critically assesses the
change in the energy paradigm.

We propose an observation-driven time-varying SVAR model where, in agreement
with the Lucas Critique, structural shocks drive both the evolution of the
macro variables and the dynamics of the VAR parameters. Contrary to existing
approaches where parameters follow a stochastic process with random and
exogenous shocks, our observation-driven specification allows the evolution of
the parameters to be driven by realized past structural shocks, thus opening
the possibility to gauge the impact of observed shocks and hypothetical policy
interventions on the future evolution of the economic system.

Hysteresis is treated as a history dependent branching, and the use of the
classical Preisach model for the analysis of macroeconomic hysteresis is first
discussed. Then, a new Preisach-type model is introduced as a macroeconomic
aggregation of more realistic microeconomic hysteresis than in the case of the
classical Preisach model. It is demonstrated that this model is endowed with a
more general mechanism of branching and may account for the continuous
evolution of the economy and its effect on hysteresis. Furthermore, it is shown
that the sluggishness of economic recovery is an intrinsic manifestation of
hysteresis branching.

Water-logging is a major challenge for Dhaka city, the capital of Bangladesh.
The rapid, unregulated, and unplanned urbanization, as well as detrimental
social, economic, infrastructural, and environmental consequences, not to
mention diseases like dengue, challenge the several crash programs combating
water-logging in the city. This study provides a brief contextual analysis of
the Dhakas topography and natural, as well as storm water drainage systems,
before concentrating on the man-made causes and effects of water-logging,
ultimately exploring a few remedial measures.

The Federal Open Market Committee within the Federal Reserve System is
responsible for managing inflation, maximizing employment, and stabilizing
interest rates. Meeting minutes play an important role for market movements
because they provide the birds eye view of how this economic complexity is
constantly re-weighed. Therefore, There has been growing interest in analyzing
and extracting sentiments on various aspects from large financial texts for
economic projection. However, Aspect-based Sentiment Analysis is not widely
used on financial data due to the lack of large labeled dataset. In this paper,
I propose a model to train ABSA on financial documents under weak supervision
and analyze its predictive power on various macroeconomic indicators.

This paper studies dynamic monopoly pricing for a class of settings that
includes multiple durable, multiple rental, or a mix of varieties. We show that
the driving force behind pricing dynamics is the seller's incentive to switch
consumers - buyers and non-buyers - to higher-valued consumption options by
lowering prices ("trading up"). If consumers cannot be traded up from the
static optimal allocation, pricing dynamics do not emerge in equilibrium. If
consumers can be traded up, pricing dynamics arise until all trading-up
opportunities are exhausted. We study the conditions under which pricing
dynamics end in finite time and characterize the final prices at which dynamics
end.

This letter proposes a data-driven sparse polynomial chaos expansion-based
surrogate model for the stochastic economic dispatch problem considering
uncertainty from wind power. The proposed method can provide accurate
estimations for the statistical information (e.g., mean, variance, probability
density function, and cumulative distribution function) for the stochastic
economic dispatch solution efficiently without requiring the probability
distributions of random inputs. Simulation studies on an integrated electricity
and gas system (IEEE 118-bus system integrated with a 20-node gas system are
presented, demonstrating the efficiency and accuracy of the proposed method
compared to the Monte Carlo simulations.

Agents, some with a bias, decide between undertaking a risky project and a
safe alternative based on information about the project's efficiency. Only a
part of that information is verifiable. Unbiased agents want to undertake only
efficient projects, while biased agents want to undertake any project. If the
project causes harm, a court examines the verifiable information, forms a
belief about the agent's type, and decides the punishment. Tension arises
between deterring inefficient projects and a chilling effect on using the
unverifiable information. Improving the unverifiable information always
increases overall efficiency, but improving the verifiable information may
reduce efficiency.

We study a model of two-player bargaining game in the shadow of a preventive
trade war that examines why states deliberately maintain trade barriers in the
age of globalization. Globalization can induce substantial power shifts between
states, which makes the threat of a preventive trade war salient. In this
situation, there may exist "healthy" levels of trade barriers that dampen the
war incentives by reducing states' expected payoffs from such a war. Thus, we
demonstrate that trade barriers can sometimes serve as breaks and cushions
necessary to sustain inefficient yet peaceful economic cooperation between
states. We assess the theoretical implications by examining the US-China trade
relations since 1972.

We provide an analysis of the recent work by Malaney-Weinstein on "Economics
as Gauge Theory" presented on November 10, 2021 at the Money and Banking
Workshop hosted by University of Chicago. In particular, we distill the
technical mathematics used in their work into a form more suitable to a wider
audience. Furthermore, we resolve the conjectures posed by Malaney-Weinstein,
revealing that they provide no discernible value for the calculation of index
numbers or rates of inflation. Our conclusion is that the main contribution of
the Malaney-Weinstein work is that it provides a striking example of how to
obscure simple concepts through an uneconomical use of gauge theory.

This paper develops a formal framework to assess policies of learning
algorithms in economic games. We investigate whether reinforcement-learning
agents with collusive pricing policies can successfully extrapolate collusive
behavior from training to the market. We find that in testing environments
collusion consistently breaks down. Instead, we observe static Nash play. We
then show that restricting algorithms' strategy space can make algorithmic
collusion robust, because it limits overfitting to rival strategies. Our
findings suggest that policy-makers should focus on firm behavior aimed at
coordinating algorithm design in order to make collusive policies robust.

Stablecoins and central bank digital currencies are on the horizon in Asia,
and in some cases have already arrived. This paper provides new analysis and a
critique of the use case for both forms of digital currency. It provides
time-varying estimates of devaluation risk for the leading stablecoin, Tether,
using data from the futures market. It describes the formidable obstacles to
widespread use of central bank digital currencies in cross-border transactions,
the context in which their utility is arguably greatest. The bottom line is
that significant uncertainties continue to dog the region's digital currency
initiatives.

Digital loans have exploded in popularity across low and middle income
countries, providing short term, high interest credit via mobile phones. This
paper reports the results of a randomized evaluation of a digital loan product
in Nigeria. Being randomly approved for digital credit (irrespective of credit
score) substantially increases subjective well-being after an average of three
months. For those who are approved, being randomly offered larger loans has an
insignificant effect. Neither treatment significantly impacts other measures of
welfare. We rule out large short-term impacts either positive or negative: on
income and expenditures, resilience, and women's economic empowerment.

China's demographic changes have important global economic and geopolitical
implications. Yet, our understanding of such transitions at the micro-spatial
scale remains limited due to spatial inconsistency of the census data caused by
administrative boundary adjustments. To fill this gap, we manually collected
and built a population census panel from 2010 to 2020 at both the county and
prefectural-city levels. We show that the massive internal migration drives
China's increasing population concentration and regional disparity, resulting
in severe population aging in shrinking cities and increasing gender imbalance
in growing cities.

We propose a deep learning approach to probabilistic forecasting of
macroeconomic and financial time series. Being able to learn complex patterns
from a data rich environment, our approach is useful for a decision making that
depends on uncertainty of large number of economic outcomes. Specifically, it
is informative to agents facing asymmetric dependence of their loss on outcomes
from possibly non-Gaussian and non-linear variables. We show the usefulness of
the proposed approach on the two distinct datasets where a machine learns the
pattern from data. First, we construct macroeconomic fan charts that reflect
information from high-dimensional data set. Second, we illustrate gains in
prediction of stock return distributions which are heavy tailed, asymmetric and
suffer from low signal-to-noise ratio.

I discuss various ways in which inference based on the estimation of the
parameters of statistical models (reduced-form estimation) can be combined with
inference based on the estimation of the parameters of economic models
(structural estimation). I discuss five basic categories of integration:
directly combining the two methods, using statistical models to simplify
structural estimation, using structural estimation to extend the validity of
reduced-form results, using reduced-form techniques to assess the external
validity of structural estimations, and using structural estimation as a sample
selection remedy. I illustrate each of these methods with examples from
corporate finance, banking, and personal finance.

Electrification of all economic sectors and solar photovoltaics (PV) becoming
the lowest-cost electricity generation technology in ever more regions give
rise to new potential gains of trade. We develop a stylized analytical model to
minimize unit energy cost in autarky, open it to different trade
configurations, and evaluate it empirically. We identify large potential gains
from interhemispheric and global electricity trade by combining complementary
seasonal and diurnal cycles. The corresponding high willingness to pay for
large-scale transmission suggests far-reaching political economy and regulatory
implications.

A leading explanation for widespread replication failures is publication
bias. I show in a simple model of selective publication that, contrary to
common perceptions, the replication rate is unaffected by the suppression of
insignificant results in the publication process. I show further that the
expected replication rate falls below intended power owing to issues with
common power calculations. I empirically calibrate a model of selective
publication and find that power issues alone can explain the entirety of the
gap between the replication rate and intended power in experimental economics.
In psychology, these issues explain two-thirds of the gap.

This paper designs a sequential repeated game of a micro-founded society with
three types of agents: individuals, insurers, and a government. Nascent to
economics literature, we use Reinforcement Learning (RL), closely related to
multi-armed bandit problems, to learn the welfare impact of a set of proposed
policy interventions per $1 spent on them. The paper rigorously discusses the
desirability of the proposed interventions by comparing them against each other
on a case-by-case basis. The paper provides a framework for algorithmic policy
evaluation using calibrated theoretical models which can assist in feasibility
studies.

Despite the increasing integration of the global economic system,
anti-dumping measures are a common tool used by governments to protect their
national economy. In this paper, we propose a methodology to detect cases of
anti-dumping circumvention through re-routing trade via a third country. Based
on the observed full network of trade flows, we propose a measure to proxy the
evasion of an anti-dumping duty for a subset of trade flows directed to the
European Union, and look for possible cases of circumvention of an active
anti-dumping duty. Using panel regression, we are able correctly classify 86%
of the trade flows, on which an investigation of anti-dumping circumvention has
been opened by the European authorities.

The subject of international institutions and power politics continues to
occupy a central position in the field of International Relations and to the
world politics. It revolves around key questions on how rising states, regional
powers and small states leverage international institutions for achieving
social, political, economic gains for themselves. Taking into account one of
the rising powers China and the role of international institutions in the
contemporary international politics, this paper aims to demonstrate, how in
pursuit of power politics, various states (Small, Regional and Great powers)
utilise international institutions by making them adapt to the new power
realities critical to world politics.

The research is done in the context of the upcoming introduction of new
European legislation for the first time for regulation of minimum wage at
European level. Its purpose is to identify the direction and strength of the
correlation amongst changes of minimum wage and unemployment rate in the
context of conflicting findings of the scientific literature. There will be
used statistical instruments for the purposes of the analysis as it
incorporates data for Bulgaria for the period 1991-2021. The significance of
the research is related to the transition to digital economy and the necessity
for complex transformation of the minimum wage functions in the context of the
new socio-economic reality.

We conduct a field experiment on a movie-recommendation platform to identify
if and how recommendations affect consumption. We use within-consumer
randomization at the good level and elicit beliefs about unconsumed goods to
disentangle exposure from informational effects. We find recommendations
increase consumption beyond its role in exposing goods to consumers. We provide
support for an informational mechanism: recommendations affect consumers'
beliefs, which in turn explain consumption. Recommendations reduce uncertainty
about goods consumers are most uncertain about and induce information
acquisition. Our results highlight the importance of recommender systems'
informational role when considering policies targeting these systems in online
marketplaces.

Even as policymakers seek to encourage economic development by addressing
misallocation due to frictions in labor markets, the associated production
externalities - such as air pollution - remain unexplored. Using a regression
discontinuity design, we show access to rural roads increases agricultural
fires and particulate emissions. Farm labor exits are a likely mechanism
responsible for the increase in agricultural fires: rural roads cause movement
of workers out of agriculture and induce farmers to use fire - a labor-saving
but polluting technology - to clear agricultural residue or to make harvesting
less labor-intensive. Overall, the adoption of fires due to rural roads
increases infant mortality rate by 5.5% in downwind locations.

Choice overload - by which larger choice sets are detrimental to a chooser's
wellbeing - is potentially of great importance to the design of economic
policy. Yet the current evidence on its prevalence is inconclusive. We argue
that existing tests are likely to be underpowered and hence that choice
overload may occur more often than the literature suggests. We propose more
powerful tests based on richer data and characterization theorems for the
Random Utility Model. These new approaches come with significant econometric
challenges, which we show how to address. We apply our tests to new
experimental data and find strong evidence of choice overload that would likely
be missed using current approaches.

This article presents evidence based on a panel of 35 countries over the past
30 years that the Phillips curve relation holds for food inflation. That is,
broader economic overheating does push up the food component of the CPI in a
systematic way. Further, general inflation expectations from professional
forecasters clearly impact food price inflation. The analysis also quantifies
the extent to which higher food production and imports, or lower food exports,
reduce food inflation. Importantly, the link between domestic and global food
prices is typically weak, with passthroughs within a year ranging from 0.07 to
0.16, after exchange rate variations are taken into account.

With the increasing pervasiveness of ICTs in the fabric of economic
activities, the corporate digital divide has emerged as a new crucial topic to
evaluate the IT competencies and the digital gap between firms and territories.
Given the scarcity of available granular data to measure the phenomenon, most
studies have used survey data. To bridge the empirical gap, we scrape the
website homepage of 182 705 Italian firms, extracting ten features related to
their digital footprint characteristics to develop a new corporate digital
assessment index. Our results highlight a significant digital divide across
dimensions, sectors and geographical locations of Italian firms, opening up new
perspectives on monitoring and near-real-time data-driven analysis.

In this work, we propose a new lecture of input-output model reconciliation
Markov chain and the dominance theory, in the field of interindustrial poles
interactions. A deeper lecture of Leontieff table in term of Markov chain is
given, exploiting spectral properties and time to absorption to characterize
production processes, then the dualities local-global/dominance- Sensitivity
analysis are established, allowing a better understanding of economic poles
arrangement. An application to the Moroccan economy is given.

When agents' information is imperfect and dispersed, existing measures of
macroeconomic uncertainty based on the forecast error variance have two
distinct drivers: the variance of the economic shock and the variance of the
information dispersion. The former driver increases uncertainty and reduces
agents' disagreement (agreed uncertainty). The latter increases both
uncertainty and disagreement (disagreed uncertainty). We use these implications
to identify empirically the effects of agreed and disagreed uncertainty shocks,
based on a novel measure of consumer disagreement derived from survey
expectations. Disagreed uncertainty has no discernible economic effects and is
benign for economic activity, but agreed uncertainty exerts significant
depressing effects on a broad spectrum of macroeconomic indicators.

This study rigorously investigates the Keynesian cross model of a national
economy with a focus on the dynamic relationship between government spending
and economic equilibrium. The model consists of two ordinary differential
equations regarding the rate of change of national income and the rate of
consumer spending. Three dynamic relationships between national income and
government spending are studied. This study aims to classify the stabilities of
equilibrium states for the economy by discussing different cases of government
spending. Furthermore, the implication of government spending on the national
economy is investigated based on phase portraits and bifurcation analysis of
the dynamical system in each scenario.

This paper investigates whether ideological indoctrination by living in a
communist regime relates to low economic performance in a market economy. We
recruit North Korean refugees and measure their implicit bias against South
Korea by using the Implicit Association Test. Conducting double auction and
bilateral bargaining market experiments, we find that North Korean refugees
with a larger bias against the capitalistic society have lower expectations
about their earning potential, exhibit trading behavior with lower target
profits, and earn less profits. These associations are robust to conditioning
on correlates of preferences, human capital, and assimilation experiences.

Economic models assume that payroll tax burdens fall fully on workers, but
where does tax incidence fall when taxes are firm-specific and time-varying?
Unemployment insurance in the United States has the key feature of varying both
across employers and over time, creating the potential for labor demand
responses if tax costs cannot be fully passed on to worker wages. Using state
policy changes and matched employer-employee job spells from the LEHD, I study
how employment and earnings respond to payroll tax increases for highly exposed
employers. I find significant drops in employment growth driven by lower
hiring, and minimal evidence of pass-through to earnings. The negative
employment effects are strongest for young and low-earning workers.

This paper investigates economic convergence in terms of real income per
capita among the autonomous regions of Spain. In order to converge, the series
should cointegrate. This necessary condition is checked using two testing
strategies recently proposed for fractional cointegration, finding no evidence
of cointegration, which rules out the possibility of convergence between all or
some of the Spanish regions. As an additional contribution, an extension of the
critical values of one of the tests of fractional cointegration is provided for
a different number of variables and sample sizes from those originally provided
by the author, fitting those considered in this paper.

Exempting soybean and rapeseed exporters from VAT has a negative effect on
the economy of $\$$44.5-60.5 million per year. The implemented policy aimed to
increase the processing of soybeans and rapeseed by Ukrainian plants. As a
result, the processors received $\$$26 million and the state budget gained
$\$$2-18 million. However, soybean farmers, mostly small and medium-sized,
received $ 88.5 million in losses, far outweighing the benefits of processors
and the state budget.

This article reviews recent advances in addressing empirical identification
issues in cross-country and country-level studies and their implications for
the identification of the effectiveness and consequences of economic sanctions.
I argue that, given the difficulties in assessing causal relationships in
cross-national data, country-level case studies can serve as a useful and
informative complement to cross-national regression studies. However, I also
warn that case studies pose a set of additional potential empirical pitfalls
which can obfuscate rather than clarify the identification of causal mechanisms
at work. Therefore, the most sensible way to read case study evidence is as a
complement rather than as a substitute to cross-national research.

A dynamic model is constructed that generalises the Hartwick and Van Long
(2020) endogenous discounting setup by introducing externalities and asks what
implications this has for optimal natural resource extraction with constant
consumption. It is shown that a modified form of the Hotelling and Hartwick
rule holds in which the externality component of price is a specific function
of the instantaneous user costs and cross price elasticities. It is
demonstrated that the externality adjusted marginal user cost of remaining
natural reserves is equal to the marginal user cost of extracted resources
invested in human-made reproducible capital. This lends itself to a discrete
form with a readily intuitive economic interpretation that illuminates the
stepwise impact of externality pricing on optimal extraction schedules.

Monitoring and enforcement considerations have been largely forgotten in the
study of fishery management. This paper discusses this issue through a model
formalization to show the impacts of costly, imperfect enforcement of law on
the behavior of fishing firms and fisheries management. Theoretical analysis
merges a standard bio-economic model of fisheries (Gordon-Schaefer) with Becker
theory of Crime and Punishment.

This article has one single purpose: introduce a new and simple, yet highly
insightful approach to capture, fully and quantitatively, the dynamics of the
circular flow of income in economies. The proposed approach relies mostly on
basic linear algebraic concepts and has deep implications for the disciplines
of economics, physics and econophysics.

Recent highly cited research uses time-series evidence to argue the decline
in interest rates led to a large rise in economic profits and markups. We show
the size of these estimates is sensitive to the sample start date: The rise in
markups from 1984 to 2019 is 14% larger than from 1980 to 2019, a difference
amounting to a $3000 change in income per worker in 2019. The sensitivity comes
from a peak in interest rates in 1984, during a period of heightened
volatility. Our results imply researchers should justify their time-series
selection and incorporate sensitivity checks in their analysis.

This study examines the relationship between ambiguity and the ideological
positioning of political parties across the political spectrum. We identify a
strong non-monotonic (inverted U-shaped) relationship between party ideology
and ambiguity within a sample of 202 European political parties. This pattern
is observed across all ideological dimensions covered in the data. To explain
this pattern, we propose a novel theory that suggests centrist parties are
perceived as less risky by voters compared to extremist parties, giving them an
advantage in employing ambiguity to attract more voters at a lower cost. We
support our explanation with additional evidence from electoral outcomes and
economic indicators in the respective party countries.

This paper applies the Hotelling model to the context of exhaustible human
resources in China. We find that over-exploitation of human resources occurs
under conditions of restricted population mobility, rigid wage levels, and
increased foreign trade demand elasticity. Conversely, the existence of
technological replacements for human resources or improvements in the
utilization rate of human resources leads to conservation. Our analysis
provides practical insights for policy-making towards sustainable development.

Aghamolla and Smith (2023) make a significant contribution to enhancing our
understanding of how managers choose financial reporting complexity. I outline
the key assumptions and implications of the theory, and discuss two empirical
implications: (1) a U-shaped relationship between complexity and returns, and
(2) a negative association between complexity and investor sophistication.
However, the robust equilibrium also implies a counterfactual positive market
response to complexity. I develop a simplified approach in which simple
disclosures indicate positive surprises, and show that this implies greater
investor skepticism toward complexity and a positive association between
investor sophistication and complexity. More work is needed to understand
complexity as an interaction of reporting and economic transactions, rather
than solely as a reporting phenomenon.

Data from national accounts show no effect of change in net saving or
consumption, in ratio to market-value capital, on change in growth rate of
market-value capital (capital acceleration). Thus it appears that capital
growth and acceleration arrive without help from net saving or consumption
restraint. We explore ways in which this is possible, and discuss implications
for economic teaching and public policy

This paper investigates how the cost of public debt shapes fiscal policy and
its effect on the economy. Using U.S. historical data, I show that when
servicing the debt creates a fiscal burden, the government responds to spending
shocks by limiting debt issuance. As a result, the initial shock triggers only
a limited increase in public spending in the short run, and even leads to
spending reversal in the long run. Under these conditions, fiscal policy loses
its ability to stimulate economic activity. This outcome arises as the fiscal
authority limits its own ability to borrow to ensure public debt
sustainability. These findings are robust to several identification and
estimation strategies.

Using as a central instrument a new database, resulting from a compilation of
historical administrative records, which covers the period 1974-2010, we can
have new evidence on how industrial companies used tax benefits, and claim that
these are decisive for the investment decision of the Uruguayan industrial
companies during that period. The aforementioned findings served as a raw
material to also affirm that the incentives to increase investment are factors
that positively influence the level of economic activity and exports, and
negatively on the unemployment rate.

Central Bank Digital Currency (CBDC) may assist New Zealand accomplish SDG 8.
I aim to evaluate if SDGs could be achieved together because of mutual
interactions between SDG 8 and other SDGs. The SDGs are categorized by their
shared qualities to affect and effect SDG 8. Also, additional SDGs may help
each other achieve. Considering the CBDC as a fundamental stimulus to achieving
decent work and economic growth, detailed study and analysis of mutual
interactions suggests that SDG 8 and other SDGs can be achieved.

In the inverted yield curve environment, I intend to assess the feasibility
of fulfilling Sustainable Development Goal (SDG) 8, decent work and economic
growth, of the United Nations by 2030 in New Zealand. Central Bank Digital
Currency (CBDC) issuance supports SDG 8, based on the Cobb-Douglas production
function, the growth accounting relation, and the Theory of Aggregate Demand.
Bright prospects exist for New Zealand.

This paper presents a method for incorporating risk aversion into existing
decision tree models used in economic evaluations. The method involves applying
a probability weighting function based on rank dependent utility theory to
reduced lotteries in the decision tree model. This adaptation embodies the fact
that different decision makers can observe the same decision tree model
structure but come to different conclusions about the optimal treatment. The
proposed solution to this problem is to compensate risk-averse decision makers
to use the efficient technology that they are reluctant to adopt.

The focus of this text is to discuss how geographical science can contribute
to an understanding of international migration in the 21st century. To this
end, in the introduction we present the central ideas, as well as the internal
structure of the text. Then, we present the theoretical and methodological
approach that guides the research and, in section three, we show the results
through texts and cartograms based on secondary data and empirical information.
Finally, in the final remarks, we summarize the text with a view to
contributing to the proposed debate.

We study the evolution of population density across Italian municipalities on
the based of their trajectories in the Moran space. We find evidence of spatial
dynamical patterns of concentrated urban growth, urban sprawl, agglomeration,
and depopulation. Over the long run, three distinct settlement systems emerge:
urban, suburban, and rural. We discuss how estimating these demographic trends
at the municipal level can help the design and validation of policies
contrasting the socio-economic decline in specific Italian areas, as in the
case of the Italian National Strategy for Inner Areas (Strategia Nazionale per
le Aree Interne, SNAI).

The sudden and rapid spread of the COVID_19 pandemic with its terrible
consequences has put the management of governments and the various world
institutions into a crisis. They have been subjected to a considerable economic
effort to be taken to combat the spread of the pandemic. The economic
investment for the research and purchase of vaccines intended for populations
is subject to cos-benefit analyzes in various situations in different cases. In
this review work, several recent models are analyzed where the appearance of
the components is coupled with the economic aspect. The analysis of these
models is detailed and the results discussed from different points of view.

This paper proposes a referencable pattern of the recovery of the consumption
sector, a new dimension to observe and evaluate the intrinsic value of the
consumption sector, and proposes the concept of sensory-based consumption and
the ranking of the weights of different categories;creates the concept of
digital consumption index, coupled with digital RMB index and China-style
digital economy index. Finally we explain the internal logic of digital
consumption as a consumption upgrade tool and a higher valuation target in the
context of China's economic performance in 2022 and the Chinese government's
policy in 2023, leading to the investment strategy of roller conduction effect.

This study investigates price seasonality in the Brazilian air transport
industry, emphasizing the impact of the COVID-19 pandemic on domestic airline
pricing strategies. Given potential shifts in demand patterns following the
global health crisis, this study explores possible long-term structural changes
in the seasonality of Brazilian airfare. We analyze an open dataset of domestic
city pairs from 2013 to 2023, employing an econometric model developed using
Stata software. Our findings indicate alterations in seasonal patterns and
long-term trends in the post-pandemic era. These changes underscore potential
shifts in the composition of leisure and business travelers, along with the
cost pressures faced by airlines.

This paper considers the problem of interpreting orthogonalization model
coefficients. We derive a causal economic interpretation of the Gram-Schmidt
orthogonalization process and provide the conditions for its equivalence to
total effects from a recursive Directed Acyclic Graph. We extend the
Gram-Schmidt process to groups of simultaneous regressors common in economic
data sets and derive its finite sample properties, finding its coefficients to
be unbiased, stable, and more efficient than those from Ordinary Least Squares.
Finally, we apply the estimator to childhood reading comprehension scores,
controlling for such highly collinear characteristics as race, education, and
income. The model expands Bohren et al.'s decomposition of systemic
discrimination into channel-specific effects and improves its coefficient
significance levels.

We estimate the short-run effects of severe weather shocks on local economic
activity and cross-border spillovers operating through economic linkages
between U.S. states. We measure weather shocks using emergency declarations
triggered by natural disasters and estimate their impacts with a monthly Global
Vector Autoregressive (GVAR) model for U.S. states. Impulse responses highlight
country-wide effects of weather shocks hitting individual regions. Taking into
account economic interconnections between states allows capturing much stronger
spillovers than those associated with mere spatial adjacency. Also,
geographical heterogeneity is critical for assessing country-wide effects of
weather shocks, and network effects amplify the local impacts of shocks.

This paper presents first results for near optimality in expectation of the
closed-loop solutions for stochastic economic MPC. The approach relies on a
recently developed turnpike property for stochastic optimal control problems at
an optimal stationary process, combined with techniques for analyzing
time-varying economic MPC schemes. We obtain near optimality in finite time as
well as overtaking and average near optimality on infinite time horizons.

We examine how monetary shocks spread throughout an economic model
characterized by sticky prices and general equilibrium, where the pricing
strategies of firms are interlinked, fostering a mutually beneficial
relationship. In this dynamic equilibrium, pricing choices of firms are
influenced by overall economic factors, which are themselves affected by these
decisions. We approach this situation using a path integral control method,
yielding several important insights. We confirm the presence and uniqueness of
the equilibrium and scrutinize the impulse response function (IRF) of output
subsequent to a shock affecting the entire economy.

The document provides an overview of financial climate risks. It delves into
how climate change impacts the global financial system, distinguishing between
physical risks (such as extreme weather events) and transition risks (stemming
from policy changes and economic transitions towards low carbon technologies).
The paper underlines the complexity of accurately defining financial climate
risk, citing the integration of climate science with financial risk analysis as
a significant challenge. The paper highlights the pivotal role of microfinance
institutions (MFIs) in addressing financial climate risk, especially for
populations vulnerable to climate change. The document emphasizes the
importance of updating risk management practices within MFIs to explicitly
include climate risk assessments and suggests leveraging technology to improve
these practices.

This article reviews the economics literature of, primarily, the last 20
years, that studies the link between income shocks and consumption fluctuations
at the household level. We identify three broad approaches through which
researchers estimate the consumption response to income shocks: 1.) structural
methods in which a fully or partially specified model helps identify the
consumption response to income shocks from the data; 2.) natural experiments in
which the consumption response of one group who receives an income shock is
compared to another group who does not; 3.) elicitation surveys in which
consumers are asked how they expect to react to various hypothetical events.

This chapter explores the role of substitutability in economic models,
particularly in the context of optimal transport and matching models. In
equilibrium models with substitutability, market-clearing prices can often be
recovered using coordinate update methods such as Jacobi's algorithm. We
provide a detailed mathematical analysis of models with substitutability
through the lens of Z- and M-functions, in particular regarding their role in
ensuring the convergence of Jacobi's algorithm. The chapter proceeds by
studying matching models using substitutability, first focusing on models with
(imperfectly) transferable utility, and then on models with non-transferable
utility. In both cases, the text reviews theoretical implications as well as
computational approaches (Sinkhorn, Gale--Shapley), and highlights a practical
economic application.

Could John Kerry have gained votes in the 2004 Presidential election by more
clearly distinguishing himself from George Bush on economic policy? At first
thought, the logic of political preferences would suggest not: the Republicans
are to the right of most Americans on economic policy, and so in a
one-dimensional space with party positions measured with no error, the optimal
strategy for the Democrats would be to stand infinitesimally to the left of the
Republicans. The median voter theorem suggests that each party should keep its
policy positions just barely distinguishable from the opposition. In a
multidimensional setting, however, or when voters vary in their perceptions of
the parties' positions, a party can benefit from putting some daylight between
itself and the other party on an issue where it has a public-opinion advantage
(such as economic policy for the Democrats). We set up a plausible theoretical
model in which the Democrats could achieve a net gain in votes by moving to the
left on economic policy, given the parties' positions on a range of issue
dimensions. We then evaluate this model based on survey data on voters'
perceptions of their own positions and those of the candidates in 2004. Under
our model, it turns out to be optimal for the Democrats to move slightly to the
right but staying clearly to the left of the Republicans' current position on
economic issues.

The concept of communicability is introduced for complex socio-economic
networks. The communicability function expresses how an impact propagates from
one place to another in the network. This function is used to define
unambiguously the concept of socio-economic community. The concept of
temperature in complex socio-economic networks is also introduced as a way of
accounting for the external stresses to which such systems are submitted. This
external stress can change dramatically the structure of the communities in a
network. We analyze here a trade network of countries exporting 'miscellaneous
manufactures of metal.' We determine the community structure of this network
showing that there are 27 communities with diverse degree of overlapping. When
only communities with less than 80% of overlap are considered we found 5
communities which are well characterized in terms of geopolitical
relationships. The analysis of external stress on these communities reveals
that several countries are very much influenced by these critical situations,
i.e., economical crisis. These weakest links are clearly identified and
represent countries that are isolated from the main trade as soon as the
external "temperature" of the system is increased. The current approach adds an
important tool for the analysis of socio-economic networks in the real-world.

The creation of open archives i.e. archives where access is regulated by open
licensing models (content, source, data), should be seen as part of a broader
socio-economic phenomenon that finds legal expression in specific
organizational and technical formats.This paper examines the origins and main
characteristics of the open archives phenomenon. We investigate the extent to
which different models of production of economic or social value can be
expressed in different forms of licensing in the context of open archives.
Through this process, we assess the extent to which the digital archive is
moving towards providing access that is deeper (meaning, that offers more
access rights) and wider (in the sense that most of the information given is in
open content licensing) or face a gradual stratification and polarization of
the content. Such stratification entails the emergence of two types of content:
content to which access is extremely limited and content to which access
remains completely open. This differentiation between classes of content is the
result of multiple factors: from purely legislative, administrative and
contractual restrictions (e.g. data protection and confidentiality
restrictions) to information economics (e.g. peer production) or social
(minimum universal access).
  We claim that with respect to the access management model, most of the
current archiving processes include elements of openness. Usually, this is the
result of economic necessity expressed in licensing instruments or
organisational arrangements. The viability and the socio-economic importance of
the digital archives also contributes to the use of open archiving practices.
In such a context, although pure forms of open digital archives may remain an
ideal, the reality of hybrid open digital archives is a necessity.

Masanao Aoki developed a new methodology for a basic problem of economics:
deducing rigorously the macroeconomic dynamics as emerging from the
interactions of many individual agents. This includes deduction of the fractal
/ intermittent fluctuations of macroeconomic quantities from the granularity of
the mezo-economic collective objects (large individual wealth, highly
productive geographical locations, emergent technologies, emergent economic
sectors) in which the micro-economic agents self-organize.
  In particular, we present some theoretical predictions, which also met
extensive validation from empirical data in a wide range of systems: - The
fractal Levy exponent of the stock market index fluctuations equals the Pareto
exponent of the investors wealth distribution. The origin of the macroeconomic
dynamics is therefore found in the granularity induced by the wealth / capital
of the wealthiest investors. - Economic cycles consist of a Schumpeter
'creative destruction' pattern whereby the maxima are cusp-shaped while the
minima are smooth. In between the cusps, the cycle consists of the sum of 2
'crossing exponentials': one decaying and the other increasing.
  This unification within the same theoretical framework of short term market
fluctuations and long term economic cycles offers the perspective of a genuine
conceptual synthesis between micro- and macroeconomics. Joining another giant
of contemporary science - Phil Anderson - Aoki emphasized the role of rare,
large fluctuations in the emergence of macroeconomic phenomena out of
microscopic interactions and in particular their non self-averaging, in the
language of statistical physics. In this light, we present a simple stochastic
multi-sector growth model.

In economic and financial networks, the strength of each node has always an
important economic meaning, such as the size of supply and demand, import and
export, or financial exposure. Constructing null models of networks matching
the observed strengths of all nodes is crucial in order to either detect
interesting deviations of an empirical network from economically meaningful
benchmarks or reconstruct the most likely structure of an economic network when
the latter is unknown. However, several studies have proved that real economic
networks and multiplexes are topologically very different from configurations
inferred only from node strengths. Here we provide a detailed analysis of the
World Trade Multiplex by comparing it to an enhanced null model that
simultaneously reproduces the strength and the degree of each node. We study
several temporal snapshots and almost one hundred layers (commodity classes) of
the multiplex and find that the observed properties are systematically well
reproduced by our model. Our formalism allows us to introduce the (static)
concept of extensive and intensive bias, defined as a measurable tendency of
the network to prefer either the formation of extra links or the reinforcement
of link weights, with respect to a reference case where only strengths are
enforced. Our findings complement the existing economic literature on (dynamic)
intensive and extensive trade margins. More in general, they show that
real-world multiplexes can be strongly shaped by layer-specific local
constraints.

Understanding cities is central to addressing major global challenges from
climate and health to economic resilience. Although increasingly perceived as
fundamental socio-economic units, the detailed fabric of urban economic
activities is only now accessible to comprehensive analyses with the
availability of large datasets. Here, we study abundances of business
categories across U.S. metropolitan statistical areas to investigate how
diversity of economic activities depends on city size. A universal structure
common to all cities is revealed, manifesting self-similarity in internal
economic structure as well as aggregated metrics (GDP, patents, crime). A
derivation is presented that explains universality and the observed empirical
distribution. The model incorporates a generalized preferential attachment
process with ceaseless introduction of new business types. Combined with
scaling analyses for individual categories, the theory quantitatively predicts
how individual business types systematically change rank with city size,
thereby providing a quantitative means for estimating their expected abundances
as a function of city size. These results shed light on processes of economic
differentiation with scale, suggesting a general structure for the growth of
national economies as integrated urban systems.

This research deals with the mathematical modeling of the physical capital
diffusion through the borders of the countries. The physical capital is
considered an important variable for the economic growth of a country. Here we
use an extension of the economic Solow model to describe how the smuggling
affects the economic growth of the countries. In this study we rely on a
production function that is non-concave instead of the classical Cobb-Douglas
production function. In order to model the physical capital diffusion through
the borders of the country, we developed a model based on a parabolic partial
differential equation that describes the dynamics of physical capital and
boundary conditions of Neumann type. Smuggling is present in many borders
between countries and may include fuel, machinery and food. This smuggling
through the borders is a problematic issue for the country's economies. The
smuggling problem usually is related mainly to a non-official exchange rate
that is different than the official rate or subsides. Numerical simulations are
obtained using an explicit finite difference scheme that shows how the physical
capital diffusion through the border of the countries. The study of physical
capital is a paramount issue for the economic growth of many countries for the
next years. The results show that the dynamics of the physical capital when
boundary conditions of Neumann type are different than zero differ from the
classical economic behavior observed in the classical spatial Solow model
without physical capital flux through the borders of countries. Finally, it can
be concluded that avoiding the smuggling through the frontiers is an important
factor that affects the economic growth of the countries.

Inspired by Adam Smith and Friedrich Hayek, many economists have postulated
the existence of invisible forces that drive economic markets. These market
forces interact in complex ways making it difficult to visualize or understand
the interactions in every detail. Here I show how these forces can transcend a
zero-sum game and become a win-win business interaction, thanks to emergent
social synergies triggered by division of labor. Computer simulations with the
model Sociodynamica show here the detailed dynamics underlying this phenomenon
in a simple virtual economy. In these simulations, independent agents act in an
economy exploiting and trading two different goods in a heterogeneous
environment. All and each of the various forces and individuals were tracked
continuously, allowing to unveil a synergistic effect on economic output
produced by the division of labor between agents. Running simulations in a
homogeneous environment, for example, eliminated all benefits of division of
labor. The simulations showed that the synergies unleashed by division of labor
arise if: Economies work in a heterogeneous environment; agents engage in
complementary activities whose optimization processes diverge; agents have
means to synchronize their activities. This insight, although trivial if viewed
a posteriori, improve our understanding of the source and nature of synergies
in real economic markets and might render economic and natural sciences more
consilient.

The literature on the costs of climate change often draws a link between
climatic 'tipping points' and large economic shocks, frequently called
'catastrophes'. The use of the phrase 'tipping points' in this context can be
misleading. In popular and social scientific discourse, 'tipping points'
involve abrupt state changes. For some climatic 'tipping points,' the
commitment to a state change may occur abruptly, but the change itself may be
rate-limited and take centuries or longer to realize. Additionally, the
connection between climatic 'tipping points' and economic losses is tenuous,
though emerging empirical and process-model-based tools provide pathways for
investigating it. We propose terminology to clarify the distinction between
'tipping points' in the popular sense, the critical thresholds exhibited by
climatic and social 'tipping elements,' and 'economic shocks'. The last may be
associated with tipping elements, gradual climate change, or non-climatic
triggers. We illustrate our proposed distinctions by surveying the literature
on climatic tipping elements, climatically sensitive social tipping elements,
and climate-economic shocks, and we propose a research agenda to advance the
integrated assessment of all three.

Emerging trends in smartphones, online maps, social media, and the resulting
geo-located data, provide opportunities to collect traces of people's
socio-economical activities in a much more granular and direct fashion,
triggering a revolution in empirical research. These vast mobile data offer new
perspectives and approaches for measurements of economic dynamics and are
broadening the research fields of social science and economics. In this paper,
we explore the potential of using mobile big data for measuring economic
activities of China. Firstly, We build indices for gauging employment and
consumer trends based on billions of geo-positioning data. Secondly, we advance
the estimation of store offline foot traffic via location search data derived
from Baidu Maps, which is then applied to predict revenues of Apple in China
and detect box-office fraud accurately. Thirdly, we construct consumption
indicators to track the trends of various industries in service sector, which
are verified by several existing indicators. To the best of our knowledge, we
are the first to measure the second largest economy by mining such
unprecedentedly large scale and fine granular spatial-temporal data. Our
research provides new approaches and insights on measuring economic activities.

This research explores the potential to analyze bank card payments and ATM
cash withdrawals in order to map and quantify how people are impacted by and
recover from natural disasters. Our approach defines a disaster-affected
community's economic recovery time as the time needed to return to baseline
activity levels in terms of number of bank card payments and ATM cash
withdrawals. For Hurricane Odile, which hit the state of Baja California Sur
(BCS) in Mexico between 15 and 17 September 2014, we measured and mapped
communities' economic recovery time, which ranged from 2 to 40 days in
different locations. We found that -- among individuals with a bank account --
the lower the income level, the shorter the time needed for economic activity
to return to normal levels. Gender differences in recovery times were also
detected and quantified. In addition, our approach evaluated how communities
prepared for the disaster by quantifying expenditure growth in food or gasoline
before the hurricane struck. We believe this approach opens a new frontier in
measuring the economic impact of disasters with high temporal and spatial
resolution, and in understanding how populations bounce back and adapt.

Cloud computing has reached significant maturity from a systems perspective,
but currently deployed solutions rely on rather basic economics mechanisms that
yield suboptimal allocation of the costly hardware resources. In this paper we
present Economic Resource Allocation (ERA), a complete framework for scheduling
and pricing cloud resources, aimed at increasing the efficiency of cloud
resources usage by allocating resources according to economic principles. The
ERA architecture carefully abstracts the underlying cloud infrastructure,
enabling the development of scheduling and pricing algorithms independently of
the concrete lower-level cloud infrastructure and independently of its
concerns. Specifically, ERA is designed as a flexible layer that can sit on top
of any cloud system and interfaces with both the cloud resource manager and
with the users who reserve resources to run their jobs. The jobs are scheduled
based on prices that are dynamically calculated according to the predicted
demand. Additionally, ERA provides a key internal API to pluggable algorithmic
modules that include scheduling, pricing and demand prediction. We provide a
proof-of-concept software and demonstrate the effectiveness of the architecture
by testing ERA over both public and private cloud systems -- Azure Batch of
Microsoft and Hadoop/YARN. A broader intent of our work is to foster
collaborations between economics and system communities. To that end, we have
developed a simulation platform via which economics and system experts can test
their algorithmic implementations.

Classic economic science is reaching the limits of its explanatory powers.
Complexity science uses an increasingly larger set of different methods to
analyze physical, biological, cultural, social, and economic factors, providing
a broader understanding of the socio-economic dynamics involved in the
development of nations worldwide. The use of tools developed in the natural
sciences, such as thermodynamics, evolutionary biology, and analysis of complex
systems, help us to integrate aspects, formerly reserved to the social
sciences, with the natural sciences. This integration reveals details of the
synergistic mechanisms that drive the evolution of societies. By doing so, we
increase the available alternatives for economic analysis and provide ways to
increase the efficiency of decision-making mechanisms in complex social
contexts. This interdisciplinary analysis seeks to deepen our understanding of
why chronic poverty is still common, and how the emergence of prosperous
technological societies can be made possible. This understanding should
increase the chances of achieving a sustainable, harmonious and prosperous
future for humanity. The analysis evidences that complex fundamental economic
problems require multidisciplinary approaches and rigorous application of the
scientific method if we want to advance significantly our understanding of
them. The analysis reveals viable routes for the generation of wealth and the
reduction of poverty, but also reveals huge gaps in our knowledge about the
dynamics of our societies and about the means to guide social development
towards a better future for all.

This paper presents a comprehensive literature review on applications of
economic and pricing theory for resource management in the evolving fifth
generation (5G) wireless networks. The 5G wireless networks are envisioned to
overcome existing limitations of cellular networks in terms of data rate,
capacity, latency, energy efficiency, spectrum efficiency, coverage,
reliability, and cost per information transfer. To achieve the goals, the 5G
systems will adopt emerging technologies such as massive Multiple-Input
Multiple-Output (MIMO), mmWave communications, and dense Heterogeneous Networks
(HetNets). However, 5G involves multiple entities and stakeholders that may
have different objectives, e.g., high data rate, low latency, utility
maximization, and revenue/profit maximization. This poses a number of
challenges to resource management designs of 5G. While the traditional
solutions may neither efficient nor applicable, economic and pricing models
have been recently developed and adopted as useful tools to achieve the
objectives. In this paper, we review economic and pricing approaches proposed
to address resource management issues in the 5G wireless networks including
user association, spectrum allocation, and interference and power management.
Furthermore, we present applications of economic and pricing models for
wireless caching and mobile data offloading. Finally, we highlight important
challenges, open issues and future research directions of applying economic and
pricing models to the 5G wireless networks.

We study an agent-based model of evolution of wealth distribution in a
macro-economic system. The evolution is driven by multiplicative stochastic
fluctuations governed by the law of proportionate growth and interactions
between agents. We are mainly interested in interactions increasing wealth
inequality that is in a local implementation of the accumulated advantage
principle. Such interactions destabilise the system. They are confronted in the
model with a global regulatory mechanism which reduces wealth inequality. There
are different scenarios emerging as a net effect of these two competing
mechanisms. When the effect of the global regulation (economic interventionism)
is too weak the system is unstable and it never reaches equilibrium. When the
effect is sufficiently strong the system evolves towards a limiting stationary
distribution with a Pareto tail. In between there is a critical phase. In this
phase the system may evolve towards a steady state with a multimodal wealth
distribution. The corresponding cumulative density function has a
characteristic stairway pattern which reflects the effect of economic
stratification. The stairs represent wealth levels of economic classes
separated by wealth gaps. As we show, the pattern is typical for macro-economic
systems with a limited economic freedom. One can find such a multimodal pattern
in empirical data, for instance, in the highest percentile of wealth
distribution for the population in urban areas of China.

Urban economists have put forward the idea that cities that are culturally
interesting tend to attract "the creative class" and, as a result, end up being
economically successful. Yet it is still unclear how economic and cultural
dynamics mutually influence each other. By contrast, that has been extensively
studied in the case of individuals. Over decades, the French sociologist Pierre
Bourdieu showed that people's success and their positions in society mainly
depend on how much they can spend (their economic capital) and what their
interests are (their cultural capital). For the first time, we adapt Bourdieu's
framework to the city context. We operationalize a neighborhood's cultural
capital in terms of the cultural interests that pictures geo-referenced in the
neighborhood tend to express. This is made possible by the mining of what users
of the photo-sharing site of Flickr have posted in the cities of London and New
York over 5 years. In so doing, we are able to show that economic capital alone
does not explain urban development. The combination of cultural capital and
economic capital, instead, is more indicative of neighborhood growth in terms
of house prices and improvements of socio-economic conditions. Culture pays,
but only up to a point as it comes with one of the most vexing urban
challenges: that of gentrification.

Methods for predicting the likely upper economic limit for the wind fleet in
the United Kingdom should be simple to use whilst being able to cope with
evolving technologies, costs and grid management strategies. This paper present
two such models, both of which use data on historical wind patterns but apply
different approaches to estimating the extent of wind shedding as a function of
the size of the wind fleet. It is clear from the models that as the wind fleet
increases in size, wind shedding will progressively increase, and as a result
the overall economic efficiency of the wind fleet will be reduced. The models
provide almost identical predictions of the efficiency loss and suggest that
the future upper economic limit of the wind fleet will be mainly determined by
the wind fleet Headroom, a concept described in some detail in the paper. The
results, which should have general applicability, are presented in graphical
form, and should obviate the need for further modelling using the primary data.
The paper also discusses the effectiveness of the wind fleet in decarbonising
the grid, and the growing competition between wind and solar fleets as sources
of electrical energy for the United Kingdom.

Many-body systems can have multiple equilibria. Though the energy of
equilibria might be the same, still systems may resist to switch from an
unfavored equilibrium to a favored one. In this paper we investigate occurrence
of such phenomenon in economic networks. In times of crisis when governments
intend to stimulate economy, a relevant question is on the proper size of
stimulus bill. To address the answer, we emphasize the role of hysteresis in
economic networks. In times of crises, firms and corporations cut their
productions; now since their level of activity is correlated, metastable
features in the network become prominent. This means that economic networks
resist against the recovery actions. To measure the size of resistance in the
network against recovery, we deploy the XY model. Though theoretically the XY
model has no hysteresis, when it comes to the kinetic behavior in the
deterministic regimes, we observe a dynamic hysteresis. We find that to
overcome the hysteresis of the network, a minimum size of stimulation is needed
for success. Our simulations show that as long as the networks are
Watts-Strogatz, such minimum is independent of the characteristics of the
networks.

The increasing integration of world economies, which organize in complex
multilayer networks of interactions, is one of the critical factors for the
global propagation of economic crises. We adopt the network science approach to
quantify shock propagation on the global trade-investment multiplex network. To
this aim, we propose a model that couples a Susceptible-Infected-Recovered
epidemic spreading dynamics, describing how economic distress propagates
between connected countries, with an internal contagion mechanism, describing
the spreading of such economic distress within a given country. At the local
level, we find that the interplay between trade and financial interactions
influences the vulnerabilities of countries to shocks. At the large scale, we
find a simple linear relation between the relative magnitude of a shock in a
country and its global impact on the whole economic system, albeit the strength
of internal contagion is country-dependent and the intercountry propagation
dynamics is non-linear. Interestingly, this systemic impact can be predicted on
the basis of intra-layer and inter-layer scale factors that we name network
multipliers, that are independent of the magnitude of the initial shock. Our
model sets-up a quantitative framework to stress-test the robustness of
individual countries and of the world economy to propagating crashes.

In science and especially in economics, agent-based modeling has become a
widely used modeling approach. These models are often formulated as a large
system of difference equations. In this study, we discuss two aspects,
numerical modeling and the probabilistic description for two agent-based
computational economic market models: the Levy-Levy-Solomon model and the
Franke-Westerhoff model. We derive time-continuous formulations of both models,
and in particular we discuss the impact of the time-scaling on the model
behavior for the Levy-Levy-Solomon model. For the Franke-Westerhoff model, we
proof that a constraint required in the original model is not necessary for
stability of the time-continuous model. It is shown that a semi-implicit
discretization of the time-continuous system preserves this unconditional
stability. In addition, this semi-implicit discretization can be computed at
cost comparable to the original model. Furthermore, we discuss possible
probabilistic descriptions of time continuous agent-based computational
economic market models. Especially, we present the potential advantages of
kinetic theory in order to derive mesoscopic desciptions of agent-based models.
Exemplified, we show two probabilistic descriptions of the Levy-Levy-Solomon
and Franke-Westerhoff model.

Since the 2007-2009 financial crisis, substantial academic effort has been
dedicated to improving our understanding of interbank lending networks (ILNs).
Because of data limitations or by choice, the literature largely lacks multiple
loan maturities. We employ a complete interbank loan contract dataset to
investigate whether maturity details are informative of the network structure.
Applying the layered stochastic block model of Peixoto (2015) and other tools
from network science on a time series of bilateral loans with multiple maturity
layers in the Russian ILN, we find that collapsing all such layers consistently
obscures mesoscale structure. The optimal maturity granularity lies between
completely collapsing and completely separating the maturity layers and depends
on the development phase of the interbank market, with a more developed market
requiring more layers for optimal description. Closer inspection of the
inferred maturity bins associated with the optimal maturity granularity reveals
specific economic functions, from liquidity intermediation to financing.
Collapsing a network with multiple underlying maturity layers or extracting one
such layer, common in economic research, is therefore not only an incomplete
representation of the ILN's mesoscale structure, but also conceals existing
economic functions. This holds important insights and opportunities for
theoretical and empirical studies on interbank market functioning, contagion,
stability, and on the desirable level of regulatory data disclosure.

Results of a convincing causal statistical inference related to
socio-economic phenomena are treated as especially desired background for
conducting various socio-economic programs or government interventions.
Unfortunately, quite often real socio-economic issues do not fulfill
restrictive assumptions of procedures of causal analysis proposed in the
literature. This paper indicates certain empirical challenges and conceptual
opportunities related to applications of procedures of data depth concept into
a process of causal inference as to socio-economic phenomena. We show, how to
apply a statistical functional depths in order to indicate factual and
counterfactual distributions commonly used within procedures of causal
inference. Thus a modification of Rubin causality concept is proposed, i.e., a
centrality-oriented causality concept. The presented framework is especially
useful in a context of conducting causal inference basing on official
statistics, i.e., basing on already existing databases. Methodological
considerations related to extremal depth, modified band depth, Fraiman-Muniz
depth, and multivariate Wilcoxon sum rank statistic are illustrated by means of
example related to a study of an impact of EU direct agricultural subsidies on
a digital development in Poland in a period of 2012-2019.

We study a novel economic network (supply chain) comprised of wire transfers
(electronic payment transactions) among the universe of firms in Brazil (6.2
million firms). We construct a directed and weighted network in which vertices
represent cities and edges connote pairwise economic dependence between cities.
Cities (vertices) represent the collection of all firms in that location and
links denote intercity wire transfers. We find a high degree of economic
integration among cities in the trade network, which is consistent with the
high degree of specialization found across Brazilian cities. We are able to
identify which cities have a dominant role in the entire supply chain process
using centrality network measures. We find that the trade network has a
disassortative mixing pattern, which is consistent with the power-law shape of
the firm size distribution in Brazil. After the Brazilian recession in 2014, we
find that the disassortativity becomes even stronger as a result of the death
of many small firms and the consequent concentration of economic flows on large
firms. Our results suggest that recessions have a large impact on the trade
network with meaningful and heterogeneous economic consequences across
municipalities. We run econometric exercises and find that courts efficiency
plays a dual role. From the customer perspective, it plays an important role in
reducing contractual frictions as it increases economic transactions between
different cities. From the supplier perspective, cities that are central
suppliers to the supply chain seem to use courts inefficiency as a lawsuit
barrier from their customers.

The decision to locate an economic activity of one or several countries is
made taking into account numerous parameters and criteria. Several studies have
been carried out in this field, but they generally use information in a reduced
context. The majority are based solely on parameters, using traditional methods
which often lead to unsatisfactory solutions.This work consists in hybridizing
through genetic algorithms, economic intelligence (EI) and multicriteria
analysis methods (MCA) to improve the decisions of territorial localization.
The purpose is to lead the company to locate its activity in the place that
would allow it a competitive advantage. This work also consists of identifying
all the parameters that can influence the decision of the economic actors and
equipping them with tools using all the national and international data
available to lead to a mapping of countries, regions or departments favorable
to the location. Throughout our research, we have as a goal the realization of
a hybrid conceptual model of economic intelligence based on multicriteria on
with genetic algorithms in order to optimize the decisions of localization, in
this perspective we opted for the method of PROMETHEE (Preference Ranking
Organization for Method of Enrichment Evaluation), which has made it possible
to obtain the best compromise between the various visions and various points of
view.

The rapid spread of the Coronavirus (COVID-19) confronts policy makers with
the problem of measuring the effectiveness of containment strategies, balancing
public health considerations with the economic costs of social distancing
measures. We introduce a modified epidemic model that we name the
controlled-SIR model, in which the disease reproduction rate evolves
dynamically in response to political and societal reactions. An analytic
solution is presented. The model reproduces official COVID-19 cases counts of a
large number of regions and countries that surpassed the first peak of the
outbreak. A single unbiased feedback parameter is extracted from field data and
used to formulate an index that measures the efficiency of containment
strategies (the CEI index). CEI values for a range of countries are given. For
two variants of the controlled-SIR model, detailed estimates of the total
medical and socio-economic costs are evaluated over the entire course of the
epidemic. Costs comprise medical care cost, the economic cost of social
distancing, as well as the economic value of lives saved. Under plausible
parameters, strict measures fare better than a hands-off policy. Strategies
based on current case numbers lead to substantially higher total costs than
strategies based on the overall history of the epidemic.

We analyze how investor expectations about economic growth and stock returns
changed during the February-March 2020 stock market crash induced by the
COVID-19 pandemic, as well as during the subsequent partial stock market
recovery. We surveyed retail investors who are clients of Vanguard at three
points in time: (i) on February 11-12, around the all-time stock market high,
(ii) on March 11-12, after the stock market had collapsed by over 20\%, and
(iii) on April 16-17, after the market had rallied 25\% from its lowest point.
Following the crash, the average investor turned more pessimistic about the
short-run performance of both the stock market and the real economy. Investors
also perceived higher probabilities of both further extreme stock market
declines and large declines in short-run real economic activity. In contrast,
investor expectations about long-run (10-year) economic and stock market
outcomes remained largely unchanged, and, if anything, improved. Disagreement
among investors about economic and stock market outcomes also increased
substantially following the stock market crash, with the disagreement
persisting through the partial market recovery. Those respondents who were the
most optimistic in February saw the largest decline in expectations, and sold
the most equity. Those respondents who were the most pessimistic in February
largely left their portfolios unchanged during and after the crash.

Most of the world poorest people come from rural areas and depend on their
local ecosystems for food production. Recent research has highlighted the
importance of self-reinforcing dynamics between low soil quality and persistent
poverty but little is known on how they affect poverty alleviation. We
investigate how the intertwined dynamics of household assets, nutrients
(especially phosphorus), water and soil quality influence food production and
determine the conditions for escape from poverty for the rural poor. We have
developed a suite of dynamic, multidimensional poverty trap models of
households that combine economic aspects of growth with ecological dynamics of
soil quality, water and nutrient flows to analyze the effectiveness of common
poverty alleviation strategies such as intensification through agrochemical
inputs, diversification of energy sources and conservation tillage. Our results
show that (i) agrochemical inputs can reinforce poverty by degrading soil
quality, (ii) diversification of household energy sources can create
possibilities for effective application of other strategies, and (iii)
sequencing of interventions can improve effectiveness of conservation tillage.
Our model-based approach demonstrates the interdependence of economic and
ecological dynamics which preclude blanket solution for poverty alleviation.
Stylized models as developed here can be used for testing effectiveness of
different strategies given biophysical and economic settings in the target
region.

To contain the pandemic of coronavirus (COVID-19) in Mainland China, the
authorities have put in place a series of measures, including quarantines,
social distancing, and travel restrictions. While these strategies have
effectively dealt with the critical situations of outbreaks, the combination of
the pandemic and mobility controls has slowed China's economic growth,
resulting in the first quarterly decline of Gross Domestic Product (GDP) since
GDP began to be calculated, in 1992. To characterize the potential shrinkage of
the domestic economy, from the perspective of mobility, we propose two new
economic indicators: the New Venues Created (NVC) and the Volumes of Visits to
Venue (V^3), as the complementary measures to domestic investments and
consumption activities, using the data of Baidu Maps. The historical records of
these two indicators demonstrated strong correlations with the past figures of
Chinese GDP, while the status quo has dramatically changed this year, due to
the pandemic. We hereby presented a quantitative analysis to project the impact
of the pandemic on economies, using the recent trends of NVC and V^3. We found
that the most affected sectors would be travel-dependent businesses, such as
hotels, educational institutes, and public transportation, while the sectors
that are mandatory to human life, such as workplaces, residential areas,
restaurants, and shopping sites, have been recovering rapidly. Analysis at the
provincial level showed that the self-sufficient and self-sustainable economic
regions, with internal supplies, production, and consumption, have recovered
faster than those regions relying on global supply chains.

Once carbon emission neutrality and other sustainability goals have been
achieved, a widespread assumption is that economic growth at current rates can
be sustained beyond the 21st century. However, even if we achieve these goals,
this article shows that the overall size of Earth's global economy is facing an
upper limit purely due to energy and thermodynamic factors. For that, we break
down global warming into two components: the greenhouse gas effect and heat
dissipation from energy consumption related to economic activities. For the
temperature increase due to greenhouse gas emissions, we take 2 {\deg}C and 5
{\deg}C as our lower and upper bounds. For the warming effect of heat
dissipation related to energy consumption, we use a simplified model for global
warming and an extrapolation of the historical correlation between global gross
domestic product (GDP) and primary energy production. Combining the two
effects, we set the acceptable global warming temperature limit to 7 {\deg}C
above pre-industrial levels. We develop four scenarios, based on the viability
of large-scale deployment of carbon-neutral energy sources. Our results
indicate that for a 2% annual GDP growth, the upper limit will be reached at
best within a few centuries, even in favorable scenarios where new energy
sources such as fusion power are deployed on a massive scale. We conclude that
unless GDP can be largely decoupled from energy consumption, thermodynamics
will put a hard cap on the size of Earth's economy. Further economic growth
would necessarily require expanding economic activities into space.

It is now widely recognised that components of the environment play the role
of economic assets, termed natural capital, that are a foundation of social and
economic development. National governments monitor the state and trends of
natural capital through a range of activities including natural capital
accounting, national ecosystem assessments, ecosystem service valuation, and
economic and environmental analyses. Indicators play an integral role in these
activities as they facilitate the reporting of complex natural capital
information. One factor that hinders the success of these activities and their
comparability across countries is the absence of a coherent framework of
indicators concerning natural capital (and its benefits) that can aid
decision-making. Here we present an integrated Natural Capital Indicator
Framework (NCIF) alongside example indicators, which provides an illustrative
structure for countries to select and organise indicators to assess their use
of and dependence on natural capital. The NCIF sits within a wider context of
indicators related to natural, human, social and manufactured capital, and
associated flows of benefits. The framework provides decision-makers with a
structured approach to selecting natural capital indicators with which to make
decisions about economic development that take into account national natural
capital and associated flows of benefits.

COVID-19 pandemic has sharply projected the globally persistent
multi-dimensional fundamental challenges in securing general socio-economic
wellbeing of the society. The problems intensify with increasing population
densities and also vary with several socio-economic-geo-cultural activity
parameters. These problems directly highlight the urgent need for accomplishing
the interdependent United Nations Sustainable Development Goals (SDGs) to
ensure that in future we do not enter into vicious loops of contracting newer
zoonotic viruses and need not search for their vaccines while incurring
socio-economic havoc. Behavioural changes in human activities/responses are
indispensable for achieving the interdependent SDGs. Using root cause analysis
approach, we have developed a yearly assessment framework for viably analysing
and identifying requisite region-specific downstream/upstream socio-economic
policies to reach the SDGs. The framework makes use of an infographic bar chart
representation based on the normalised values of 20 human activity/impact
parameters classified under three categories as - negative, limiting and
positive. With a holistic view encompassing the SDGs, we illustrate through
this framework the impact and urgent need of region-specific human behavioural
reforms. This framework enables the foresight about policies regarding their
potential in bringing down the negative parameter values to the desired zero
level for accomplishing the SDGs through planetary health.

In this paper it is demonstrated that the application of principal components
analysis for regional cluster modelling and analysis is essential in the
situations where there is significant multicollinearity among several
parameters, especially when the dimensionality of regional data is measured in
tens. The proposed principal components model allows for same-quality
representation of the clustering of regions. In fact, the clusters become more
distinctive and the apparent outliers become either more pronounced with the
component model clustering or are alleviated with the respective hierarchical
cluster. Thus, a five-component model was obtained and validated upon 85
regions of Russian Federation and 19 socio-economic parameters. The principal
components allowed to describe approximately 75 percent of the initial
parameters variation and enable further simulations upon the studied variables.
The cluster analysis upon the principal components modelling enabled better
exposure of regional structure and disparity in economic development in Russian
Federation, consisting of four main clusters: the few-numbered highest
development regions, the clusters with mid-to-high and low economic
development, and the "poorest" regions. It is observable that the development
in most regions relies upon resource economy, and the industrial potential as
well as inter-regional infrastructural potential are not realized to their
fullest, while only the wealthiest regions show highly developed economy, while
the industry in other regions shows signs of stagnation which is scaled further
due to the conditions entailed by economic sanctions and the recent Covid-19
pandemic. Most Russian regions are in need of additional public support and
industrial development, as their capital assets potential is hampered and,
while having sufficient labor resources, their donorship will increase.

In this study we present a dynamical agent-based model to investigate the
interplay between the socio-economy of and SEIRS-type epidemic spreading over a
geographical area, divided to smaller area districts and further to smallest
area cells. The model treats the populations of cells and authorities of
districts as agents, such that the former can reduce their economic activity
and the latter can recommend economic activity reduction both with the overall
goal to slow down the epidemic spreading. The agents make decisions with the
aim of attaining as high socio-economic standings as possible relative to other
agents of the same type by evaluating their standings based on the local and
regional infection rates, compliance to the authorities' regulations, regional
drops in economic activity, and efforts to mitigate the spread of epidemic. We
find that the willingness of population to comply with authorities'
recommendations has the most drastic effect on the epidemic spreading: periodic
waves spread almost unimpeded in non-compliant populations, while in compliant
ones the spread is minimal with chaotic spreading pattern and significantly
lower infection rates. Health and economic concerns of agents turn out to have
lesser roles, the former increasing their efforts and the latter decreasing
them.

Financial inclusion and inclusive growth are the buzzwords today. Inclusive
growth empowers people belonging to vulnerable sections. This in turn depends
upon a variety of factors, the most important being financial inclusion, which
plays a strategic role in promoting inclusive growth and helps in reducing
poverty by providing regular and reliable sources of finance to the vulnerable
sections. In this direction, the Government of India in its drive for financial
inclusion has taken several measures to increase the access to and availing of
formal financial services by unbanked households. The purpose of this paper is
to assess the nature and extent of financial inclusion and its impact on the
socio-economic status of households belonging to vulnerable sections focusing
on inclusive growth. This has been analyzed with the theoretical background on
financial access and economic growth, and by analyzing the primary data
collected from the Revenue Divisions of Karnataka. The results show that there
is a disparity in nature and extent of financial inclusion. Access to, availing
of formal banking services pave the way to positive changes in the
socio-economic status of households belonging to vulnerable sections which are
correlated, leading to inclusive growth based on which the paper proposes a
model to make the financial system more inclusive and pro-poor.

New autonomous driving technologies are emerging every day and some of them
have been commercially applied in the real world. While benefiting from these
technologies, autonomous trucks are facing new challenges in short-term
maintenance planning, which directly influences the truck operator's profit. In
this paper, we implement a vehicle health management system by addressing the
maintenance planning issues of autonomous trucks on a transport mission. We
also present a maintenance planning model using a risk-based decision-making
method, which identifies the maintenance decision with minimal economic risk of
the truck company. Both availability losses and maintenance costs are
considered when evaluating the economic risk. We demonstrate the proposed model
by numerical experiments illustrating real-world scenarios. In the experiments,
compared to three baseline methods, the expected economic risk of the proposed
method is reduced by up to $47\%$. We also conduct sensitivity analyses of
different model parameters. The analyses show that the economic risk
significantly decreases when the estimation accuracy of remaining useful life,
the maximal allowed time of delivery delay before order cancellation, or the
number of workshops increases. The experiment results contribute to identifying
future research and development attentions of autonomous trucks from an
economic perspective.

Our study is one of the first examples of multidimensional and longitudinal
disciplinary analysis at the national level based on Crossref data. We present
a large-scale quantitative analysis of Ukrainian economics. This study is not
yet another example of research aimed at ranking of local journals, authors or
institutions, but rather exploring general tendencies that can be compared to
other countries or regions. We study different aspects of Ukrainian economics
output. In particular, the collaborative nature, geographic landscape and some
peculiarities of citation statistics are investigated. We have found that
Ukrainian economics is characterized by a comparably small share of co-authored
publications, however, it demonstrates the tendency towards more collaborative
output. Based on our analysis, we discuss specific and universal features of
Ukrainian economic research. The importance of supporting various initiatives
aimed at enriching open scholarly metadata is considered. A comprehensive and
high-quality meta description of publications is probably the shortest path to
a better understanding of national trends, especially for non-English speaking
countries. The results of our analysis can be used to better understand
Ukrainian economic research and support research policy decisions.

The penetration of the lithium-ion battery energy storage system (BESS) into
the power system environment occurs at a colossal rate worldwide. This is
mainly because it is considered as one of the major tools to decarbonize,
digitalize, and democratize the electricity grid. The economic viability and
technical reliability of projects with batteries require appropriate assessment
because of high capital expenditures, deterioration in charging/discharging
performance and uncertainty with regulatory policies. Most of the power system
economic studies employ a simple power-energy representation coupled with an
empirical description of degradation to model the lithium-ion battery. This
approach to modelling may result in violations of the safe operation and
misleading estimates of the economic benefits. Recently, the number of
publications on techno-economic analysis of BESS with more details on the
lithium-ion battery performance has increased. The aim of this review paper is
to explore these publications focused on the grid-scale BESS applications and
to discuss the impacts of using more sophisticated modelling approaches. First,
an overview of the three most popular battery models is given, followed by a
review of the applications of such models. The possible directions of future
research of employing detailed battery models in power systems' techno-economic
studies are then explored.

Research in Data Envelopment Analysis has created rankings of the ecological
efficiency of countries' economies. At the same time, research in economic
complexity has provided new methods to depict productive structures and has
analyzed how economic diversification and sophistication affect environmental
pollution indicators. However, no research so far has compared the ecological
efficiency of countries with similar productive structures and levels of
economic complexity, combining the strengths of both approaches. In this
article, we use data on 774 different types of exports, CO2 emissions, and the
ecological footprint of 99 countries to create a relative ecological pollution
ranking (REPR). Moreover, we use methods from network science to reveal a
benchmark network of the best learning partners based on country pairs with a
large extent of export similarity, yet significant differences in pollution
values. This is important because it helps to reveal adequate benchmark
countries for efficiency improvements and cleaner production, considering that
countries may specialize in substantially different types of economic
activities. Finally, the article (i) illustrates large efficiency improvements
within current global output levels, (ii) helps to identify countries that can
best learn from each other, and (iii) improves the information base in
international negotiations for the sake of a clean global production system.

The spread of COVID-19 and ensuing containment measures have accentuated the
profound interdependence among nations or regions. This has been particularly
evident in tourism, one of the sectors most affected by uncoordinated mobility
restrictions. The impact of this interdependence on the tendency to adopt less
or more restrictive measures is hard to evaluate, more so if diversity in
economic exposures to citizens' mobility are considered. Here, we address this
problem by developing an analytical and computational game-theoretical model
encompassing the conflicts arising from the need to control the economic
effects of global risks, such as in the COVID-19 pandemic. The model includes
the individual costs derived from severe restrictions imposed by governments,
including the resulting economic interdependence among all the parties involved
in the game. By using tourism-based data, the model is enriched with actual
heterogeneous income losses, such that every player has a different economic
cost when applying restrictions. We show that economic interdependence enhances
cooperation because of the decline in the expected payoffs by free-riding
parties (i.e., those neglecting the application of mobility restrictions).
Furthermore, we show (analytically and through numerical simulations) that
these cross-exposures can transform the nature of the cooperation dilemma each
region or country faces, modifying the position of the fixed points and the
size of the basins of attraction that characterize this class of games.
Finally, our results suggest that heterogeneity among regions may be used to
leverage the impact of intervention policies by ensuring an agreement among the
most relevant initial set of cooperators.

Agricultural systems experience land-use changes that are driven by
population growth and intensification of technological inputs. This results in
land-use and cover change (LUCC) dynamics representing a complex landscape
transformation process. In order to study the LUCC process we developed a
spatially explicit agent-based model in the form of a Cellular Automata
implemented with the Cell-DEVS formalism. The resulting model called AgroDEVS
is used for predicting LUCC dynamics along with their associated economic and
environmental changes. AgroDEVS is structured using behavioral rules and
functions representing a) crop yields, b) weather conditions, c) economic
profit, d) farmer preferences, e) technology level adoption and f) natural
resources consumption based on embodied energy accounting. Using data from a
typical location of the Pampa region (Argentina) for the 1988-2015 period,
simulation exercises showed that the economic goals were achieved, on average,
each 6 out of 10 years, but the environmental thresholds were only achieved in
1.9 out of 10 years. In a set of 50-years simulations, LUCC patterns quickly
converge towards the most profitable crop sequences, with no noticeable
tradeoff between the economic and environmental conditions.

This work applies Matrix Completion (MC) -- a class of machine-learning
methods commonly used in the context of recommendation systems -- to analyse
economic complexity. MC is applied to reconstruct the Revealed Comparative
Advantage (RCA) matrix, whose elements express the relative advantage of
countries in given classes of products, as evidenced by yearly trade flows. A
high-accuracy binary classifier is derived from the application of MC, with the
aim of discriminating between elements of the RCA matrix that are,
respectively, higher or lower than one. We introduce a novel Matrix cOmpletion
iNdex of Economic complexitY (MONEY) based on MC, which is related to the
predictability of countries' RCA (the lower the predictability, the higher the
complexity). Differently from previously-developed indices of economic
complexity, the MONEY index takes into account the various singular vectors of
the matrix reconstructed by MC, whereas other indices are based only on one/two
eigenvectors of a suitable symmetric matrix, derived from the RCA matrix.
Finally, MC is compared with a state-of-the-art economic complexity index
(GENEPY). We show that the false positive rate per country of a binary
classifier constructed starting from the average entry-wise output of MC can be
used as a proxy of GENEPY.

The UN states that inequalities are determined along with income by other
factors - gender, age, origin, ethnicity, disability, sexual orientation,
class, and religion. India, since the ancient period, has socio-political
stratification that induced socio-economic inequality and continued till now.
There have been attempts to reduce socio-economic inequality through policy
interventions since the first plan, still there are evidences of social and
economic discrimination. This paper examines earning gaps between the forward
castes and the traditionally disadvantaged caste workers in the Indian labour
market using two distinct estimation methods. First, we interpret the
inequality indicator of the Theil index and decompose Theil to show within and
between-group inequalities. Second, a Threefold Oaxaca Decomposition is
employed to break the earnings differentials into components of endowment,
coefficient and interaction. Earnings gaps are examined separately in urban and
rural divisions. Within-group, inequalities are found larger than between
groups across variables; with a higher overall inequality for forward castes. A
high endowment is observed which implies pre-market discrimination in human
capital investment such as nutrition and education. Policymakers should first
invest in basic quality education and simultaneously expand post-graduate
diploma opportunities, subsequently increasing the participation in the labour
force for the traditionally disadvantaged in disciplines and occupations where
the forward castes have long dominated.

The Covid-19 pandemic has intersected with the opioid epidemic to create a
unique public health crisis, with the health and economic consequences of the
virus and associated lockdowns compounding pre-existing social and economic
stressors associated with rising opioid and heroin use and abuse. In order to
better understand these interlocking crises, we use social media data to
extract qualitative and quantitative insights on the experiences of opioid
users during the Covid-19 pandemic. In particular, we use an unsupervised
learning approach to create a rich geolocated data source for public health
surveillance and analysis. To do this we first infer the location of 26,000
Reddit users that participate in opiate-related sub-communities (subreddits) by
combining named entity recognition, geocoding, density-based clustering, and
heuristic methods. Our strategy achieves 63 percent accuracy at state-level
location inference on a manually-annotated reference dataset. We then leverage
the geospatial nature of our user cohort to answer policy-relevant questions
about the impact of varying state-level policy approaches that balance economic
versus health concerns during Covid-19. We find that state government
strategies that prioritized economic reopening over curtailing the spread of
the virus created a markedly different environment and outcomes for opioid
users. Our results demonstrate that geospatial social media data can be used
for agile monitoring of complex public health crises.

For centuries, national economies created wealth by engaging in international
trade and production. The resulting international supply networks not only
increase wealth for countries, but also create systemic risk: economic shocks,
triggered by company failures in one country, may propagate to other countries.
Using global supply network data on the firm-level, we present a method to
estimate a country's exposure to direct and indirect economic losses caused by
the failure of a company in another country. We show the network of systemic
risk-flows across the world. We find that rich countries expose poor countries
much more to systemic risk than the other way round. We demonstrate that higher
systemic risk levels are not compensated with a risk premium in GDP, nor do
they correlate with economic growth. Systemic risk around the globe appears to
be distributed more unequally than wealth. These findings put the often praised
benefits for developing countries from globalized production in a new light,
since they relate them to the involved risks in the production processes.
Exposure risks present a new dimension of global inequality, that most affects
the poor in supply shock crises. It becomes fully quantifiable with the
proposed method.

Large scale directed energy offers the possibility of radical transformation
in a variety of areas, including the ability to achieve relativistic flight
that will enable the first interstellar missions, as well as rapid
interplanetary transit. In addition, the same technology will allow for
long-range beamed power for ion, ablation, and thermal engines, as well as
long-range recharging of distant spacecraft, long-range and ultra high
bandwidth laser communications, and many additional applications that include
remote composition analysis, manipulation of asteroids, and full planetary
defense. Directed energy relies on photonics which, like electronics, is an
exponentially expanding growth area driven by diverse economic interests that
allows transformational advances in space exploration and capability. We have
made enormous technological progress in the last few years to enable this
long-term vision. In addition to the technological challenges, we must face the
economic challenges to bring the vision to reality. The path ahead requires a
fundamental change in the system designs to allow for the radical cost
reductions required. To afford the full-scale realization of this vision we
will need to bring to fore integrated photonics and mass production as a path
forward. Fortunately, integrated photonics is a technology driven by vast
consumer need for high speed data delivery. We outline the fundamental physics
that drive the economics and derive an analytic cost model that allows us to
logically plan the path ahead.

Rapid increases in food supplies have reduced global hunger, while rising
burdens of diet-related disease have made poor diet quality the leading cause
of death and disability around the world. Today's "double burden" of
undernourishment in utero and early childhood then undesired weight gain and
obesity later in life is accompanied by a third less visible burden of
micronutrient imbalances. The triple burden of undernutrition, obesity, and
unbalanced micronutrients that underlies many diet-related diseases such as
diabetes, hypertension and other cardiometabolic disorders often coexist in the
same person, household and community. All kinds of deprivation are closely
linked to food insecurity and poverty, but income growth does not always
improve diet quality in part because consumers cannot directly or immediately
observe the health consequences of their food options, especially for newly
introduced or reformulated items. Even after direct experience and
epidemiological evidence reveals relative risks of dietary patterns and
nutritional exposures, many consumers may not consume a healthy diet because
food choice is driven by other factors. This chapter reviews the evidence on
dietary transition and food system transformation during economic development,
drawing implications for how research and practice in agricultural economics
can improve nutritional outcomes.

Most representative decision tree ensemble methods have been used to examine
the variable importance of Treasury term spreads to predict US economic
recessions with a balance of generating rules for US economic recession
detection. A strategy is proposed for training the classifiers with Treasury
term spreads data and the results are compared in order to select the best
model for interpretability. We also discuss the use of SHapley Additive
exPlanations (SHAP) framework to understand US recession forecasts by analyzing
feature importance. Consistently with the existing literature we find the most
relevant Treasury term spreads for predicting US economic recession and a
methodology for detecting relevant rules for economic recession detection. In
this case, the most relevant term spread found is 3 month to 6 month, which is
proposed to be monitored by economic authorities. Finally, the methodology
detected rules with high lift on predicting economic recession that can be used
by these entities for this propose. This latter result stands in contrast to a
growing body of literature demonstrating that machine learning methods are
useful for interpretation comparing many alternative algorithms and we discuss
the interpretation for our result and propose further research lines aligned
with this work.

This paper presents a conceptual model describing the medium and long-term
co-evolution of natural and socio-economic subsystems of Earth. An economy is
viewed as an out-of-equilibrium dissipative structure that can only be
maintained with a flow of energy and matter. The distinctive approach
emphasized here consists in capturing the economic impact of natural ecosystems
being depleted and destroyed by human activities via a pinch of thermodynamic
potentials. This viewpoint allows: (i) the full-blown integration of a limited
quantity of primary resources into a non-linear macrodynamics that is
stock-flow consistent both in terms of matter-energy as well as economic
transactions; (ii) the inclusion of natural and forced recycling; (iii) the
inclusion of a friction term which reflects the impossibility of producing
goods and services in high metabolising intensity without exuding energy and
matter wastes; (iv) the computation of the anthropically produced entropy as a
function of intensity and friction. Analysis and numerical computations confirm
the role played by intensity and friction as key factors for sustainability.
Our approach is flexible enough to allow for various economic models to be
embedded into our thermodynamic framework.

Measures of economic mobility represent aggregated values for how wealth
ranks of individuals change over time. Therefore, in certain circumstances
mobility measures may not describe the feasibility of the typical individual to
change their wealth ranking. To address this issue, we introduce mixing, a
concept from statistical physics, as a relevant phenomenon for quantifying the
ability of individuals to move across the whole wealth distribution. We display
the relationship between mixing and mobility by studying the relaxation time, a
statistical measure for the degree of mixing, in reallocating geometric
Brownian motion (RGBM). RGBM is an established model of wealth in a growing and
reallocating economy that distinguishes between a mixing and a non-mixing
wealth dynamics regime. We show that measures of mixing are inherently
connected to the concept of economic mobility: while certain individuals can
move across the distribution when wealth is a non-mixing observable, only in
the mixing case every individual is able to move across the whole wealth
distribution. Then, there is also a direct equivalence between measures of
mixing and the magnitude of the standard measures of economic mobility. On the
other hand, the opposite is not true. Wealth dynamics, are however, best
modeled as non-mixing. Hence, measuring mobility using standard measures in a
non-mixing system may lead to misleading conclusions about the extent of
mobility across the whole distribution.

Socio-economic indicators provide context for assessing a country's overall
condition. These indicators contain information about education, gender,
poverty, employment, and other factors. Therefore, reliable and accurate
information is critical for social research and government policing. Most data
sources available today, such as censuses, have sparse population coverage or
are updated infrequently. Nonetheless, alternative data sources, such as call
data records (CDR) and mobile app usage, can serve as cost-effective and
up-to-date sources for identifying socio-economic indicators.
  This work investigates mobile app data to predict socio-economic features. We
present a large-scale study using data that captures the traffic of thousands
of mobile applications by approximately 30 million users distributed over
550,000 km square and served by over 25,000 base stations. The dataset covers
the whole France territory and spans more than 2.5 months, starting from 16th
March 2019 to 6th June 2019. Using the app usage patterns, our best model can
estimate socio-economic indicators (attaining an R-squared score upto 0.66).
Furthermore, using models' explainability, we discover that mobile app usage
patterns have the potential to reveal socio-economic disparities in IRIS.
Insights of this study provide several avenues for future interventions,
including user temporal network analysis to understand evolving network
patterns and exploration of alternative data sources.

The study of US-China relations has always been a crucial topic in our
economic development [4][5][7], and the US presidential election plays an
integral role in shaping these relations. The presidential election is held
every four years, and it is crucial to assess the impact of the 2020 election
on China to prepare for the potential effects of the 2024 US presidential
election on the Chinese economy [8][16][20]. To achieve this, we have gathered
statistical data from nearly 70 years and analyzed data related to the US
economy. We have classified the collected data and utilized the analytic
hierarchy process [1][2][3] to evaluate the President's policy
implementation.This approach allowed us to obtain a comprehensive ranking of
the indicators [6][9][11][33]. We then quantified the index data and employed
the entropy weight method to calculate the weight of each index data. Finally,
we used the weighted total score calculation to evaluate the economic status of
the United States in a hierarchical manner after the election of Presidents
Trump and Biden [15][18]. We optimized the index system by incorporating
additional dimension indexes such as "foreign policy". We then crawled China's
specific development data from 1990-2020 and substituted it into the model for
analysis and evaluation. This enabled us to obtain detailed quantitative index
data of the degree of influence [10][12][14]. To address China's shortcomings
in science and technology innovation, we recommend strengthening economic
cooperation with developed countries, diversifying market development, and
actively expanding the domestic market through feasible solutions
[13][16][23][36].

Low carbon synfuel can displace transport fossil fuels such as diesel and jet
fuel and help achieve the decarbonization of the transportation sector at a
global scale, but large-scale cost-effective production facilities are needed.
Meanwhile, nuclear power plants are closing due to economic difficulties:
electricity prices are too low and variable to cover their operational costs.
Using existing nuclear power plants to produce synfuels might prevent loss of
these low-carbon assets while producing synfuels at scale, but no
technoeconomic analysis of this Integrated Energy System exist. We quantify the
technoeconomic potential of coupling a synthetic fuel production process with
five example nuclear power plants across the U.S. to explore the influence of
different electricity markets, access to carbon dioxide sources, and fuel
markets. Coupling synfuel production increases nuclear plant profitability by
up to 792 million USD(2020) in addition to a 10 percent rate of return on
investment over a 20 year period. Our analysis identifies drivers for the
economic profitability of the synfuel IES. The hydrogen production tax credit
from the 2022 Inflation Reduction Act is essential to its overall profitability
representing on average three quarters of its revenues. The carbon feedstock
transportation is the highest cost - more than a third on average - closely
followed by the synfuel production process capital costs. Those results show
the key role of incentive policies for the decarbonization of the
transportation sector and the economic importance of the geographic location of
Integrated Energy Systems.

Taxation and government spending are crucial tools for governments to promote
economic growth and maintain social equity. However, the difficulty in
accurately predicting the dynamic strategies of diverse self-interested
households presents a challenge for governments to implement effective tax
policies. Given its proficiency in modeling other agents in partially
observable environments and adaptively learning to find optimal policies,
Multi-Agent Reinforcement Learning (MARL) is highly suitable for solving
dynamic games between the government and numerous households. Although MARL
shows more potential than traditional methods such as the genetic algorithm and
dynamic programming, there is a lack of large-scale multi-agent reinforcement
learning economic simulators. Therefore, we propose a MARL environment, named
\textbf{TaxAI}, for dynamic games involving $N$ households, government, firms,
and financial intermediaries based on the Bewley-Aiyagari economic model. Our
study benchmarks 2 traditional economic methods with 7 MARL methods on TaxAI,
demonstrating the effectiveness and superiority of MARL algorithms. Moreover,
TaxAI's scalability in simulating dynamic interactions between the government
and 10,000 households, coupled with real-data calibration, grants it a
substantial improvement in scale and reality over existing simulators.
Therefore, TaxAI is the most realistic economic simulator for optimal tax
policy, which aims to generate feasible recommendations for governments and
individuals.

Ecosystems enjoy increasing attention due to their flexibility and innovative
power. It is well known, however, that this type of network-based economic
governance structures occupies a potentially unstable position between the two
stable (governance) endpoints, namely the firm (i.e., hierarchical governance)
and the (open) market (i.e., coordination through the monetary system).
  This paper develops a formal (mathematical) theory of the economic value of
(generic) ecosystem by extending transaction costs economics using certain
elements from service-dominant logic.
  Within a first-best setting of rational actors, we derive analytical
solutions for the hub-and-spoke and generic ecosystem configurations under some
uniformity assumptions of ecosystem participants. Additionally, we are able to
infer a generic condition for the welfare-maximizing and utility-maximizing
price of the hub-and-spoke configuration in the familiar form of Lerner indices
and elasticities.
  Relinquishing a first-best rational actors approach, we additionally derive
several general propositions on (i) necessary conditions for the economic
feasibility of ecosystem-based transactions, (ii) scaling requirements for
ecosystem stability, and (iii) a generic feasibility condition for arbitrary
provider-consumer ecosystems.
  Finally, we present an algebraic definition of business ecosystems and relate
it to existing informal definition attempts. Thereby we demonstrate that the
property of "being an ecosystem" of a network of transacting actors cannot be
decided on structural grounds alone.

For most developing countries, increasing the equalization of basic public
services is widely recognized as an effective channel to improve people's sense
of contentment. However, for many emerging economies like China, the
equalization level of basic public services may often be neglected in the
trade-off between the speed and quality of development. Taking the Yangtze
River Delta region of China as an example, this paper first adopts the coupling
coordination degree model to explore current status of basic public services in
this region, and then uses Moran's I index to study the overall equalization
level of development there. Moreover, this paper uses the Theil index to
analyze the main reasons for the spatial differences in the level of public
services, followed by the AF method to accurately identify the exact weaknesses
of the 40 counties of 10 cities with the weakest level of basic public service
development. Based on this, this paper provides targeted optimization
initiatives and continues to explore the factors affecting the growth of the
level of public service equalization through the convergence model, verifying
the convergence trend of the degree of public service equalization, and
ultimately providing practical policy recommendations for promoting the
equalization of basic public services.

During the COVID-19 pandemic, governments faced the challenge of managing
population behavior to prevent their healthcare systems from collapsing. Sweden
adopted a strategy centered on voluntary sanitary recommendations while Belgium
resorted to mandatory measures. Their consequences on pandemic progression and
associated economic impacts remain insufficiently understood. This study
leverages the divergent policies of Belgium and Sweden during the COVID-19
pandemic to relax the unrealistic -- but persistently used -- assumption that
social contacts are not influenced by an epidemic's dynamics. We develop an
epidemiological-economic co-simulation model where pandemic-induced behavioral
changes are a superposition of voluntary actions driven by fear, prosocial
behavior or social pressure, and compulsory compliance with government
directives. Our findings emphasize the importance of early responses, which
reduce the stringency of measures necessary to safeguard healthcare systems and
minimize ensuing economic damage. Voluntary behavioral changes lead to a
pattern of recurring epidemics, which should be regarded as the natural
long-term course of pandemics. Governments should carefully consider prolonging
lockdown longer than necessary because this leads to higher economic damage and
a potentially higher second surge when measures are released. Our model can aid
policymakers in the selection of an appropriate long-term strategy that
minimizes economic damage.

Economic withholding in electricity markets refers to generators bidding
higher than their true marginal fuel cost, and is a typical approach to
exercising market power. However, existing market designs require storage to
design bids strategically based on their own future price predictions,
motivating storage to conduct economic withholding without assuming market
power. As energy storage takes up more significant roles in wholesale
electricity markets, understanding its motivations for economic withholding and
the consequent effects on social welfare becomes increasingly vital. This paper
derives a theoretical framework to study the economic capacity withholding
behavior of storage participating in competitive electricity markets and
validate our results in simulations based on the ISO New England system. We
demonstrate that storage bids can reach unbounded high levels under conditions
where future price predictions show bounded expectations but unbounded
deviations. Conversely, in scenarios with peak price limitations, we show the
upper bounds of storage bids are grounded in bounded price expectations. Most
importantly, we show that storage capacity withholding can potentially lower
the overall system cost when price models account for system uncertainties. Our
paper reveals energy storage is not a market manipulator but an honest player
contributing to the social welfare. It helps electricity market researchers and
operators better understand the economic withholding behavior of storage and
reform market policies to maximize storage contributing to a cost-efficient
decolonization.

Most health economic analyses are undertaken with the aid of computers.
However, the research ethics of implementing health economic models as software
(or computational health economic models (CHEMs)) are poorly understood. We
propose that developers and funders of CHEMs should adhere to research ethics
principles and pursue the goals of: (i) socially acceptable user requirements
and design specifications; (ii) fit for purpose implementations; and (iii)
socially beneficial post-release use. We further propose that a transparent
(T), reusable (R) and updatable (U) CHEM is suggestive of a project team that
has largely met these goals. We propose six criteria for assessing TRU CHEMs:
(T1) software files are publicly available; (T2) developer contributions and
judgments on appropriate use are easily identified; (R1) programming practices
facilitate independent reuse of model components; (R2) licenses permit reuse
and derivative works; (U1) maintenance infrastructure is in place; and (U2)
releases are systematically retested and deprecated. Few existing CHEMs would
meet all TRU criteria. Addressing these limitations will require the
development of new and updated good practice guidelines and investments by
governments and other research funders in enabling infrastructure and human
capital.

Agent-based models (ABMs) are simulation models used in economics to overcome
some of the limitations of traditional frameworks based on general equilibrium
assumptions. However, agents within an ABM follow predetermined, not fully
rational, behavioural rules which can be cumbersome to design and difficult to
justify. Here we leverage multi-agent reinforcement learning (RL) to expand the
capabilities of ABMs with the introduction of fully rational agents that learn
their policy by interacting with the environment and maximising a reward
function. Specifically, we propose a 'Rational macro ABM' (R-MABM) framework by
extending a paradigmatic macro ABM from the economic literature. We show that
gradually substituting ABM firms in the model with RL agents, trained to
maximise profits, allows for a thorough study of the impact of rationality on
the economy. We find that RL agents spontaneously learn three distinct
strategies for maximising profits, with the optimal strategy depending on the
level of market competition and rationality. We also find that RL agents with
independent policies, and without the ability to communicate with each other,
spontaneously learn to segregate into different strategic groups, thus
increasing market power and overall profits. Finally, we find that a higher
degree of rationality in the economy always improves the macroeconomic
environment as measured by total output, depending on the specific rational
policy, this can come at the cost of higher instability. Our R-MABM framework
is general, it allows for stable multi-agent learning, and represents a
principled and robust direction to extend existing economic simulators.

We make an attempt to map a simple economically motivated model for the price
evolution [J. Phys. A: Gen. Math 33, 3637 (2000)] to the phenomenological
renormalization group scaling of stock markets. This mapping gives insight into
the critical exponents and the renormalization group predictions for the
log-periodic oscillations preceding some stock market crashes from the
perspective of non-linear changes in `the level of stocks'.

The opinion dynamics of economic agents is modeled with the link structure
influenced by the resulting opinions: Links between people of nearly the same
opinion are more stable than those between people of vastly different opinions.
A simple scaling law describes the number of surviving final opinion as a
function of the numbers of agents and of possible opinions.

A number $n$ is said to be economical if the prime power factorisation of $n$
can be written with no more digits than $n$ itself. We show that under a
plausible hypothesis, related to the twin prime conjecture, there are
arbitrarily long sequences of consecutive economial numbers, and exhibit such a
sequence of length 9.

We develop a model of the behaviour of a dynamically optimizing economic
agent who makes consumption-saving and spatial relocation decisions. We
formulate an existence result for the model, derive the necessary conditions
for optimality and study the behaviour of the economic agent, focusing on the
case of a wage distribution with a single maximum.

Using the fractional integration and differentiation on R we build the
fractional jet fibre bundle on a differentiable manifold and we emphasize some
important geometrical objects. Euler-Lagrange fractional equations are
described. Some significant examples from mechanics and economics are
presented.

Using Caputo fractional derivative of order $\alpha $ we build the fractional
jet bundle of order $\alpha $ and its main geometrical structures. Defined on
that bundle, some fractional dynamical systems with applications to economics
are studied.

I argue that the current financial crisis highlights the crucial need of a
change of mindset in economics and financial engineering, that should move away
from dogmatic axioms and focus more on data, orders of magnitudes, and
plausible, albeit non rigorous, arguments.

The sustainability conditions for the market participants with a different
ownership model were also determined. It was revealed, that the nonlinear form
of the equations describing the market behavior with the prevailing private
capital, predetermines the development of such a market according to the
subharmonic cascade scenario. The latter is presumably the reason of the
periodically arising economic crises.

In this paper we investigate a stochastic model for an economic game. To
describe this model we have used a Wiener process, as the noise has a
stabilization effect. The dynamics are studied in terms of stochastic stability
in the stationary state, by constructing the Lyapunov exponent, depending on
the parameters that describe the model. The numerical simulation that we did
justifies the theoretical results.

Exponential distribution is ubiquitous in the framework of multi-agent
systems. An alternative approach with an economic motivation to derive the
exponential distribution in the framework of iterations in the space of
distributions is disclosed.

This work's purpose is to understand the dynamics of some social systems
whose properties can be captured by certain iterated function systems. To
achieve this intension, we start from the theory of iterated function systems,
and then we study two specific economic models on random utility function and
optimal stochastic growth.

Despite all our great advances in science, technology and financial
innovations, many societies today are struggling with a financial, economic and
public spending crisis, over-regulation, and mass unemployment, as well as lack
of sustainability and innovation. Can we still rely on conventional economic
thinking or do we need a new approach?
  I argue that, as the complexity of socio-economic systems increases,
networked decision-making and bottom-up self-regulation will be more and more
important features. It will be explained why, besides the "homo economicus"
with strictly self-regarding preferences, natural selection has also created a
"homo socialis" with other-regarding preferences. While the "homo economicus"
optimizes the own prospects in separation, the decisions of the "homo socialis"
are self-determined, but interconnected, a fact that may be characterized by
the term "networked minds". Notably, the "homo socialis" manages to earn higher
payoffs than the "homo economicus".
  I show that the "homo economicus" and the "homo socialis" imply a different
kind of dynamics and distinct aggregate outcomes. Therefore, next to the
traditional economics for the "homo economicus" ("economics 1.0"), a
complementary theory must be developed for the "homo socialis". This economic
theory might be called "economics 2.0" or "socionomics". The names are
justified, because the Web 2.0 is currently promoting a transition to a new
market organization, which benefits from social media platforms and could be
characterized as "participatory market society". To thrive, the "homo socialis"
requires suitable institutional settings such a particular kinds of reputation
systems, which will be sketched in this paper. I also propose a new kind of
money, so-called "qualified money", which may overcome some of the problems of
our current financial system.

In this paper the introduction of notion of reference vector paves the way
for a combination of classical and social approaches in the framework of
referential preferences given by matrix groups. It is shown that individual
demand issue from rational decision does not depend on that reference.

A 3-fold terminal quotient singularity X=C^3/G admits the economic resolution
Y-> X, which is "close to being crepant". This paper proves that the economic
resolution Y is isomorphic to a distinguished component of a moduli space of
certain G-equivariant objects using the the King stability condition $\theta$
introduced by K\k{e}dzierski.

My recent book Antieigenvalue Analysis, World-Scientific, 2012, presented the
theory of antieigenvalues from its inception in 1966 up to 2010, and its
applications within those forty-five years to Numerical Analysis, Wavelets,
Statistics, Quantum Mechanics, Finance, and Optimization. Here I am able to
offer three further areas of application: Continuum Mechanics, Economics, and
Number Theory.

Mathematical methods of analysis of data and of predicting growth are
discussed. The starting point is the analysis of the growth rates, which can be
expressed as a function of time or as a function of the size of the growing
entity. Application of these methods is illustrated using the world economic
growth but they can be applied to any other type of growth.

There are many viable combinations of texture zeros in lepton mass matrices.
We propose an economical and stable mass texture. Analytical and numerical
results on mixing parameters and the effective mass of neutrinos are obtained.
These results satisfy new constraints from neutrinos oscillation experiments
and cosmological observations. Our proposition reveals that in the complex
forest of neutrinos mixing models, a simple and robust one is still possible.

By analysing the restrictions that ensure the existence of capital market
equilibrium, we show that the coefficient of relative risk aversion and the
subjective discount factor cannot be high simultaneously as they are supposed
to be to make the standard asset pricing consistent with financial stylised
facts.

We define Walrasian economic equilibrium in convex regions. We introduce the
definition of strict-proper quasimonotone functions, and derive some necessary
and sufficient conditions for the existence and uniqueness of Walrasian
equilibrium vectors in convex regions.

Pontrygin-type maximum principle is extended for the present value
Hamiltonian systems and current value Hamiltonian systems of nonlinear
difference equations for uniform time step $h$. A new method termed as a
discrete time current value Hamiltonian method is established for the
construction of first integrals for current value Hamiltonian systems of
ordinary difference equations arising in Economic growth theory.

We revisit the results of Harvie (2000) and show how correcting for a
reporting mistake in some of the estimated parameter values leads to
significantly different conclusions, including realistic parameter values for
the Philips curve and estimated equilibrium employment rates exhibiting on
average one tenth of the relative error of those obtained in Harvie (2000).

In this article, a game-theoretic model is constructed that is related to the
problem of optimal assignments. Examples are considered. A compromise point is
found, the Nash equilibriums and the decision of the Nash arbitration scheme
are constructed.

Social and political polarization is a significant source of conflict and
poor governance in many societies. Thus, understanding its causes has become a
priority of scholars across many disciplines. Here we demonstrate that shifts
in socialization strategies analogous to political polarization and identity
politics can arise as a locally-beneficial response to both rising wealth
inequality and economic decline. Adopting a perspective of cultural evolution,
we develop a framework to study the emergence of polarization under shifting
economic environments. In many contexts, interacting with diverse out-groups
confers benefits from innovation and exploration greater than those that arise
from interacting exclusively with a homogeneous in-group. However, when the
economic environment favors risk-aversion, a strategy of seeking low-risk
interactions can be important to maintaining individual solvency. To capture
this dynamic, we assume that in-group interactions have a lower expected
outcome, but a more certain one. Thus in-group interactions are less risky than
out-group interactions. Our model shows that under conditions of economic
decline or increasing wealth inequality, some members of the population benefit
from adopting a risk-averse, in-group favoring strategy. Moreover, we show that
such in-group polarization can spread rapidly to the whole population and
persist even when the conditions that produced it have reversed. Finally we
offer empirical support for the role of income inequality as a driver of
affective polarization in the United States, mirroring findings on a panel of
developed democracies. Our work provides a framework for studying how disparate
forces interplay, via cultural evolution, to shape patterns of identity, and
unifies what are often seen as conflicting explanations for political
polarization: identity threat versus economic anxiety.

We analyze the classical model of compound interest with a constant
per-period payment and interest rate. We examine the outstanding balance
function as well as the periodic payment function and show that the outstanding
balance function is not generally concave in the interest rate, but instead may
be initially convex on its domain and then concave.

The current article unveils and analyzes some important factors, influencing
diversity in strategic decision-making approaches in local companies.
Researcher's attention is oriented to survey important characteristics of the
strategic moves, undertaken by leading companies in Bulgaria.

Although the integration of two-sided matching markets using stable
mechanisms generates expected gains from integration, I show that there are
worst-case scenarios in which these are negative. The losses from integration
can be large enough that the average rank of an agent's spouse decreases by
37.5% of the length of their preference list in any stable matching mechanism.

We point out a simple equities trading strategy that allows a sufficiently
large, market-neutral, quantitative hedge fund to achieve outsized returns
while simultaneously contributing significantly to increasing global wealth
inequality. Overnight and intraday return distributions in major equity indices
in the United States, Canada, France, Germany, and Japan suggest a few such
firms have been implementing this strategy successfully for more than
twenty-five years.

This note considers cartel stability when the cartelized products are
vertically differentiated. If market shares are maintained at pre-collusive
levels, then the firm with the lowest competitive price-cost margin has the
strongest incentive to deviate from the collusive agreement. The lowest-quality
supplier has the tightest incentive constraint when the difference in unit
production costs is sufficiently small.

Every year, 10 million people die from lack of access to treatment for
curable diseases, specially in developing countries. Meanwhile, legal but
unsafe drugs cause 130 thousand deaths per year. How can this be happening in
21st Century? What role does the pharmaceutical industry play in this tragedy?
In this research, WHO reports are analyzed and primary information gathered so
as to answer this questions.

This study suggests a new decomposition of the effect of Foreign Direct
Investment (FDI) on long-term growth in developing countries. It reveals that
FDI not only have a positive direct effect on growth, but also increase the
latter by reducing the recessionary effect resulting from a banking crisis.
Even more, they reduce its occurrence. JEL: F65, F36, G01, G15

Given a purely atomic probability measure with support on n points, P, any
mean-preserving contraction (mpc) of P, Q, with support on m > n points is a
mixture of mpcs of P, each with support on most n points. We illustrate an
application of this result in economics.

The presence of \b{eta}-convergence in European regions is an important issue
to be analyzed. In this paper, we adopt a quantile regression approach in
analyzing economic convergence. While previous work has performed quantile
regression at the national level, we focus on 187 European NUTS2 regions for
the period 1981-2009 and use spatial quantile regression to account for spatial
dependence.

We propose a new model for regulation to achieve AI safety: global regulatory
markets. We first sketch the model in general terms and provide an overview of
the costs and benefits of this approach. We then demonstrate how the model
might work in practice: responding to the risk of adversarial attacks on AI
models employed in commercial drones.

The implementation of Goods and Services Tax(GST) is often attributed as the
main cause of the rising prices of goods and services. The main objective of
this study is to estimate the extent of GST implementation impact on the costs
of production, which in turn have implication on households living costs.

While the coronavirus spreads, governments are attempting to reduce contagion
rates at the expense of negative economic effects. Market expectations
plummeted, foreshadowing the risk of a global economic crisis and mass
unemployment. Governments provide huge financial aid programmes to mitigate the
economic shocks. To achieve higher effectiveness with such policy measures, it
is key to identify the industries that are most in need of support. In this
study, we introduce a data-mining approach to measure industry-specific risks
related to COVID-19. We examine company risk reports filed to the U.S.
Securities and Exchange Commission (SEC). This alternative data set can
complement more traditional economic indicators in times of the fast-evolving
crisis as it allows for a real-time analysis of risk assessments. Preliminary
findings suggest that the companies' awareness towards corona-related business
risks is ahead of the overall stock market developments. Our approach allows to
distinguish the industries by their risk awareness towards COVID-19. Based on
natural language processing, we identify corona-related risk topics and their
perceived relevance for different industries. The preliminary findings are
summarised as an up-to-date online index. The CoRisk-Index tracks the
industry-specific risk assessments related to the crisis, as it spreads through
the economy. The tracking tool is updated weekly. It could provide relevant
empirical data to inform models on the economic effects of the crisis. Such
complementary empirical information could ultimately help policymakers to
effectively target financial support in order to mitigate the economic shocks
of the crisis.

We extend the model of Parenti (2018) on large and small firms by introducing
cost heterogeneity among small firms. We propose a novel necessary and
sufficient condition for the existence of such a mixed market structure.
Furthermore, in contrast to Parenti (2018), we show that in the presence of
cost heterogeneity among small firms, trade liberalization may raise or reduce
the mass of small firms in operation.

The dependence of world GDP on current energy consumption and total energy
produced over the previous period and materialized in the form of production
infrastructure is studied. The dependence describes empirical data with high
accuracy over the entire observation interval 1965-2018.

A negative interest rate policy is often accompanied by tiered remuneration,
which allows for exemption from negative rates. This study proposes a basic
model of interest rates formed in the interbank market with a tiering system.
The results predicted by the model largely mirror actual market developments in
late 2019, when the European Central Bank introduced, and the Switzerland
National Bank modified, the tiering system.

This paper examines necessary and sufficient conditions for the uniqueness of
dynamic Groves mechanisms when the domain of valuations is restricted. Our
approach is to appropriately define the total valuation function, which is the
expected discounted sum of each period's valuation function from the allocation
and thus a dynamic counterpart of the static valuation function, and then to
port the results for static Groves mechanisms to the dynamic setting.

Black and Cox (1976) claim that the value of junior debt is increasing in
asset risk when the firm's value is low. We show, using closed-form solution,
that the junior debt's value is hump-shaped. This has interesting implications
for the market-discipline role of banks' junior debt.

Failure to receive post-natal care within first week of delivery causes a 3%
increase in the possibility of Acute Respiratory Infection in children under
five. Mothers with unpaid maternity leave put their children at a risk of 3.9%
increase in the possibility of ARI compared to those with paid maternity leave.

After the first lockdown in response to the COVID-19 outbreak, many countries
faced difficulties in balancing infection control with economics. Due to
limited prior knowledge, economists began researching this issue using
cost-benefit analysis and found that infection control processes significantly
affect economic efficiency. A UK study used economic parameters to numerically
demonstrate an optimal balance in the process, including keeping the infected
population stationary. However, universally applicable knowledge, which is
indispensable for the guiding principles of infection control, has not yet been
clearly developed because of the methodological limitations of simulation
studies. Here, we propose a simple model and theoretically prove the universal
result of economic irreversibility by applying the idea of thermodynamics to
pandemic control. This means that delaying infection control measures is more
expensive than implementing infection control measures early while keeping
infected populations stationary. This implies that once the infected population
increases, society cannot return to its previous state without extra
expenditures. This universal result is analytically obtained by focusing on the
infection-spreading phase of pandemics, and is applicable not just to COVID-19,
regardless of "herd immunity." It also confirms the numerical observation of
stationary infected populations in its optimally efficient process. Our
findings suggest that economic irreversibility is a guiding principle for
balancing infection control with economic effects.

We present a model of political competition in which an incumbent politician,
may implement a costly policy to prevent a possible threat to, for example,
national security or a natural disaster.

We show that under plausible levels of background risk, no theory of choice
under risk -- such as expected utility theory, prospect theory, or rank
dependent utility -- can simultaneously satisfy the following three economic
postulates: (i) Decision makers are risk-averse over small gambles, (ii) they
respect stochastic dominance, and (iii) they account for background risk.

This paper examines how the group membership fee influences the formation of
groups and the cooperation rate within the socialized groups. We found that
monetary transactions do not ruin the establishment of social ties and the
formation of group relations.

EIP-1559 is a proposal to make several tightly coupled additions to
Ethereum's transaction fee mechanism, including variable-size blocks and a
burned base fee that rises and falls with demand. This report assesses the
game-theoretic strengths and weaknesses of the proposal and explores some
alternative designs.

In seeking to explain aspects of real-world economies that defy easy
understanding when analysed via conventional means, Nobel Laureate Robert
Shiller has since 2017 introduced and developed the idea of Narrative
Economics, where observable economic factors such as the dynamics of prices in
asset markets are explained largely as a consequence of the narratives (i.e.,
the stories) heard, told, and believed by participants in those markets.
Shiller argues that otherwise irrational and difficult-to-explain behaviors,
such as investors participating in highly volatile cryptocurrency markets, are
best explained and understood in narrative terms: people invest because they
believe, because they have a heartfelt opinions, about the future prospects of
the asset, and they tell to themselves and others stories (narratives) about
those beliefs and opinions. In this paper we describe what is, to the best of
our knowledge, the first ever agent-based modelling platform that allows for
the study of issues in narrative economics. We have created this by integrating
and synthesizing research in two previously separate fields: opinion dynamics
(OD), and agent-based computational economics (ACE) in the form of
minimally-intelligent trader-agents operating in accurately modelled financial
markets. We show here for the first time how long-established models in OD and
in ACE can be brought together to enable the experimental study of issues in
narrative economics, and we present initial results from our system. The
program-code for our simulation platform has been released as freely-available
open-source software on GitHub, to enable other researchers to replicate and
extend our work

A planner aims to target individuals who exceed a threshold in a
characteristic, such as wealth or ability. The individuals can rank their
friends according to the characteristic. We study a strategy-proof mechanism
for the planner to use the rankings for targeting. We discuss how the mechanism
works in practice, when the rankings may contain errors.

In Japan, teacher and student is randomly matched in the first year of
elementary school. Under the quasi-natural experimental setting, we examine how
learning in female teacher homeroom class in the elementary school influence
pupils' smoking behavior after they become adult. We found that pupils are
unlikely to smoke later in life if they belonged to female teacher homeroom
class in pupil's first year of school.

The article examines both the legal responsibility itself and its types, and
in various aspects. The authors apply legal analysis, as well as the principles
of consistency and integrity. The contradictions of administrative
responsibility, as well as legal gaps in its interpretation, are highlighted.

We consider a global market constituted by several submarkets, each with its
own assets and num\'eraire. We provide theoretical foundations for the
existence of equivalent martingale measures and results on superreplication
prices which allows to take into account difference of features between
submarkets.

The authors of the article analyze the policy of the Russian government in
the field of family support, paying attention to legal programs at the federal
and regional levels. The maternity capital program is considered separately, as
well as measures aimed at supporting large families.

In this article we explain why the November 2021 election for the Ward 2 city
council seat in Minneapolis, MN, may be the mathematically most interesting
ranked choice election in US history.

We propose a parametric specification of the probability of tax penalisation
faced by a taxpayer, based on the amount of deduction chosen by her to reduce
total taxation. Comparative analyses lead to a closed-form solution for the
optimum tax deduction, and provide the maximising conditions with respect to
the probability parameters.

This document details a dataset that contains all unconsolidated annual
financial statements of the universe of Norwegian private and public limited
liability companies. It also includes all financial statements of other company
types reported to the Norwegian authorities.

Agrivoltaics (AV) is a dual land-use approach to collocate solar energy
generation with agriculture for preserving the terrestrial ecosystem and
enabling food-energy-water synergies. Here, we present a systematic approach to
model the economic performance of AV relative to standalone ground-mounted PV
(GMPV) and explore how the module design configuration can affect the dual
food-energy economic performance. A remarkably simple criterion for economic
feasibility is quantified that relates the land preservation cost to dual
food-energy profit. We explore case studies including both high and low value
crops under fixed tilt bifacial modules oriented either along the conventional
North/South (N/S) facings or vertical East/West (E/W) facings. For each module
configuration, the array density is varied to explore an economically feasible
design space relative to GMPV for a range of module to land cost ratio (M_L) -
a location-specific indicator relating the module technology (hardware and
installation) costs to the soft (land acquisition, tax, overheads, etc.) costs.
To offset a typically higher AV module cost needed to preserve the cropland,
both E/W and N/S orientated modules favor high value crops, reduced (<60%)
module density, and higher M_L (>25). In contrast, higher module density and an
increased feed-in-tariff (FIT) relative to GMPV are desirable at lower M_L. The
economic trends vary sharply for M_L< 10 but tend to saturate for M_L> 20. For
low value crops, ~15% additional FIT can enable economic equivalence to GMPV at
standard module density. The proposed modeling framework can provide a valuable
tool for AV stakeholders to assess, predict, and optimize the techno-economic
design for AV

We examine the allocation of a limited pool of matching funds to public good
projects using Quadratic Funding. In particular, we consider a variation of the
Capital Constrained Quadratic Funding (CQF) mechanism proposed by Buterin,
Hitzig and Weyl (2019) where only funds in the matching pool are distributed
among projects. We show that this mechanism achieves a socially optimal
allocation of limited funds.

This paper discusses the relevance of information overload for explaining
environmental degradation. Our argument goes that information overload and
detachment from nature, caused by energy abundance, have made individuals
unaware of the unsustainable effects of their choices and lifestyles.

Galichon, Samuelson and Vernet (2022) introduced a class of problems,
equilibrium flow problems, that nests several classical economic models such as
bipartite matching models, minimum-cost flow problems and hedonic pricing
models. We establish conditions for the existence of equilibrium prices in the
equilibrium flow problem, in the process generalizing Hall's theorem.

The Bayesian posterior probability of the true state is stochastically
dominated by that same posterior under the probability law of the true state.
This generalizes to notions of "optimism" about posterior probabilities.

This paper presents the results of further exploration of Crossref data
related to Ukrainian Economics research (the first part can be found in
[Mryglod, O., Nazarovets, S. & Kozmenko, S. (2021) Scientometrics, 126, 8187]).
Our purpose is to supplement the quantitative portrait of Ukrainian Economics
discipline with the results of gender and author ordering analysis at the level
of individual authors, special methods of working with bibliographic data with
a predominant share of non-English authors are used. The properties of gender
mixing, the likelihood of male and female authors occupying the first position
in the authorship list, as well as the arrangements of names are studied. A
data set containing bibliographic records related to Ukrainian journal
publications in the field of Economics is constructed using Crossref metadata.
The described stages for working with such specific data help to work at the
level of authors and analyse, in particular, gender issues. Despite the larger
number of female authors, gender equality is more likely to be reported at the
individual level for the discipline of Ukrainian Economics. The tendencies
towards collaborative or solo-publications and gender mixing patterns are found
to be dependent on the journal: the differences for publications indexed in
Scopus and/or Web of Science databases are found. It has also been found that
Ukrainian Economics research is characterized by rather a non-alphabetical
order of authors. To our knowledge, this is the first large-scale quantitative
study of Ukrainian Economic discipline. The results obtained are valuable not
only at the national level, but also contribute to general knowledge about
Economic research, gender issues and authors' names ordering. Here, for the
first time, attention is drawn to the explicit use of the features of the
Slavic authors' names.

In the allocation of indivisible goods, the maximum Nash welfare (MNW) rule,
which chooses an allocation maximizing the product of the agents' utilities,
has received substantial attention for its fairness. We characterize MNW as the
only additive welfarist rule that satisfies envy-freeness up to one good. Our
characterization holds even in the simplest setting of two agents.

In the allocation of indivisible goods, the maximum Nash welfare rule has
recently been characterized as the only rule within the class of additive
welfarist rules that satisfies envy-freeness up to one good. We extend this
characterization to the class of all welfarist rules.

We illustrate the point with an empirical analysis of assortative mating in
the US, namely, that the outcome of comparing two distant groups can be
sensitive to whether comparing the groups directly, or indirectly via a series
of counterfactual decompositions involving the groups' comparisons to some
intermediate groups. We argue that the latter approach is typically more fit
for its purpose.

We extract cyclic information in turnover and find it can explain the
momentum echo. The reversal in recent month momentum is the key factor that
cancels out the recent month momentum and excluding it makes the echo regress
to a damped shape. Both rational and behavioral theories can explain the
reversal. This study is the first explanation of the momentum echo in U.S.
stock markets.

This study examines the relationship between automation and income inequality
across different countries, taking into account the varying levels of
technological adoption and labor market institutions. The research employs a
panel data analysis using data from the World Bank, the International Labour
Organization, and other reputable sources. The findings suggest that while
automation leads to an increase in productivity, its effect on income
inequality depends on the country's labor market institutions and social
policies.

Adam Smith developed a version of moral philosophy where better decisions are
made by interrogating an impartial spectator within us. We discuss the
possibility of using an external non-human-based substitute tool that would
augment our internal mental processes and play the role of the impartial
spectator. Such tool would have more knowledge about the world, be more
impartial, and would provide a more encompassing perspective on moral
assessment.

This study examines the impact of Total Quality Management (TQM) practices on
organizational outcomes. Results show a significant relationship between TQM
practices such as top executive commitment, education and teaching, process
control, and continuous progress, and how they can be leveraged to enhance
performance outcomes.

Adjustments to public health policy are common. This paper investigates the
impact of COVID-19 policy ambiguity on specific groups' insurance consumption.
The results show that sensitive groups' willingness to pay (WTP) for insurance
is 12.2% above the benchmark. Groups that have experienced income disruptions
are more likely to suffer this. This paper offers fresh perspectives on the
effects of pandemic control shifts.

We obtain an elementary characterization of expected utility based on a
representation of choice in terms of psychological gambles, which requires no
assumption other than coherence between ex-ante and ex-post preferences. Weaker
version of coherence are associated with various attitudes towards complexity
and lead to a characterization of minimax or Choquet expected utility.

In a New Keynesian model where the trade-off between stabilising the
aggregate inflation rate and the output gap arises from sectoral asymmetries,
the gains from commitment are either zero or negligible. Thus, to the extent
that economic fluctuations are caused by sectoral shocks, policies designed to
overcome the stabilisation bias are aiming to correct an unimportant problem.

Constructing theorems can help to determine the shape of certain utility
curves that make up the new definitions in financial markets. The aim of this
study was to present proofs for these theorems. Although the terms of
risk-averse, risk-loving, and risk-neutral are equivalent to strict concavity,
strict convexity, and linearity, respectively, in standard theory, certain new
definitions satisfy strict concavity or strict convexity, or linearity.

An increasingly salient public policy challenge is how to manage the growing
number of satellites in orbit, including large constellations. Many policy
initiatives have been proposed that attempt to address the problem from
different angles, but there is a paucity of analytical tools to help
policymakers evaluate the efficacy of these different proposals and any
potential counterproductive outcomes. To help address this problem, this paper
summarizes work done to develop an experimental integrated assessment model --
Orbital Debris Propagators Unified with Economic Systems (OPUS) -- that
combines both astrodynamics of the orbital population and economic behavior of
space actors. For a given set of parameters, the model first utilizes a given
astrodynamic propagator to assess the state of objects in orbit. It then uses a
set of user-defined economic and policy parameters -- e.g. launch prices,
disposal regulations -- to model how actors will respond to the economic
incentives created by a given scenario. For the purposes of testing, the MIT
Orbital Capacity Tool (MOCAT) version 4S was used as the primary astrodynamics
propagator to simulate the true expected target collision probability ($p_c$)
for a given end-of-life (EOL) disposal plan. To demonstrate
propagator-agnosticism, a Gaussian mixture probability hypothesis density
(GMPHD) filter was also used to simulate $p_c$. We also explore economic policy
instruments to improve both sustainability of and economic welfare from orbit
use. In doing so, we demonstrate that this hybrid approach can serve as a
useful tool for evaluating policy proposals for managing orbital congestion. We
also discuss areas where this work can be made more robust and expanded to
include additional policy considerations.

Using a firm-level dataset from the Spanish Technological Innovation Panel
(2003-2016), this study explores the characteristics of environmentally
innovative firms and quantifies the effects of pursuing different types of
environmental innovation strategies (resource-saving, pollution-reducing, and
regulation-driven innovations) on sales, employment, and productivity dynamics.

The pursuit of excellence seems to be the True North of academia. What is
meant by excellence? Can excellence be measured? This article discusses the
concept of excellence in the context of research and competition.

Based on supermodularity ordering properties, we show that convex risk
measures of credit losses are nondecreasing w.r.t. credit-credit and, in a
wrong-way risk setup, credit-market, covariances of elliptically distributed
latent factors. These results support the use of such setups for computing
credit provisions and economic capital or for conducting stress test exercises
and risk management analysis.

A new version of the database for the meta-analysis of estimates of the
social cost of carbon is presented. New records were added, and new fields on
the impact of climate change and the shape of the welfare function. The
database was extended to co-author and citation networks.

This survey article is dedicated to the life of the famous American economist
H. Markowitz (1927--2023). We do revisit the main statements of the portfolio
selection theory in terms of mathematical completeness including all the
necessary auxiliary details.

The quest for understanding the complex phenomena of the world has led to the
development of various fields of science, each with its own methods, models,
and assumptions. However, sometimes these fields can intersect and inspire each
other, revealing new insights and perspectives that transcend the boundaries of
their domains. In this paper, we explore one such fascinating intersection: the
connection between economic theory and stochastic game dynamics, with
applications to both firms and soccer players.

The majority of estimates of the social cost of carbon use preference
parameters calibrated to data for North America and Europe. We here use
representative data for attitudes to time and risk across the world. The social
cost of carbon is substantially higher in the global north than in the south.
The difference is more pronounced if we count people rather than countries.

In economics comparative analysis plays the same role as experimental
research in physics. In this paper we closely examine several methodological
problems related to comparative analysis by investigating the specific example
of grain markets in China and France respectively. This enables us to answer a
question in economic history which has so far remained pending, namely whether
or not market integration progressed in the 18th century. In economics as in
physics, before being accepted any new result has to be checked and re-checked
by different researchers. This is what we call the replication and comparison
procedures. We show how these procedures should (and can) be implemented.

We studied the statistical distribution of student's performance, which is
measured through their marks, in university entrance examination (Vestibular)
of UNESP (Universidade Estadual Paulista) with respect to (i) period of study -
day vs. night period (ii) teaching conditions - private vs. public school (iii)
economical conditions - high vs. low family income. This examination reflect
quality of high schools education. We observed long ubiquitous power law tails
in Physical and Biological sciences in all cases. The mean value increases with
better study conditions followed by better teaching and economical conditions.
In humanities, the distribution is close to normal distribution with very small
tail. This indicate that these power law tail in science subjects are due to
the nature of the subject itself. Further better study, teaching and economical
conditions are more important for physical and biological sciences in
comparison to humanities. We explain these statistical distributions through
Gradually Truncated Power law distributions. We discuss the possible reason for
this peculiar behaviour and make suggestions to improve science education at
high school level.

A microscopic approach to macroeconomic features is intended. A model for
macroeconomic behavior under heterogeneous spatial economic conditions is
reviewed. A birth-death lattice gas model taking into account the influence of
an economic environment on the fitness and concentration evolution of economic
entities is numerically and analytically examined. The reaction-diffusion model
can be also mapped onto a high order logistic map. The role of the selection
pressure along various dynamics with entity diffusion on a square symmetry
lattice has been studied by Monte-Carlo simulation. The model leads to a sort
of phase transition for the fitness gap as a function of the selection pressure
and to cycles. The control parameter is a (scalar) ''business plan''. The
business plan(s) allows for spin-offs or merging and enterprise survival
evolution law(s), whence bifurcations, cycles and chaotic behavior.

Two classical problems in economics, the existence of a market equilibrium
and the existence of social choice functions, are formalized here by the
properties of a family of cones associated with the economy. It was recently
established that a necessary and sufficient condition for solving the former is
the nonempty intersection of the family of cones, and one such condition for
solving the latter is the acyclicity of the unions of its subfamilies. We show
an unexpected but clear connection between the two problems by establishing a
duality property of the homology groups of the nerve defined by the family of
cones. In particular, we prove that the intersection of the family of cones is
nonempty if and only if every subfamily has acyclic unions, thus identifying
the two conditions that solve the two economic problems. In addition to their
applications to economics, the results are shown to extend significantly
several classical theorems, providing unified and simple proofs: Helly's
theorem, Caratheodory's representation theorem, the
Knaster-Kuratowski-Marzukiewicz theorem, Brouwer's fixed point theorem, and
Leray's theorem on acyclic covers.

Kolmogorov nonlinear averaging is complemented by a natural axiom. For this
averaging, we prove a theorem on large deviations as well as establish the
relationship to the tunnel canonical operator.

We report quantitative relations between corruption level and economic
factors, such as country wealth and foreign investment per capita, which are
characterized by a power law spanning multiple scales of wealth and investments
per capita. These relations hold for diverse countries, and also remain stable
over different time periods. We also observe a negative correlation between
level of corruption and long-term economic growth. We find similar results for
two independent indices of corruption, suggesting that the relation between
corruption and wealth does not depend on the specific measure of corruption.
The functional relations we report have implications when assessing the
relative level of corruption for two countries with comparable wealth, and for
quantifying the impact of corruption on economic growth and foreign
investments.

We believe that economists have much to learn from educational research
practices and related pedagogical innovations in other disciplines, in
particular physics education. In this paper we identify three key features of
physics education research that distinguish it from economics education
research - (1) the intentional grounding of physics education research in
learning science principles, (2) a shared conceptual research framework focused
on how students learn physics concepts, and (3) a cumulative process of
knowledge-building in the discipline - and describe their influence on new
teaching pedagogies, instructional activities, and curricular design in physics
education. In addition, we highlight four specific examples of successful
pedagogical innovations drawn from physics education - context-rich problems,
concept tests, just-in-time teaching, and interactive lecture demonstrations -
and illustrate how these practices can be adapted for economic education.

In this essay, I attempt to provide supporting evidence as well as some
balance for the thesis on `Transforming socio-economics with a new
epistemology' presented by Hollingworth and Mueller (2008). First, I review a
personal highlight of my own scientific path that illustrates the power of
interdisciplinarity as well as unity of the mathematical description of natural
and social processes. I also argue against the claim that complex systems are
in general `not susceptible to mathematical analysis, but must be understood by
letting them evolve over time or with simulation analysis'. Moreover, I present
evidence of the limits of the claim that scientists working within Science II
do not make predictions about the future because it is too complex. I stress
the potentials for a third `Quantum Science' and its associated conceptual and
philosophical revolutions, and finally point out some limits of the `new'
theory of networks.

We propose a simple quantitative model of Schumpeterian economic dynamics.
New goods and services are endogenously produced through combinations of
existing goods. As soon as new goods enter the market they may compete against
already existing goods, in other words new products can have destructive
effects on existing goods. As a result of this competition mechanism existing
goods may be driven out from the market - often causing cascades of secondary
defects (Schumpeterian gales of destruction). The model leads to a generic
dynamics characterized by phases of relative economic stability followed by
phases of massive restructuring of markets - which could be interpreted as
Schumpeterian business `cycles'. Model timeseries of product diversity and
productivity reproduce several stylized facts of economics timeseries on long
timescales such as GDP or business failures, including non-Gaussian fat tailed
distributions, volatility clustering etc. The model is phrased in an open,
non-equilibrium setup which can be understood as a self organized critical
system. Its diversity dynamics can be understood by the time-varying topology
of the active production networks.

The quantitative aspirations of economists and financial analysts have for
many years been based on the belief that it should be possible to build models
of economic systems - and financial markets in particular - that are as
predictive as those in physics. While this perspective has led to a number of
important breakthroughs in economics, "physics envy" has also created a false
sense of mathematical precision in some cases. We speculate on the origins of
physics envy, and then describe an alternate perspective of economic behavior
based on a new taxonomy of uncertainty. We illustrate the relevance of this
taxonomy with two concrete examples: the classical harmonic oscillator with
some new twists that make physics look more like economics, and a quantitative
equity market-neutral strategy. We conclude by offering a new interpretation of
tail events, proposing an "uncertainty checklist" with which our taxonomy can
be implemented, and considering the role that quants played in the current
financial crisis.

In statistical physics, the conservation of particle number results in the
equalization of the chemical potential throughout a system at equilibrium. In
contrast, the homogeneity of utility in socio-economic models is usually
thought to rely on the competition between individuals, leading to Nash
equilibrium. We show that both views can be reconciled by introducing a notion
of chemical potential in a wide class of socio-economic models, and by relating
it in a direct way to the equilibrium value of the utility. This approach also
allows the dependence of utility across the system to be determined when agents
take decisions in a probabilistic way. Numerical simulations of a urban
economic model also suggest that our result is valid beyond the initially
considered class of solvable models.

Econophysics is a new research field, which makes an attempt to bring
economics in the fold of natural sciences or specifically attempts for a
"physics of economics". The term Econophysics was formally born in Kolkata in
1995. The entry on Econophysics in The New Palgrave Dictionary of Economics,
2nd Ed., Vol 2, Macmillan, NY (2008), pp 729-732, begins with "... the term
'econophysics' was neologized in 1995 at the second Statphys- Kolkata
conference in Kolkata (formerly Calcutta), India ...". The Econophysics
research therefore formally completes fifteen years of research by the end of
this year! The importance and proliferation of the interdisciplinary research
of Econophysics is highlighted in the special issue of Science & Culture, which
presents a collection of twenty nine papers (giving country wise perspectives,
reviews of the recent developments and original research communications),
written by more than forty renowned experts in physics, mathematics or
economics, from all over the world. We present here the list of contents and
the editorial. The manuscript files are available at
http://fiquant.mas.ecp.fr/chakraboa for preview. This special issue will be
published online at http://www.scienceandculture-isna.org/journal.htm, at the
end of October 2010.

This paper develops a structural credit risk model to characterize the
difference between the economic and recorded default times for a firm. Recorded
default occurs when default is recorded in the legal system. The economic
default time is the last time when the firm is able to pay off its debt prior
to the legal default time. It has been empirically documented that these two
times are distinct (see Guo, Jarrow, and Lin (2008)). In our model, the
probability distribution for the time span between economic and recorded
defaults follows a mixture of Arcsine Laws, which is consistent with the
results contained in Guo, Jarrow, and Lin. In addition, we show that the
classical structural model is a limiting case of our model as the time period
between debt repayment dates goes to zero. As a corollary, we show how the firm
value process's parameters can be estimated using the tail index and
correlation structure of the firm's return.

The green area of economy is the key of healthy living. It is necessary to
convene economic and ecologic framework to establish a market attentive to
drastic reduction of emissions damaging our climate and landscapes in rural
areas, to the protection of biological diversity of the planet, to stop
producing nuclear waste, etc. This paper tries to demonstrate human concern for
a waste recycling economy that will provide new jobs, will create economic and
social stability and will ensure a healthier and cleaner environment. Green
Economy and its support system (planetary ecosystem) won't be in conflict
anymore. Green Economy will be able to support economic progress for future.

It is demonstrated that the US economy has on the long-term in reality been
governed by the Keynesian approach to economics independent of the current
official economical policy. This is done by calculating the two-point
correlation function between the fluctuations of the DJIA and the US public
debt. We find that the origin of this condition is mainly related to the wars
that the USA has fought during the time period investigated. Wars mean a large
influx of public money into the economy, thus as a consequence creating a
significant economical upturn in the DJIA. A reason for this straight-cut
result of our analysis, is that very few wars have been fought on US-territory
and those that have, were in the 18th century, when the partial destruction of
cities, factories, railways and so on, was more limited and with less effect on
the over-all economy.

The conventional economic approaches explore very little about the dynamics
of the economic systems. Since such systems consist of a large number of agents
interacting nonlinearly they exhibit the properties of a complex system.
Therefore the tools of statistical physics and nonlinear dynamics has been
proved to be very useful the underlying dynamics of the system. In this paper
we introduce the concept of the multidisciplinary field of econophysics, a
neologism that denotes the activities of Physicists who are working on economic
problems to test a variety of new conceptual approaches deriving from the
physical science and review the recent developments in the discipline and
possible future trends.

Physics and economics are two disciplines that share the common challenge of
linking microscopic and macroscopic behaviors. However, while physics is based
on collective dynamics, economics is based on individual choices. This
conceptual difference is one of the main obstacles one has to overcome in order
to characterize analytically economic models. In this paper, we build both on
statistical mechanics and the game theory notion of Potential Function to
introduce a rigorous generalization of the physicist's free energy, which
includes individual dynamics. Our approach paves the way to analytical
treatments of a wide range of socio-economic models and might bring new
insights into them. As first examples, we derive solutions for a congestion
model and a residential segregation model.

This work aims to study the Portuguese regional agglomeration process, using
the linear form the New Economic Geography models that emphasize the importance
of spatial factors (distance, costs of transport and communication) in
explaining of the concentration of economic activity in certain locations. In a
theoretical context, it is intended to explain the complementarily of
clustering models, associated with the New Economic Geography, and polarization
associated with the Keynesian tradition, describing the mechanisms by which
these processes are based. As a summary conclusion, we can say which the
agglomeration process shows some signs of concentration in Lisboa e Vale do
Tejo (which is evidence of regional divergence in Portugal) and the
productivity factor significantly improves the results that explain the
regional clustering in Portugal (despite being ignored in the models of New
Economic Geography).

Many scholars have recently begun to dispute the assumed link between
individual wellbeing and economic conditions and the extent to which the latter
matters (Easterlin, 1995; Stevenson and Wolfers 2008; Tella and MacCulloch
2008). This dilemma is empirically demonstrated in the Latin America Public
Opinion Project (LAPOP, 2011), which surveyed North and Latin America in terms
of perceived life satisfaction. Higher measures found in the less developed
countries of Brazil, Costa Rica, and Panama than in North America pose an
intriguing quandary to traditional economic theory. In light of this
predicament this paper aims to construct a sensible measure of the national
happiness level for the United States on a year by year basis; and regress this
against indicators of the national economy to provide insight into this
puzzling enigma between national happiness and economic forces

Understanding how spatial configurations of economic activity emerge is
important when formulating spatial planning and economic policy. Not only
micro-simulation and agent-based model such as UrbanSim, ILUMAS and SIMFIRMS,
but also Simon's model of hierarchical concentration have widely applied, for
this purpose. These models, however, have limitations with respect to
simulating structural changes in spatial economic systems and the impact of
proximity. The present paper proposes a model of firm development that is based
on behavioural rules such as growth, closure, spin-off and relocation. An
important aspect of the model is that locational preferences of firms are based
on agglomeration advantages, accessibility of markets and congestion, allowing
for a proper description of concentration and deconcentration tendencies. By
comparing the outcomes of the proposed model with real world data, we will
calibrate the parameters and assess how well the model predicts existing
spatial configurations and decide. The model is implemented as an agent-based
simulation model describing firm development in the Netherlands in 21
industrial sectors from 1950 to 2004.

Economic growth is unpredictable unless demand is quantified. We solve this
problem by introducing the demand for unpaid spare time and a user quantity
named human capacity. It organizes and amplifies spare time required for
enjoying affluence like physical capital, the technical infrastructure for
production, organizes and amplifies working time for supply. The sum of annual
spare and working time is fixed by the universal flow of time. This yields the
first macroeconomic equilibrium condition. Both storable quantities form
stabilizing feedback loops. They are driven with the general and technical
knowledge embodied with parts of the supply by education and construction.
Linear amplification yields S-functions as only analytic solutions.
Destructible physical capital controls medium-term recoveries from disaster.
Indestructible human capacity controls the collective long-term industrial
evolution. It is immune even to world wars and runs from 1800 to date parallel
to the unisex life expectancy in the pioneering nations. This is the first
quantitative information on long-term demand. The theory is self-consistent. It
reproduces all peaceful data from 1800 to date without adjustable parameter. It
has full forecasting power since the decisive parameters are constants of the
human species. They predict an asymptotic maximum for the economic level per
capita. Long-term economic growth appears as a part of natural science.

One 'problem' with the 21st century world, particularly the economic and
business worlds, is the phenomenal and increasing number of interconnections
between economic agents (consumers, firms, banks, markets, national economies).
This implies that such agents are all interacting and consequently giving raise
to enormous degrees of non-linearity, a.k.a. complexity. Complexity often
brings with it unexpected phenomena, such as chaos and emerging behaviour, that
can become challenges for the survival of economic agents and systems.
Developing econophysics approaches are beginning to apply, to the 'economic
web', methods and models that have been used in physics and/or systems theory
to tackle non-linear domains. The paper gives an account of the research in
progress in this field and shows its implications for enteprise information
systems, anticipating the emergence of software that will allow to reflect the
complexity of the business world, as holistic risk management becomes a mandate
for financial institutions and business organizations.

We propose an axiomatic approach to constructing the dynamics of systems, in
which one the main elements is the consciousness of a subject. The main axiom
is the statements that the state of consciousness is completely determined by
the results of measurements performed on it. In case of economic systems we
propose to consider an offer of transaction as a fundamental measurement.
Transactions with delayed choice, discussed in this paper, represent a logical
generalization of incomplete transactions and allow for a more rigorous
approach to studying the properties of the algebra of economic measurements.
Considering the behavior of an object as a sequence of actions and choices
allows extending the obtained results to random systems, which include the
subject's consciousness as one of its elements. The specifics, on which the
description of fundamental measurements is based, allows easily transferring
the formalism of Schwinger's theory of selective measurements (and the
consequent quantum-mechanical formalism) to the economic and social systems.

Decision makers need economical information to drive their decisions. The
Company Actualis SARL is specialized in the production and distribution of a
press review about French regional economic actors. This economic review
represents for a client a prospecting tool on partners and competitors. To
reduce the overload of useless information, the company is moving towards a
customized review for each customer. Three issues appear to achieve this goal.
First, how to identify the elements in the text in order to extract objects
that match with the recommendation's criteria presented? Second, How to define
the structure of these objects, relationships and articles in order to provide
a source of knowledge usable by the extraction process to produce new knowledge
from articles? The latter issue is the feedback on customer experience to
identify the quality of distributed information in real-time and to improve the
relevance of the recommendations. This paper presents a new type of
recommendation based on the semantic description of both articles and user
profile.

Nanotechnology is the first major worldwide research initiative of the 21st
century and probably is the solution vector in the economic environment. Also,
innovation is widely recognized as a key factor in the economic development of
nations, and is essential for the competitiveness of the industrial firms as
well. Policy and management of innovation are necessary in order to develop
innovation and it involves processes. It is essential to develop new methods
for nanotechnology development for better understanding of nanotechnology based
innovation. Nanotechnologies reveal commercialization processes, from start ups
to large firms in collaboration with public sector research. In the current
paper, a study in the present status of innovation in nanotechnology and the
affection of global economic crisis in this section is made and also the
potential of increase the innovation via the presence of clusters in a small
country like Greece which is in the eye of tornado from the global crisis is
studied.

We have analyzed the Indices of Industrial Production (Seasonal Adjustment
Index) for a long period of 240 months (January 1988 to December 2007) to
develop a deeper understanding of the economic shocks. The angular frequencies
estimated using the Hilbert transformation, are almost identical for the 16
industrial sectors. Moreover, the partial phase locking was observed for the 16
sectors. These are the direct evidence of the synchronization in the Japanese
business cycle. We also showed that the information of the economic shock is
carried by the phase time-series. The common shock and individual shocks are
separated using phase time-series. The former dominates the economic shock in
all of 1992, 1998 and 2001. The obtained results suggest that the business
cycle may be described as a dynamics of the coupled limit-cycle oscillators
exposed to the common shocks and random individual shocks.

This article derives prognostic expressions for the evolution of globally
aggregated economic wealth, productivity, inflation, technological change,
innovation and growth. The approach is to treat civilization as an open,
non-equilibrium thermodynamic system that dissipates energy and diffuses matter
in order to sustain existing circulations and to further its material growth.
Appealing to a prior result that established a fixed relationship between a
very general representation of global economic wealth and rates of global
primary energy consumption, physically derived expressions for economic
quantities follow. The analysis suggests that wealth can be expressed in terms
of the length density of civilization's networks and the availability of energy
resources. Rates of return on wealth are accelerated by energy reserve
discovery, improvements to human and infrastructure longevity, and a more
common culture, or a lowering of the amount of energy required to diffuse raw
materials into civilization's bulk. According to a logistic equation, rates of
return are slowed by past growth, and if rates of return approach zero, such
"slowing down" makes civilization fragile with respect to externally imposed
network decay. If past technological change has been especially rapid, then
civilization is particularly vulnerable to newly unfavorable conditions that
might force a switch into a mode of accelerating collapse.

As the Software Defined Networking (SDN) paradigm gains momentum, every
network operator faces the obvious dilemma: when and how to migrate from
existing IP routers to SDN compliant equipments. A single step complete
overhaul of a fully functional network is impractical, while at the same time,
the immediate benefits of SDN are obvious. A viable solution is thus a gradual
migration over time, where questions of which routers should migrate first, and
whether the order of migration makes a difference, can be analyzed from techno
economic and traffic engineering perspective. In this paper, we address these
questions from the techno economic perspective, and establish the importance of
migration scheduling. We propose optimization techniques and greedy algorithms
to plan an effective migration schedule, based on various techno economic
aspects, such as technological gains in combinations with CapEx limitations. We
demonstrate the importance of an effective migration sequence through two
relevant network management metrics, namely, number of alternative paths
availed by a node on migration, and network capacity savings. Our results
suggest that the sequence of migration plays a vital role, especially in the
early stages of network migration to SDN.

The dynamics of economies and infectious disease are inexorably linked:
economic well-being influences health (sanitation, nutrition, treatment
capacity, etc.) and health influences economic well-being (labor productivity
lost to sickness and disease). Often societies are locked into "poverty traps"
of poor health and poor economy. Here, using a simplified coupled
disease-economic model with endogenous capital growth we demonstrate the
formation of poverty traps, as well as ways to escape them. We suggest two
possible mechanisms of escape both motivated by empirical data: one, through an
influx of capital (development aid), and another through changing the
percentage of GDP spent on healthcare. We find that a large influx of capital
is successful in escaping the poverty trap, but increasing health spending
alone is not. Our results demonstrate that escape from a poverty trap may be
possible, and carry important policy implications in the world-wide
distribution of aid and within-country healthcare spending.

Biondi et al. (2012) develop an analytical model to examine the emergent
dynamic properties of share market price formation over time, capable to
capture important stylized facts. These latter properties prove to be sensitive
to regulatory regimes for fundamental information provision, as well as to
market confidence conditions among actual and potential investors. Regimes
based upon mark-to-market (fair value) measurement of traded security, while
generating higher linear correlation between market prices and fundamental
signals, also involve higher market instability and volatility. These regimes
also incur more relevant episodes of market exuberance and vagary in some
regions of the market confidence space, where lower market liquidity further
occurs.

This paper investigates how economic shocks propagate and amplify through the
input-output network connecting industrial sectors in developed economies. We
study alternative models of diffusion on networks and we calibrate them using
input-output data on real-world inter-sectoral dependencies for several
European countries before the Great Depression. We show that the impact of
economic shocks strongly depends on the nature of the shock and country size.
Shocks that impact on final demand without changing production and the
technological relationships between sectors have on average a large but very
homogeneous impact on the economy. Conversely, when shocks change also the
magnitudes of input-output across-sector interdependencies (and possibly sector
production), the economy is subject to predominantly large but more
heterogeneous avalanche sizes. In this case, we also find that: (i) the more a
sector is globally central in the country network, the largest its impact; (ii)
the largest European countries, such as those constituting the core of the
European Union's economy, typically experience the largest avalanches,
signaling their intrinsic higher vulnerability to economic shocks.

Historically, the banking multiplier has been in a range of 4 to 100, with
25% to 1% reserve ratios at most layers of the banking system encompassing the
majority of its range in recent centuries. Here it is shown that multipliers
over 1 000 can occur from a new mechanism in banking. This new multiplier uses
a default insurance note to insure an outstanding loan in order to return the
value of the insured amount into capital. The economic impact of this invention
is calculably greater than the original invention of reserve banking. The
consequence of this lending invention is to render the existing money
multiplier equations of reserve banking obsolete where it occurs. The equations
describing this new multiplier do not converge. Each set of parameters for
reserve percentage, nesting depth, etc. creates a unique logarithmic curve
rather than approaching a limit. Thus it is necessary to show the behavior of
this new equation by numerical methods. Understanding this new multiplier and
associated issues is necessary for economic analyses of the Global Financial
Crisis.

Being one of the most important factors of economic growth of the country,
innovations became one of the key vectors in Russian economic policy. In this
field technology parks are one of the most effective instruments which can
provide growth of innovative activity in sectors, regions and economies. In
this paper, we made a model that allows us to evaluate the effect of technology
parks in the economy of the country and its potential for small and medium
enterprises. The model is based on a system of coupled equations, whose
parameters are estimated on the statistical data that reflect the activity of
the economic entity, in an environment of this entity the technology parks are
acting. Typically, there are regression equations linking a number of economic
factors with some output indicators. We analyzed the property of increasing the
share of surviving small and medium enterprises for Russian conditions as one
of the effect of technology parks and built a working model for estimating the
maximum (limit) values of the effect.

Growing economic inequalities are observed in several countries throughout
the world. Following Pareto, the power-law structure of these inequalities has
been the subject of much theoretical and empirical work. But their
nonequilibrium dynamics, e.g. after a policy change, remains incompletely
understood. Here we introduce a thermodynamical theory of inequalities based on
the analogy between economic stratification and statistical entropy. Within
this framework we identify the combination of upward mobility with
precariousness as a fundamental driver of inequality. We formalize this
statement by a "second-law" inequality displaying upward mobility and
precariousness as thermodynamic conjugate variables. We estimate the time scale
for the "relaxation" of the wealth distribution after a sudden change of the
after-tax return on capital. Our method can be generalized to gain insight into
the dynamics of inequalities in any Markovian model of socioeconomic
interactions.

We formulate a flexible micro-to-macro kinetic model which is able to explain
the emergence of income profiles out of a whole of individual economic
interactions. The model is expressed by a system of several nonlinear
differential equations which involve parameters defined by probabilities.
Society is described as an ensemble of individuals divided into income classes;
the individuals exchange money through binary and ternary interactions, leaving
the total wealth unchanged. The ternary interactions represent taxation and
redistribution effects. Dynamics is investigated through computational
simulations, the focus being on the effects that different fiscal policies and
differently weighted welfare policies have on the long-run income
distributions. The model provides a tool which may contribute to the
identification of the most effective actions towards a reduction of economic
inequality. We find for instance that, under certain hypotheses, the Gini index
is more affected by a policy of reduction of the welfare and subsidies for the
rich classes than by an increase of the upper tax rate. Such a policy also has
the effect of slightly increasing the total tax revenue.

Economies are instances of complex socio-technical systems that are shaped by
the interactions of large numbers of individuals. The individual behavior and
decision-making of consumer agents is determined by complex psychological
dynamics that include their own assessment of present and future economic
conditions as well as those of others, potentially leading to feedback loops
that affect the macroscopic state of the economic system. We propose that the
large-scale interactions of a nation's citizens with its online resources can
reveal the complex dynamics of their collective psychology, including their
assessment of future system states. Here we introduce a behavioral index of
Chinese Consumer Confidence (C3I) that computationally relates large-scale
online search behavior recorded by Google Trends data to the macroscopic
variable of consumer confidence. Our results indicate that such computational
indices may reveal the components and complex dynamics of consumer psychology
as a collective socio-economic phenomenon, potentially leading to improved and
more refined economic forecasting.

The exceptional benefits of wind power as an environmentally responsible
renewable energy resource have led to an increasing penetration of wind energy
in today's power systems. This trend has started to reshape the paradigms of
power system operations, as dealing with uncertainty caused by the highly
intermittent and uncertain wind power becomes a significant issue. Motivated by
this, we present a new framework using adaptive robust optimization for the
economic dispatch of power systems with high level of wind penetration. In
particular, we propose an adaptive robust optimization model for multi-period
economic dispatch, and introduce the concept of dynamic uncertainty sets and
methods to construct such sets to model temporal and spatial correlations of
uncertainty. We also develop a simulation platform which combines the proposed
robust economic dispatch model with statistical prediction tools in a rolling
horizon framework. We have conducted extensive computational experiments on
this platform using real wind data. The results are promising and demonstrate
the benefits of our approach in terms of cost and reliability over existing
robust optimization models as well as recent look-ahead dispatch models.

Socio-economic inequalities are manifested in different aspects of our social
life. We discuss various aspects, beginning with the evolutionary and
historical origins, and discussing the major issues from the social and
economic point of view. The subject has attracted scholars from across various
disciplines, including physicists, who bring in a unique perspective to the
field. The major attempts to analyze the results, address the causes, and
understand the origins using statistical tools and statistical physics concepts
are discussed.

There is, among the economist ecosystem, the idea of virtuous public spending
as a form of promotion of economic growth. If we think on the way GDP is
measured, it is not possible to get that conclusion because it becomes
circular: measuring the money flow obviously will detect directly the public
spending but always mixed with the flow of money from other sources. The
question is how virtuous is public spending per se? Can it promote economic
growth? Is there multiplicative effect in GDP bigger than 1? In this paper, we
make use of the first principles of Economics to show that government spending
is, at the most, as virtuous as private consumption and can be a source of
economic depression and inequality if it is not restricted to fundamental
services.

This is a chapter of the forthcoming Oxford Handbook on the Economics of
Networks.

Despite more than 50 years of human space exploration, no paper in the field
of economics has been published regarding the theory of a space-based economy.
The aim of this paper is to develop quantitative techniques to estimate
conditions of the human heliospheric expansion. An empirical analysis of
current space commercialization and reasoning from first economic principles
yields an evolutionary prisoner's dilemma game on a dynamically scaled
heterogeneous Newman-Watts Small World Network to generate a new space. The
analysis allows for scalar measurements of behavior, market structures, wealth,
and technological prowess, with time measured relative to the system. Four
major phases of heliospheric expansion become evident, in which the dynamic of
the economic environment drives further exploration. Further research could
combine empirical estimations of parameters with computer simulations to prove
results to inform long-term business plans or public policy to further
incentivize human heliospheric domination.

The key characteristic of a true free market economy is that exchanges are
entirely voluntary. When there is a monopoly in the creation of currency as we
have in today's markets, you no longer have a true free market. Features of the
current economic system such as central banking and taxation would be
nonexistent in a free market. This paper examines how currency monopoly leads
to the instabilities and imbalances that we see in today's economy. It also
proposes that currencies should emerge from the voluntary exchange of goods and
services, and studies economic interaction across all scales, by considering
economic action in cases where the self-interests of individuals are
coincident. By examining the voluntary exchange of goods and services at the
scale of an entire society, it is shown that a new currency system, which
resolves a lot of the problems caused by the current fiat currency system,
emerges naturally from the free market. The new currency system is robust and
efficient, and provides a way for public goods and services to be provided, and
its providers compensated, without the need for direct taxation.

General equilibrium equations in economics play the same role with many-body
Newtonian equations in physics. Accordingly, each solution of the general
equilibrium equations can be regarded as a possible microstate of the economic
system. Since Arrow's Impossibility Theorem and Rawls' principle of social
fairness will provide a powerful support for the hypothesis of equal
probability, then the principle of maximum entropy is available in a just and
equilibrium economy so that an income distribution will occur spontaneously
(with the largest probability). Remarkably, some scholars have observed such an
income distribution in some democratic countries, e.g. USA. This result implies
that the hypothesis of equal probability may be only suitable for some "fair"
systems (economic or physical systems). From this meaning, the non-equilibrium
systems may be "unfair" so that the hypothesis of equal probability is
unavailable.

Growth rate of the world Growth Domestic Product (GDP) is analysed to
determine possible pathways of the future economic growth. The analysis is
based on using the latest data of the World Bank and it reveals that the growth
rate between 1960 and 2014 was following a trajectory approaching
asymptotically a constant value. The most likely prediction is that the world
economic growth will continue to increase exponentially and that it will become
unsustainable possibly even during the current century. A more optimistic but
less realistic prediction is based on the assumption that the growth rate will
start to decrease linearly. In this case, the world economic growth is
predicted to reach a maximum, if the growth rate is going to decrease linearly
with time, or to follow a logistic trajectory, if the growth rate is going to
decrease linearly with the size of the world GDP.

This paper provides a general characterization of subgame perfect equilibria
for strategic timing problems, where two firms have the (real) option to make
an irreversible investment. Profit streams are uncertain and depend on the
market structure. The analysis is based directly on the inherent economic
structure of the model. In particular, the determination of equilibria with
preemptive investment is reduced to solving a single class of constrained
optimal stopping problems. The general results are applied to typical
state-space models, completing commonly insufficient equilibrium arguments,
showing when uncertainty leads to qualitatively different behavior, and
establishing additional equilibria that are Pareto improvements.

Warning signs about the developing economic crisis in Greece were present in
the growth rate of the Gross Domestic Product (GDP) and in the growth of the
GDP well before the economic collapse. The growth rate was strongly unstable.
On average, in less than 50 years, it decreased 10-folds but after reaching a
low minimum it quickly increased 6-folds only to crash before completing the
full cycle. The decreasing growth rate was leading to an asymptotic maximum of
the GDP but it was soon replaced by a fast-increasing growth rate propelling
the GDP along a pseudo-hyperbolic trajectory, which if continued would have
escaped to infinity in 2017. Such a growth could not have been possibly
supported. Under these conditions, the economic collapse in Greece was
inevitable.

China's rapid economic growth resulted in serious air pollution, which caused
substantial losses to economic development and residents' health. In
particular, the road transport sector has been blamed to be one of the major
emitters. During the past decades, fluctuation in the international oil prices
has imposed significant impacts on the China's road transport sector.
Therefore, we propose an assumption that China's provincial economies are
independent "economic entities". Based on this assumption, we investigate the
China's road transport fuel (i.e., gasoline and diesel) demand system by using
the panel data of all 31 Chinese provinces except Hong Kong, Macau and Taiwan.
To connect the fuel demand system and the air pollution emissions, we propose
the concept of pollution emissions elasticities to estimate the air pollution
emissions from the road transport sector, and residents' health losses by a
simplified approach consisting of air pollution concentrations and health loss
assessment models under different scenarios based on real-world oil price
fluctuations. Our framework, to the best of our knowledge, is the first attempt
to address the transmission mechanism between the fuel demand system in road
transport sector and residents' health losses in the transitional China.

Mechanized reasoning uses computers to verify proofs and to help discover new
theorems. Computer scientists have applied mechanized reasoning to economic
problems but -- to date -- this work has not yet been properly presented in
economics journals. We introduce mechanized reasoning to economists in three
ways. First, we introduce mechanized reasoning in general, describing both the
techniques and their successful applications. Second, we explain how mechanized
reasoning has been applied to economic problems, concentrating on the two
domains that have attracted the most attention: social choice theory and
auction theory. Finally, we present a detailed example of mechanized reasoning
in practice by means of a proof of Vickrey's familiar theorem on second-price
auctions.

Although not a formal pricing consideration, gap risk or hedging errors are
the norm of derivatives businesses. Starting with the gap risk during a margin
period of risk of a repurchase agreement (repo), this article extends the
Black-Scholes-Merton option pricing framework by introducing a reserve capital
approach to the hedging error's irreducible variability. An extended partial
differential equation is derived with two new terms for expected gap loss and
economic capital charge, leading to the gap risk economic value adjustment and
capital valuation adjustment (KVA) respectively. Practical repo pricing
formulae is obtained showing that the break-even repo rate decomposes into cost
of fund and economic capital charge in KVA. At zero haircut, a one-year term
repo on main equities could command a capital charge as large as 50 basis
points for a 'BBB' rated borrower.

Economic dispatch and frequency regulation are typically viewed as
fundamentally different problems in power systems and, hence, are typically
studied separately. In this paper, we frame and study a joint problem that co-
optimizes both slow timescale economic dispatch resources and fast timescale
frequency regulation resources. We show how the joint problem can be decomposed
without loss of optimality into slow and fast timescale sub-problems that have
appealing interpretations as the economic dispatch and frequency regulation
problems respectively. We solve the fast timescale sub-problem using a
distributed frequency control algorithm that preserves the stability of the
network during transients. We solve the slow timescale sub-problem using an
efficient market mechanism that coordinates with the fast timescale
sub-problem. We investigate the performance of the decomposition on the IEEE
24-bus reliability test system.

We report the first ex post study of the economic impact of sea level rise.
We apply two econometric approaches to estimate the past effects of sea level
rise on the economy of the USA, viz. Barro type growth regressions adjusted for
spatial patterns and a matching estimator. Unit of analysis is 3063 counties of
the USA. We fit growth regressions for 13 time periods and we estimated
numerous varieties and robustness tests for both growth regressions and
matching estimator. Although there is some evidence that sea level rise has a
positive effect on economic growth, in most specifications the estimated
effects are insignificant. We therefore conclude that there is no stable,
significant effect of sea level rise on economic growth. This finding
contradicts previous ex ante studies.

The current framework of Internet interconnections, based on transit and
settlement-free peering relations, has systemic problems that often cause
peering disputes. We propose a new techno-economic interconnection framework
called Nash-Peering, which is based on the principles of Nash Bargaining in
game theory and economics. Nash-Peering constitutes a radical departure from
current interconnection practices, providing a broader and more economically
efficient set of interdomain relations. In particular, the direction of payment
is not determined by the direction of traffic or by rigid customer-provider
relationships but based on which AS benefits more from the interconnection. We
argue that Nash-Peering can address the root cause of various types of peering
disputes.

Socio-economic inequality is measured using various indices. The Gini ($g$)
index, giving the overall inequality is the most commonly used, while the
recently introduced Kolkata ($k$) index gives a measure of $1-k$ fraction of
population who possess top $k$ fraction of wealth in the society. This article
reviews the character of such inequalities, as seen from a variety of data
sources, the apparent relationship between the two indices, and what toy models
tell us. These socio-economic inequalities are also investigated in the context
of man-made social conflicts or wars, as well as in natural disasters. Finally,
we forward a proposal for an international institution with sufficient fund for
visitors, where natural and social scientists from various institutions of the
world can come to discuss, debate and formulate further developments.

A widely applied diversification paradigm is the naive diversification choice
heuristic. It stipulates that an economic agent allocates equal decision
weights to given choice alternatives independent of their individual
characteristics. This article provides mathematically and economically sound
choice theoretic foundations for the naive approach to diversification. We
axiomatize naive diversification by defining it as a preference for equality
over inequality and derive its relationship to the classical diversification
paradigm. In particular, we show that (i) the notion of permutation invariance
lies at the core of naive diversification and that an economic agent is a naive
diversifier if and only if his preferences are convex and permutation
invariant; (ii) Schur-concave utility functions capture the idea of being
inequality averse on top of being risk averse; and (iii) the transformations,
which rebalance unequal decision weights to equality, are characterized in
terms of their implied turnover.

Long and short memory in economic processes is usually described by the
so-called discrete fractional differencing and fractional integration. We prove
that the discrete fractional differencing and integration are the
Grunwald-Letnikov fractional differences of non-integer order d. Equations of
ARIMA(p,d,q) and ARFIMA(p,d,q) models are the fractional-order difference
equations with the Grunwald-Letnikov differences of order d. We prove that the
long and short memory with power law should be described by the exact
fractional-order differences, for which the Fourier transform demonstrates the
power law exactly. The fractional differencing and the Grunwald-Letnikov
fractional differences cannot give exact results for the long and short memory
with power law, since the Fourier transform of these discrete operators satisfy
the power law in the neighborhood of zero only. We prove that the economic
processes with the continuous time long and short memory, which is
characterized by the power law, should be described by the fractional
differential equations.

A generalization of the economic model of natural growth, which takes into
account the power-law memory effect, is suggested. The memory effect means the
dependence of the process not only on the current state of the process, but
also on the history of changes of this process in the past. For the
mathematical description of the economic process with power-law memory we used
the theory of derivatives of non-integer order and fractional-order
differential equation. We propose equations take into account the effects of
memory with one-parameter power-law damping. Solutions of these fractional
differential equations are suggested. We proved that the growth and downturn of
output depend on the memory effects. We demonstrate that the memory effect can
lead to decrease of output instead of its growth, which is described by model
without memory effect. Memory effect can lead to increase of output, rather
than decrease, which is described by model without memory effect.

In this paper, we demonstrate that a consumer's marginal system impact is
only determined by their demand profile rather than their demand level. Demand
profile clustering is identical to cluster consumers according to their
marginal impacts on system costs. A profile-based uniform-rate price is
economically efficient as real-time pricing. We develop a criteria system to
evaluate the economic efficiency of an implemented retail price scheme in a
distribution system by comparing profile clustering and daily-average
clustering. Our criteria system can examine the extent of a retail price
scheme's inefficiency even without information about the distribution system's
daily cost structure. We analyze data from a real distribution system in China.
In this system, targeting each consumer's high-impact days is more efficient
than target high-impact consumers.

In this article, we discuss a dynamical stochastic model that represents the
time evolution of income distribution of a population, where the dynamics
develop from an interplay of multiple economic exchanges in the presence of
multiplicative noise. The model remit stretches beyond the conventional
framework of a Langevin-type kinetic equation in that our model dynamics is
self-consistently constrained by dynamical conservation laws emerging from
population and wealth conservation. This model is numerically solved and
analyzed to interpret the inequality of income as a function of relevant
dynamical parameters like the {\it mobility} $M$ and the {\it total income}
$\mu$. In our model, inequality is quantified by the {\it Gini index} $G$. In
particular, correlations between any two of the mobility index $M$ and/or the
total income $\mu$ with the Gini index $G$ are investigated and compared with
the analogous correlations resulting from an equivalent additive noise model.
Our findings highlight the importance of a multiplicative noise based economic
modeling structure in the analysis of inequality. The model also depicts the
nature of correlation between mobility and total income of a population from
the perspective of inequality measure.

The paper proposes a method of financial time series forecasting taking into
account the semantics of news. For the semantic analysis of financial news the
sampling of negative and positive words in economic sense was formed based on
Loughran McDonald Master Dictionary. The sampling included the words with high
frequency of occurrence in the news of financial markets. For single-root words
it has been left only common part that allows covering few words for one
request. Neural networks were chosen for modeling and forecasting. To automate
the process of extracting information from the economic news a script was
developed in the MATLAB Simulink programming environment, which is based on the
generated sampling of positive and negative words. Experimental studies with
different architectures of neural networks showed a high adequacy of
constructed models and confirmed the feasibility of using information from news
feeds to predict the stock prices.

We show that the space in which scientific, technological and economic
developments interplay with each other can be mathematically shaped using
pioneering multilayer network and complexity techniques. We build the
tri-layered network of human activities (scientific production, patenting, and
industrial production) and study the interactions among them, also taking into
account the possible time delays. Within this construction we can identify
which capabilities and prerequisites are needed to be competitive in a given
activity, and even measure how much time is needed to transform, for instance,
the technological know-how into economic wealth and scientific innovation,
being able to make predictions with a very long time horizon. Quite
unexpectedly, we find empirical evidence that the naive knowledge flow from
science, to patents, to products is not supported by data, being instead
technology the best predictor for industrial and scientific production for the
next decades.

Classical game theory addresses decision problems in multi-agent environment
where one rational agent's decision affects other agents' payoffs. Game theory
has widespread application in economic, social and biological sciences. In
recent years quantum versions of classical games have been proposed and
studied. In this paper, we consider a quantum version of the classical
Barro-Gordon game which captures the problem of time inconsistency in monetary
economics. Such time inconsistency refers to the temptation of weak policy
maker to implement high inflation when the public expects low inflation. The
inconsistency arises when the public punishes the weak policy maker in the next
cycle. We first present a quantum version of the Barro-Gordon game. Next, we
show that in a particular case of the quantum game, time-consistent Nash
equilibrium could be achieved when public expects low inflation, thus resolving
the game.

In this paper we investigate the possibility of spontaneous segregation into
groups of traders that have to choose among several markets. Even in the
simplest case of two markets and Zero Intelligence traders, we are able to
observe segregation effects below a critical value Tc of the temperature T; the
latter regulates how strongly traders bias their decisions towards choices with
large accumulated scores. It is notable that segregation occurs even though the
traders are statistically homogeneous. Traders can in principle change their
loyalty to a market, but the relevant persistence times become long below Tc.

This paper presents an intertemporal bimodal network to analyze the evolution
of the semantic content of a scientific field within the framework of topic
modeling, namely using the Latent Dirichlet Allocation (LDA). The main
contribution is the conceptualization of the topic dynamics and its
formalization and codification into an algorithm. To benchmark the
effectiveness of this approach, we propose three indexes which track the
transformation of topics over time, their rate of birth and death, and the
novelty of their content. Applying the LDA, we test the algorithm both on a
controlled experiment and on a corpus of several thousands of scientific papers
over a period of more than 100 years which account for the history of the
economic thought.

Exergy is a thermodynamic quantity useful to obtain information on the work
from any process. The analyses of irreversibility are important in the
designing and development of the productive processes for the economic growth,
but they play a fundamental role also in the analysis of socio-economic
context. The link between the wasted exergy and the energy cost for maintain
the productive processes are obtained in the bioengineering thermodynamics.
This link holds to the fundamental role of fluxes and to the exergy exchanged
in the interaction between the system and its environment. The equivalent
wasted primary resource value is suggested as an indicator to support the
economic considerations on the biofuel production by using biomass and
bacteria. Moreover, the technological considerations can be developed by using
the exergy inefficiency. Consequently, bacteria use can be compared with other
way to obtain biofuels, by comparing both the technologies and the economic
considerations.

In recent years, methods from network science are gaining rapidly interest in
economics and finance. A reason for this is that in a globalized world the
interconnectedness among economic and financial entities are crucial to
understand and networks provide a natural framework for representing and
studying such systems. In this paper, we are surveying the use of networks and
network-based methods for studying economy related questions. We start with a
brief overview of graph theory and basic definitions. Then we discuss
descriptive network measures and network complexity measures for quantifying
structural properties of economic networks. Finally, we discuss different
network and tree structures as relevant for applications.

Payment channel networks are supposed to overcome technical scalability
limitations of blockchain infrastructure by employing a special overlay network
with fast payment confirmation and only sporadic settlement of netted
transactions on the blockchain. However, they introduce economic routing
constraints that limit decentralized scalability and are currently not well
understood. In this paper, we model the economic incentives for participants in
payment channel networks. We provide the first formal model of payment channel
economics and analyze how the cheapest path can be found. Additionally, our
simulation assesses the long-term evolution of a payment channel network. We
find that even for small routing fees, sometimes it is cheaper to settle the
transaction directly on the blockchain.

Achieving international food security requires improved understanding of how
international trade networks connect countries around the world through the
import-export flows of food commodities. The properties of food trade networks
are still poorly documented, especially from a multi-network perspective. In
particular, nothing is known about the community structure of food networks,
which is key to understanding how major disruptions or 'shocks' would impact
the global food system. Here we find that the individual layers of this network
have densely connected trading groups, a consistent characteristic over the
period 2001 to 2011. We also fit econometric models to identify social,
economic and geographic factors explaining the probability that any two
countries are co-present in the same community. Our estimates indicate that the
probability of country pairs belonging to the same food trade community depends
more on geopolitical and economic factors -- such as geographical proximity and
trade agreements co-membership -- than on country economic size and/or income.
This is in sharp contrast with what we know about bilateral-trade determinants
and suggests that food country communities behave in ways that can be very
different from their non-food counterparts.

This study briefly introduces the development of Shantou Special Economic
Zone under Reform and Opening-Up Policy from 1980 through 2016 with a focus on
policy making issues and its influences on local economy. This paper is divided
into two parts, 1980 to 1991, 1992 to 2016 in accordance with the separation of
the original Shantou District into three cities: Shantou, Chaozhou and Jieyang
in the end of 1991. This study analyzes the policy making issues in the
separation of the original Shantou District, the influences of the policy on
Shantou's economy after separation, the possibility of merging the three cities
into one big new economic district in the future and reasons that lead to the
stagnant development of Shantou in recent 20 years. This paper uses statistical
longitudinal analysis in analyzing economic problems with applications of
non-parametric statistics through generalized additive model and time series
forecasting methods. The paper is authored by Bowen Cai solely, who is the
graduate student in the PhD program of Applied and Computational Mathematics
and Statistics at the University of Notre Dame with concentration in big data
analysis.

Collective learning in economic development has been revealed by recent
empirical studies, however, investigations on how to benefit most from its
effects remain still lacking. In this paper, we explore the maximization of the
collective learning effects using a simple propagation model to study the
diversification of industries on real networks built on Brazilian labor data.
For the inter-regional learning, we find an optimal strategy that makes a
balance between core and periphery industries in the initial activation,
considering the core-periphery structure of the industry space--a network
representation of the relatedness between industries. For the inter-regional
learning, we find an optimal strategy that makes a balance between nearby and
distant regions in establishing new spatial connections, considering the
spatial structure of the integrated adjacent network that connects all regions.
Our findings suggest that the near to by random strategies are likely to make
the best use of the collective learning effects in advancing regional economic
development practices.

Intersectoral dynamic models with power-law memory are proposed. The
equations of open and closed intersectoral models, in which the memory effects
are described by the Caputo derivatives of non-integer orders, are derived. We
suggest solutions of these equations, which have the form of linear
combinations of the Mittag-Leffler functions and which are characterized by
different effective growth rates. Examples of intersectoral dynamics with
power-law memory are suggested for two sectoral cases. We formulate two
principles of intersectoral dynamics with memory: the principle of changing of
technological growth rates and the principle of domination change. It has been
shown that in the input-output economic dynamics the effects of fading memory
can change the economic growth rate and dominant behavior of economic sectors.

In this paper we discuss a concept of dynamic memory and an application of
fractional calculus to describe the dynamic memory. The concept of memory is
considered from the standpoint of economic models in the framework of
continuous time approach based on fractional calculus. We also describe some
general restrictions that can be imposed on the structure and properties of
dynamic memory. These restrictions include the following three principles: (a)
the principle of fading memory; (b) the principle of memory homogeneity on time
(the principle of non-aging memory); (c) the principle of memory reversibility
(the principle of memory recovery). Examples of different memory functions are
suggested by using the fractional calculus. To illustrate an application of the
concept of dynamic memory in economics we consider a generalization of the
Harrod-Domar model, where the power-law memory is taken into account.

Off-chain transaction channels represent one of the leading techniques to
scale the transaction throughput in cryptocurrencies. However, the economic
effect of transaction channels on the system has not been explored much until
now.
  We study the economics of Bitcoin transaction channels, and present a
framework for an economic analysis of the lightning network and its effect on
transaction fees on the blockchain. Our framework allows us to reason about
different patterns of demand for transactions and different topologies of the
lightning network, and to derive the resulting fees for transacting both on and
off the blockchain.
  Our initial results indicate that while the lightning network does allow for
a substantially higher number of transactions to pass through the system, it
does not necessarily provide higher fees to miners, and as a result may in fact
lead to lower participation in mining within the system.

This document collects the lecture notes from my mini-course "Complexity
Theory, Game Theory, and Economics," taught at the Bellairs Research Institute
of McGill University, Holetown, Barbados, February 19--23, 2017, as the 29th
McGill Invitational Workshop on Computational Complexity.
  The goal of this mini-course is twofold: (i) to explain how complexity theory
has helped illuminate several barriers in economics and game theory; and (ii)
to illustrate how game-theoretic questions have led to new and interesting
complexity theory, including recent several breakthroughs. It consists of two
five-lecture sequences: the Solar Lectures, focusing on the communication and
computational complexity of computing equilibria; and the Lunar Lectures,
focusing on applications of complexity theory in game theory and economics. No
background in game theory is assumed.

A new concept of an equilibrium in games is introduced that solves an open
question posed by A. Neyman.

Countries tend to diversify their exports by entering products that are
related to their current exports. Yet this average behavior is not
representative of every diversification path. In this paper, we introduce a
method to identify periods when countries enter unrelated products. We analyze
the economic diversification paths of 93 countries between 1965 and 2014 and
find that countries enter unrelated products in only about 7.2% of all
observations. We find that countries enter more unrelated products when they
are at an intermediate level of economic development, and when they have higher
levels of human capital. Finally, we ask whether countries entering more
unrelated products grow faster than those entering only related products. The
data shows that countries that enter more unrelated activities experience a
small but significant increase in future economic growth, compared to countries
with a similar level of income, human capital, capital stock per worker, and
economic complexity.

A short and direct proof of the Gibbard-Satterthwaite theorem \`{a} la
Amartya Sen's proof of Arrow's impossibility theorem is given.

Opinion polls have been the bridge between public opinion and politicians in
elections. However, developing surveys to disclose people's feedback with
respect to economic issues is limited, expensive, and time-consuming. In recent
years, social media such as Twitter has enabled people to share their opinions
regarding elections. Social media has provided a platform for collecting a
large amount of social media data. This paper proposes a computational public
opinion mining approach to explore the discussion of economic issues in social
media during an election. Current related studies use text mining methods
independently for election analysis and election prediction; this research
combines two text mining methods: sentiment analysis and topic modeling. The
proposed approach has effectively been deployed on millions of tweets to
analyze economic concerns of people during the 2012 US presidential election.

We introduce a new diffusion process Xt to describe asset prices within an
economic bubble cycle. The main feature of the process, which differs from
existing models, is the drift term where a mean-reversion is taken based on an
exponential decay of the scaled price. Our study shows the scaling factor on Xt
is crucial for modelling economic bubbles as it mitigates the dependence
structure between the price and parameters in the model. We prove both the
process and its first passage time are well-defined. An efficient calibration
scheme, together with the probability density function for the process are
given. Moreover, by employing the perturbation technique, we deduce the
closed-form density for the downward first passage time, which therefore can be
used in estimating the burst time of an economic bubble. The object of this
study is to understand the asset price dynamics when a financial bubble is
believed to form, and correspondingly provide estimates to the bubble crash
time. Calibration examples on the US dot-com bubble and the 2007 Chinese stock
market crash verify the effectiveness of the model itself. The example on
BitCoin prediction confirms that we can provide meaningful estimate on the
downward probability for asset prices.

This paper proposes a convex non-linear cost saving model for optimal
economic dispatch in a microgrid. The mod-el incorporates energy storage
degradation cost and intermittent renewable generation. Cell degradation cost
being a non-linear model, its incorporation in an objective function alters the
convexity of the optimization problem and stochastic algorithms are required
for its solution. This paper builds on the scope for usage of macroscopically
semi-empirical models for degradation cost in economic dispatch problems and
proves that these cost models derived from the existing semi-empirical capacity
fade equations for LiFePO4 cells are convex under some operating condi-tions.
The proposed non-linear model was tested on two data sets of varying size which
portray different trends of seasonality. The results show that the model
reflects the trends of seasonality existing in the data sets and it mini-mizes
the total fuel cost globally when compared to conventional systems of economic
dispatch. The results thus indicate that the model achieves a more accurate
estimate of fuel cost in the system and can be effectively utilized for cost
analysis in power system applications.

This paper discusses how public research organizations consume funding for
research, applying a new approach based on economic metabolism of research
labs, in a broad analogy with biology. This approach is applied to a case study
in Europe represented by one of the biggest European public research
organizations, the National Research council of Italy. Results suggest that
funding for research (state subsidy and public contracts) of this public
research organization is mainly consumed for the cost of personnel. In
addition, the analysis shows a disproportionate growth of the cost of personnel
in public research labs in comparison with total revenue from government. In
the presence of shrinking public research lab budgets, this organizational
behavior generates inefficiencies and stress. R&D management and public policy
implications are suggested for improving economic performance of public
research organizations in turbulent markets.

This study provides the theoretical framework and empirical model for
productivity growth evaluations in agricultural sector as one of the most
important sectors in Iran's economic development plan. We use the Solow
residual model to measure the productivity growth share in the value-added
growth of the agricultural sector. Our time series data includes value-added
per worker, employment, and capital in this sector. The results show that the
average total factor productivity growth rate in the agricultural sector is
-0.72% during 1991-2010. Also, during this period, the share of total factor
productivity growth in the value-added growth is -19.6%, while it has been
forecasted to be 33.8% in the fourth development plan. Considering the
effective role of capital in the agricultural low productivity, we suggest
applying productivity management plans (especially in regards of capital
productivity) to achieve future growth goals.

Geography, including climatic factors, have long been considered potentially
important elements in shaping socio-economic activities, alongside other
determinants, such as institutions. Here we demonstrate that geography and
climate satisfactorily explain worldwide economic activity as measured by the
per capita Gross Cell Product (GCP-PC) at a fine geographical resolution,
typically much higher than country average. A 1{\deg} by 1{\deg} GCP-PC dataset
has been key for establishing and testing a direct relationship between 'local'
geography/climate and GCP-PC. Not only have we tested the geography/climate
hypothesis using many possible explanatory variables, importantly we have also
predicted and reconstructed GCP-PC worldwide by retaining the most significant
predictors. While this study confirms that latitude is the most important
predictor for GCP-PC when taken in isolation, the accuracy of the GCP-PC
prediction is greatly improved when other factors mainly related to variations
in climatic variables, such as the variability in air pressure, rather than
average climatic conditions as typically used, are considered. Implications of
these findings include an improved understanding of why economically better-off
societies are geographically placed where they are

When studying social, economic and biological systems, one has often access
to only limited information about the structure of the underlying networks. An
example of paramount importance is provided by financial systems: information
on the interconnections between financial institutions is privacy-protected,
dramatically reducing the possibility of correctly estimating crucial systemic
properties such as the resilience to the propagation of shocks. The need to
compensate for the scarcity of data, while optimally employing the available
information, has led to the birth of a research field known as network
reconstruction. Since the latter has benefited from the contribution of
researchers working in disciplines as different as mathematics, physics and
economics, the results achieved so far are still scattered across heterogeneous
publications. Most importantly, a systematic comparison of the network
reconstruction methods proposed up to now is currently missing. This review
aims at providing a unifying framework to present all these studies, mainly
focusing on their application to economic and financial networks.

Historical examination of the Bretton Woods system allows comparisons to be
made with the current evolution of the EMS.

I analyze factory worker households in the early 1920s in Osaka to examine
idiosyncratic income shocks and consumption. Using the household-level monthly
panel dataset, I find that while households could not fully cope with
idiosyncratic income shocks at that time, they mitigated fluctuations in
indispensable consumption during economic hardship. In terms of risk-coping
mechanisms, I find suggestive evidence that savings institutions helped
mitigate vulnerabilities and that both using borrowing institutions and
adjusting labor supply served as risk-coping strategies among households with
less savings.

In an economic market, sellers, infomediaries and customers constitute an
economic network. Each seller has her own customer group and the seller's
private customers are unobservable to other sellers. Therefore, a seller can
only sell commodities among her own customers unless other sellers or
infomediaries share her sale information to their customer groups. However, a
seller is not incentivized to share others' sale information by default, which
leads to inefficient resource allocation and limited revenue for the sale. To
tackle this problem, we develop a novel mechanism called customer sharing
mechanism (CSM) which incentivizes all sellers to share each other's sale
information to their private customer groups. Furthermore, CSM also
incentivizes all customers to truthfully participate in the sale. In the end,
CSM not only allocates the commodities efficiently but also optimizes the
seller's revenue.

Empirical research often cites observed choice responses to variation that
shifts expected discounted future utilities, but not current utilities, as an
intuitive source of information on time preferences. We study the
identification of dynamic discrete choice models under such economically
motivated exclusion restrictions on primitive utilities. We show that each
exclusion restriction leads to an easily interpretable moment condition with
the discount factor as the only unknown parameter. The identified set of
discount factors that solves this condition is finite, but not necessarily a
singleton. Consequently, in contrast to common intuition, an exclusion
restriction does not in general give point identification. Finally, we show
that exclusion restrictions have nontrivial empirical content: The implied
moment conditions impose restrictions on choices that are absent from the
unconstrained model.

Wealth inequality is an important matter for economic theory and policy.
Ongoing debates have been discussing recent rise in wealth inequality in
connection with recent development of active financial markets around the
world. Existing literature on wealth distribution connects the origins of
wealth inequality with a variety of drivers. Our approach develops a minimalist
modelling strategy that combines three featuring mechanisms: active financial
markets; individual wealth accumulation; and compound interest structure. We
provide mathematical proof that accumulated financial investment returns
involve ever-increasing wealth concentration and inequality across individual
investors through time. This cumulative effect through space and time depends
on the financial accumulation process and holds also under efficient financial
markets, which generate some fair investment game that individual investors do
repeatedly play through time.

The Internet of Things (IoT) propagates the paradigm of interconnecting
billions of heterogeneous devices by various manufacturers. To enable IoT
applications, the communication between IoT devices follows specifications
defined by standard developing organizations. In this paper, we present a case
study that investigates disclosed insecurities of the popular IoT standard
ZigBee, and derive general lessons about security economics in IoT
standardization efforts. We discuss the motivation of IoT standardization
efforts that are primarily driven from an economic perspective, in which large
investments in security are not considered necessary since the consumers do not
reward them. Success at the market is achieved by being quick-to-market,
providing functional features and offering easy integration for complementors.
Nevertheless, manufacturers should not only consider economic reasons but also
see their responsibility to protect humans and technological infrastructures
from being threatened by insecure IoT products. In this context, we propose a
number of recommendations to strengthen the security design in future IoT
standardization efforts, ranging from the definition of a precise security
model to the enforcement of an update policy.

Research has highlighted relationships between size and scaled growth across
a large variety of biological and social organisms, ranging from bacteria,
through animals and plants, to cities an companies. Yet, heretofore,
identifying a similar relationship at the country level has proven challenging.
One reason is that, unlike the former, countries have predefined borders, which
limit their ability to grow "organically." This paper addresses this issue by
identifying and validating an effective measure of organic growth at the
country level: nighttime light emissions, which serve as a proxy of energy
allocations where more productive activity takes place. This indicator is
compared to population size to illustrate that while nighttime light emissions
are associated with superlinear growth, population size at the country level is
associated with sublinear growth. These relationships and their implications
for economic inequalities are then explored using high-resolution geospatial
datasets spanning the last three decades.

We show that a convex body admits a translative dense packing in
$\mathbb{R}^d$ if and only if it admits a translative economical covering.

Almost a decade on from the launch of Bitcoin, cryptocurrencies continue to
generate headlines and intense debate. What started as an underground
experiment by a rag tag group of programmers armed with a Libertarian manifesto
has now resulted in a thriving $230 billion ecosystem, with constant on-going
innovation. Scholars and researchers alike are realizing that cryptocurrencies
are far more than mere technical innovation; they represent a distinct and
revolutionary new economic paradigm tending towards decentralization.
Unfortunately, this bold new universe is little explored from the perspective
of Islamic economics and finance. Our work aims to address these deficiencies.
Our paper makes the following distinct contributions We significantly expand
the discussion on whether cryptocurrencies qualify as "money" from an Islamic
perspective and we argue that this debate necessitates rethinking certain
fundamental definitions. We conclude that the cryptocurrency phenomenon, with
its radical new capabilities, may hold considerable opportunity which merits
deeper investigation.

The purpose of this paper is to study the time average behavior of Markov
chains with transition probabilities being kernels of completely continuous
operators, and therefore to provide a sufficient condition for a class of
Markov chains that are frequently used in dynamic economic models to be
ergodic. The paper reviews the time average convergence of the quasi-weakly
complete continuity Markov operators to a unique projection operator. Also, it
shows that a further assumption of quasi-strongly complete continuity reduces
the dependence of the unique invariant measure on its corresponding initial
distribution through ergodic decomposition, and therefore guarantees the Markov
chain to be ergodic up to multiplication of constant coefficients. Moreover, a
sufficient and practical condition is provided for the ergodicity in economic
state Markov chains that are induced by exogenous random shocks and a
correspondence between the exogenous space and the state space.

We propose a multivariate elastic net regression forecast model for German
quarter-hourly electricity spot markets. While the literature is diverse on
day-ahead prediction approaches, both the intraday continuous and intraday
call-auction prices have not been studied intensively with a clear focus on
predictive power. Besides electricity price forecasting, we check for the
impact of early day-ahead (DA) EXAA prices on intraday forecasts. Another
novelty of this paper is the complementary discussion of economic benefits. A
precise estimation is worthless if it cannot be utilized. We elaborate possible
trading decisions based upon our forecasting scheme and analyze their monetary
effects. We find that even simple electricity trading strategies can lead to
substantial economic impact if combined with a decent forecasting technique.

Recently there has been considerable progress on the analysis of stability
and performance properties of so-called economic Nonlinear Model Predictive
Control (NMPC) schemes; i.e. NMPC schemes employing stage costs that are not
directly related to distance measures of pre-computed setpoints. At the same
time, with respect to the energy transition, the use of NMPC schemes is
proposed and investigated in a plethora of papers in different contexts. For
example receding-horizon approaches to generator dispatch problems, which is
also known as multi-stage Optimal Power Flow (OPF), naturally lead to economic
NMPC schemes based on non-convex discrete-time Optimal Control Problems (OCP).
The present paper investigates the transfer of analytic results available for
general economic NMPC schemes to receding-horizon multistage OPF. We propose a
blueprint formulation of multi-stage opf including AC power flow equations.
Based on this formulation we present results on the dissipativity and recursive
feasibility properties of the underlying OCP. Finally, we draw upon simulations
using a 5 bus system and a 118 bus system to illustrate our findings.

This paper examines how subsistence farmers respond to extreme heat. Using
micro-data from Peruvian households, we find that high temperatures reduce
agricultural productivity, increase area planted, and change crop mix. These
findings are consistent with farmers using input adjustments as a short-term
mechanism to attenuate the effect of extreme heat on output. This response
seems to complement other coping strategies, such as selling livestock, but
exacerbates the drop in yields, a standard measure of agricultural
productivity. Using our estimates, we show that accounting for land adjustments
is important to quantify damages associated with climate change.

In this paper we study the impact of errors in wind and solar power forecasts
on intraday electricity prices. We develop a novel econometric model which is
based on day-ahead wholesale auction curves data and errors in wind and solar
power forecasts. The model shifts day-ahead supply curves to calculate intraday
prices. We apply our model to the German EPEX SPOT SE data. Our model
outperforms both linear and non-linear benchmarks. Our study allows us to
conclude that errors in renewable energy forecasts exert a non-linear impact on
intraday prices. We demonstrate that additional wind and solar power capacities
induce non-linear changes in the intraday price volatility. Finally, we comment
on economical and policy implications of our findings.

At the zero lower bound, the New Keynesian model predicts that output and
inflation collapse to implausibly low levels, and that government spending and
forward guidance have implausibly large effects. To resolve these anomalies, we
introduce wealth into the utility function; the justification is that wealth is
a marker of social status, and people value status. Since people partly save to
accrue social status, the Euler equation is modified. As a result, when the
marginal utility of wealth is sufficiently large, the dynamical system
representing the zero-lower-bound equilibrium transforms from a saddle to a
source---which resolves all the anomalies.

We consider a two-person trading game in continuous time whereby each player
chooses a constant rebalancing rule $b$ that he must adhere to over $[0,t]$. If
$V_t(b)$ denotes the final wealth of the rebalancing rule $b$, then Player 1
(the `numerator player') picks $b$ so as to maximize
$\mathbb{E}[V_t(b)/V_t(c)]$, while Player 2 (the `denominator player') picks
$c$ so as to minimize it. In the unique Nash equilibrium, both players use the
continuous-time Kelly rule $b^*=c^*=\Sigma^{-1}(\mu-r\textbf{1})$, where
$\Sigma$ is the covariance of instantaneous returns per unit time, $\mu$ is the
drift vector of the stock market, and $\textbf{1}$ is a vector of ones. Thus,
even over very short intervals of time $[0,t]$, the desire to perform well
relative to other traders leads one to adopt the Kelly rule, which is
ordinarily derived by maximizing the asymptotic exponential growth rate of
wealth. Hence, we find agreement with Bell and Cover's (1988) result in
discrete time.

Recent advances in computing power and the potential to make more realistic
assumptions due to increased flexibility have led to the increased prevalence
of simulation models in economics. While models of this class, and particularly
agent-based models, are able to replicate a number of empirically-observed
stylised facts not easily recovered by more traditional alternatives, such
models remain notoriously difficult to estimate due to their lack of tractable
likelihood functions. While the estimation literature continues to grow,
existing attempts have approached the problem primarily from a frequentist
perspective, with the Bayesian estimation literature remaining comparatively
less developed. For this reason, we introduce a Bayesian estimation protocol
that makes use of deep neural networks to construct an approximation to the
likelihood, which we then benchmark against a prominent alternative from the
existing literature. Overall, we find that our proposed methodology
consistently results in more accurate estimates in a variety of settings,
including the estimation of financial heterogeneous agent models and the
identification of changes in dynamics occurring in models incorporating
structural breaks.

This article shows a new focus of mathematic analysis for the Solow-Swan
economic growth model, using the generalized conformal derivative Katugampola
(KGCD). For this, under the same Solow-Swan model assumptions, the Inada
conditions are extended, which, for the new model shown here, depending on the
order of the KGCD. This order plays an important role in the speed of
convergence of the closed solutions obtained with this derivative for capital
(k) and for per-capita production (y) in the cases without migration and with
negative migration. Our approach to the model with the KGCD adds a new
parameter to the Solow-Swan model, the order of the KGCD and not a new state
variable. In addition, we propose several possible economic interpretations for
that parameter.

In this note, we consider the pricing problem of a profit-maximizing
monopolist who faces naive consumers with convex self-control preferences.

Recently, the French Senate approved a law that imposes a 3% tax on revenue
generated from digital services by companies above a certain size. While there
is a lot of political debate about economic consequences of this action, it is
actually interesting to reverse the question: We consider the long-term
implications of an economy with no such digital tax. More generally, we can
think of digital services as a special case of products with low or zero cost
of transportation. With basic economic models we show that a market with no
transportation costs is prone to monopolization as minuscule, random
differences in quality are rewarded disproportionally. We then propose a
distance-based tax to counter-balance the tendencies of random centralisation.
Unlike a tax that scales with physical (cardinal) distance, a ranked (ordinal)
distance tax leverages the benefits of digitalization while maintaining a
stable economy.

Many large cities are found at locations with certain first nature
advantages. Yet, those exogenous locational features may not be the most potent
forces governing the spatial pattern of cities. In particular, population size,
spacing and industrial composition of cities exhibit simple, persistent and
monotonic relationships. Theories of economic agglomeration suggest that this
regularity is a consequence of interactions between endogenous agglomeration
and dispersion forces. This paper reviews the extant formal models that explain
the spatial pattern together with the size distribution of cities, and
discusses the remaining research questions to be answered in this literature.
To obtain results about explicit spatial patterns of cities, a model needs to
depart from the most popular two-region and systems-of-cities frameworks in
urban and regional economics in which there is no variation in interregional
distance. This is one of the major reasons that only few formal models have
been proposed in this literature. To draw implications as much as possible from
the extant theories, this review involves extensive discussions on the behavior
of the many-region extension of these models. The mechanisms that link the
spatial pattern of cities and the diversity in city sizes are also discussed in
detail.

In this work, the control of snake robot locomotion via economic model
predictive control (MPC) is studied. Only very few examples of applications of
MPC to snake robots exist and rigorous proofs for recursive feasibility and
convergence are missing. We propose an economic MPC algorithm that maximizes
the robot's forward velocity and integrates the choice of the gait pattern into
the closed loop. We show recursive feasibility of the MPC optimization problem,
where some of the developed techniques are also applicable for the analysis of
a more general class of system. Besides, we provide performance results and
illustrate the achieved performance by numerical simulations. We thereby show
that the economic MPC algorithm outperforms a standard lateral undulation
controller and achieves constraint satisfaction. Surprisingly, a gait pattern
different to lateral undulation results from the optimization.

We conduct a sequential social-learning experiment where subjects each guess
a hidden state based on private signals and the guesses of a subset of their
predecessors. A network determines the observable predecessors, and we compare
subjects' accuracy on sparse and dense networks. Accuracy gains from social
learning are twice as large on sparse networks compared to dense networks.
Models of naive inference where agents ignore correlation between observations
predict this comparative static in network density, while the finding is
difficult to reconcile with rational-learning models.

The increasing difficulties in financing the welfare state and in particular
public retirement pensions have been one of the outcomes both of the decrease
of fertility and birth rates combined with the increase of life expectancy. The
dynamics of retirement pensions are usually studied in Economics using
overlapping generation models. These models are based on simplifying
assumptions like the use of a representative agent to ease the problem of
tractability. Alternatively, we propose to use agent-based modelling (ABM),
relaxing the need for those assumptions and enabling the use of interacting and
heterogeneous agents assigning special importance to the study of
inter-generational relations. We treat pension dynamics both in economics and
political perspectives. The model we build, following the ODD protocol, will
try to understand the dynamics of choice of public versus private retirement
pensions resulting from the conflicting preferences of different agents but
also from the cooperation between them. The aggregation of these individual
preferences is done by voting. We combine a microsimulation approach following
the evolution of synthetic populations along time, with the ABM approach
studying the interactions between the different agent types. Our objective is
to depict the conditions for the survival of the public pensions system
emerging from the relation between egoistic and altruistic individual and
collective behaviours.

We investigate state-dependent effects of fiscal multipliers and allow for
endogenous sample splitting to determine whether the US economy is in a slack
state. When the endogenized slack state is estimated as the period of the
unemployment rate higher than about 12 percent, the estimated cumulative
multipliers are significantly larger during slack periods than non-slack
periods and are above unity. We also examine the possibility of time-varying
regimes of slackness and find that our empirical results are robust under a
more flexible framework. Our estimation results point out the importance of the
heterogenous effects of fiscal policy and shed light on the prospect of fiscal
policy in response to economic shocks from the current COVID-19 pandemic.

In this paper we derive the maximum entropy characteristics of a particular
rank order distribution, namely the discrete generalized beta distribution,
which has recently been observed to be extremely useful in modelling many
several rank-size distributions from different context in Arts and Sciences, as
a two-parameter generalization of Zipf's law. Although it has been seen to
provide excellent fits for several real world empirical datasets, the
underlying theory responsible for the success of this particular rank order
distribution is not explored properly. Here we, for the first time, provide its
generating process which describes it as a natural maximum entropy distribution
under an appropriate bivariate utility constraint. Further, considering the
similarity of the proposed utility function with the usual logarithmic utility
function from economic literature, we have also explored its acceptability in
universal modeling of different types of socio-economic factors within a
country as well as across the countries. The values of distributional
parameters estimated through a rigorous statistical estimation method, along
with the $entropy$ values, are used to characterize the distributions of all
these socio-economic factors over the years.

An important question in economics is how people choose between different
payments in the future. The classical normative model predicts that a decision
maker discounts a later payment relative to an earlier one by an exponential
function of the time between them. Descriptive models use non-exponential
functions to fit observed behavioral phenomena, such as preference reversal.
Here we propose a model of discounting, consistent with standard axioms of
choice, in which decision makers maximize the growth rate of their wealth. Four
specifications of the model produce four forms of discounting -- no
discounting, exponential, hyperbolic, and a hybrid of exponential and
hyperbolic -- two of which predict preference reversal. Our model requires no
assumption of behavioral bias or payment risk.

The Total Economic Time Capacity of a Year 525600 minutes is postulated as a
time standard for a new Monetary Minute currency in this evaluation study.
Consequently, the Monetary Minute MonMin is defined as a 1/525600 part of the
Total Economic Time Capacity of a Year. The Value CMonMin of the Monetary
Minute MonMin is equal to a 1/525600 part of the GDP, p.c., expressed in a
specific state currency C. There is described how the Monetary Minutes MonMin
are determined, and how their values CMonMin are calculated based on the GDP
and all the population in specific economies. The Monetary Minutes trace
different aggregate productivity, i.e. exploitation of the total time capacity
of a year for generating of the GDP in economies of different states.

We analyze pricing mechanisms in electricity markets with AC power flow
equations that define a nonconvex feasible set for the economic dispatch
problem. Specifically, we consider two possible pricing schemes. The first
among these prices are derived from Lagrange multipliers that satisfy
Karush-Kuhn-Tucker conditions for local optimality of the nonconvex market
clearing problem. The second is derived from optimal dual multipliers of the
convex semidefinite programming (SDP) based relaxation of the market clearing
problem. Relationships between these prices, their revenue adequacy and market
equilibrium properties are derived and compared. The SDP prices are shown to
equal distribution locational marginal prices derived with second-order conic
relaxations of power flow equations over radial distribution networks. We
illustrate our theoretical findings through numerical experiments.

Input Output (IO) tables provide a standardised way of looking at monetary
flows between all industries in an economy. IO tables can be thought of as
networks - with the nodes being different industries and the edges being the
flows between them. We develop a network-based analysis to consider a
multi-regional IO network at regional and subregional level within a country.
We calculate both traditional matrix-based IO measures (e.g. 'multipliers') and
new network theory-based measures at this higher spatial resolution. We
contrast these methods with the results of a disruption model applied to the
same IO data in order to demonstrate that betweenness centrality gives a good
indication of flow on economic disruption, while eigenvector-type centrality
measures give results comparable to traditional IO multipliers.We also show the
effects of treating IO networks at different levels of spatial aggregation.

The fall of the Berlin Wall in 1989, modified the relations between cities of
the former communist bloc. The European and worldwide reorientation of
interactions that followed raises the question of the actual state of
historical relationships between Central Eastern European cities, but also with
ex-USSR and ex-Yugoslavian ones. Do Central and Eastern European cities
reproduce trajectories from the past in a new economic context? This paper will
examine their evolution in terms of trade exchanges and air traffic connexions
since 1989. They are confronted with transnational firm networks for the recent
years. The main contribution is to show a progressive formation of several
economic regions in Central and Eastern Europe as a result of integration into
Braudel's \'economie-monde.

We discuss the idea of a purely algorithmic universal world iCurrency set
forth in [Kakushadze and Liew, 2014] (https://ssrn.com/abstract=2542541) and
expanded in [Kakushadze and Liew, 2017] (https://ssrn.com/abstract=3059330) in
light of recent developments, including Libra. Is Libra a contender to become
iCurrency? Among other things, we analyze the Libra proposal, including the
stability and volatility aspects, and discuss various issues that must be
addressed. For instance, one cannot expect a cryptocurrency such as Libra to
trade in a narrow band without a robust monetary policy. The presentation in
the main text of the paper is intentionally nontechnical. It is followed by an
extensive appendix with a mathematical description of the dynamics of
(crypto)currency exchange rates in target zones, mechanisms for keeping the
exchange rate from breaching the band, the role of volatility, etc.

We consider the problem of predicting human players' actions in repeated
strategic interactions. Our goal is to predict the dynamic step-by-step
behavior of individual players in previously unseen games. We study the ability
of neural networks to perform such predictions and the information that they
require. We show on a dataset of normal-form games from experiments with human
participants that standard neural networks are able to learn functions that
provide more accurate predictions of the players' actions than established
models from behavioral economics. The networks outperform the other models in
terms of prediction accuracy and cross-entropy, and yield higher economic
value. We show that if the available input is only of a short sequence of play,
economic information about the game is important for predicting behavior of
human agents. However, interestingly, we find that when the networks are
trained with long enough sequences of history of play, action-based networks do
well and additional economic details about the game do not improve their
performance, indicating that the sequence of actions encode sufficient
information for the success in the prediction task.

The paper argues that attracting more economists and adopting a more-precise
definition of dynamic complexity might help econophysics acquire more attention
in the economics community and bring new lymph to economic research. It may be
necessary to concentrate less on the applications than on the basics of
economic complexity, beginning with expansion and deepening of the study of
small systems with few interacting components, while until thus far complexity
has been assumed to be a prerogative of complicated systems only. It is
possible that without a thorough analysis at that level, the understanding of
systems that are at the same time complex and complicated will continue to
elude economics and econophysics research altogether. To that purpose, the
paper initiates and frames a definition of dynamic complexity grounded on the
concept of non-linear dynamical system.

The indirect transactions between sectors of an economic system has been a
long-standing open problem. There have been numerous attempts to conceptually
define and mathematically formulate this notion in various other scientific
fields in literature as well. The existing direct and indirect effects
formulations, however, can neither determine the direct and indirect
transactions separately nor quantify these transactions between two individual
sectors of interest in a multisectoral economic system. The novel concepts of
the direct, indirect and transfer (total) transactions between any two sectors
are introduced, and the corresponding requirements matrices and coefficients
are systematically formulated relative to both final demands and gross outputs
based on the system decomposition theory in the present manuscript. It is
demonstrated theoretically and through illustrative examples that the proposed
requirements matrices accurately define and correctly quantify the
corresponding direct, indirect, and total interactions and relationships. The
proposed requirements matrices for the US economy using aggregated input-output
tables for multiple years are then presented and briefly analyzed.

Multimodal agglomerations, in the form of the existence of many cities,
dominate modern economic geography. We focus on the mechanism by which
multimodal agglomerations realize endogenously. In a spatial model with
agglomeration and dispersion forces, the spatial scale (local or global) of the
dispersion force determines whether endogenous spatial distributions become
multimodal. Multimodal patterns can emerge only under a global dispersion
force, such as competition effects, which induce deviations to locations
distant from an existing agglomeration and result in a separate agglomeration.
A local dispersion force, such as the local scarcity of land, causes the
flattening of existing agglomerations. The resulting spatial configuration is
unimodal if such a force is the only source of dispersion. This view allows us
to categorize extant models into three prototypical classes: those with only
global, only local, and local and global dispersion forces. The taxonomy
facilitates model choice depending on each study's objective.

We propose a regularized factor-augmented vector autoregressive (FAVAR) model
that allows for sparsity in the factor loadings. In this framework, factors may
only load on a subset of variables which simplifies the factor identification
and their economic interpretation. We identify the factors in a data-driven
manner without imposing specific relations between the unobserved factors and
the underlying time series. Using our approach, the effects of structural
shocks can be investigated on economically meaningful factors and on all
observed time series included in the FAVAR model. We prove consistency for the
estimators of the factor loadings, the covariance matrix of the idiosyncratic
component, the factors, as well as the autoregressive parameters in the dynamic
model. In an empirical application, we investigate the effects of a monetary
policy shock on a broad range of economically relevant variables. We identify
this shock using a joint identification of the factor model and the structural
innovations in the VAR model. We find impulse response functions which are in
line with economic rationale, both on the factor aggregates and observed time
series level.

This is the first study that attempts to assess the regional economic impacts
of the European Institute of Innovation and Technology (EIT) investments in a
spatially explicit macroeconomic model, which allows us to take into account
all key direct, indirect and spatial spillover effects of EIT investments via
inter-regional trade and investment linkages and a spatial diffusion of
technology via an endogenously determined global knowledge frontier with
endogenous growth engines driven by investments in knowledge and human capital.
Our simulation results of highly detailed EIT expenditure data suggest that,
besides sizable direct effects in those regions that receive the EIT investment
support, there are also significant spatial spillover effects to other
(non-supported) EU regions. Taking into account all key indirect and spatial
spillover effects is a particular strength of the adopted spatial general
equilibrium methodology; our results suggest that they are important indeed and
need to be taken into account when assessing the impacts of EIT investment
policies on regional economies.

Satellite imagery has long been an attractive data source that provides a
wealth of information on human-inhabited areas. While super resolution
satellite images are rapidly becoming available, little study has focused on
how to extract meaningful information about human habitation patterns and
economic scales from such data. We present READ, a new approach for obtaining
essential spatial representation for any given district from high-resolution
satellite imagery based on deep neural networks. Our method combines transfer
learning and embedded statistics to efficiently learn critical spatial
characteristics of arbitrary size areas and represent them into a fixed-length
vector with minimal information loss. Even with a small set of labels, READ can
distinguish subtle differences between rural and urban areas and infer the
degree of urbanization. An extensive evaluation demonstrates the model
outperforms the state-of-the-art in predicting economic scales, such as
population density for South Korea (R^2=0.9617), and shows a high potential use
for developing countries where district-level economic scales are not known.

This paper proposes a hybrid Gaussian process (GP) approach to robust
economic model predictive control under unknown future disturbances in order to
reduce the conservatism of the controller. The proposed hybrid GP is a
combination of two well-known methods, namely, kernel composition and nonlinear
auto-regressive. A switching mechanism is employed to select one of these
methods for disturbance prediction after analyzing the prediction outcomes. The
hybrid GP is intended to detect not only patterns but also unexpected behaviors
in the unknown disturbances by using past disturbance measurements.
\textcolor{black}{A novel forgetting factor concept is also utilized in the
hybrid GP, giving less weight to older measurements, in order to increase
prediction accuracy based on recent disturbances values.} The detected
disturbance information is used to reduce prediction uncertainty in economic
model predictive controllers systematically. The simulation results show that
the proposed method can improve the overall performance of an economic model
predictive controller compared to other GP-based methods in cases when
disturbances have discernible patterns.

Economists are showing increasing interest in the use of text as an input to
economic research. Here, we analyse online text to construct a real time metric
of welfare. For purposes of description, we call it the Feel Good Factor (FGF).
The particular example used to illustrate the concept is confined to data from
the London area, but the methodology is readily generalisable to other
geographical areas. The FGF illustrates the use of online data to create a
measure of welfare which is not based, as GDP is, on value added in a
market-oriented economy. There is already a large literature which measures
wellbeing/happiness. But this relies on conventional survey approaches, and
hence on the stated preferences of respondents. In unstructured online media
text, users reveal their emotions in ways analogous to the principle of
revealed preference in consumer demand theory. The analysis of online media
offers further advantages over conventional survey-based measures of sentiment
or well-being. It can be carried out in real time rather than with the lags
which are involved in survey approaches. In addition, it is very much cheaper.

We develop a network reconstruction model based on entropy maximization
considering the sparsity of networks. We reconstruct the interbank network in
Japan from financial data in individual banks' balance sheets using the
developed reconstruction model from 2000 to 2016. The observed sparsity of the
interbank network is successfully reproduced. We examine the characteristics of
the reconstructed interbank network by calculating important network
attributes. We obtain the following characteristics, which are consistent with
the previously known stylized facts. Although we do not introduce the mechanism
to generate the core and peripheral structure, we impose the constraints to
consider the sparsity that is no transactions within the same bank category
except for major commercial banks, the core and peripheral structure has
spontaneously emerged. We identify major nodes in each community using the
value of PageRank and degree to examine the changing role of each bank
category. The observed changing role of banks is considered a result of the
quantitative and qualitative monetary easing policy started by the Bank of
Japan in April 2013.

We consider an economic geography model with two inter-regional proximity
structures: one governing goods trade and the other governing production
externalities across regions. We investigate how the introduction of the latter
affects the timing of endogenous agglomeration and the spatial distribution of
workers across regions. As transportation costs decline, the economy undergoes
a progressive dispersion process. Mono-centric agglomeration emerges when
inter-regional trade and/or production externalities incur high transportation
costs, while uniform dispersion occurs when these costs become negligibly small
(i.e., when distance dies). In multi-regional geography, the network structure
of production externalities can determine the geographical distribution of
workers as economic integration increases. If production externalities are
governed solely by geographical distance, a mono-centric spatial distribution
emerges in the form of suburbanization. However, if geographically distant
pairs of regions are connected through tight production linkages, multi-centric
spatial distribution can be sustainable.

We propose a novel explanation for classic international macro puzzles
regarding capital flows and portfolio investment, which builds on modern
macro-finance models of experience-based belief formation. Individual
experiences of past macroeconomic outcomes have been shown to exert a
long-lasting influence on beliefs about future realizations, and to explain
domestic stock-market investment. We argue that experience effects can explain
the tendency of investors to hold an over proportional fraction of their equity
wealth in domestic stocks (home bias), to invest in domestic equity markets in
periods of domestic crises (retrenchment), and to withdraw capital from foreign
equity markets in periods of foreign crises (fickleness). Experience-based
learning generates additional implications regarding the strength of these
puzzles in times of higher or lower economic activity and depending on the
demographic composition of market participants. We test and confirm these
predictions in the data.

Technological developments worldwide are contributing to the improvement of
transport infrastructures and they are helping to reduce the overall transport
costs. At the same time, such developments along with the reduction in
transport costs are affecting the spatial interdependence between the regions
and countries, a fact inducing significant effects on their economies and, in
general, on their growth-rates. A specific class of transport infrastructures
contributing significantly to overcoming the spatial constraints is the
airtransport infrastructures. Nowadays, the importance of air-transport
infrastructures in the economic development is determinative, especially for
the geographically isolated regions, such as for the island regions of Greece.
Within this context, this paper studies the Greek airports and particularly the
evolution of their overall transportation imprint, their geographical
distribution, and the volume of the transport activity of each airport. Also,
it discusses, in a broad context, the seasonality of the Greek airport
activity, the importance of the airports for the local and regional
development, and it formulates general conclusions.

Empirical distributions of wealth and income can be reproduced using
simplified agent-based models of economic interactions, analogous to
microscopic collisions of gas particles. Building upon these models of freely
interacting agents, we explore the effect of a segregated economic network in
which interactions are restricted to those between agents of similar wealth.
Agents on a 2D lattice undergo kinetic exchanges with their nearest neighbours,
while continuously switching places to minimize local wealth differences. A
spatial concentration of wealth leads to a steady state with increased global
inequality and a magnified distinction between local and global measures of
combatting poverty. Individual saving propensity proves ineffective in the
segregated economy, while redistributive taxation transcends the spatial
inhomogeneity and greatly reduces inequality. Adding fluctuations to the
segregation dynamics, we observe a sharp phase transition to lower inequality
at a critical temperature, accompanied by a sudden change in the distribution
of the wealthy elite.

This article attempts to highlight the importance that transportation has in
the economic development of Greece and in particular the importance of the
transportation infrastructure and transportation networks, which suggest a
fixed structured capital covering the total of the country. For this purpose,
longitudinal and cross-sectoral statistical data are examined over a set of
fundamental macroeconomic measures and metrics. Furthermore, the study attempts
to highlight the structural and functional aspects composing the concept of
transportation networks and to highlight the necessity of their joint
consideration on the relevant research. The transportation networks that are
examined in this paper are the Greek road (GRN), rail (GRAN), maritime (GMN)
and air transport network (GAN), which are studied both in terms of their
geometry and technical characteristics, as well as of their historical, traffic
and political framework. For the empirical assessment of the transportation
networks importance in Greece an econometric model is constructed, expressing
the welfare level of the Greek regions as a multivariate function of their
transportation infrastructure and of their socioeconomic environment. The
further purpose of the article is to highlight, macroscopically, all the
aspects related the study of transportation infrastructure and networks.

This survey develops a dual analysis, consisting, first, in a bibliometric
examination and, second, in a close literature review of all the scientific
production around cryptocurrencies conducted in economics so far. The aim of
this paper is twofold. On the one hand, proposes a methodological hybrid
approach to perform comprehensive literature reviews. On the other hand, we
provide an updated state of the art in cryptocurrency economic literature. Our
methodology emerges as relevant when the topic comprises a large number of
papers, that make unrealistic to perform a detailed reading of all the papers.
This dual perspective offers a full landscape of cryptocurrency economic
research. Firstly, by means of the distant reading provided by machine learning
bibliometric techniques, we are able to identify main topics, journals, key
authors, and other macro aggregates. Secondly, based on the information
provided by the previous stage, the traditional literature review provides a
closer look at methodologies, data sources and other details of the papers. In
this way, we offer a classification and analysis of the mounting research
produced in a relative short time span.

This study quantifies the economic effect of a possible lockdown of Tokyo to
prevent spread of COVID-19. The negative effect of the lockdown may propagate
to other regions through supply chains because of shortage of supply and
demand. Applying an agent-based model to the actual supply chains of nearly 1.6
million firms in Japan, we simulate what would happen to production activities
outside Tokyo when production activities that are not essential to citizens'
survival in Tokyo were shut down for a certain period. We find that when Tokyo
is locked down for a month, the indirect effect on other regions would be twice
as large as the direct effect on Tokyo, leading to a total production loss of
27 trillion yen in Japan, or 5.3% of its annual GDP. Although the production
shut down in Tokyo accounts for 21% of the total production in Japan, the
lockdown would result in a reduction of the daily production in Japan by 86% in
a month.

A data intermediary acquires signals from individual consumers regarding
their preferences. The intermediary resells the information in a product market
wherein firms and consumers tailor their choices to the demand data. The social
dimension of the individual data -- whereby a consumer's data are predictive of
others' behavior -- generates a data externality that can reduce the
intermediary's cost of acquiring the information. The intermediary optimally
preserves the privacy of consumers' identities if and only if doing so
increases social surplus. This policy enables the intermediary to capture the
total value of the information as the number of consumers becomes large.

This position paper discusses emerging behavioral, social, and economic
dynamics related to the COVID-19 pandemic and puts particular emphasis on two
emerging issues: First, delayed effects (or second strikes) of pandemics caused
by dread risk effects are discussed whereby two factors which might influence
the existence of such effects are identified, namely the accessibility of
(mis-)information and the effects of policy decisions on adaptive behavior.
Second, the issue of individual preparedness to hazardous events is discussed.
As events such as the COVID-19 pandemic unfolds complex behavioral patterns
which are hard to predict, sophisticated models which account for behavioral,
social, and economic dynamics are required to assess the effectivity and
efficiency of decision-making.

Economic Scenario Generators (ESGs) simulate economic and financial variables
forward in time for risk management and asset allocation purposes. It is often
not feasible to calibrate the dynamics of all variables within the ESG to
historical data alone. Calibration to forward-information such as future
scenarios and return expectations is needed for stress testing and portfolio
optimization, but no generally accepted methodology is available. This paper
introduces the Conditional Scenario Simulator, which is a framework for
consistently calibrating simulations and projections of economic and financial
variables both to historical data and forward-looking information. The
framework can be viewed as a multi-period, multi-factor generalization of the
Black-Litterman model, and can embed a wide array of financial and
macroeconomic models. Two practical examples demonstrate this in a frequentist
and Bayesian setting.

Systems engineering approaches use high-level models to capture the
architecture and behavior of the system. However, when safety engineers conduct
safety and reliability analysis, they have to create formal models, such as
fault-trees, according to the behavior described by the high-level engineering
models and environmental/fault assumptions. Instead of creating low-level
analysis models, our approach builds on engineering models in safety analysis
by exploiting the simulation capabilities of recent probabilistic programming
and simulation advancements. Thus, it could be applied in accordance with
standards and best practices for the analysis of a critical automotive system
as part of an industrial collaboration, while leveraging high-level block
diagrams and statechart models created by engineers. We demonstrate the
applicability of our approach in a case study adapted from the automotive
system from the collaboration.

We develop a mathematical framework to study the economic impact of
infectious diseases by integrating epidemiological dynamics with a kinetic
model of wealth exchange. The multi-agent description leads to study the
evolution over time of a system of kinetic equations for the wealth densities
of susceptible, infectious and recovered individuals, whose proportions are
driven by a classical compartmental model in epidemiology. Explicit
calculations show that the spread of the disease seriously affects the
distribution of wealth, which, unlike the situation in the absence of
epidemics, can converge towards a stationary state with a bimodal form.
Furthermore, simulations confirm the ability of the model to describe different
phenomena characteristics of economic trends in situations compromised by the
rapid spread of an epidemic, such as the unequal impact on the various wealth
classes and the risk of a shrinking middle class.

How much and when should we limit economic and social activity to ensure that
the health-care system is not overwhelmed during an epidemic? We study a
setting where ICU resources are constrained while suppression is costly (e.g.,
limiting economic interaction). Providing a fully analytical solution we show
that the common wisdom of "flattening the curve", where suppression measures
are continuously taken to hold down the spread throughout the epidemic, is
suboptimal. Instead, the optimal suppression is discontinuous. The epidemic
should be left unregulated in a first phase and when the ICU constraint is
approaching society should quickly lock down (a discontinuity). After the
lockdown regulation should gradually be lifted, holding the rate of infected
constant thus respecting the ICU resources while not unnecessarily limiting
economic activity. In a final phase, regulation is lifted. We call this
strategy "filling the box".

We investigate structural change in the PR China during a period of
particularly rapid growth 1998-2014. For this, we utilize sectoral data from
the World Input-Output Database and firm-level data from the Chinese Industrial
Enterprise Database. Starting with correlation laws known from the literature
(Fabricant's laws), we investigate which empirical regularities hold at the
sectoral level and show that many of these correlations cannot be recovered at
the firm level. For a more detailed analysis, we propose a multi-level
framework, which is validated with empirically. For this, we perform a robust
regression, since various input variables at the firm-level as well as the
residuals of exploratory OLS regressions are found to be heavy-tailed. We
conclude that Fabricant's laws and other regularities are primarily
characteristics of the sectoral level which rely on aspects like
infrastructure, technology level, innovation capabilities, and the knowledge
base of the relevant labor force. We illustrate our analysis by showing the
development of some of the larger sectors in detail and offer some policy
implications in the context of development economics, evolutionary economics,
and industrial organization.

This paper uses transaction data from a large bank in Scandinavia to estimate
the effect of social distancing laws on consumer spending in the COVID-19
pandemic. The analysis exploits a natural experiment to disentangle the effects
of the virus and the laws aiming to contain it: Denmark and Sweden were
similarly exposed to the pandemic but only Denmark imposed significant
restrictions on social and economic activities. We estimate that aggregate
spending dropped by around 25 percent in Sweden and, as a result of the
shutdown, by 4 additional percentage points in Denmark. This implies that most
of the economic contraction is caused by the virus itself and occurs regardless
of social distancing laws. The age gradient in the estimates suggest that
social distancing reinforces the virus-induced drop in spending for low
health-risk individuals but attenuates it for high-risk individuals by lowering
the overall prevalence of the virus in the society.

We develop a result on expected posteriors for Bayesians with heterogenous
priors, dubbed information validates the prior (IVP). Under familiar ordering
requirements, Anne expects a (Blackwell) more informative experiment to bring
Bob's posterior mean closer to Anne's prior mean. We apply the result in two
contexts of games of asymmetric information: voluntary testing or
certification, and costly signaling or falsification. IVP can be used to
determine how an agent's behavior responds to additional exogenous or
endogenous information. We discuss economic implications.

We argue that recent developments in proof-of-work consensus mechanisms can
be used in accordance with advancements in formal verification techniques to
build a distributed payment protocol that addresses important economic
drawbacks from cost efficiency, scalability and adaptablity common to current
decentralized record-keeping systems. We enable the protocol to autonomously
adjust system throughput according to a feasibly computable statistic - system
difficulty. We then provide a formal economic analysis of a decentralized
market place for record-keeping that is consistent with our protocol design and
show that, when block rewards are zero, the system admits stable,
self-regulating levels of transaction fees and wait-times across varying levels
of demand. We also provide an analysis of the various technological
requirements needed to instantiate such a system in a commercially viable
setting, and identify relevant research directions.

Social mobility captures the extent to which socio-economic status of
children, is independent of status of their respective parents. In order to
measure social mobility, most widely used indicators of socio-economic status
are income, education and occupation. While social mobility measurement based
on income is less contested, data availability in Indian context limits us to
observing mobility patterns along the dimensions of either education or
occupation. In this study we observe social mobility patterns for different
social groups along these two main dimensions, and find that while upward and
downward mobility prospects in education for SCs/STs is somewhat improving in
the recent times, occupational mobility patterns are rather worrisome. These
results motivate the need for reconciling disparate trends along education and
occupation, in order to get a more comprehensive picture of social mobility in
the country.

Mass public quarantining, colloquially known as a lock-down, is a
non-pharmaceutical intervention to check spread of disease. This paper presents
ESOP (Epidemiologically and Socio-economically Optimal Policies), a novel
application of active machine learning techniques using Bayesian optimization,
that interacts with an epidemiological model to arrive at lock-down schedules
that optimally balance public health benefits and socio-economic downsides of
reduced economic activity during lock-down periods. The utility of ESOP is
demonstrated using case studies with VIPER
(Virus-Individual-Policy-EnviRonment), a stochastic agent-based simulator that
this paper also proposes. However, ESOP is flexible enough to interact with
arbitrary epidemiological simulators in a black-box manner, and produce
schedules that involve multiple phases of lock-downs.

Integrating renewable energy production into the electricity grid is an
important policy goal to address climate change. However, such an integration
faces economic and technological challenges. As power generation by renewable
sources increases, power transmission patterns over the electric grid change.
Due to physical laws, these new transmission patterns lead to non-intuitive
grid congestion externalities. We derive the conditions under which negative
network externalities due to power trades occur. Calibration using a stylized
framework and data from Europe shows that each additional unit of power traded
between northern and western Europe reduces transmission capacity for the
southern and eastern regions by 27% per unit traded. Such externalities suggest
that new investments in the electric grid infrastructure cannot be made
piecemeal. In our example, power infrastructure investment in northern and
western Europe needs an accompanying investment in southern and eastern Europe
as well. An economic challenge is regions facing externalities do not always
have the financial ability to invest in infrastructure. Power transit fares can
help finance power infrastructure investment in regions facing network
congestion externalities. The resulting investment in the overall electricity
grid facilitates integration of renewable energy production.

Attitudes toward risk underlie virtually every important economic decision an
individual makes. In this experimental study, I examine how introducing a time
delay into the execution of an investment plan influences individuals' risk
preferences. The field experiment proceeded in three stages: a decision stage,
an execution stage and a payout stage. At the outset, in the Decision Stage
(Stage 1), each subject was asked to make an investment plan by splitting a
monetary investment amount between a risky asset and a safe asset. Subjects
were informed that the investment plans they made in the Decision Stage are
binding and will be executed during the Execution Stage (Stage 2). The Payout
Stage (Stage 3) was the payout date. The timing of the Decision Stage and
Payout Stage was the same for each subject, but the timing of the Execution
Stage varied experimentally. I find that individuals who were assigned to
execute their investment plans later (i.e., for whom there was a greater delay
prior to the Execution Stage) invested a greater amount in the risky asset
during the Decision Stage.

Evidence on educational returns and the factors that determine the demand for
schooling in developing countries is extremely scarce. Building on previous
studies that show individuals underestimating the returns to schooling, we use
two surveys from Tanzania to estimate both the actual and perceived schooling
returns and subsequently examine what factors drive individual misperceptions
regarding actual returns. Using ordinary least squares and instrumental
variable methods, we find that each additional year of schooling in Tanzania
increases earnings, on average, by 9 to 11 percent. We find that on average
individuals underestimate returns to schooling by 74 to 79 percent and three
factors are associated with these misperceptions: income, asset poverty and
educational attainment. Shedding light on what factors relate to individual
beliefs about educational returns can inform policy on how to structure
effective interventions in order to correct individual misperceptions.

Against the background of renewed interest in vertical support policies
targeting specific industries or technologies, we investigate the effects of
vertical vs. horizontal policies in a combinatorial model of economic
development. In the framework we propose, an economy develops by acquiring new
capabilities allowing for the production of an ever greater variety of products
with an increasing complexity. Innovation policy can aim to expand the number
of capabilities (vertical policy) or the ability to combine capabilities
(horizontal policy). The model shows that for low-income countries, the two
policies are complementary. For high-income countries that are specialised in
the most complex products, focusing on horizontal policy only yields the
highest returns. We reflect on the model results in the light of the
contemporary debate on vertical policy.

At the end of 2012 the International Monetary Fund (IMF) has suspended its
financial assistance to the Democratic Republic of the Congo (DRC). Due to
inflationary pressures which occurred in the last quarter of 2016, several
decision-makers called for a reopening of a formal cooperation with the IMF.
This process was formally completed in December 2019. The restart of IMF
programs was greeted with satisfaction by politicians and widely commented in
the media. However, recent history shows that the DRC managed to achieve
exceptional economic performance, between 2012 and 2016, without being in a
formal cooperation with the IMF. Some people wonder whether IMF assistance is a
curse for recipient countries? We argue that the underlying problem has nothing
to do with accepting or not the IMF assistance, but rather in the ability of
policy makers to establish effective leadership and good governance for the
development and implementation supporting structural reforms.

We consider a possibility of socioeconomic collapse caused by the spread of
epidemic in a basic dynamical model with negative feedback between the infected
population size and a formal collective economic resource. The
epidemic-resource coupling is supposed to be of activation type, with the
recovery rate governed by the Arrhenius-like law and resource playing the role
of temperature. Such a coupling can result in the collapsing effect opposite to
thermal explosion because of the limited resource. In this case, the system can
no longer stabilize and return to the stable pre- or post-epidemic states. We
demonstrate that such a collapse can partially be mitigated by means of a
negative resource or debt.

Economic theory has provided an estimable intuition in understanding the
perplexing ideologies in law, in the areas of economic law, tort law, contract
law, procedural law and many others. Most legal systems require the parties
involved in a legal dispute to exchange information through a process called
discovery. The purpose is to reduce the relative optimisms developed by
asymmetric information between the parties. Like a head or tail phenomenon in
stochastic processes, uncertainty in the adjudication affects the decisions of
the parties in a legal negotiation. This paper therefore applies the principles
of aleatory analysis to determine how negotiations fail in the legal process,
introduce the axiological concept of optimal transaction cost and formulates a
numerical methodology based on backwards induction and stochastic options
pricing economics in estimating the reasonable and fair bargain in order to
induce settlements thereby increasing efficiency and reducing social costs.

With non-controllable auto-regressive shocks, the welfare of Ramsey optimal
policy is the solution of a single Riccati equation of a linear quadratic
regulator. The existing theory by Hansen and Sargent (2007) refers to an
additional Sylvester equation but miss another equation for computing the block
matrix weighting the square of non-controllable variables in the welfare
function. There is no need to simulate impulse response functions over a long
period, to compute period loss functions and to sum their discounted value over
this long period, as currently done so far. Welfare is computed for the case of
the new-Keynesian Phillips curve with an auto-regressive cost-push shock. JEL
classification numbers: C61, C62, C73, E47, E52, E61, E63.

Entering and exiting the Pandemic Recession, I study the high-frequency
real-activity signals provided by a leading nowcast, the ADS Index of Business
Conditions produced and released in real time by the Federal Reserve Bank of
Philadelphia. I track the evolution of real-time vintage beliefs and compare
them to a later-vintage chronology. Real-time ADS plunges and then swings as
its underlying economic indicators swing, but the ADS paths quickly converge to
indicate a return to brisk positive growth by mid-May. I show, moreover, that
the daily real activity path was highly correlated with the daily COVID-19
cases. Finally, I provide a comparative assessment of the real-time ADS signals
provided when exiting the Great Recession.

This chapter examines how positivity and order play out in two important
questions in mathematical economics, and in so doing, subjects the postulates
of continuity, additivity and monotonicity to closer scrutiny. Two sets of
results are offered: the first departs from Eilenberg's (1941) necessary and
sufficient conditions on the topology under which an anti-symmetric, complete,
transitive and continuous binary relation exists on a topologically connected
space; and the second, from DeGroot's (1970) result concerning an additivity
postulate that ensures a complete binary relation on a {\sigma}-algebra to be
transitive. These results are framed in the registers of order, topology,
algebra and measure-theory; and also beyond mathematics in economics: the
exploitation of Villegas' notion of monotonic continuity by Arrow-Chichilnisky
in the context of Savage's theorem in decision theory, and the extension of
Diamond's impossibility result in social choice theory by Basu-Mitra. As such,
this chapter has a synthetic and expository motivation, and can be read as a
plea for inter-disciplinary conversations, connections and collaboration.

This paper aims to review the different impacts of income inequality drivers
on the Gini coefficient, depending on institutional specificities. In this
context, we divided the European Union member states in two clusters (the
cluster of member states with inclusive institutions / extractive institutions)
using the institutional pillar as a clustering criterion. In both cases, we
assesed the impact of income inequality drivers on Gini coefficient by using a
fixed effects model in order to examine the role and importance of the
institutions in the dynamics of income disparities.The models were estimated by
applying the Panel Estimated Generalized Least Squares (EGLS) method, this
being weighted by Cross-section weights option. The separate assessment of the
income inequality reactivity to the change in its determinants according to the
institutional criterion represents a new approach in this field of research and
the results show that the impact of moderating income inequality strategies is
limitedin the case of member states with extractive institutions.

In this paper I discuss truthful equilibria in common agency models.
Specifically, I provide general conditions under which truthful equilibria are
plausible, easy to calculate and efficient. These conditions generalize similar
results in the literature and allow the use of truthful equilibria in novel
economic applications. Moreover, I provide two such applications. The first
application is a market game in which multiple sellers sell a uniform good to a
single buyer. The second application is a lobbying model in which there are
externalities in contributions between lobbies. This last example indicates
that externalities between principals do not necessarily prevent efficient
equilibria. In this regard, this paper provides a set of conditions, under
which, truthful equilibria in common agency models with externalities are
efficient.

Technological change is essential to balance economic growth and
environmental sustainability. This study documents energy-saving technological
change to understand the trends and differences therein in OECD countries. We
estimate sector-level production functions with factor-augmenting technologies
using cross-country and cross-industry panel data and shift-share instruments,
thereby measuring energy-saving technological change for each country and
sector. Our results show how the levels and growth rates of energy-saving
technology vary across countries, sectors, and time. In addition, we evaluate
the extent to which factor-augmenting technologies contribute to economic
growth and how this contribution differs across countries and sectors.

Global historical series spanning the last two centuries recently became
available for primary energy consumption (PEC) and Gross Domestic Product
(GDP). Based on a thorough analysis of the data, we propose a new, simple
macroeconomic model whereby physical power is fueling economic power. From 1820
to 1920, the linearity between global PEC and world GDP justifies basic
equations where, originally, PEC incorporates unskilled human labor that
consumes and converts energy from food. In a consistent model, both physical
capital and human capital are fed by PEC and represent a form of stored energy.
In the following century, from 1920 to 2016, GDP grows quicker than PEC.
Periods of quasi-linearity of the two variables are separated by distinct
jumps, which can be interpreted as radical technology shifts. The GDP to PEC
ratio accumulates game-changing innovation, at an average growth rate
proportional to PEC. These results seed alternative strategies for modeling and
for political management of the climate crisis and the energy transition.

During a pandemic, there are conflicting demands arising from public health
and economic cost. Lockdowns are a common way of containing infections, but
they adversely affect the economy. We study the question of how to minimise the
economic damage of a lockdown while still containing infections. Our analysis
is based on the SIR model, which we analyse using a clock set by the virus.
This use of the "virus time" permits a clean mathematical formulation of our
problem. We optimise the economic cost for a fixed health cost and arrive at a
strategy for navigating the pandemic. This involves adjusting the level of
lockdowns in a controlled manner so as to minimise the economic cost.

Data are invaluable. How can we assess the value of data objectively,
systematically and quantitatively? Pricing data, or information goods in
general, has been studied and practiced in dispersed areas and principles, such
as economics, marketing, electronic commerce, data management, data mining and
machine learning. In this article, we present a unified, interdisciplinary and
comprehensive overview of this important direction. We examine various
motivations behind data pricing, understand the economics of data pricing and
review the development and evolution of pricing models according to a series of
fundamental principles. We discuss both digital products and data products. We
also consider a series of challenges and directions for future work.

This paper uses a new textual data index for predicting stock market data.
The index is applied to a large set of news to evaluate the importance of one
or more general economic-related keywords appearing in the text. The index
assesses the importance of the economic-related keywords, based on their
frequency of use and semantic network position. We apply it to the Italian
press and construct indices to predict Italian stock and bond market returns
and volatilities in a recent sample period, including the COVID-19 crisis. The
evidence shows that the index captures the different phases of financial time
series well. Moreover, results indicate strong evidence of predictability for
bond market data, both returns and volatilities, short and long maturities, and
stock market volatility.

Reliable data about the stock of physical capital and infrastructure in
developing countries is typically very scarce. This is particular a problem for
data at the subnational level where existing data is often outdated, not
consistently measured or coverage is incomplete. Traditional data collection
methods are time and labor-intensive costly, which often prohibits developing
countries from collecting this type of data. This paper proposes a novel method
to extract infrastructure features from high-resolution satellite images. We
collected high-resolution satellite images for 5 million 1km $\times$ 1km grid
cells covering 21 African countries. We contribute to the growing body of
literature in this area by training our machine learning algorithm on
ground-truth data. We show that our approach strongly improves the predictive
accuracy. Our methodology can build the foundation to then predict subnational
indicators of economic development for areas where this data is either missing
or unreliable.

Regional promotion and centralized correction in Tokyo have long been the
goals of the Government of Japan. Furthermore, in the wake of the recent new
coronavirus (COVID-19) epidemic, the momentum for rural migration is
increasing, to prevent the risk of infection with the help of penetration of
remote work. However, there is not enough debate about what kind of land will
attract the population. Therefore, in this paper, we will consider this problem
by performing correlation analysis and multiple regression analysis with the
inflow rate and the excess inflow rate of the population as the dependent
variables, using recent government statistics for each prefecture. As a result
of the analysis, in addition to economic factor variables, variables of
climatic, amenity, and human factors correlated with the inflow rate, and it
was shown that the model has the greatest explanatory power when multiple
factors were used in addition to specific factors. Therefore, local prefectures
are required to take regional promotion measures focusing on not only economic
factors but also multifaceted factors to attract the outside population.

Economic growth is measured as the rate of relative change in gross domestic
product (GDP) per capita. Yet, when incomes follow random multiplicative
growth, the ensemble-average (GDP per capita) growth rate is higher than the
time-average growth rate achieved by each individual in the long run. This
mathematical fact is the starting point of ergodicity economics. Using the
atypically high ensemble-average growth rate as the principal growth measure
creates an incomplete picture. Policymaking would be better informed by
reporting both ensemble-average and time-average growth rates. We analyse
rigorously these growth rates and describe their evolution in the United States
and France over the last fifty years. The difference between the two growth
rates gives rise to a natural measure of income inequality, equal to the mean
logarithmic deviation. Despite being estimated as the average of individual
income growth rates, the time-average growth rate is independent of income
mobility.

This paper presents preliminary summary results from a longitudinal study of
participants in seven U.S. states during the COVID-19 pandemic. In addition to
standard socio-economic characteristics, we collect data on various economic
preference parameters: time, risk, and social preferences, and risk perception
biases. We pay special attention to predictors that are both important drivers
of social distancing and are potentially malleable and susceptible to policy
levers. We note three important findings: (1) demographic characteristics exert
the largest influence on social distancing measures and mask-wearing, (2) we
show that individual risk perception and cognitive biases exert a critical role
in influencing the decision to adopt social distancing measures, (3) we
identify important demographic groups that are most susceptible to changing
their social distancing behaviors. These findings can help inform the design of
policy interventions regarding targeting specific demographic groups, which can
help reduce the transmission speed of the COVID-19 virus.

This paper investigates the impact of economic policy uncertainty (EPU) on
the crash risk of US stock market during the COVID-19 pandemic. To this end, we
use the GARCH-S (GARCH with skewness) model to estimate daily skewness as a
proxy for the stock market crash risk. The empirical results show the
significantly negative correlation between EPU and stock market crash risk,
indicating the aggravation of EPU increase the crash risk. Moreover, the
negative correlation gets stronger after the global COVID-19 outbreak, which
shows the crash risk of the US stock market will be more affected by EPU during
the pandemic.

This article identifies how scarcity, abundance, and sufficiency influence
exchange behavior. Analyzing the mechanisms governing exchange of resources
constitutes the foundation of several social-science perspectives. Neoclassical
economics provides one of the most well-known perspectives of how rational
individuals allocate and exchange resources. Using Rational Choice Theory
(RCT), neoclassical economics assumes that exchange between two individuals
will occur when resources are scarce and that these individuals interact
rationally to satisfy their requirements (i.e., preferences). While RCT is
useful to characterize interaction in closed and stylized systems, it proves
insufficient to capture social and psychological reality where culture,
emotions, and habits play an integral part in resource exchange. Social
Resource Theory (SRT) improves on RCT in several respects by making the social
nature of resources the object of study. SRT shows how human interaction is
driven by an array of psychological mechanisms, from emotions to heuristics.
Thus, SRT provides a more realistic foundation for analyzing and explaining
social exchange than the stylized instrumental rationality of RCT. Yet SRT has
no clear place for events of abundance and sufficiency as additional
motivations to exchange resources. This article synthesize and formalize a
foundation for SRT using not only scarcity but also abundance and sufficiency.

This work is dedicated to finding the determinants of voting behavior in
Poland at the poviat level. 2019 parliamentary election has been analyzed and
an attempt to explain vote share for the winning party (Law and Justice) has
been made. Sentiment analysis of tweets in Polish (original) and English
(machine-translations), collected in the period around the election, has been
applied. Amid multiple machine learning approaches tested, the best
classification accuracy has been achieved by Huggingface BERT on
machine-translated tweets. OLS regression, with sentiment of tweets and
selected socio-economic features as independent variables, has been utilized to
explain Law and Justice vote share in poviats. Sentiment of tweets has been
found to be a significant predictor, as stipulated by the literature of the
field.

To design, evaluate and tune policies for all-inclusive human development,
the primary requisite is to assess the true state of affairs of the society.
Statistical indices like GDP, Gini Coefficients have been developed to
accomplish the evaluation of the socio-economic systems. They have remained
prevalent in the conventional economic theories but little do they have in the
offing regarding true well-being and development of humans. Human Development
Index (HDI) and thereafter Inequality-adjusted Human Development Index (IHDI)
has been the path changing composite-index having the focus on human
development. However, even though its fundamental philosophy has an
all-inclusive human development focus, the composite-indices appear to be
unable to grasp the actual assessment in several scenarios. This happens due to
the dynamic non-linearity of social-systems where superposition principle
cannot be applied between all of its inputs and outputs of the system as the
system's own attributes get altered upon each input. We would discuss the
apparent shortcomings and probable refinement of the existing index using an
agent based computational system model approach.

Methods and applications are inextricably linked in science, and in
particular in the domain of text-as-data. In this paper, we examine one such
text-as-data application, an established economic index that measures economic
policy uncertainty from keyword occurrences in news. This index, which is shown
to correlate with firm investment, employment, and excess market returns, has
had substantive impact in both the private sector and academia. Yet, as we
revisit and extend the original authors' annotations and text measurements we
find interesting text-as-data methodological research questions: (1) Are
annotator disagreements a reflection of ambiguity in language? (2) Do
alternative text measurements correlate with one another and with measures of
external predictive validity? We find for this application (1) some annotator
disagreements of economic policy uncertainty can be attributed to ambiguity in
language, and (2) switching measurements from keyword-matching to supervised
machine learning classifiers results in low correlation, a concerning
implication for the validity of the index.

The current knowledge system of macroeconomics is built on interactions among
a small number of variables, since traditional macroeconomic models can mostly
handle a handful of inputs. Recent work using big data suggests that a much
larger number of variables are active in driving the dynamics of the aggregate
economy. In this paper, we introduce a knowledge graph (KG) that consists of
not only linkages between traditional economic variables but also new
alternative big data variables. We extract these new variables and the linkages
by applying advanced natural language processing (NLP) tools on the massive
textual data of academic literature and research reports. As one example of the
potential applications, we use it as the prior knowledge to select variables
for economic forecasting models in macroeconomics. Compared to statistical
variable selection methods, KG-based methods achieve significantly higher
forecasting accuracy, especially for long run forecasts.

This work examines the economic benefits of learning a new skill from a
different domain: cross-skilling. To assess this, a network of skills from the
job profiles of 14,790 online freelancers is constructed. Based on this skill
network, relationships between 3,480 different skills are revealed and marginal
effects of learning a new skill can be calculated via workers' wages. The
results indicate that learning in-demand skills, such as popular programming
languages, is beneficial in general, and that diverse skill sets tend to be
profitable, too. However, the economic benefit of a new skill is individual, as
it complements the existing skill bundle of each worker. As technological and
social transformation is reshuffling jobs' task profiles at a fast pace, the
findings of this study help to clarify skill sets required for designing
individual re-skilling pathways. This can help to increase employability and
reduce labour market shortages.

The quest of this work is to present discussions of some fundamental
questions of economics in the era of quantum technology, which require a
treatment different from economics studied thus far in the literature. A study
of quantum economic behavior will become the center of attention of economists
in the coming decades. We analyze a quantum economy in which players produce
and consume quantum goods. They meet randomly and barter with neighbors
bilaterally for quantum goods they produced. We clarify the conditions where
certain quantum goods emerge endogenously as media of exchange, called quantum
commodity money. As quantum strategies are entangled, we find distinctive
aspects of quantum games that cannot be explained by conventional classical
games. In some situations a quantum player can acquire a quantum good from
people regardless of their strategies, while on the other hand people can find
quantum strategies that improve their welfare based on an agreement. Those
novel properties imply that quantum games also shed new light on theories of
mechanism design, auction and contract in the quantum era.

I study the economic effects of testing during the outbreak of a novel
disease. I propose a model where testing permits isolation of the infected and
provides agents with information about the prevalence and lethality of the
disease. Additional testing reduces the perceived lethality of the disease, but
might increase the perceived risk of infection. As a result, more testing could
increase the perceived risk of dying from the disease - i.e. "stoke fear" - and
cause a fall in economic activity, despite improving health outcomes. Two main
insights emerge. First, increased testing is beneficial to the economy and pays
for itself if performed at a sufficiently large scale, but not necessarily
otherwise. Second, heterogeneous risk perceptions across age-groups can have
important aggregate consequences. For a SARS-CoV-2 calibration of the model,
heterogeneous risk perceptions across young and old individuals mitigate GDP
losses by 50% and reduce the death toll by 30% relative to a scenario in which
all individuals have the same perceptions of risk.

To battle with economic challenges during the COVID-19 pandemic, the US
government implemented various measures to mitigate economic loss. From
issuance of stimulus checks to reopening businesses, consumers had to
constantly alter their behavior in response to government policies. Using
anonymized card transactions and mobile device-based location tracking data, we
analyze the factors that contribute to these behavior changes, focusing on
stimulus check issuance and state-wide reopening. Our finding suggests that
stimulus payment has a significant immediate effect of boosting spending, but
it typically does not reverse a downward trend. State-wide reopening had a
small effect on spending. Foot traffic increased gradually after stimulus check
issuance, but only increased slightly after reopening, which also coincided or
preceded several policy changes and confounding events (e.g., protests) in the
US. We also find differences in the reaction to these policies in different
regions in the US. Our results may be used to inform future economic recovery
policies and their potential consumer response.

This study presents for the first time the SWB-J index, a subjective
well-being indicator for Japan based on Twitter data. The index is composed by
eight dimensions of subjective well-being and is estimated relying on Twitter
data by using human supervised sentiment analysis. The index is then compared
with the analogous SWB-I index for Italy, in order to verify possible analogies
and cultural differences. Further, through structural equation models, a causal
assumption is tested to see whether the economic and health conditions of the
country influence the well-being latent variable and how this latent dimension
affects the SWB-J and SWB-I indicators. It turns out that, as expected, the
economic and health welfare is only one aspect of the multidimensional
well-being that is captured by the Twitter-based indicator.

A Renewable Energy and Materials Economy (REME) is proposed as the solution
to the climate change threat. REME mimics nature to produce carbon neutral
liquid fuels and chemicals as well as carbon negative materials by using water,
CO$_2$ from the atmosphere and renewable energy as inputs. By being in harmony
with nature REME has a positive feedback between economic development and
climate change protection. In REME the feedback driven accelerated rate of
economic growth enables the climate change threat to be addressed in a timely
manner. It is also cost-effective protection because it sequesters by
monetizing the carbon removed from the air in carbon-based building materials.
Thus, addressing the climate change threat is not a cost to the economy but a
result of REME driven prosperity.

Toward explaining the persistence of biased inferences, we propose a
framework to evaluate competing (mis)specifications in strategic settings.
Agents with heterogeneous (mis)specifications coexist and draw Bayesian
inferences about their environment through repeated play. The relative
stability of (mis)specifications depends on their adherents' equilibrium
payoffs. A key mechanism is the learning channel: the endogeneity of perceived
best replies due to inference. We characterize when a rational society is only
vulnerable to invasion by some misspecification through the learning channel.
The learning channel leads to new stability phenomena, and can confer an
evolutionary advantage to otherwise detrimental biases in economically relevant
applications.

The transformation of the electricity sector is a main element of the
transition to a decarbonized economy. Conventional generators powered by fossil
fuels have to be replaced by variable renewable energy (VRE) sources in
combination with electricity storage and other options for providing temporal
flexibility. We discuss the market dynamics of increasing VRE penetration and
their integration in the electricity system. We describe the merit-order effect
(decline of wholesale electricity prices as VRE penetration increases) and the
cannibalization effect (decline of VRE value as their penetration increases).
We further review the role of electricity storage and other flexibility options
for integrating variable renewables, and how storage can contribute to
mitigating the two mentioned effects. We also use a stylized open-source model
to provide some graphical intuition on this. While relatively high shares of
VRE are achievable with moderate amounts of electricity storage, the role of
long-term storage increases as the VRE share approaches 100%.

Indeed, the global production (as a system of creating values) is eventually
forming like a gigantic and complex network/web of value chains that explains
the transitional structures of global trade and development of the global
economy. It's truly a new wave of globalisation, and we term it as the global
value chains (GVCs), creating the nexus among firms, workers and consumers
around the globe. The emergence of this new scenario asks: how an economy's
firms, producers and workers connect in the global economy. And how are they
capturing the gains out of it in terms of different dimensions of economic
development? This GVC approach is very crucial for understanding the
organisation of the global industries and firms. It requires the statics and
dynamics of diverse players involved in this complex global production network.
Its broad notion deals with different global issues (including regional value
chains also) from the top down to the bottom up, founding a scope for policy
analysis (Gereffi & Fernandez-Stark 2011). But it is true that, as Feenstra
(1998) points out, any single computational framework is not sufficient to
quantification this whole range of economic activities. We should adopt an
integrative framework for accurate projection of this dynamic multidimensional
phenomenon.

Bounce Back Loan is amongst a number of UK business financial support schemes
launched by UK Government in 2020 amidst pandemic lockdown. Through these
schemes, struggling businesses are provided financial support to weather
economic slowdown from pandemic lockdown. {\pounds}43.5bn loan value has been
provided as of 17th Dec2020. However, with no major checks for granting these
loans and looming prospect of loan losses from write-offs from failed
businesses and fraud, this paper theorizes prospect of applying spatiotemporal
modelling technique to explore if geospatial patterns and temporal analysis
could aid design of loan grant criteria for schemes. Application of Clustering
and Visual Analytics framework to business demographics, survival rate and
Sector concentration shows Inner and Outer London spatial patterns which
historic business failures and reversal of the patterns under COVID-19 implying
sector influence on spatial clusters. Combination of unsupervised clustering
technique with multinomial logistic regression modelling on research datasets
complimented by additional datasets on other support schemes, business
structure and financial crime, is recommended for modelling business
vulnerability to certain types of financial market or economic condition. The
limitations of clustering technique for high dimensional is discussed along
with relevance of an applicable model for continuing the research through next
steps.

Decentralized Finance (DeFi), a blockchain powered peer-to-peer financial
system, is mushrooming. Two years ago the total value locked in DeFi systems
was approximately 700m USD, now, as of April 2022, it stands at around 150bn
USD. The frenetic evolution of the ecosystem has created challenges in
understanding the basic principles of these systems and their security risks.
In this Systematization of Knowledge (SoK) we delineate the DeFi ecosystem
along the following axes: its primitives, its operational protocol types and
its security. We provide a distinction between technical security, which has a
healthy literature, and economic security, which is largely unexplored,
connecting the latter with new models and thereby synthesizing insights from
computer science, economics and finance. Finally, we outline the open research
challenges in the ecosystem across these security types.

This paper examines deposits of individuals ("retail") and large companies
("wholesale") in the U.S. banking industry, and how these deposit types are
impacted by macroeconomic factors, such as quantitative easing (QE). Actual
data for deposits by holder are unavailable. We use a dataset on banks'
financial information and probabilistic generative model to predict industry
retail-wholesale deposit split from 2000 to 2020. Our model assumes account
balances arise from separate retail and wholesale lognormal distributions and
fit parameters of distributions by minimizing error between actual bank metrics
and simulated metrics using the model's generative process. We use time-series
regression to forward predict retail-wholesale deposits as function of loans,
retail loans, and reserve balances at Fed banks. We find increase in reserves
(representing QE) increases wholesale but not retail deposits, and increase in
loans increase both wholesale and retail deposits evenly. The result shows that
QE following the 2008 financial crisis benefited large companies more than
average individuals, a relevant finding for economic decision making. In
addition, this work benefits bank management strategy by providing forecasting
capability for retail-wholesale deposits.

We consider optimal intervention in the Elliott-Golub-Jackson network model
\cite{jackson14} and we show that it can be transformed into an influence
maximization-like form, interpreted as the reverse of a default cascade. Our
analysis of the optimal intervention problem extends well-established targeting
results to the economic network setting, which requires additional theoretical
steps. We prove several results about optimal intervention: it is NP-hard and
cannot be approximated to a constant factor in polynomial time. In turn, we
show that randomizing failure thresholds leads to a version of the problem
which is monotone submodular, for which existing powerful approximations in
polynomial time can be applied. In addition to optimal intervention, we also
show practical consequences of our analysis to other economic network problems:
(1) it is computationally hard to calculate expected values in the economic
network, and (2) influence maximization algorithms can enable efficient
importance sampling and stress testing of large failure scenarios. We
illustrate our results on a network of firms connected through input-output
linkages inferred from the World Input Output Database.

The global production (as a system of creating values) is eventually forming
a vast web of value chains that explains the transitional structures of global
trade and development of the world economy. It is truly a new wave of
globalisation, and we can term it as the global value chains (GVCs), creating
the nexus among firms, workers and consumers around the globe. The emergence of
this new scenario is asking how an economy's businesses, producers and
employees are connecting to the global economy and capturing the gains out of
it regarding different dimensions of economic development. Indeed, this GVC
approach is very crucial for understanding the organisation of the global
industries (including firms) through analysing the statics and dynamics of
different economic players involved in this complex global production network.
Its widespread notion deals with various global issues (including regional
value chains also) from the top down to the bottom up, founding a scope for
policy analysis.

Are deep convolutional neural networks (CNNs) for image classification
explainable by utility maximization with information acquisition costs? We
demonstrate that deep CNNs behave equivalently (in terms of necessary and
sufficient conditions) to rationally inattentive utility maximizers, a
generative model used extensively in economics for human decision making. Our
claim is based by extensive experiments on 200 deep CNNs from 5 popular
architectures. The parameters of our interpretable model are computed
efficiently via convex feasibility algorithms. As an application, we show that
our economics-based interpretable model can predict the classification
performance of deep CNNs trained with arbitrary parameters with accuracy
exceeding 94% . This eliminates the need to re-train the deep CNNs for image
classification. The theoretical foundation of our approach lies in Bayesian
revealed preference studied in micro-economics. All our results are on GitHub
and completely reproducible.

Analysis of policies for managing epidemics require simultaneously an
economic and epidemiological perspective. We adopt a cost-of-policy framework
to model both the virus spread and the cost of handling the pandemic. Because
it is harder and more costly to fight the pandemic when the circulation is
higher, we find that the optimal policy is to go to zero or near-zero case
numbers. Without imported cases, if a region is willing to implement measures
to prevent spread at one level in number of cases, it must also be willing to
prevent the spread with at a lower level, since it will be cheaper to do so and
has only positive other effects. With imported cases, if a region is not
coordinating with other regions, we show the cheapest policy is continually low
but nonzero cases due to decreasing cost of halting imported cases. When it is
coordinating, zero is cost-optimal. Our analysis indicates that within Europe
cooperation targeting a reduction of both within country transmission, and
between country importation risk, should help achieve lower transmission and
reduced costs.

The tourism industry is one of the potential revenues and has an important
role in economics in Indonesia. The tourism Industry brings job and business
opportunities, foreign exchange earnings, and infrastructure development,
tourism also plays the role of one of the main drivers in socio-economic
progress in Indonesia. The number of foreign tourists visiting Indonesia
increase cumulatively and has reached 10.41 million visits or an increase of
10.46 percent from the same period in the previous year. Government trying to
increase the number of tourists to visit Indonesia by promoting many Indonesian
tourist attractions.

A multi-regional input-output table (MRIOT) containing the transactions among
the region-sectors in an economy defines a weighted and directed network. Using
network analysis tools, we analyze the regional and sectoral structure of the
Chinese economy and their temporal dynamics from 2007 to 2012 via the MRIOTs of
China. Global analyses are done with network topology measures. Growth-driving
province-sector clusters are identified with community detection methods.
Influential province-sectors are ranked by weighted PageRank scores. The
results revealed a few interesting and telling insights. The level of
inter-province-sector activities increased with the rapid growth of the
national economy, but not as fast as that of intra-province economic
activities. Regional community structures were deeply associated with
geographical factors. The community heterogeneity across the regions was high
and the regional fragmentation increased during the study period. Quantified
metrics assessing the relative importance of the province-sectors in the
national economy echo the national and regional economic development policies
to a certain extent.

The economic approach to determine optimal legal policies involves maximizing
a social welfare function. We propose an alternative: a consent-approach that
seeks to promote consensual interactions and deter non-consensual interactions.
The consent-approach does not rest upon inter-personal utility comparisons or
value judgments about preferences. It does not require any additional
information relative to the welfare-approach. We highlight the contrast between
the welfare-approach and the consent-approach using a stylized model inspired
by seminal cases of harassment and the #MeToo movement. The social welfare
maximizing penalty for harassment in our model can be zero under the
welfare-approach but not under the consent-approach.

The coronavirus is a global event of historical proportions and just a few
months changed the time series properties of the data in ways that make many
pre-covid forecasting models inadequate. It also creates a new problem for
estimation of economic factors and dynamic causal effects because the
variations around the outbreak can be interpreted as outliers, as shifts to the
distribution of existing shocks, or as addition of new shocks. I take the
latter view and use covid indicators as controls to 'de-covid' the data prior
to estimation. I find that economic uncertainty remains high at the end of 2020
even though real economic activity has recovered and covid uncertainty has
receded. Dynamic responses of variables to shocks in a VAR similar in magnitude
and shape to the ones identified before 2020 can be recovered by directly or
indirectly modeling covid and treating it as exogenous. These responses to
economic shocks are distinctly different from those to a covid shock which are
much larger but shorter lived. Disentangling the two types of shocks can be
important in macroeconomic modeling post-covid.

The initial period of vaccination shows strong heterogeneity between
countries' vaccinations rollout, both in the terms of the start of the
vaccination process and in the dynamics of the number of people that are
vaccinated. A predominant thesis in the ongoing debate on the drivers of this
observed heterogeneity is that a key determinant of the swift and extensive
vaccine rollout is state capacity. Here, we utilize two measures that quantify
different aspects of the state capacity: i) the external capacity (measured
through the soft power and the economic power of the country) and ii) the
internal capacity (measured via the country's government effectiveness) and
investigate their relationship with the coronavirus vaccination outcome in the
initial period (up to 30th January 2021). By using data on 189 countries and a
two-step Heckman approach, we find that the economic power of the country and
its soft power are robust determinants of whether a country has started with
the vaccination process. In addition, the government effectiveness is a key
factor that determines vaccine roll-out. Altogether, our findings are in line
with the hypothesis that state capacity determines the observed heterogeneity
between countries in the initial period of COVID-19 vaccines rollout.

This paper describes an approach to economics that is inspired by quantum
computing, and is motivated by the need to develop a consistent quantum
mathematical framework for economics. The traditional neoclassical approach
assumes that rational utility-optimisers drive market prices to a stable
equilibrium, subject to external perturbations. While this approach has been
highly influential, it has come under increasing criticism following the
financial crisis of 2007/8. The quantum approach, in contrast, is inherently
probabilistic and dynamic. Decision-makers are described, not by a utility
function, but by a propensity function which specifies the probability of
transacting. We show how a number of cognitive phenomena such as preference
reversal and the disjunction effect can be modelled by using a simple quantum
circuit to generate an appropriate propensity function. Conversely, a general
propensity function can be quantized to incorporate effects such as
interference and entanglement that characterise human decision-making.
Applications to some common problems in economics and finance are discussed.

We introduce a model of the diffusion of an epidemic with demographically
heterogeneous agents interacting socially on a spatially structured network.
Contagion-risk averse agents respond behaviorally to the diffusion of the
infections by limiting their social interactions. Schools and workplaces also
respond by allowing students and employees to attend and work remotely. The
spatial structure induces local herd immunities along socio-demographic
dimensions, which significantly affect the dynamics of infections. We study
several non-pharmaceutical interventions; e.g., i) lockdown rules, which set
thresholds on the spread of the infection for the closing and reopening of
economic activities; ii) neighborhood lockdowns, leveraging granular
(neighborhood-level) information to improve the effectiveness public health
policies; iii) selective lockdowns, which restrict social interactions by
location (in the network) and by the demographic characteristics of the agents.
Substantiating a "Lucas critique" argument, we assess the cost of naive
discretionary policies ignoring agents and firms' behavioral responses.

This paper investigates the impact of Reconstruction-era amnesty policy on
the officeholding and wealth of elites in the postbellum South. Amnesty policy
restricted the political and economic rights of Southern elites for nearly
three years during Reconstruction. I estimate the effect of being excluded from
amnesty on elites' future wealth and political power using a regression
discontinuity design that compares individuals just above and below a wealth
threshold that determined exclusion from amnesty. Results on a sample of
Reconstruction convention delegates show that exclusion from amnesty
significantly decreased the likelihood of ex-post officeholding. I find no
evidence that exclusion impacted later census wealth for Reconstruction
delegates or for a larger sample of known slaveholders who lived in the South
in 1860. These findings are in line with previous studies evidencing both
changes to the identity of the political elite, and the continuity of economic
mobility among the planter elite across the Civil War and Reconstruction.

The form of political polarization where citizens develop strongly negative
attitudes towards out-party policies and members has become increasingly
prominent across many democracies. Economic hardship and social inequality, as
well as inter-group and racial conflict, have been identified as important
contributing factors to this phenomenon known as "affective polarization." Such
partisan animosities are exacerbated when these interests and identities become
aligned with existing party cleavages. In this paper we use a model of cultural
evolution to study how these forces combine to generate and maintain affective
political polarization. We show that economic events can drive both affective
polarization and sorting of group identities along party lines, which in turn
can magnify the effects of underlying inequality between those groups. But on a
more optimistic note, we show that sufficiently high levels of wealth
redistribution through the provision of public goods can counteract this
feedback and limit the rise of polarization. We test some of our key
theoretical predictions using survey data on inter-group polarization, sorting
of racial groups and affective polarization in the United States over the past
50 years.

Knowledge of the spatial organisation of economic activity within a city is
key to policy concerns. However, in developing cities with high levels of
informality, this information is often unavailable. Recent progress in machine
learning together with the availability of street imagery offers an affordable
and easily automated solution. Here we propose an algorithm that can detect
what we call 'visible firms' using street view imagery. Using Medell\'in,
Colombia as a case study, we illustrate how this approach can be used to
uncover previously unseen economic activity. Applying spatial analysis to our
dataset we detect a polycentric structure with five distinct clusters located
in both the established centre and peripheral areas. Comparing the density of
visible and registered firms, we find that informal activity concentrates in
poor but densely populated areas. Our findings highlight the large gap between
what is captured in official data and the reality on the ground.

This study leverages narrative from global newspapers to construct
theme-based knowledge graphs about world events, demonstrating that features
extracted from such graphs improve forecasts of industrial production in three
large economies compared to a number of benchmarks. Our analysis relies on a
filtering methodology that extracts "backbones" of statistically significant
edges from large graph data sets. We find that changes in the eigenvector
centrality of nodes in such backbones capture shifts in relative importance
between different themes significantly better than graph similarity measures.
We supplement our results with an interpretability analysis, showing that the
theme categories "disease" and "economic" have the strongest predictive power
during the time period that we consider. Our work serves as a blueprint for the
construction of parsimonious - yet informative - theme-based knowledge graphs
to monitor in real time the evolution of relevant phenomena in socio-economic
systems.

We estimate the short- to medium term impact of six major past pandemic
crises on the CO2 emissions and energy transition to renewable electricity. The
results show that the previous pandemics led on average to a 3.4-3.7% fall in
the CO2 emissions in the short-run (1-2 years since the start of the pandemic).
The effect is present only in the rich countries, as well as in countries with
the highest pandemic death toll (where it disappears only after 8 years) and in
countries that were hit by the pandemic during economic recessions. We found
that the past pandemics increased the share of electricity generated from
renewable sources within the fiveyear horizon by 1.9-2.3 percentage points in
the OECD countries and by 3.2-3.9 percentage points in countries experiencing
economic recessions. We discuss the implications of our findings in the context
of CO2 emissions and the transition to renewable energy in the post-COVID-19
era.

The present work generalizes the analytical results of Petrikaite (2016) to a
market where more than two firms interact. As a consequence, for a generic
number of firms in the oligopoly model described by Janssen et al (2005), the
relationship between the critical discount factor which sustains the monopoly
collusive allocation and the share of perfectly informed buyers is
non-monotonic, reaching a unique internal point of minimum. The first section
locates the work within the proper economic framework. The second section hosts
the analytical computations and the mathematical reasoning needed to derive the
desired generalization, which mainly relies on the Leibniz rule for the
differentiation under the integral sign and the Bounded Convergence Theorem.

How are economies in a modern age impacted by epidemics? In what ways is
economic life disrupted? How can pandemics be modeled? What can be done to
mitigate and manage the danger? Does the threat of pandemics increase or
decrease in the modern world? The Covid-19 pandemic has demonstrated the
importance of these questions and the potential of complex systems science to
provide answers. This article offers a broad overview of the history of
pandemics, of established facts, and of models of infection diffusion,
mitigation strategies, and economic impact. The example of the Covid-19
pandemic is used to illustrate the theoretical aspects, but the article also
includes considerations concerning other historic epidemics and the danger of
more infectious and less controllable outbreaks in the future.

Economic Policy Uncertainty (EPU) is a critical indicator in economic
studies, while it can be used to forecast a recession. Under higher levels of
uncertainty, firms' owners cut their investment, which leads to a longer
post-recession recovery. EPU index is computed by counting news articles
containing pre-defined keywords related to policy-making and economy and convey
uncertainty. Unfortunately, this method is sensitive to the original keyword
set, its richness, and the news coverage. Thus, reproducing its results for
different countries is challenging. In this paper, we propose an unsupervised
text mining method that uses word-embedding representation space to select
relevant keywords. This method is not strictly sensitive to the semantic
similarity threshold applied to the word embedding vectors and does not require
a pre-defined dictionary. Our experiments using a massive repository of Persian
news show that the EPU series computed by the proposed method precisely follows
major events affecting Iran's economy and is compatible with the World
Uncertainty Index (WUI) of Iran.

In this paper, we analyze the effect of transport infrastructure investments
in railways. As a testing ground, we use data from a new historical database
that includes annual panel data on approximately 2,400 Swedish rural
geographical areas during the period 1860-1917. We use a staggered event study
design that is robust to treatment effect heterogeneity. Importantly, we find
extremely large reduced-form effects of having access to railways. For real
nonagricultural income, the cumulative treatment effect is approximately 120%
after 30 years. Equally important, we also show that our reduced-form effect is
likely to reflect growth rather than a reorganization of existing economic
activity since we find no spillover effects between treated and untreated
regions. Specifically, our results are consistent with the big push hypothesis,
which argues that simultaneous/coordinated investment, such as large
infrastructure investment in railways, can generate economic growth if there
are strong aggregate demand externalities (e.g., Murphy et al. 1989). We used
plant-level data to further corroborate this mechanism. Indeed, we find that
investments in local railways dramatically, and independent of initial
conditions, increase local industrial production and employment on the order of
100-300% across almost all industrial sectors.

This work presents the analysis of the impact of restrictions on mobility in
Italy, with a focus on the period from 6 November 2020 to 31 January 2021, when
a three-tier system based on different levels of risk was adopted and applied
at regional level to contrast the second wave of COVID-19. The impact is first
evaluated on mobility using Mobile Network Operator anonymised and aggregate
data shared in the framework of a Business-to-Government initiative with the
European Commission. Mobility data, alongside additional information about
electricity consuption, are then used to assess the impacts on an economic
level of the three-tier system in different areas of the country.

Green Management (GM) is now one of many methods proposed to achieve new,
more ecological, and sustainable economic models. The paper is focused on the
impact of the developing human population on the environment measured by
researched variables. Anthropopressure can have both a positive and a negative
dimension. This paper aims to present an econometric model of the Green
Industrial Revolution (GIR) impact on the Labour Market. The GIR is similar to
the Fourth Industrial Revolution (FIR) and takes place as the next stage in the
development of humanity in the perception of both machines and devices and the
natural environment. The processes of the GIR in the European Union can be
identified based on selected indicators of Sustainable Development (SD), in
particular with the use of indicators of the Green Economy (GE) using taxonomic
methods and regression analysis. The GM strives to implement the idea of the SD
in many areas, to transform the whole economy, and elements of this process are
visible Green Labour Market (GLM). The adopted direction of economic
development depends on the as-sumptions of strategic management, which can be
defined, for example, with green management, which is mainly manifested in the
creation of green jobs.

The question of "Justice" still divides social research and moral philosophy.
Several Theories of Justice and conceptual approaches compete here, and
distributive justice remains a major societal controversy. From an evolutionary
point of view, fair and just exchange can be nothing but "equivalent", and this
makes "strict" reciprocity (merit, equity) the foundational principle of
justice, both theoretically and empirically. But besides being just, justice
must be effective, efficient, and communicable. Moral reasoning is a
communicative strategy for resolving conflict, enhancing status, and
maintaining cooperation, thereby making justice rather a social bargain and an
optimization problem. Social psychology (intuitions, rules of thumb,
self-bindings) can inform us when and why the two auxiliary principles equality
and need are more likely to succeed than merit would. Nevertheless, both
equality and need are governed by reciprocal considerations, and self-bindings
help to interpret altruism as "very generalized reciprocity". The Meritocratic
Principle can be implemented, and its controversy avoided, by concentrating on
"non-merit", i.e., institutionally draining the wellsprings of undeserved
incomes (economic rents). Avoiding or taxing away economic rents is an
effective implementation of justice in liberal democracies. This would enable
market economies to bring economic achievement and income much more in line,
thus becoming more just.

We show how the Shannon entropy function can be used as a basis to set up
complexity measures weighting the economic efficiency of countries and the
specialization of products beyond bare diversification. This entropy function
guarantees the existence of a fixed point which is rapidly reached by an
iterative scheme converging to our self-consistent measures. Our approach
naturally allows to decompose into inter-sectorial and intra-sectorial
contributions the country competitivity measure if products are partitioned
into larger categories. Besides outlining the technical features and advantages
of the method, we describe a wide range of results arising from the analysis of
the obtained rankings and we benchmark these observations against those
established with other economical parameters. These comparisons allow to
partition countries and products into various main typologies, with
well-revealed characterizing features. Our methods have wide applicability to
general problems of ranking in bipartite networks.

We consider an observational learning model with exogenous public payoff
shock. We show that confounded learning doesn't arise for almost all private
signals and almost all shocks, even if players have sufficiently divergent
preferences.

Artificial neural networks (ANNs) have been the catalyst to numerous advances
in a variety of fields and disciplines in recent years. Their impact on
economics, however, has been comparatively muted. One type of ANN, the long
short-term memory network (LSTM), is particularly wellsuited to deal with
economic time-series. Here, the architecture's performance and characteristics
are evaluated in comparison with the dynamic factor model (DFM), currently a
popular choice in the field of economic nowcasting. LSTMs are found to produce
superior results to DFMs in the nowcasting of three separate variables; global
merchandise export values and volumes, and global services exports. Further
advantages include their ability to handle large numbers of input features in a
variety of time frequencies. A disadvantage is the inability to ascribe
contributions of input features to model outputs, common to all ANNs. In order
to facilitate continued applied research of the methodology by avoiding the
need for any knowledge of deep-learning libraries, an accompanying Python
library was developed using PyTorch, https://pypi.org/project/nowcast-lstm/.

The COVID-19 pandemic has forced changes in production and especially in
human interaction, with "social distancing" a standard prescription for slowing
transmission of the disease. This paper examines the economic effects of social
distancing at the aggregate level, weighing both the benefits and costs to
prolonged distancing. Specifically we fashion a model of economic recovery when
the productive capacity of factors of production is restricted by social
distancing, building a system of equations where output growth and social
distance changes are interdependent. The model attempts to show the complex
interactions between output levels and social distancing, developing cycle
paths for both variables. Ultimately, however, defying gravity via prolonged
social distancing shows that a lower growth path is inevitable as a result.

Economic Model Predictive Control has recently gained popularity due to its
ability to directly optimize a given performance criterion, while enforcing
constraint satisfaction for nonlinear systems. Recent research has developed
both numerical algorithms and stability analysis for the undiscounted case. The
introduction of a discount factor in the cost, however, can be desirable in
some cases of interest, e.g., economics, stochastically terminating processes,
Markov decision processes, etc. Unfortunately, the stability theory in this
case is still not fully developed. In this paper we propose a new dissipativity
condition to prove asymptotic stability in the infinite horizon case and we
connect our results with existing ones in the literature on discounted economic
optimal control. Numerical examples are provided to illustrate the theoretical
results.

Economic model predictive control (EMPC) has attracted significant attention
in recent years and is recognized as a promising advanced process control
method for the next generation smart manufacturing. It can lead to improving
economic performance but at the same time increases the computational
complexity significantly. Model approximation has been a standard approach for
reducing computational complexity in process control. In this work, we perform
a study on three types of representative model approximation methods applied to
EMPC, including model reduction based on available first-principle models
(e.g., proper orthogonal decomposition), system identification based on
input-output data (e.g., subspace identification) that results in an explicitly
expressed mathematical model, and neural networks based on input-output data. A
representative algorithm from each model approximation method is considered.
Two processes that are very different in dynamic nature and complexity were
selected as benchmark processes for computational complexity and economic
performance comparison, namely an alkylation process and a wastewater treatment
plant (WWTP). The strengths and drawbacks of each method are summarized
according to the simulation results, with future research direction regarding
control oriented model approximation proposed at the end.

An important issue for many economic experiments is how the experimenter can
ensure sufficient power for rejecting one or more hypotheses. Here, we apply
methods developed mainly within the area of clinical trials for testing
multiple hypotheses simultaneously in adaptive, two-stage designs. Our main
goal is to illustrate how this approach can be used to improve the power of
economic experiments. Having briefly introduced the relevant theory, we perform
a simulation study supported by the open source R package asd in order to
evaluate the power of some different designs. The simulations show that the
power to reject at least one hypothesis can be improved while still ensuring
strong control of the overall Type I error probability, and without increasing
the total sample size and thus the costs of the study. The derived designs are
further illustrated by applying them to two different real-world data sets from
experimental economics.

Welfare economics relies on access to agents' utility functions: we revisit
classical questions in welfare economics, assuming access to data on agents'
past choices instead of their utilities. Our main result considers the
existence of utilities that render a given allocation Pareto optimal. We show
that a candidate allocation is efficient for some utilities consistent with the
choice data if and only if it is efficient for an incomplete relation derived
from the revealed preference relations and convexity. Similar ideas are used to
make counterfactual choices for a single consumer, policy comparisons by the
Kaldor criterion, and determining which allocations, and which prices, may be
part of a Walrasian equilibrium.

Designing waterfront redevelopment generally focuses on attractiveness,
leisure, and beauty, resulting in various types of building and block shapes
with limited considerations on environmental aspects. However, increasing
climate change impacts necessitate these buildings to be sustainable,
resilient, and zero CO2 emissions. By producing five scenarios (plus existing
buildings) with constant floor areas, we investigated how building and district
form with building integrated photovoltaics (BIPV) affect energy consumption
and production, self-sufficiency, CO2 emission, and energy costs in the context
of waterfront redevelopment in Tokyo. From estimated hourly electricity demands
of the buildings, techno-economic analyses are conducted for rooftop PV systems
for 2018 and 2030 with declining costs of rooftop PV systems. We found that
environmental building designs with rooftop PV system are increasingly
economical in Tokyo with CO2 emission reduction of 2-9% that depends on rooftop
sizes. Payback periods drop from 14 years in 2018 to 6 years in 2030. Toward
net-zero CO2 emissions by 2050, immediate actions are necessary to install
rooftop PVs on existing and new buildings with energy efficiency improvements
by construction industry and building owners. To facilitate such actions,
national and local governments need to adopt appropriate policies.

We use the logistic equation to model the dynamics of the GDP and the trade
of the six countries with the highest GDP in the world, namely, USA, China,
Japan, Germany, UK and India. From the modelling of the economic data, which
are made available by the World Bank, we predict the maximum values of the
growth of GDP and trade, as well as the duration over which exponential growth
can be sustained. We set up the correlated growth of GDP and trade as the phase
solutions of an autonomous second-order dynamical system. GDP and trade are
related to each other by a power law, whose exponent seems to differentiate the
six national economies into two types. Under conducive conditions for economic
growth, our conclusions have general validity.

Fair division is a significant, long-standing problem and is closely related
to social and economic justice. The conventional division methods such as
cut-and-choose are hardly applicable to realworld problems because of their
complexity and unrealistic assumptions about human behaviors. Here we propose a
fair division method from a completely different perspective, using the
Boltzmann distribution. The Boltzmann distribution adopted from the physical
sciences gives the most probable and unbiased distribution derived from a
goods-centric, rather than a player-centric, division process. The mathematical
model of the Boltzmann fair division was developed for both homogeneous and
heterogeneous division problems, and the players' key factors (contributions,
needs, and preferences) could be successfully integrated. We show that the
Boltzmann fair division is a well-balanced division method maximizing the
players' total utility, and it could be easily finetuned and applicable to
complex real-world problems such as income/wealth redistribution or
international negotiations on fighting climate change.

The Sundarban Reserve Forest (SRF) of Bangladesh provides tourism services to
local and international visitors. Indeed, tourism is one of the major ecosystem
services that this biodiversity-rich mangrove forest provides. Through a
convenient sampling technique, 421 tourist respondents were interviewed to
assess their willingness to pay for the tourism services of the Sundarban,
using the Zonal Travel Cost Method (ZTCM). The estimated annual economic
contribution of tourism in the Sundarban mangroves to the Bangladesh economy is
USD 53 million. The findings of this study showed that facilities for watching
wildlife and walking inside the forest can increase the number of tourists in
the SRF. The findings also show that the availability of information like
forest maps, wildlife precautionary signs, and danger zones would increase the
number of tourists as well. Thus, the government of Bangladesh should consider
increasing visitor entry fees to fund improvements and to enhance the
ecotourism potential of the Sundarban mangroves.

Para-Aminophenol is one of the key chemicals required for the synthesis of
Paracetamol, an analgesic and antipyretic drug. Data shows a large fraction of
India's demand for Para-Aminophenol being met through imports from China. The
uncertainty in the India-China relations would affect the supply and price of
this "Key Starting Material." This report is a detailed business plan for
setting up a plant and producing Para-Aminophenol in India at a competitive
price. The plant is simulated in AspenPlus V8 and different Material Balances
and Energy Balances calculations are carried out. The plant produces 22.7 kmols
Para-Aminophenol per hour with a purity of 99.9%. Along with the simulation,
economic analysis is carried out for this plant to determine the financial
parameters like Payback Period and Return on Investment.

This paper proposes a spatial model with a realistic geography where a
continuous distribution of agents (e.g., farmers) engages in economic
interactions with one location from a finite set (e.g., cities). The spatial
structure of the equilibrium consists of a tessellation, i.e., a partition of
space into a collection of mutually exclusive market areas. After proving the
existence of a unique equilibrium, we characterize how the location of borders
and, in the case with mobile labor, the set of inhabited cities change in
response to economic shocks. To deal with a two-dimensional space, we draw on
tools from computational geometry and from the theory of shape optimization.
Finally, we provide an empirical application to illustrate the usefulness of
the framework for applied work.

Humans exhibit irrational decision-making patterns in response to
environmental triggers, such as experiencing an economic loss or gain. In this
paper we investigate whether algorithms exhibit the same behavior by examining
the observed decisions and latent risk and rationality parameters estimated by
a random utility model with constant relative risk-aversion utility function.
We use a dataset consisting of 10,000 hands of poker played by Pluribus, the
first algorithm in the world to beat professional human players and find (1)
Pluribus does shift its playing style in response to economic losses and gains,
ceteris paribus; (2) Pluribus becomes more risk-averse and rational following a
trigger but the humans become more risk-seeking and irrational; (3) the
difference in playing styles between Pluribus and the humans on the dimensions
of risk-aversion and rationality are particularly differentiable when both have
experienced a trigger. This provides support that decision-making patterns
could be used as "behavioral signatures" to identify human versus algorithmic
decision-makers in unlabeled contexts.

International economics has a long history of improving our understanding of
factors causing trade, and the consequences of free flow of goods and services
across countries. The recent shocks to the free trade regime, especially trade
disputes among major economies, as well as black swan events, such as trade
wars and pandemics, raise the need for improved predictions to inform policy
decisions. AI methods are allowing economists to solve such prediction problems
in new ways. In this manuscript, we present novel methods that predict and
associate food and agricultural commodities traded internationally. Association
Rules (AR) analysis has been deployed successfully for economic scenarios at
the consumer or store level, such as for market basket analysis. In our work
however, we present analysis of imports and exports associations and their
effects on commodity trade flows. Moreover, Ensemble Machine Learning methods
are developed to provide improved agricultural trade predictions, outlier
events' implications, and quantitative pointers to policy makers.

This paper quantifies the international spillovers of US monetary policy by
exploiting the high-frequency movement of multiple financial assets around FOMC
announcements. I use the identification strategy introduced by Jarocinski &
Karadi (2022) to identify two FOMC shocks: a pure US monetary policy and an
information disclosure shock. These two FOMC shocks have intuitive and very
different international spillovers. On the one hand, a US tightening caused by
a pure US monetary policy shock leads to an economic recession, an exchange
rate depreciation and tighter financial conditions. On the other hand, a
tightening of US monetary policy caused by the FOMC disclosing positive
information about the state of the US economy leads to an economic expansion,
an exchange rate appreciation and looser financial conditions. Ignoring the
disclosure of information by the FOMC biases the impact of a US monetary policy
tightening and may explain recent atypical findings.

Technology codes are assigned to each patent for classification purposes and
to identify the components of its novelty. Not all the technology codes are
used with the same frequency - if we study the use frequency of codes in a
year, we can find predominant technologies used in many patents and technology
codes not so frequent as part of a patent. In this paper, we measure that
inequality in the use frequency of patent technology codes. First, we analyze
the total inequality in that use frequency considering the patent applications
filed under the Patent Co-operation Treaty at international phase, with the
European Patent Office as designated office, in the period 1977-2018, on a
yearly basis. Then, we analyze the decomposition of that inequality by grouping
the technology codes by productive economic activities. We show that total
inequality had an initial period of growth followed by a phase of relative
stabilization, and that it tends to be persistently high. We also show that
total inequality was mainly driven by inequality within productive economic
activities, with a low contribution of the between-activities component.

Depressive disorders, in addition to causing direct negative impacts on
health, are also responsible for imposing substantial costs on society. In
relation to the treatment of depression, antidepressants have proven effective,
and, to the World Health Organization, access to psychotropic drugs for people
with mental illnesses offers a chance of improved health and an opportunity for
reengagement in society. The aim of this study is to analyze the use of and
access to antidepressants in Brazil, according to macro-regions and to
demographic, social and economic conditions of the population, using the
National Survey on Access, Use and Promotion of Rational Use of Medicines
(PNAUM 2013/2014). The results show that there is a high prevalence of
antidepressant use in individuals with depression in Brazil. The main profile
of use of these drugs is: female individuals, between 20 and 59 years old,
white, from the Southeast region, of the economic class D/E, with a high
schooling level, in a marital situation, without health insurance coverage,
without limitations derived from depression, and who self-evaluated health as
regular.

We introduce a novel machine learning approach to leverage historical and
contemporary maps and systematically predict economic statistics. Our simple
algorithm extracts meaningful features from the maps based on their color
compositions for predictions. We apply our method to grid-level population
levels in Sub-Saharan Africa in the 1950s and South Korea in 1930, 1970, and
2015. Our results show that maps can reliably predict population density in the
mid-20th century Sub-Saharan Africa using 9,886 map grids (5km by 5 km).
Similarly, contemporary South Korean maps can generate robust predictions on
income, consumption, employment, population density, and electric consumption.
In addition, our method is capable of predicting historical South Korean
population growth over a century.

We provide a necessary and sufficient condition for rationalizable
implementation of social choice functions, i.e., we offer a complete answer
regarding what social choice functions can be rationalizably implemented.

We propose a scalable method for computing global solutions of nonlinear,
high-dimensional dynamic stochastic economic models. First, within a time
iteration framework, we approximate economic policy functions using an
adaptive, high-dimensional model representation scheme, combined with adaptive
sparse grids to address the ubiquitous challenge of the curse of
dimensionality. Moreover, the adaptivity within the individual component
functions increases sparsity since grid points are added only where they are
most needed, that is, in regions with steep gradients or at
nondifferentiabilities. Second, we introduce a performant vectorization scheme
for the interpolation compute kernel. Third, the algorithm is hybrid
parallelized, leveraging both distributed- and shared-memory architectures. We
observe significant speedups over the state-of-the-art techniques, and almost
ideal strong scaling up to at least $1,000$ compute nodes of a Cray XC$50$
system at the Swiss National Supercomputing Center. Finally, to demonstrate our
method's broad applicability, we compute global solutions to two variates of a
high-dimensional international real business cycle model up to $300$ continuous
state variables. In addition, we highlight a complementary advantage of the
framework, which allows for a priori analysis of the model complexity.

The view that altruistic punishment plays an important role in supporting
public cooperation among human beings and other species has been widely
accepted by the public. However, the positive role of altruistic punishment in
enhancing cooperation will be undermined if corruption is considered. Recently,
behavioral experiments have confirmed this finding and further investigated the
effects of the leader's punitive power and the economic potential.
Nevertheless, there are relatively few studies focusing on how these factors
affect the evolution of cooperation from a theoretical perspective. Here, we
combine institutional punishment public goods games with bribery games to
investigate the effects of the above factors on the evolution of cooperation.
Theoretical and numerical results reveal that the existence of corruption will
reduce the level of cooperation when cooperators are more inclined to provide
bribes. In addition, we demonstrate that stronger leader and richer economic
potential are both important to enhance cooperation. In particular, when
defectors are more inclined to provide bribes, stronger leaders can sustain the
contributions of public goods from cooperators if the economic potential is
weak.

Russia's attack on Ukraine on Thursday 24 February 2022 hitched financial
markets and the increased geopolitical crisis. In this paper, we select some
main economic indexes, such as Gold, Oil (WTI), NDAQ, and known currency which
are involved in this crisis and try to find the quantitative effect of this war
on them. To quantify the war effect, we use the correlation feature and the
relationships between these economic indices, create datasets, and compare the
results of forecasts with real data. To study war effects, we use Machine
Learning Linear Regression. We carry on empirical experiments and perform on
these economic indices datasets to evaluate and predict this war tolls and its
effects on main economics indexes.

The goal of this paper is to evaluate the informational content of sentiment
extracted from news articles about the state of the economy. We propose a
fine-grained aspect-based sentiment analysis that has two main characteristics:
1) we consider only the text in the article that is semantically dependent on a
term of interest (aspect-based) and, 2) assign a sentiment score to each word
based on a dictionary that we develop for applications in economics and finance
(fine-grained). Our data set includes six large US newspapers, for a total of
over 6.6 million articles and 4.2 billion words. Our findings suggest that
several measures of economic sentiment track closely business cycle
fluctuations and that they are relevant predictors for four major macroeconomic
variables. We find that there are significant improvements in forecasting when
sentiment is considered along with macroeconomic factors. In addition, we also
find that sentiment matters to explains the tails of the probability
distribution across several macroeconomic variables.

In 2016, the Dutch National Health Care Institute issued new guidelines that
aggregated and updated previous recommendations on key elements for conducting
economic evaluation. However, the impact on standard practice after the
introduction of the guidelines in terms of design, methodology and reporting
choices, is still uncertain. To assess this impact, we examine and compare key
analysis components of economic evaluations conducted in the Netherlands before
(2010-2015) and after (2016-2020) the introduction of the guidelines. We
specifically focus on two aspects of the analysis that are crucial in
determining the plausibility of the results: statistical methodology and
missing data handling. Our review shows how many components of economic
evaluations have changed in accordance with the new recommendations towards
more transparent and advanced analytic approaches. However, potential
limitations are identified in terms of the statistical software and information
provided to support the choice of missing data methods.

We examine whether a company's corporate reputation gained from their CSR
activities and a company leader's reputation, one that is unrelated to his or
her business acumen, can impact economic action fairness appraisals. We provide
experimental evidence that good corporate reputation causally buffers
individuals' negative fairness judgment following the firm's decision to
profiteer from an increase in the demand. Bad corporate reputation does not
make the decision to profiteer as any less acceptable. However, there is
evidence that individuals judge as more unfair an ill-reputed firm's decision
to raise their product's price to protect against losses. Thus, our results
highlight the importance of a good reputation in protecting a firm against
severe negative judgments from making an economic decision that the public
deems unfair.

Explaining changes in bitcoin's price and predicting its future have been the
foci of many research studies. In contrast, far less attention has been paid to
the relationship between bitcoin's mining costs and its price. One popular
notion is the cost of bitcoin creation provides a support level below which
this cryptocurrency's price should never fall because if it did, mining would
become unprofitable and threaten the maintenance of bitcoin's public ledger.
Other research has used mining costs to explain or forecast bitcoin's price
movements. Competing econometric analyses have debunked this idea, showing that
changes in mining costs follow changes in bitcoin's price rather than preceding
them, but the reason for this behavior remains unexplained in these analyses.
This research aims to employ economic theory to explain why econometric studies
have failed to predict bitcoin prices and why mining costs follow movements in
bitcoin prices rather than precede them. We do so by explaining the chain of
causality connecting a bitcoin's price to its mining costs.

The COVID-19 pandemic created enormous public health and socioeconomic
challenges. The health effects of vaccination and non-pharmaceutical
interventions (NPIs) were often contrasted with significant social and economic
costs. We describe a general framework aimed to derive adaptive cost-effective
interventions, adequate for both recent and emerging pandemic threats. We also
quantify the net health benefits and propose a reinforcement learning approach
to optimise adaptive NPIs. The approach utilises an agent-based model
simulating pandemic responses in Australia, and accounts for a heterogeneous
population with variable levels of compliance fluctuating over time and across
individuals. Our analysis shows that a significant net health benefit may be
attained by adaptive NPIs formed by partial social distancing measures, coupled
with moderate levels of the society's willingness to pay for health gains
(health losses averted). We demonstrate that a socially acceptable balance
between health effects and incurred economic costs is achievable over a long
term, despite possible early setbacks.

We prove that supply correspondences are characterized by two properties: the
law of supply and being homogeneous of degree zero.

We compute confidence intervals for recursive impact factors, that take into
account that some citations are more prestigious than others, as well as for
the associated ranks of journals, applying the methods to the population of
economics journals. The Quarterly Journal of Economics is clearly the journal
with greatest impact, the confidence interval for its rank only includes one.
Based on the simple bootstrap, the remainder of the Top5 journals are in the
top 6 together with the Journal of Finance, while the Xie et al. (2009), and
Mogstad et al. (2022) methods generally broaden estimated confidence intervals,
particularly for mid-ranking journals. All methods agree that most apparent
differences in journal quality are, in fact, mostly insignificant.

Using U.S. nationwide travel surveys for 1995, 2001, 2009 and 2017, this
study compares Millennials with their previous generation (Gen Xers) in terms
of their automobile travel across different neighborhood patterns. At the age
of 16 to 28 years old, Millennials have lower daily personal vehicle miles
traveled and car trips than Gen Xers in urban (higher-density) and suburban
(lower-density) neighborhoods. Such differences remain unchanged after
adjusting for the socio-economic, vehicle ownership, life cycle, year-specific
and regional-specific factors. In addition, the associations between
residential density and automobile travel for the 16- to 28-year-old
Millennials are flatter than that for Gen Xers, controlling for the
aforementioned covariates. These generational differences remain for the 24- to
36-year-old Millennials, during the period when the U.S. economy was recovering
from the recession. These findings show that, in both urban and suburban
neighborhoods, Millennials in the U.S. are less auto-centric than the previous
generation during early life stages, regardless of economic conditions. Whether
such difference persists over later life stages remains an open question and is
worth continuous attention.

Using images containing information on wealth, this research investigates
that pictures are capable of reliably predicting the economic prosperity of
households. Without surveys on wealth-related information and human-made
standard of wealth quality that the traditional wealth-based approach relied
on, this novel approach makes use of only images posted on Dollar Street as
input data on household wealth across 66 countries and predicts the consumption
or income level of each household using the Convolutional Neural Network (CNN)
method. The best result predicts the log of consumption level with root mean
squared error of 0.66 and R-squared of 0.80 in CNN regression problem. In
addition, this simple model also performs well in classifying extreme poverty
with an accuracy of 0.87 and F-beta score of 0.86. Since the model shows a
higher performance in the extreme poverty classification when I applied the
different threshold of poverty lines to countries by their income group, it is
suggested that the decision of the World Bank to define poverty lines
differently by income group was valid.

We generate a continuous measure of health to estimate a non-parametric model
of health dynamics, showing that adverse health shocks are highly persistent
when suffered by people in poor health. Canonical models cannot account for
this pattern. We incorporate this health dynamic into a life-cycle model of
consumption, savings, and labor force participation. After estimating the model
parameters, we simulate the effects of health shocks on economic outcomes. We
find that bad health shocks have long-term adverse economic effects that are
more extreme for those in poor health. Furthermore, bad health shocks also
increase the disparity of asset accumulation among this group of people. A
canonical model of health dynamics would not reveal these effects.

Forecast combination -- the aggregation of individual forecasts from multiple
experts or models -- is a proven approach to economic forecasting. To date,
research on economic forecasting has concentrated on local combination methods,
which handle separate but related forecasting tasks in isolation. Yet, it has
been known for over two decades in the machine learning community that global
methods, which exploit task-relatedness, can improve on local methods that
ignore it. Motivated by the possibility for improvement, this paper introduces
a framework for globally combining forecasts while being flexible to the level
of task-relatedness. Through our framework, we develop global versions of
several existing forecast combinations. To evaluate the efficacy of these new
global forecast combinations, we conduct extensive comparisons using synthetic
and real data. Our real data comparisons, which involve forecasts of core
economic indicators in the Eurozone, provide empirical evidence that the
accuracy of global combinations of economic forecasts can surpass local
combinations.

Predicting the economy's short-term dynamics -- a vital input to economic
agents' decision-making process -- often uses lagged indicators in linear
models. This is typically sufficient during normal times but could prove
inadequate during crisis periods. This paper aims to demonstrate that
non-traditional and timely data such as retail and wholesale payments, with the
aid of nonlinear machine learning approaches, can provide policymakers with
sophisticated models to accurately estimate key macroeconomic indicators in
near real-time. Moreover, we provide a set of econometric tools to mitigate
overfitting and interpretability challenges in machine learning models to
improve their effectiveness for policy use. Our models with payments data,
nonlinear methods, and tailored cross-validation approaches help improve
macroeconomic nowcasting accuracy up to 40\% -- with higher gains during the
COVID-19 period. We observe that the contribution of payments data for economic
predictions is small and linear during low and normal growth periods. However,
the payments data contribution is large, asymmetrical, and nonlinear during
strong negative or positive growth periods.

Duality is the heart of advanced microeconomics. It exists everywhere
throughout advanced microeconomics, from the beginning of consumer theory to
the end of production theory. The complex, circular relationships among various
theoretical microeconomic concepts involved in the setting of duality theory
have led it to be called the "wheel of pain" by many graduate economics
students . Put simply, the main aim of this paper is to turn this "wheel of
pain" into a "wheel of joy". To be more specific, the primary purpose of this
paper is to graphically decode the logical, complex relationships among a
quartet of dual functions which present preferences as well as a quartet of
demand-related functions in a visual manner.

We examine the effect of civil war in Syria on economic growth, human
development and institutional quality. Building on the synthetic control
method, we estimate the missing counterfactual scenario in the hypothetical
absence of the armed conflict that led to unprecedented humanitarian crisis and
population displacement in modern history. By matching Syrian growth and
development trajectories with the characteristics of the donor pool of 66
countries with no armed internal conflict in the period 1996-2021, we estimate
a series of growth and development gaps attributed to civil war. Syrian civil
war appears to have had a temporary negative effect on the trajectory of
economic growth that almost disappeared before the onset of COVID19 pandemic.
By contrast, the civil war led to unprecedented losses in human development,
rising infant mortality and rampantly deteriorating institutional quality. Down
to the present day, each year of the conflict led to 5,700 additional
under-five child deaths with permanently derailed negative effect on longevity.
The civil war led to unprecedent and permanent deterioration in institutional
quality indicated by pervasive weakening of the rule of law and deleterious
impacts on government effectiveness, civil liberties and widespread escalation
of corruption. The estimated effects survive a battery of placebo checks.

The paper proposes a time-varying parameter global vector autoregressive
(TVP-GVAR) framework for predicting and analysing developed region economic
variables. We want to provide an easily accessible approach for the economy
application settings, where a variety of machine learning models can be
incorporated for out-of-sample prediction. The LASSO-type technique for
numerically efficient model selection of mean squared errors (MSEs) is
selected. We show the convincing in-sample performance of our proposed model in
all economic variables and relatively high precision out-of-sample predictions
with different-frequency economic inputs. Furthermore, the time-varying
orthogonal impulse responses provide novel insights into the connectedness of
economic variables at critical time points across developed regions. We also
derive the corresponding asymptotic bands (the confidence intervals) for
orthogonal impulse responses function under standard assumptions.

Distributed renewable resources owned by prosumers can be an effective way of
fortifying grid resilience and enhancing sustainability. However, prosumers
serve their own interests and their objectives are unlikely to align with that
of society. This paper develops a bilevel model to study the optimal design of
retail electricity tariffs considering the balance between economic efficiency
and energy equity. The retail tariff entails a fixed charge and a volumetric
charge tied to electricity usage to recover utilities' fixed costs. We analyze
solution properties of the bilevel problem and prove an optimal rate design,
which is to use fixed charges to recover fixed costs and to balance energy
equity among different income groups. This suggests that programs similar to
CARE (California Alternative Rate of Energy), which offer lower retail rates to
low-income households, are unlikely to be efficient, even if they are
politically appealing.

Over the years, the growing availability of extensive datasets about
registered patents allowed researchers to better understand technological
innovation drivers. In this work, we investigate how the technological contents
of patents characterise the development of metropolitan areas and how
innovation is related to GDP per capita. Exploiting worldwide data from 1980 to
2014, and through network-based techniques that only use information about
patents, we identify coherent distinguished groups of metropolitan areas,
either clustered in the same geographical area or similar from an economic
point of view. We also extend the concept of coherent diversification to patent
production by showing how it represents a decisive factor in the economic
growth of metropolitan areas. These results confirm a picture in which
technological innovation can lead and steer the economic development of cities,
opening, in this way, the possibility of adopting the tools introduced here to
investigate the interplay between urban development and technological
innovation.

Wetland quality is a critical factor in determining values of wetland goods
and services. However, in many studies on wetland valuation, wetland quality
has been ignored. While those studies might give useful information to the
local people for decision-making, their lack of wetland quality information may
lead to the difficulty in integrating wetland quality into a cross-studies
research like meta-analysis. In a meta-analysis, a statistical regression
function needs withdrawing from those individual studies for analysis and
prediction. This research introduces the wetland quality factor, a critical but
frequently missed factor, into a meta-analysis and simultaneously considers
other influential factors, such as study method, socio-economic state and other
wetland site characteristics, as well. Thus, a more accurate and valid
meta-regression function is expected. Due to no obvious quality information in
primary studies, we extract two kinds of wetland states from the study context
as relative but globally consistent quality measurement to use in the analysis,
as the first step to explore the effect of wetland quality on values of
ecosystem services.

Shadow prices are well understood and are widely used in economic
applications. However, there are limits to where shadow prices can be applied
assuming their natural interpretation and the fact that they reflect the first
order optimality conditions (FOC). In this paper, we present a simple ad-hoc
example demonstrating that marginal cost associated with exercising an optimal
control may exceed the respective cost estimated from a ratio of shadow prices.
Moreover, such cost estimation through shadow prices is arbitrary and depends
on a particular (mathematically equivalent) formulation of the optimization
problem. These facts render a ratio of shadow prices irrelevant to estimation
of optimal marginal cost. The provided illustrative optimization problem links
to a similar approach of calculating social cost of carbon (SCC) in the widely
used dynamic integrated model of climate and the economy (DICE).

We study Nash implementation by stochastic mechanisms, and provide a
surprisingly simple full characterization, which is in sharp contrast to the
classical, albeit complicated, full characterization in Moore and Repullo
(1990).

The motivation for focusing on economic sanctions is the mixed evidence of
their effectiveness. We assess the role of sanctions on the Russian
international trade flow of agricultural products after 2014. We use a
differences-in-differences model of trade flows data for imported and exported
agricultural products from 2010 to 2020 in Russia. The main expectation was
that the Russian economy would take a hit since it had lost its importers. We
assess the economic impact of the Russian food embargo on agricultural
commodities, questioning whether it has achieved its objective and resulted in
a window of opportunity for the development of the domestic agricultural
sector. Our results confirm that the sanctions have significantly impacted
foodstuff imports; they have almost halved in the first two years since the
sanctions were imposed. However, Russia has embarked on a path to reduce
dependence on food imports and managed self-sufficient agricultural production.

Large amounts of evidence suggest that trust levels in a country are an
important determinant of its macroeconomic growth. In this paper, we
investigate one channel through which trust might support economic performance:
through the levels of patience, also known as time preference in the economics
literature. Following Gabaix and Laibson (2017), we first argue that time
preference can be modelled as optimal Bayesian inference based on noisy signals
about the future, so that it is affected by the perceived certainty of future
outcomes. Drawing on neuroscience literature, we argue that the mechanism
linking trust and patience could be facilitated by the neurotransmitter
oxytocin. On the one hand, it is a neural correlate of trusting behavior. On
the other, it has an impact on the brain's encoding of prediction error, and
could therefore increase the perceived certainty of a neural representation of
a future event. The relationship between trust and time preference is tested
experimentally using the Trust Game. While the paper does not find a
significant effect of trust on time preference or the levels of certainty, it
proposes an experimental design that can successfully manipulate people's
short-term levels of trust for experimental purposes.

Non-Fungible Tokens (NFTs) promise to revolutionize how content creators
(e.g., artists) price and sell their work. One core feature of NFTs is the
option to embed creator royalties which earmark a percentage of future sale
proceeds to creators, each time their NFTs change hands. As popular as this
feature is in practice, its utility is often questioned because buyers, the
argument goes, simply ``price it in at the time of purchase''. As intuitive as
this argument sounds, it is incomplete. We find royalties can add value to
creators in at least three distinct ways. (i) Risk sharing: when creators and
buyers are risk sensitive, royalties can improve trade by splitting the risks
associated with future price volatility; (ii) Dynamic pricing: in the presence
of information asymmetry, royalties can extract more revenues from
better-informed speculators over time, mimicking the benefits of ``dynamic
pricing''; (iii) Price discrimination: when creators sell multi-unit NFT
collections, royalties can better capture value from heterogeneous buyers. Our
results suggest creator royalties play an important and sometimes overlooked
role in the economics of NFTs.

We study the causal effects of lockdown measures on uncertainty and sentiment
on Twitter. To this end, we exploit the quasi-experimental framework created by
the first COVID-19 lockdown in a high-income economy--the unexpected Italian
lockdown in February 2020. We measure changes in public sentiment using deep
learning and dictionary-based methods on the text of daily tweets geolocated
within and near the locked-down areas, before and after the treatment. We
classify tweets into four categories--economics, health, politics, and lockdown
policy--to examine how the policy affected emotions heterogeneously. Using a
staggered difference-in-differences approach, we show that the lockdown did not
have a significantly robust impact on economic uncertainty and sentiment.
However, the policy came at the price of higher uncertainty on health and
politics and more negative political sentiments. These results, which are
robust to a battery of robustness tests, show that lockdowns have relevant
non-health related implications.

Understanding the structure and formation of networks is a central topic in
complexity science. Economic networks are formed by decisions of individual
agents and thus not properly described by established random graph models. In
this article, we establish a model for the emergence of trade networks that is
based on rational decisions of individual agents. The model incorporates key
drivers for the emergence of trade, comparative advantage and economic scale
effects, but also the heterogeneity of agents and the transportation or
transaction costs. Numerical simulations show three macroscopically different
regimes of the emerging trade networks. Depending on the specific
transportation costs and the heterogeneity of individual preferences, we find
centralized production with a star-like trade network, distributed production
with all-to-all trading or local production and no trade. Using methods from
statistical mechanics, we provide an analytic theory of the transitions between
these regimes and estimates for critical parameters values.

Homophily and heterophobia, the tendency for people with similar
characteristics to preferentially interact with (or avoid) each other are
pervasive in human social networks. Here, we develop an extension of the
mathematical theory of urban scaling which describes the effects of homophily
and heterophobia on social interactions and resulting economic outputs of
cities. Empirical tests of our model show that increased residential racial
heterophobia and segregation in U.S. cities are associated with reduced
economic outputs and that the strength of this relationship increased
throughout the 2010s. Our findings provide the means for the formal
incorporation of general homophilic and heterophobic effects into theories of
modern urban science and suggest that racial segregation is increasingly and
adversely impacting the economic performance and connectivity of urban
societies in the U.S.

National Association of Securities Dealers Automated Quotations(NASDAQ) is an
American stock exchange based. It is one of the most valuable stock economic
indices in the world and is located in New York City \cite{pagano2008quality}.
The volatility of the stock market and the influence of economic indicators
such as crude oil, gold, and the dollar in the stock market, and NASDAQ shares
are also affected and have a volatile and chaotic nature
\cite{firouzjaee2022lstm}.In this article, we have examined the effect of oil,
dollar, gold, and the volatility of the stock market in the economic market,
and then we have also examined the effect of these indicators on NASDAQ stocks.
Then we started to analyze the impact of the feedback on the past prices of
NASDAQ stocks and its impact on the current price. Using PCA and Linear
Regression algorithm, we have designed an optimal dynamic learning experience
for modeling these stocks. The results obtained from the quantitative analysis
are consistent with the results of the qualitative analysis of economic
studies, and the modeling done with the optimal dynamic experience of machine
learning justifies the current price of NASDAQ shares.

We uncover the connection between the Fitness-Complexity algorithm, developed
in the economic complexity field, and the Sinkhorn-Knopp algorithm, widely used
in diverse domains ranging from computer science and mathematics to economics.
Despite minor formal differences between the two methods, both converge to the
same fixed-point solution up to normalization. The discovered connection allows
us to derive a rigorous interpretation of the Fitness and the Complexity
metrics as the potentials of a suitable energy function. Under this
interpretation, high-energy products are unfeasible for low-fitness countries,
which explains why the algorithm is effective at displaying nested patterns in
bipartite networks. We also show that the proposed interpretation reveals the
scale invariance of the Fitness-Complexity algorithm, which has practical
implications for the algorithm's implementation in different datasets. Further,
analysis of empirical trade data under the new perspective reveals three
categories of countries that might benefit from different development
strategies.

We present a survey to evaluate crypto-political, crypto-economic, and
crypto-governance sentiment in people who are part of a blockchain ecosystem.
Based on 3710 survey responses, we describe their beliefs, attitudes, and modes
of participation in crypto and investigate how self-reported political
affiliation and blockchain ecosystem affiliation are associated with these. We
observed polarization in questions on perceptions of the distribution of
economic power, personal attitudes towards crypto, normative beliefs about the
distribution of power in governance, and external regulation of blockchain
technologies. Differences in political self-identification correlated with
opinions on economic fairness, gender equity, decision-making power and how to
obtain favorable regulation, while blockchain affiliation correlated with
opinions on governance and regulation of crypto and respondents' semantic
conception of crypto and personal goals for their involvement. We also find
that a theory-driven constructed political axis is supported by the data and
investigate the possibility of other groupings of respondents or beliefs
arising from the data.

The thing about inflation is that it ravages your income if you don not keep
up with it and you do not know when it will stop.

This paper deals with the control of pumps in large-scale water distribution
networks with the aim of minimizing economic costs while satisfying operational
constraints. Finding a control algorithm in combination with a model that can
be applied in real-time is a challenging problem due to the nonlinearities
presented by the pipes and the network sizes. We propose a predictive control
algorithm with a periodic horizon. The method provides a way for the economic
operation of large water networks with a small linear model. Economic
Predictive control with a periodic horizon and a terminal state constraint is
constructed to keep the state trajectories close to an optimal periodic
trajectory. Barrier terms are also included in the cost function to prevent
constraint violations. The proposed method is tested on the EPANET
implementation of the water network of a medium size Danish town (Randers) and
shown to perform as intended under varying conditions.

One of the key tasks in the economy is forecasting the economic agents'
expectations of the future values of economic variables using mathematical
models. The behavior of mathematical models can be irregular, including
chaotic, which reduces their predictive power. In this paper, we study the
regimes of behavior of two economic models and identify irregular dynamics in
them. Using these models as an example, we demonstrate the effectiveness of
evolutionary algorithms and the continuous deep Q-learning method in
combination with Pyragas control method for deriving a control action that
stabilizes unstable periodic trajectories and suppresses chaotic dynamics. We
compare qualitative and quantitative characteristics of the model's dynamics
before and after applying control and verify the obtained results by numerical
simulation. Proposed approach can improve the reliability of forecasting and
tuning of the economic mechanism to achieve maximum decision-making efficiency.

Monthly and weekly economic indicators are often taken to be the largest
common factor estimated from high and low frequency data, either separately or
jointly. To incorporate mixed frequency information without directly modeling
them, we target a low frequency diffusion index that is already available, and
treat high frequency values as missing. We impute these values using multiple
factors estimated from the high frequency data. In the empirical examples
considered, static matrix completion that does not account for serial
correlation in the idiosyncratic errors yields imprecise estimates of the
missing values irrespective of how the factors are estimated. Single equation
and systems-based dynamic procedures that account for serial correlation yield
imputed values that are closer to the observed low frequency ones. This is the
case in the counterfactual exercise that imputes the monthly values of consumer
sentiment series before 1978 when the data was released only on a quarterly
basis. This is also the case for a weekly version of the CFNAI index of
economic activity that is imputed using seasonally unadjusted data. The imputed
series reveals episodes of increased variability of weekly economic information
that are masked by the monthly data, notably around the 2014-15 collapse in oil
prices.

Efficient risk transfer is an important condition for ensuring the
sustainability of a market according to the established economics literature.
In an inefficient market, significant financial imbalances may develop and
potentially jeopardise the solvency of some market participants. The constantly
evolving nature of cyber-threats and lack of public data sharing mean that the
economic conditions required for quoted cyber-insurance premiums to be
considered efficient are highly unlikely to be met. This paper develops Monte
Carlo simulations of an artificial cyber-insurance market and compares the
efficient and inefficient outcomes based on the informational setup between the
market participants. The existence of diverse loss distributions is justified
by the dynamic nature of cyber-threats and the absence of any reliable and
centralised incident reporting. It is shown that the limited involvement of
reinsurers when loss expectations are not shared leads to increased premiums
and lower overall capacity. This suggests that the sustainability of the
cyber-insurance market requires both better data sharing and external sources
of risk tolerant capital.

In recent years, European regulators have debated restricting the time an
online tracker can track a user to protect consumer privacy better. Despite the
significance of these debates, there has been a noticeable absence of any
comprehensive cost-benefit analysis. This article fills this gap on the cost
side by suggesting an approach to estimate the economic consequences of
lifetime restrictions on cookies for publishers. The empirical study on cookies
of 54,127 users who received 128 million ad impressions over 2.5 years yields
an average cookie lifetime of 279 days, with an average value of EUR 2.52 per
cookie. Only 13% of all cookies increase their daily value over time, but their
average value is about four times larger than the average value of all cookies.
Restricting cookies lifetime to one year (two years) decreases their lifetime
value by 25% (19%), which represents a decrease in the value of all cookies of
9% (5%). In light of the EUR 10.60 billion cookie-based display ad revenue in
Europe, such restrictions would endanger EUR 904 million (EUR 576 million)
annually, equivalent to EUR 2.08 (EUR 1.33) per EU internet user. The article
discusses these results' marketing strategy challenges and opportunities for
advertisers and publishers.

In the thirty years since the end of real socialism, Bulgaria's went from
having a rather radically 'different' tax system to adopting flat-rate taxation
with marginal tax rates that fell from figures as high as 40% to 10% for both
the corporate-income tax and the personal-income tax. Crucially, the
econometric forecasting models in use at the Bulgarian Ministry of Finance
hinted at an increase in tax revenue compatible with the so-called 'Laffer
curve'. Similarly, many economists held the view that revenues would have
increased. However, reality fell short of those expectations based on
forecasting models and rooted in mainstream economic theory. Thus, this paper
asks whether there are betterperforming forecasting models for personal-and
corporate-income tax-revenues in Bulgaria that are readily implementable and
overperform the ones currently in use. After articulating a constructive
critique of the current forecasting models, the paper offers readily
implementable, transparent alternatives and proves their superiority.

We present a universal concept for the Value of Information (VoI), based on
the works of Claude Shannon's and Ruslan Stratonovich that can take into
account very general preferences of the agents and results in a single number.
As such it is convenient for applications and also has desirable properties for
decision theory and demand analysis. The Shannon/Stratonovich VoI concept is
compared to alternatives and applied in examples. In particular we apply the
concept to a circular spatial structure well known from many economic models
and allow for various economic transport costs.

Financial market volatility is a crucial factor for investment planning,
option pricing, and financial market regulation, and technology is widely
recognized as a key driver of economic growth. In this project, we investigate
the co-movement of technology and non-technology sectors over the last two
decades. We identify a decoupling phenomenon in the levels and volatility of
the two sectors after 2015 and argue that this cannot be attributed to the
COVID-19 shock. Furthermore, we demonstrate that the technology sector serves
as a leading indicator of growth for the rest of the economy. Using ARIMA
modeling and stationarity tests, we process time series data to test our
hypotheses, finding that the technology sector follows an ARIMA(3,1,3) model,
while the non-technology sector follows an ARIMA(2,1,4) model. Our analysis
encompasses data wrangling, pre-processing, missing value treatment, and
exploratory data analysis, and we discuss the merits and shortcomings of our
work to aid in the interpretation of our results. Overall, our findings shed
new light on the relationship between technology and economic growth. Our
results can be used for understanding labor market fluctuations in the
technology sector and investment planning assessments.

Pursuing educational qualifications later in life is an increasingly common
phenomenon within OECD countries since technological change and automation
continues to drive the evolution of skills needed in many professions. We focus
on the causal impacts to economic returns of degrees completed later in life,
where motivations and capabilities to acquire additional education may be
distinct from education in early years. We find that completing an additional
degree leads to more than \$3000 (AUD, 2019) extra income per year compared to
those who do not complete additional study. For outcomes, treatment and
controls we use the extremely rich and nationally representative longitudinal
data from the Household Income and Labour Dynamics Australia survey (HILDA). To
take full advantage of the complexity and richness of this data we use a
Machine Learning (ML) based methodology for causal effect estimation. We are
also able to use ML to discover sources of heterogeneity in the effects of
gaining additional qualifications. For example, those younger than 45 years of
age when obtaining additional qualifications tend to reap more benefits (as
much as \$50 per week more) than others.

We correct a gap in the proof of Theorem 2 in Matsushima et al. (2010).

Research on residential segregation has been active since the 1950s and
originated in a desire to quantify the level of racial/ethnic segregation in
the United States. The Index of Concentration at the Extremes (ICE), an
operationalization of racialized economic segregation that simultaneously
captures spatial, racial, and income polarization, has been a popular topic in
public health research, with a particular focus on social epidemiology.
However, the construction of the ICE metric usually ignores the spatial
autocorrelation that may be present in the data, and it is usually presented
without indicating its degree of statistical and spatial uncertainty. To
address these issues, we propose reformulating the ICE metric using Bayesian
modeling methodologies. We use a simulation study to evaluate the performance
of each method by considering various segregation scenarios. The application is
based on racialized economic segregation in Georgia, and the proposed modeling
approach will help determine whether racialized economic segregation has
changed over two non-overlapping time points.

I introduce a survey of economic expectations formed by querying a large
language model (LLM)'s expectations of various financial and macroeconomic
variables based on a sample of news articles from the Wall Street Journal
between 1984 and 2021. I find the resulting expectations closely match existing
surveys including the Survey of Professional Forecasters (SPF), the American
Association of Individual Investors, and the Duke CFO Survey. Importantly, I
document that LLM based expectations match many of the deviations from
full-information rational expectations exhibited in these existing survey
series. The LLM's macroeconomic expectations exhibit under-reaction commonly
found in consensus SPF forecasts. Additionally, its return expectations are
extrapolative, disconnected from objective measures of expected returns, and
negatively correlated with future realized returns. Finally, using a sample of
articles outside of the LLM's training period I find that the correlation with
existing survey measures persists -- indicating these results do not reflect
memorization but generalization on the part of the LLM. My results provide
evidence for the potential of LLMs to help us better understand human beliefs
and navigate possible models of nonrational expectations.

This note shows that in Bauer's maximum principle, the assumed convexity of
the objective function can be relaxed to quasiconvexity.

The importance of unspanned macroeconomic variables for Dynamic Term
Structure Models has been intensively discussed in the literature. To our best
knowledge the earlier studies considered only linear interactions between the
economy and the real-world dynamics of interest rates in DTSMs. We propose a
generalized modelling setup for Gaussian DTSMs which allows for unspanned
nonlinear associations between the two and we exploit it in forecasting.
Specifically, we construct a custom sequential Monte Carlo estimation and
forecasting scheme where we introduce Gaussian Process priors to model
nonlinearities. Sequential scheme we propose can also be used with dynamic
portfolio optimization to assess the potential of generated economic value to
investors. The methodology is presented using US Treasury data and selected
macroeconomic indices. Namely, we look at core inflation and real economic
activity. We contrast the results obtained from the nonlinear model with those
stemming from an application of a linear model. Unlike for real economic
activity, in case of core inflation we find that, compared to linear models,
application of nonlinear models leads to statistically significant gains in
economic value across considered maturities.

This thesis provides an in-depth exploration of the Decentralized
Co-governance Crowdfunding (DCC) Ecosystem, a novel solution addressing
prevailing challenges in conventional crowdfunding methods faced by MSMEs and
innovative projects. Among the problems it seeks to mitigate are high
transaction costs, lack of transparency, fraud, and inefficient resource
allocation. Leveraging a comprehensive review of the existing literature on
crowdfunding economic activities and blockchain's impact on organizational
governance, we propose a transformative socio-economic model based on digital
tokens and decentralized co-governance. This ecosystem is marked by a
tripartite community structure - the Labor, Capital, and Governance communities
- each contributing uniquely to the ecosystem's operation. Our research unfolds
the evolution of the DCC ecosystem through distinct phases, offering a novel
understanding of socioeconomic dynamics in a decentralized digital world. It
also delves into the intricate governance mechanism of the ecosystem, ensuring
integrity, fairness, and a balanced distribution of value and wealth.

In this paper, I prove that existence of pure-strategy Nash equilibrium in
games with infinitely many players is equivalent to the axiom of choice.

This paper introduces and formalizes the classical view on supply and demand,
which, we argue, has an integrity independent and distinct from the
neoclassical theory. Demand and supply, before the marginal revolution, are
defined not by an unobservable criterion such as a utility function, but by an
observable monetary variable, the reservation price: the buyer's (maximum)
willingness to pay (WTP) value (a potential price) and the seller's (minimum)
willingness to accept (WTA) value (a potential price) at the marketplace.
Market demand and supply are the cumulative distribution of the buyers' and
sellers' reservation prices, respectively. This WTP-WTA classical view of
supply and demand formed the means whereby market participants were motivated
in experimental economics although experimentalists (trained in neoclassical
economics) were not cognizant of their link to the past. On this foundation was
erected a vast literature on the rules of trading for a host of institutions,
modern and ancient. This paper documents textually this reappraisal of
classical economics and then formalizes it mathematically. A follow-up paper
will articulate a theory of market price formation rooted in this classical
view on supply and demand and in experimental findings on market behavior.

The increasing use of data in various parts of the economic and social
systems is creating a new form of monopoly: data monopolies. We illustrate that
the companies using these strategies, Datalists, are challenging the existing
definitions used within Monopoly Capital Theory (MCT). Datalists are pursuing a
different type of monopoly control than traditional multinational corporations.
They are pursuing monopolistic control over data to feed their productive
processes, increasingly controlled by algorithms and Artificial Intelligence
(AI). These productive processes use information about humans and the creative
outputs of humans as the inputs but do not classify those humans as employees,
so they are not paid or credited for their labour. This paper provides an
overview of this evolution and its impact on monopoly theory. It concludes with
an outline for a research agenda for economics in this space.

We face unprecedented resource stresses in the 21st Century such as global
climate disruptions, freshwater scarcity, expanding energy demands, and the
threat of global pandemics. Historically, societies have relieved resource
stress by increasing trade, innovating technologically, expanding
territorially, regulating, redistributing, making alliances, creating new
economic models, training new skills, as well as conducting war. Do we continue
depleting our already strained resources leading to more regulation,
redistribution, alliances, new economics, and war or do we grow our resources
using innovation, expansion, new economics, and new skills? We present the
argument for evolving space development using asteroid mining as the primary
activity for frontier expansion aided by Low Earth Orbit (LEO), Moon, and Mars
waystations. Forecast space weather is a necessary technology baseline for
developing this pathway. All activity off Earth will require a fundamental
knowledge of how the energetics of space will affect technological progress. We
discuss the critical elements this space economy expansion, including technical
feasibility and infrastructure development, economic and geopolitical viability
complete with the US National Space Weather Program dialogue, ethical and legal
considerations, and risk management. This discussion helps us understand how a
space economy is feasible with the aggregation of many existing and new
technologies into more advanced systems engineering projects.

In spite of being one of the smallest and wealthiest countries in the
European Union in terms of GDP per capita, Luxembourg is facing socio-economic
challenges due to recent rapid urban transformations. This article contributes
by approaching this phenomenon at the most granular and rarely analysed
geographical level - the neighbourhoods of the capital, Luxembourg City. Based
on collected empirical data covering various socio-demographic dimensions for
2020-2021, an ascending hierarchical classification on principal components is
set out to establish neighbourhoods' socio-spatial patterns. In addition, Chi2
tests are carried out to examine residents' socio-demographic characteristics
and determine income inequalities in neighbourhoods. The results reveal a clear
socio-spatial divide along a north-west south-east axis. Moreover, classical
factors such as gender or citizenship differences are revealed to be poorly
determinant of income inequalities compared with the proportion of social
benefits recipients and single residents.

This study explores the marriage matching of only-child individuals and its
outcome. Specifically, we analyze two aspects. First, we investigate how
marital status (i.e., marriage with an only child, that with a non-only child
and remaining single) differs between only children and non-only children. This
analysis allows us to know whether people choose mates in a positive or a
negative assortative manner regarding only-child status, and to predict whether
only-child individuals benefit from marriage matching premiums or are subject
to penalties regarding partner attractiveness. Second, we measure the
premium/penalty by the size of the gap in partner's socio economic status (SES,
here, years of schooling) between only-child and non--only-child individuals.
The conventional economic theory and the observed marriage patterns of positive
assortative mating on only-child status predict that only-child individuals are
subject to a matching penalty in the marriage market, especially when their
partner is also an only child. Furthermore, our estimation confirms that among
especially women marrying an only-child husband, only children are penalized in
terms of 0.57-years-lower educational attainment on the part of the partner.

Air pollution generates substantial health damages and economic costs
worldwide. Pollution exposure varies greatly, both between countries and within
them. However, the degree of air quality inequality and its' trajectory over
time have not been quantified at a global level. Here I use economic inequality
indices to measure global inequality in exposure to ambient fine particles with
2.5 microns or less in diameter (PM2.5). I find high and rising levels of
global air quality inequality. The global PM2.5 Gini Index increased from 0.32
in 2000 to 0.36 in 2020, exceeding levels of income inequality in many
countries. Air quality inequality is mostly driven by differences between
countries and less so by variation within them, as decomposition analysis
shows. A large share of people facing the highest levels of PM2.5 exposure are
concentrated in only a few countries. The findings suggest that research and
policy efforts that focus only on differences within countries are overlooking
an important global dimension of environmental justice.

In economics, risk aversion is modeled via a concave Bernoulli utility within
the expected-utility paradigm. We propose a simple test of expected utility and
concavity. We find little support for either: only 30 percent of the choices
are consistent with a concave utility, only two out of 72 subjects are
consistent with expected utility, and only one of them fits the economic model
of risk aversion. Our findings contrast with the preponderance of seemingly
"risk-averse" choices that have been elicited using the popular multiple-price
list methodology, a result we replicate in this paper. We demonstrate that this
methodology is unfit to measure risk aversion, and that the high prevalence of
risk aversion it produces is due to parametric misspecification.

Economic Complexity (EC) methods have gained increasing popularity across
fields and disciplines. In particular, the EC toolbox has proved particularly
promising in the study of complex and interrelated phenomena, such as the
transition towards a greener economy. Using the EC approach, scholars have been
investigating the relationship between EC and sustainability, proposing to
identify the distinguishing characteristics of green products and to assess the
readiness of productive and technological structures for the sustainability
transition. This article proposes to review and summarize the data, methods,
and empirical literature that are relevant to the study of the sustainability
transition from an EC perspective. We review three distinct but connected
blocks of literature on EC and environmental sustainability. First, we survey
the evidence linking measures of EC to indicators related to environmental
sustainability. Second, we review articles that strive to assess the green
competitiveness of productive systems. Third, we examine evidence on green
technological development and its connection to non-green knowledge bases.
Finally, we summarize the findings for each block and identify avenues for
further research in this recent and growing body of empirical literature.

In today's complex healthcare landscape, the pursuit of delivering optimal
patient care while navigating intricate economic dynamics poses a significant
challenge for healthcare service providers (HSPs). In this already complex
dynamics, the emergence of clinically promising personalized medicine based
treatment aims to revolutionize medicine. While personalized medicine holds
tremendous potential for enhancing therapeutic outcomes, its integration within
resource-constrained HSPs presents formidable challenges. In this study, we
investigate the economic feasibility of implementing personalized medicine. The
central objective is to strike a balance between catering to individual patient
needs and making economically viable decisions. Unlike conventional binary
approaches to personalized treatment, we propose a more nuanced perspective by
treating personalization as a spectrum. This approach allows for greater
flexibility in decision-making and resource allocation. To this end, we propose
a mathematical framework to investigate our proposal, focusing on Bladder
Cancer (BC) as a case study. Our results show that while it is feasible to
introduce personalized medicine, a highly efficient but highly expensive one
would be short-lived relative to its less effective but cheaper alternative as
the latter can be provided to a larger cohort of patients, optimizing the HSP's
objective better.

Unemployment insurance provides temporary cash benefits to eligible
unemployed workers. Benefits are sometimes extended by discretion during
economic slumps. In a model that features temporary benefits and sequential job
opportunities, a worker's reservation wages are studied when policymakers can
make discretionary extensions to benefits. A worker's optimal labor-supply
choice is characterized by a sequence of reservation wages that increases with
weeks of remaining benefits. The possibility of an extension raises the entire
sequence of reservation wages, meaning a worker is more selective when
accepting job offers throughout their spell of unemployment. The welfare
consequences of misperceiving the probability and length of an extension are
investigated. Properties of the model can help policymakers interpret data on
reservation wages, which may be important if extended benefits are used more
often in response to economic slumps, virus pandemics, extreme heat, and
natural disasters.

Labor share, the fraction of economic output accrued as wages, is
inexplicably declining in industrialized countries. Whilst numerous prior works
attempt to explain the decline via economic factors, our novel approach links
the decline to biological factors. Specifically, we propose a theoretical
macroeconomic model where labor share reflects a dynamic equilibrium between
the workforce automating existing outputs, and consumers demanding new output
variants that require human labor. Industrialization leads to an aging
population, and while cognitive performance is stable in the working years it
drops sharply thereafter. Consequently, the declining cognitive performance of
aging consumers reduces the demand for new output variants, leading to a
decline in labor share. Our model expresses labor share as an algebraic
function of median age, and is validated with surprising accuracy on historical
data across industrialized economies via non-linear stochastic regression.

This study aims to use simultaneous quantile regression (SQR) to examine the
impact of macroeconomic and financial uncertainty including global pandemic,
geopolitical risk on the futures returns of crude oil (ROC). The data for this
study is sourced from the FRED (Federal Reserve Economic Database) economic
dataset; the importance of the factors have been validated by using variation
inflation factor (VIF) and principal component analysis (PCA). To fully
understand the combined effect of these factors on WTI, study includes
interaction terms in the multi-factor model. Empirical results suggest that
changes in ROC can have varying impacts depending on the specific period and
market conditions. The results can be used for informed investment decisions
and to construct portfolios that are well-balanced in terms of risk and return.
Structural breaks, such as changes in global economic conditions or shifts in
demand for crude oil, can cause return on crude oil to be sensitive to changes
in different time periods. The unique aspect ness of this study also lies in
its inclusion of explanatory factors related to the pandemic, geopolitical
risk, and inflation.

We critique the formulation of Arrow's no-dictator condition to show that it
does not correspond to the accepted informal/intuitive interpretation. This has
implications for the theorem's scope of applicability.

This paper studies the links between the descriptions of macroeconomic
variables and statistical moments of market trade, price, and return. The
randomness of market trade values and volumes during the averaging interval
{\Delta} results in the random properties of price and return. We describe how
averages and volatilities of price and return depend on the averages,
volatilities, and correlations of market trade values and volumes. The
averages, volatilities, and correlations of market trade, price, and return can
behave randomly during the long interval {\Delta}2>>{\Delta}. To describe their
statistical properties during the long interval {\Delta}2, we introduce the
secondary averaging procedure of trade, price, and return. We explain why, in
the coming years, predictions of market-based probabilities of price and return
will be limited by Gaussian distributions. We discuss the roots of the internal
weakness of the commonly used hedging tool, Value-at-Risk, that cannot be
solved and remains the source of additional risks and losses. One should
consider theoretical economics as a set of successive approximations, each of
which describes the next array of the n-th statistical moments of market
trades, price, return, and macroeconomic variables, which are repeatedly
averaged during the sequence of increasing time intervals.

Coordination development across various subsystems, particularly economic,
social, cultural, and human resources subsystems, is a key aspect of urban
sustainability that has a direct impact on the quality of urbanization.
Hangzhou Metropolitan Circle composing Hangzhou, Huzhou, Jiaxing, Shaoxing, was
the first metropolitan circle approved by National Development and Reform
Commission (NDRC) as a demonstration of economic transformation. To evaluate
the coupling degree of the four cities and to analyze the coordinative
development in the three systems (Digital Economy System, Regional Innovation
System, and Talent Employment System), panel data of these four cities during
the period 2015-2022 were collected. The development level of these three
systems were evaluated using standard deviation and comprehensive development
index evaluation. The results are as follows: (1) the coupling coordination
degree of the four cities in Hangzhou Metropolitan Circle has significant
regional differences, with Hangzhou being a leader while Huzhou, Jiaxing,
Shaoxing have shown steady but slow progress in the coupling development of the
three systems; and (2) the development of digital economy and talent employment
are the breakthrough points for construction in Huzhou, Jiaxing, Shaoxing.
Related suggestions are made based on the coupling coordination results of the
Hangzhou Metropolitan Circle.

Green ammonia is poised to be a key part in the hydrogen economy. This paper
discusses green ammonia supply chains from a higher-level industry perspective
with a focus on market structures. The architecture of upstream and downstream
supply chains are explored. Potential ways to accelerate market emergence are
discussed. Market structure is explored based on transaction cost economics and
lessons from the oil and gas industry. Three market structure prototypes are
developed for different phases. In the infancy, a highly vertically integrated
structure is proposed to reduce risks and ensure capital recovery. A
restructuring towards a disintegrated structure is necessary in the next stage
to improve the efficiency. In the late stage, a competitive structure
characterized by a separation between asset ownership and production activities
and further development of short-term and spot markets is proposed towards a
market-driven industry. Finally, a multi-linear regression model is developed
to evaluate the developed structures using a case in the gas industry. Results
indicate that high asset specificity and uncertainty and low frequency lead to
a more disintegrated market structure, and vice versa, thus supporting the
structures designed. We assume the findings and results contribute to
developing green ammonia supply chains and the hydrogen economy.

To assess the impact of climate change on the Canadian economy, we
investigate and model the relationship between seasonal climate variables and
economic growth across provinces and economic sectors. We further provide
projections of climate change impacts up to the year 2050, taking into account
the diverse climate change patterns and economic conditions across Canada. Our
results indicate that rising Fall temperature anomalies have a notable adverse
impact on Canadian economic growth. Province-wide, Saskatchewan and Manitoba
are anticipated to experience the most substantial declines, whereas British
Columbia and the Maritime provinces will be less impacted. Industry-wide,
Mining is projected to see the greatest benefits, while Agriculture and
Manufacturing are projected to have the most significant downturns. The
disparities of climate change effects between provinces and industries
highlight the need for governments to tailor their policies accordingly, and
offer targeted assistance to regions and industries that are particularly
vulnerable in the face of climate change. Targeted approaches to climate change
mitigation are likely to be more effective than one-size-fits-all policies for
the whole economy.

This review paper focuses on enhancing organizational economic sustainability
through process optimization and human capital effective management utilizing
the soft systems methodology (SSM) approach which offers a holistic approach
for understanding complex real-world challenges. By emphasizing systems
thinking and engaging diverse stakeholders in problem-solving, SSM provides a
comprehensive understanding of the problem's context and potential solutions.
The approach guides a systematic process of inquiry that leads to feasible and
desirable changes in tackling complex problems effectively. Our paper employs
the bibliometric analysis based on the sample of 5171 research articles,
proceedings papers, and book chapters indexed in Web of Science (WoS) database.
We carry out the network cluster analysis using the text data and the
bibliometric data with the help of VOSViewer software. Our results confirm that
as the real-world situations are becoming more complex and the new challenges
such as the global warming and climate change are threatening many economic and
social processes, SSM approach is currently getting back at the forefront of
academic research related to such topics as organizational management and
sustainable human capital efficiency.

This paper addresses a key question in economic forecasting: does pure noise
truly lack predictive power? Economists typically conduct variable selection to
eliminate noises from predictors. Yet, we prove a compelling result that in
most economic forecasts, the inclusion of noises in predictions yields greater
benefits than its exclusion. Furthermore, if the total number of predictors is
not sufficiently large, intentionally adding more noises yields superior
forecast performance, outperforming benchmark predictors relying on dimension
reduction. The intuition lies in economic predictive signals being densely
distributed among regression coefficients, maintaining modest forecast bias
while diversifying away overall variance, even when a significant proportion of
predictors constitute pure noises. One of our empirical demonstrations shows
that intentionally adding 300~6,000 pure noises to the Welch and Goyal (2008)
dataset achieves a noteworthy 10% out-of-sample R square accuracy in
forecasting the annual U.S. equity premium. The performance surpasses the
majority of sophisticated machine learning models.

Control barrier functions (CBFs) and safety-critical control have seen a
rapid increase in popularity in recent years, predominantly applied to systems
in aerospace, robotics and neural network controllers. Control barrier
functions can provide a computationally efficient method to monitor arbitrary
primary controllers and enforce state constraints to ensure overall system
safety. One area that has yet to take advantage of the benefits offered by CBFs
is the field of finance and economics. This manuscript re-introduces three
applications of traditional control to economics, and develops and implements
CBFs for such problems. We consider the problem of optimal advertising for the
deterministic and stochastic case and Merton's portfolio optimization problem.
Numerical simulations are used to demonstrate the effectiveness of using
traditional control solutions in tandem with CBFs and stochastic CBFs to solve
such problems in the presence of state constraints.

Recent discussions on the future of metropolitan cities underscore the
pivotal role of (social) equity, driven by demographic and economic trends.
More equal policies can foster and contribute to a city's economic success and
social stability. In this work, we focus on identifying metropolitan areas with
distinct economic and social levels in the greater Los Angeles area, one of the
most diverse yet unequal areas in the United States. Utilizing American
Community Survey data, we propose a Bayesian model for boundary detection based
on income distributions. The model identifies areas with significant income
disparities, offering actionable insights for policymakers to address social
and economic inequalities. Our approach formalized as a Bayesian structural
learning framework, models areal densities through finite mixture models.
Efficient posterior computation is facilitated by a transdimensional Markov
Chain Monte Carlo sampler. The methodology is validated via extensive
simulations and applied to the income distributions in the greater Los Angeles
area. We identify several boundaries in the income distributions which can be
explained in light of other social dynamics such as crime rates and healthcare,
showing the usefulness of such an analysis to policymakers.

Climate (change) affects the prices of goods and services in different
countries or regions differently. Simply relying on aggregate measures or
summary statistics, such as the impact of average country temperature changes
on HICP headline inflation, conceals a large heterogeneity across (sub-)sectors
of the economy. Additionally, the impact of a weather anomaly on consumer
prices depends not only on its sign and magnitude, but also on its location and
the size of the area affected by the shock. This is especially true for larger
countries or regions with diverse climate zones, since the geographical
distribution of climatic effects plays a role in shaping economic outcomes.
Using time series data of geolocations, we demonstrate that relying solely on
country averages fails to adequately capture and explain the influence of
weather on consumer prices in the euro area. We conclude that the information
content hidden in rich and complex surface data can provide valuable insights
into the role of weather and climate variables for price stability, and more
generally may help to inform economic policy.

Economic periods and financial crises have highlighted the importance of
evaluating financial markets to investors and researchers in recent decades.

We present a classroom game that integrates economics and data-science
competencies. In the first two parts of the game, participants assume the roles
of firms in a procurement market, where they must either adopt competitive
behaviors or have the option to engage in collusion. Success in these parts
hinges on their comprehension of market dynamics. In the third part of the
game, participants transition to the role of competition-authority members.
Drawing from recent literature on machine-learning-based cartel detection, they
analyze the bids for patterns indicative of collusive (cartel) behavior. In
this part of the game, success depends on data-science skills. We offer a
detailed discussion on implementing the game, emphasizing considerations for
accommodating diverging levels of preexisting knowledge in data science.

The COVID-19 pandemic has significantly exacerbated existing educational
disparities in Georgia's K-12 system, particularly in terms of racial and
ethnic achievement gaps. Utilizing machine learning methods, the study conducts
a comprehensive analysis of student achievement rates across different
demographics, regions, and subjects. The findings highlight a significant
decline in proficiency in English and Math during the pandemic, with a
noticeable contraction in score distribution and a greater impact on
economically disadvantaged and Black students. Socio-economic status, as
represented by the Directly Certified Percentage -- the percentage of students
eligible for free lunch, emerges as the most crucial factor, with additional
insights drawn from faculty resources such as teacher salaries and expenditure
on instruction. The study also identifies disparities in achievement rates
between urban and rural settings, as well as variations across counties,
underscoring the influence of geographical and socio-economic factors. The data
suggests that targeted interventions and resource allocation, particularly in
schools with higher percentages of economically disadvantaged students, are
essential for mitigating educational disparities.

Economic structure comparisons between China and Japan have long captivated
development economists. To delve deeper into their sectoral differences from
1995 to 2018, we used the annual input-output tables (IOTs) of both nations to
construct weighted and directed input-output networks (IONs). This facilitated
deeper network analyses. Strength distributions underscored variations in
inter-sector economic interactions. Weighted, directed assortativity
coefficients encapsulated the homophily among connecting sectors' features. By
adjusting emphasis in PageRank centrality, key sectors were identified.
Community detection revealed their clustering tendencies among the sectors. As
anticipated, the analysis pinpointed manufacturing as China's central sector,
while Japan favored services. Yet, at a finer level of the specific sectors,
both nations exhibited varied structural evolutions. Contrastingly, sectoral
communities in both China and Japan demonstrated commendable stability over the
examined duration.

Drawing on work spanning economics, public health, education, sociology, and
law, I formalize theoretically what makes systemic discrimination "systemic."
Injustices do not occur in isolation, but within a complex system of
interdependent factors; and their effects may amplify as a consequence. I
develop a taxonomy of these amplification mechanisms, connecting them to
well-understood concepts in economics that are precise, testable and
policy-oriented. This framework reveals that these amplification mechanisms can
either be directly disrupted, or exploited to amplify the effects of
equity-focused interventions instead. In other words, it shows how to use the
machinery of systemic discrimination against itself. Real-world examples
discussed include but are not limited to reparations for slavery and Jim Crow,
vouchers or place-based neighborhood interventions, police shootings,
affirmative action, and Covid-19.

Effective macroeconomic policies play a crucial role in promoting economic
growth and social stability. This paper models the optimal macroeconomic policy
problem based on the \textit{Stackelberg Mean Field Game} (SMFG), where the
government acts as the leader in policy-making, and large-scale households
dynamically respond as followers. This modeling method captures the asymmetric
dynamic game between the government and large-scale households, and
interpretably evaluates the effects of macroeconomic policies based on
microfoundations, which is difficult for existing methods to achieve. We also
propose a solution for SMFGs, incorporating pre-training on real data and a
model-free \textit{Stackelberg mean-field reinforcement learning }(SMFRL)
algorithm, which operates independently of prior environmental knowledge and
transitions. Our experimental results showcase the superiority of the SMFG
method over other economic policies in terms of performance, efficiency-equity
tradeoff, and SMFG assumption analysis. This paper significantly contributes to
the domain of AI for economics by providing a powerful tool for modeling and
solving optimal macroeconomic policies.

Over the last two decades, a growing body of experimental research has
provided evidence that linguistic frames influence human behaviour in economic
games, beyond the economic consequences of the available actions. This article
proposes a novel framework that transcends the traditional confines of
outcome-based preference models. According to the LENS model, the Linguistic
description of the decision problem triggers Emotional responses and suggests
potential Norms of behaviour, which then interact to shape an individual's
Strategic choice. The article reviews experimental evidence that supports each
path of the LENS model. Furthermore, it identifies and discusses several
critical research questions that arise from this model, pointing towards
avenues for future inquiry.

A natural way of quantifying the ``amount of information'' in decision
problems yields a globally concave value for information. Another (in contrast,
adversarial) way almost never does.

This study aims to develop a framework for multicriteria analysis to evaluate
alternatives for sustainable corn agricultural area planning, considering the
integration of ecological, economic, and social aspects as pillars of
sustainability. The research method uses qualitative and quantitative
approaches to integrate ecological, economic, and social aspects in the
multicriteria analysis. The analysis involves land evaluation, subcriteria
identification, and data integration using Multidimensional Scaling and
Analytical Hierarchy Process methods to prioritize developing sustainable corn
agricultural areas. Based on the results of the RAP-Corn analysis, it indicates
that the ecological dimension depicts less sustainability. The AHP results
yield weight distribution and highly relevant scores that describe tangible
preferences. Priority directions are grouped as strategic steps toward
achieving the goals of sustainable corn agricultural area planning.

The Cold War was the defining episode of geopolitical fragmentation in the
twentieth century. Trade between East and West across the Iron Curtain (a
symbolical and physical barrier dividing Europe into two distinct areas) was
restricted, but the severity of these restrictions varied over time. We
quantify the trade and welfare effects of the Iron Curtain and show how the
difficulty of trading across the Iron Curtain fluctuated throughout the Cold
War. Using a novel dataset on trade between the two economic blocs and a
quantitative trade model, we find that while the Iron Curtain at its height
represented a tariff equivalent of 48% in 1951, trade between East and West
gradually became easier until the fall of the Berlin Wall in 1989. Despite the
easing of trade restrictions, we estimate that the Iron Curtain roughly halved
East-West trade flows and caused substantial welfare losses in the Eastern bloc
countries that persisted until the end of the Cold War. Conversely, the Iron
Curtain led to an increase in intra-bloc trade, especially in the Eastern bloc,
which outpaced the integration of Western Europe in the run-up to the formation
of the European Union.

The renowned van der Waals (VDW) state equation quantifies the equilibrium
relationship between pressure $P$, volume $V$ and temperature $k_{B}T$ of a
real gas. We assign new variable interpretations adapted to the economic
context: $P \rightarrow Y$, representing price; $V \rightarrow X$, representing
demand; and $k_{B}T \rightarrow \kappa$, representing income, to describe an
economic state equilibrium. With this reinterpretation, the price elasticity of
demand (PED) and the income elasticity of demand (YED) are non-constant factors
and may exhibit a singularity of the cusp-catastrophe type. Within this
economic framework, the counterpart of VDW liquid-gas phase transition
illustrates a substitution mechanism where one product or service is replaced
by an alternative substitute. The conceptual relevance of this reinterpretation
is discussed qualitatively and quantitatively via several illustrations ranging
from transport (carpooling), medical context (generic versus original
medication) and empirical data drawn from the electricity market in Germany.

Generative artificial intelligence (AI) systems are trained on large data
corpora to generate new pieces of text, images, videos, and other media. There
is growing concern that such systems may infringe on the copyright interests of
training data contributors. To address the copyright challenges of generative
AI, we propose a framework that compensates copyright owners proportionally to
their contributions to the creation of AI-generated content. The metric for
contributions is quantitatively determined by leveraging the probabilistic
nature of modern generative AI models and using techniques from cooperative
game theory in economics. This framework enables a platform where AI developers
benefit from access to high-quality training data, thus improving model
performance. Meanwhile, copyright owners receive fair compensation, driving the
continued provision of relevant data for generative model training. Experiments
demonstrate that our framework successfully identifies the most relevant data
sources used in artwork generation, ensuring a fair and interpretable
distribution of revenues among copyright owners.

The unfolding climate crisis is a physical manifestation of the damage that
market economy, driven by the high intensity consumption of fossil fuels, has
inflicted on the Earth System and on the stability conditions that were
established by a complex conjugation of natural factors during the Holoecene.
The magnitude of the human activities and its predatory nature is such that it
is no longer possible to consider the Earth System and the services it provides
for the habitability of the planet, the so-called natural capital, as an
economical externality. Thus one is left with two main choices in what concerns
the sustaintability of the planet's habitability: radical economic degrowth or
highly efficient solutions to internalise the maintenance and the restoration
of ecosystems and the services of the Earth System. It is proposed that an
interesting strategy for the latter is to consider the natural capital as a
stock option.

This study reevaluates the traditional understanding of the "political
resource curse" by examining the unique impact of energy transition metals,
specifically cobalt, on local-level conflicts in Africa. Contrary to previous
studies that primarily focus on high-value minerals and their political
outcomes resulted from substantial economic revenues, this study investigates
cobalt's influence on local conflict. Despite its strategic importance,
cobalt's limited commercial value presents a unique yet critical case for
analysis. Different with the prevailing view that links mineral reserves with
increased conflict, this research finds that regions rich in cobalt experience
a reduction in conflict. This decrease is attributed to enhanced government
security measures, which are implemented independently of the economic benefits
derived from cobalt as a commodity. The study utilizes a combination of
georeferenced data and a difference-in-difference design to analyze the causal
relationship between cobalt deposits and regional conflict. The findings
suggest that the presence of cobalt deposits leads to enhanced security
interventions by governments, effectively reducing the likelihood of
non-governmental actors taking control of these territories. This pattern
offers a new perspective on the role of energy transition metals in shaping
conflict and governance, highlighting the need to reassess theoretical
frameworks related to the political implications of natural resources with the
ongoing energy revolution.

The COVID-19 pandemic has disrupted traditional academic collaboration
patterns, prompting a unique opportunity to analyze the influence of peer
effects and coauthorship dynamics on research output. Using a novel dataset,
this paper endeavors to make a first cut at investigating the role of peer
effects on the productivity of economics scholars, measured by the number of
publications, in both pre-pandemic and pandemic times. Results show that peer
effect is significant for the pre-pandemic time but not for the pandemic time.
The findings contribute to our understanding of how research collaboration
influences knowledge production and may help guide policies aimed at fostering
collaboration and enhancing research productivity in the academic community.

We propose a high level network architecture for an economic system that
integrates money, governance and reputation. We introduce a method for issuing,
and redeeming a digital coin using a mechanism to create a sustainable global
economy and a free market. To maintain a currency's value over time, and
therefore be money proper, we claim it must be issued by the buyer and backed
for value by the seller, exchanging the products of labour, in a free market.
We also claim that a free market and sustainable economy cannot be maintained
using economically arbitrary creation and allocation of money. Nakamoto, with
Bitcoin, introduced a new technology called the cryptographic blockchain to
operate a decentralised and distributed accounts ledger without the need for an
untrusted third party. This blockchain technology creates and allocates new
digital currency as a reward for "proof-of-work", to secure the network.
However, no currency, digital or otherwise, has solved how to create and
allocate money in an economically non-arbitrary way, or how to govern and trust
a world-scale free enterprise money system. We propose an "Ontologically
Networked Exchange" (ONE), with purpose as its highest order domain. Each
purpose is defined in a contract, and the entire economy of contracts is
structured in a unified ontology. We claim to secure the ONE network using
economically non-arbitrary methodologies and economically incented human
behaviour. Decisions influenced by reputation help to secure the network
without an untrusted third party. The stack of contracts, organised in a
unified ontology, functions as a super recursive algorithm, with individual use
programming the algorithm, acting as the "oracle". The state of the algorithm
becomes the "memory" of a scalable and trustable artificial intelligence (AI).
This AI offers a new platform for what we call the "Autonomy-of-Things" (AoT).

A macroeconomic model based on the economic variables (i) assets, (ii)
leverage (defined as debt over asset) and (iii) trust (defined as the maximum
sustainable leverage) is proposed to investigate the role of credit in the
dynamics of economic growth, and how credit may be associated with both
economic performance and confidence. Our first notable finding is the mechanism
of reward/penalty associated with patience, as quantified by the return on
assets. In regular economies where the EBITA/Assets ratio is larger than the
cost of debt, starting with a trust higher than leverage results in the highest
long-term return on assets (which can be seen as a proxy for economic growth).
Our second main finding concerns a recommendation for the reaction of a central
bank to an external shock that affects negatively the economic growth. We find
that late policy intervention in the model economy results in the highest
long-term return on assets and largest asset value. But this comes at the cost
of suffering longer from the crisis until the intervention occurs. The
phenomenon can be ascribed to the fact that postponing intervention allows
trust to increase first, and it is most effective to intervene when trust is
high. These results derive from two fundamental assumptions underlying our
model: (a) trust tends to increase when it is above leverage; (b) economic
agents learn optimally to adjust debt for a given level of trust and amount of
assets. Using a Markov Switching Model for the EBITA/Assets ratio, we have
successfully calibrated our model to the empirical data of the return on equity
of the EURO STOXX 50 for the time period 2000-2013. We find that dynamics of
leverage and trust can be highly non-monotonous with curved trajectories, as a
result of the nonlinear coupling between the variables.

The COVID-19 pandemic has caused a massive economic shock across the world
due to business interruptions and shutdowns from social-distancing measures. To
evaluate the socio-economic impact of COVID-19 on individuals, a micro-economic
model is developed to estimate the direct impact of distancing on household
income, savings, consumption, and poverty. The model assumes two periods: a
crisis period during which some individuals experience a drop in income and can
use their precautionary savings to maintain consumption; and a recovery period,
when households save to replenish their depleted savings to pre-crisis level.
The San Francisco Bay Area is used as a case study, and the impacts of a
lockdown are quantified, accounting for the effects of unemployment insurance
(UI) and the CARES Act federal stimulus. Assuming a shelter-in-place period of
three months, the poverty rate would temporarily increase from 17.1% to 25.9%
in the Bay Area in the absence of social protection, and the lowest income
earners would suffer the most in relative terms. If fully implemented, the
combination of UI and CARES could keep the increase in poverty close to zero,
and reduce the average recovery time, for individuals who suffer an income
loss, from 11.8 to 6.7 months. However, the severity of the economic impact is
spatially heterogeneous, and certain communities are more affected than the
average and could take more than a year to recover. Overall, this model is a
first step in quantifying the household-level impacts of COVID-19 at a regional
scale. This study can be extended to explore the impact of indirect
macroeconomic effects, the role of uncertainty in households' decision-making
and the potential effect of simultaneous exogenous shocks (e.g., natural
disasters).

A growing number of countries have established programs to attract immigrants
who can contribute to their economy. Research suggests that an immigrant's
initial arrival location plays a key role in shaping their economic success.
Yet immigrants currently lack access to personalized information that would
help them identify optimal destinations. Instead, they often rely on
availability heuristics, which can lead to the selection of sub-optimal landing
locations, lower earnings, elevated outmigration rates, and concentration in
the most well-known locations. To address this issue and counteract the effects
of cognitive biases and limited information, we propose a data-driven decision
helper that draws on behavioral insights, administrative data, and machine
learning methods to inform immigrants' location decisions. The decision helper
provides personalized location recommendations that reflect immigrants'
preferences as well as data-driven predictions of the locations where they
maximize their expected earnings given their profile. We illustrate the
potential impact of our approach using backtests conducted with administrative
data that links landing data of recent economic immigrants from Canada's
Express Entry system with their earnings retrieved from tax records.
Simulations across various scenarios suggest that providing location
recommendations to incoming economic immigrants can increase their initial
earnings and lead to a mild shift away from the most populous landing
destinations. Our approach can be implemented within existing institutional
structures at minimal cost, and offers governments an opportunity to harness
their administrative data to improve outcomes for economic immigrants.

A substantial increase in illegal extraction of the benthic resources in
central Chile is likely driven by an interplay of numerous socio-economic local
factors that threatens the success of the fisheries management areas (MA)
system. To assess this problem, the exploitation state of a commercially
important benthic resource (i.e., keyhole limpet) in the MAs was related with
socio-economic drivers of the small-scale fisheries. The potential drivers of
illegal extraction included rebound effect of fishing effort displacement by
MAs, level of enforcement, distance to surveillance authorities, wave exposure
and land-based access to the MA, and alternative economic activities in the
fishing village. The exploitation state of limpets was assessed by the
proportion of the catch that is below the minimum legal size, with high
proportions indicating a poor state, and by the relative median size of limpets
fished within the MAs in comparison with neighbouring OA areas, with larger
relative sizes in the MA indicating a good state. A Bayesian-Belief Network
approach was adopted to assess the effects of potential drivers of illegal
fishing on the status of the benthic resource in the MAs. Results evidenced the
absence of a direct link between the level of enforcement and the status of the
resource, with other socio-economic (e.g., alternative economic activities in
the village) and context variables (e.g., fishing effort or distance to
surveillance authorities) playing important roles. Scenario analysis explored
variables that are susceptible to be managed, evidencing that BBN is a powerful
approach to explore the role of multiple external drivers, and their impact on
marine resources, in complex small-scale fisheries.

The agent-based Yard-Sale model of wealth inequality is generalized to
incorporate exponential economic growth and its distribution. The distribution
of economic growth is nonuniform and is determined by the wealth of each agent
and a parameter $\lambda$. Our numerical results indicate that the model has a
critical point at $\lambda=1$ between a phase for $\lambda < 1$ with economic
mobility and exponentially growing wealth of all agents and a non-stationary
phase for $\lambda \geq 1$ with wealth condensation and no mobility. We define
the energy of the system and show that the system can be considered to be in
thermodynamic equilibrium for $\lambda < 1$. Our estimates of various critical
exponents are consistent with a mean-field theory (see following paper). The
exponents do not obey the usual scaling laws unless a combination of parameters
that we refer to as the Ginzburg parameter is held fixed as the transition is
approached. The model illustrates that both poorer and richer agents benefit
from economic growth if its distribution does not favor the richer agents too
strongly. This work and the accompanying theory paper contribute to
understanding whether the methods of equilibrium statistical mechanics can be
applied to economic systems.

This article is a supplement to my main contribution to the Routledge
Handbook of Complexity Economics (2023). On the basis of three recent papers,
it presents an unconventional perspective on economic inequality from a
statistical physics point of view. One section demonstrates empirical evidence
for the exponential distribution of income in 67 countries around the world.
The exponential distribution was not familiar to mainstream economists until it
was introduced by physicists by analogy with the Boltzmann-Gibbs distribution
of energy and subsequently confirmed in empirical data for many countries.
Another section reviews the two-class structure of income distribution in the
USA. While the exponential law describes the majority of population (the lower
class), the top tail of income distribution (the upper class) is characterized
by the Pareto power law, and there is no clearly defined middle class in
between. As a result, the whole distribution can be very well fitted by using
only three parameters. Historical evolution of these parameters and inequality
trends are analyzed from 1983 to 2018. Finally, global inequality in energy
consumption and CO2 emissions per capita is studied using the empirical data
from 1980 to 2017. Global inequality, as measured by the Gini coefficient G,
has been decreasing until around 2010, but then saturated at the level G=0.5.
The saturation at this level was theoretically predicted on the basis of the
maximal entropy principle, well before the slowdown of the global inequality
decrease became visible in the data. This effect is attributed to accelerated
mixing of the world economy due to globalization, which brings it to the state
of maximal entropy and thus results in global economic stagnation. This
observation has profound consequences for social and geopolitical stability and
the efforts to deal with the climate change.

We develop a model for the evolution of economic entities within a
geographical type of framework. On a square symmetry lattice made of three
(economic) regions, firms, described by a scalar fitness, are allowed to move,
adapt, merge or create spin-offs under predetermined rules, in a space and time
dependent economic environment. We only consider here one timely variation of
the ''external economic field condition''. For the firm fitness evolution we
take into account a constraint such that the disappearance of a firm modifies
the fitness of nearest neighboring ones, as in Bak-Sneppen population fitness
evolution model. The concentration of firms, the averaged fitness, the regional
distribution of firms, and fitness for different time moments, the number of
collapsed, merged and new firms as a function of time have been recorded and
are discussed. Also the asymptotic values of the number of firms present in the
three regions together with their average fitness, as well as the number of
respective births and collapses in the three regions are examined. It appears
that a sort of $critical$ selection pressure exists. A power law dependence,
signature of self-critical organization is seen in the birth and collapse
asymptotic values for a high selection pressure only. A lack of
self-organization is also seen at region borders.

"What Should be Hidden and Open in Computer Security: Lessons from Deception,
the Art of War, Law, and Economic Theory" Peter P. Swire, George Washington
University.
  Imagine a military base. It is defended against possible attack. Do we expect
the base to reveal the location of booby traps and other defenses? No. But for
many computer applications,a software developer will need to reveal a great
deal about the code to get other system owners to trust the code and know how
to operate with it.
  This article examines these conflicting intuitions and develops a theory
about what should be open and hidden in computer security. Part I of the paper
shows how substantial openness is typical for major computer security topics,
such as firewalls, packaged software, and encryption. Part II shows what
factors will lead to openness or hiddenness in computer security.
  Part III presents an economic analysis of the issue of what should be open in
computer security. The owner who does not reveal the booby traps is like a
monopolist, while the open-source software supplier is in a competitive market.
This economic approach allows us to identify possible market failures in how
much openness occurs for computer security.
  Part IV examines the contrasting approaches of Sun Tzu and Clausewitz to the
role of hiddenness and deception in military strategy. The computer security,
economic, and military strategy approaches thus each show factors relevant to
what should be kept hidden in computer security. Part V then applies the theory
to a range of current legal and technical issues.

Notions of Darwinian selection have been implicit in economic theory for at
least sixty years. Richard Nelson and Sidney Winter have argued that while
evolutionary thinking was prevalent in prewar economics, the postwar
Neoclassical school became almost entirely preoccupied with equilibrium
conditions and their mathematical conditions. One of the problems with the
economic interpretation of firm selection through competition has been a weak
grasp on an incomplete scientific paradigm. As I.F. Price notes, "The
biological metaphor has long lurked in the background of management theory
largely because the message of 'survival of the fittest' (usually wrongly
attributed to Charles Darwin rather than Herbert Spencer) provides a seemingly
natural model for market competition (e.g. Alchian 1950, Merrell 1984,
Henderson 1989, Moore 1993), without seriously challenging the underlying
paradigms of what an organisation is." In this paper we examine the application
of dynamic fitness landscape models to economic theory, particularly the theory
of technology substitution, drawing on recent work by Kauffman, Arthur,
McKelvey, Nelson and Winter, and Windrum and Birchenhall. In particular we use
Professor Post's early work with John Holland on the genetic algorithm to
explain some of the key differences between static and dynamic approaches to
economic modeling.

The financial crisis of 2008, which started with an initially well-defined
epicenter focused on mortgage backed securities (MBS), has been cascading into
a global economic recession, whose increasing severity and uncertain duration
has led and is continuing to lead to massive losses and damage for billions of
people. Heavy central bank interventions and government spending programs have
been launched worldwide and especially in the USA and Europe, with the hope to
unfreeze credit and boltster consumption. Here, we present evidence and
articulate a general framework that allows one to diagnose the fundamental
cause of the unfolding financial and economic crisis: the accumulation of
several bubbles and their interplay and mutual reinforcement has led to an
illusion of a "perpetual money machine" allowing financial institutions to
extract wealth from an unsustainable artificial process. Taking stock of this
diagnostic, we conclude that many of the interventions to address the so-called
liquidity crisis and to encourage more consumption are ill-advised and even
dangerous, given that precautionary reserves were not accumulated in the "good
times" but that huge liabilities were. The most "interesting" present times
constitute unique opportunities but also great challenges, for which we offer a
few recommendations.

Using the formalism of Lyapunov potential function it is shown that the
stability principles for biomass in the ecosystem and for employment in
economics are mathematically similar. The ecosystem is found to have a stable
and an unstable stationary state with high (forest) and low (grasslands)
biomass, respectively. In economics, there is a stable stationary state with
high employment, which corresponds to mass production of conventional goods
sold at low cost price, and an unstable stationary state with lower employment,
which corresponds to production of novel goods appearing in the course of
technological progress. An additional stable stationary state is described for
economics, the one corresponding to very low employment in production of life
essentials such as energy and raw materials. In this state the civilization
currently pays 10% of global GDP for energy produced by a negligible minority
of the working population (currently ~0.2%) and sold at prices greatly
exceeding the cost price by 40 times. It is shown that economic ownership over
energy sources is equivalent to equating measurable variables of different
dimensions (stores and fluxes), which leads to effective violation of the laws
of energy and matter conservation.

Understanding how spatial configurations of economic activity emerge is
important when formulating spatial planning and economic policy. A simple model
was proposed by Simon, who assumed that firms grow at a rate proportional to
their size, and that new divisions of firms with certain probabilities relocate
to other firms or to new centres of economic activity. Simon's model produces
realistic results in the sense that the sizes of economic centres follow a Zipf
distribution, which is also observed in reality. It lacks realism in the sense
that mechanisms such as cluster formation, congestion (defined as an overly
high density of the same activities) and dependence on the spatial distribution
of external parties (clients, labour markets) are ignored.
  The present paper proposed an extension of the Simon model that includes both
centripetal and centrifugal forces. Centripetal forces are included in the
sense that firm divisions are more likely to settle in locations that offer a
higher accessibility to other firms. Centrifugal forces are represented by an
aversion of a too high density of activities in the potential location. The
model is implemented as an agent-based simulation model in a simplified spatial
setting. By running both the Simon model and the extended model, comparisons
are made with respect to their effects on spatial configurations. To this end a
series of metrics are used, including the rank-size distribution and indices of
the degree of clustering and concentration.

In this chapter the complex systems are discussed in the context of economic
and business policy and decision making. It will be showed and motivated that
social systems are typically chaotic, non-linear and/or non-equilibrium and
therefore complex systems. It is discussed that the rapid change in global
consumer behaviour is underway, that further increases the complexity in
business and management. For policy making under complexity, following
principles are offered: openness and international competition, tolerance and
variety of ideas, self-reliability and low dependence on external help. The
chapter contains four applications that build on the theoretical motivation of
complexity in social systems. The first application demonstrates that small
economies have good prospects to gain from the global processes underway, if
they can demonstrate production flexibility, reliable business ethics and good
risk management. The second application elaborates on and discusses the
opportunities and challenges in decision making under complexity from macro and
micro economic perspective. In this environment, the challenges for corporate
management are being also permanently changed: the balance between short term
noise and long term chaos whose attractor includes customers, shareholders and
employees must be found. The emergence of chaos in economic relationships is
demonstrated by a simple system of differential equations that relate the
stakeholders described above. The chapter concludes with two financial
applications: about debt and risk management. The non-equilibrium economic
establishment leads to additional problems by using excessive borrowing;
unexpected downturns in economy can more easily kill companies. Finally, the
demand for quantitative improvements in risk management is postulated.

For those concerned with the long-term value of their accounts, it can be a
challenge to plan in the present for inflation-adjusted economic growth over
coming decades. Here, I argue that there exists an economic constant that
carries through time, and that this can help us to anticipate the more distant
future: global economic wealth has a fixed link to civilization's overall rate
of energy consumption from all sources; the ratio of these two quantities has
not changed over the past 40 years that statistics are available. Power
production and wealth rise equally quickly because civilization, like any other
system in the universe, must consume and dissipate its energy reserves in order
to sustain its current size. One perspective might be that financial wealth
must ultimately collapse as we deplete our energy reserves. However, we can
also expect that highly aggregated quantities like global wealth have inertia,
and that growth rates must persist. Exceptionally rapid innovation in the two
decades following 1950 allowed for unprecedented acceleration of
inflation-adjusted rates of return. But today, real innovation rates are more
stagnant. This means that, over the coming decade or so, global GDP and wealth
should rise fairly steadily at an inflation-adjusted rate of about 2.2% per
year.

The field of study of complex systems considers that the dynamics of complex
systems are founded on universal principles that may be used to describe a
great variety of scientific and technological approaches of different types of
natural, artificial, and social systems. Several authors have suggested that
earthquake dynamics and the dynamics of economic (financial) systems can be
analyzed within similar mathematical frameworks. We apply concepts of the
nonextensive statistical physics, on time-series data of observable
manifestations of the underlying complex processes ending up to these different
extreme events, in order to support the suggestion that a dynamical analogy
exists between a financial crisis (in the form of share or index price
collapse) and a single earthquake. We also investigate the existence of such an
analogy by means of scale-free statistics (the Gutenberg-Richter distribution
of event sizes). We show that the populations of: (i) fracto-electromagnetic
events rooted in the activation of a single fault, emerging prior to a
significant earthquake, (ii) the trade volume events of different shares /
economic indices, prior to a collapse, and (iii) the price fluctuation
(considered as the difference of maximum minus minimum price within a day)
events of different shares / economic indices, prior to a collapse, follow both
the traditional Gutenberg-Richter law as well as a nonextensive model for
earthquake dynamics, with similar parameter values. The obtained results imply
the existence of a dynamic analogy between earthquakes and economic crises,
which moreover follow the dynamics of seizures, magnetic storms and solar
flares.

Economic and financial networks play a crucial role in various important
processes, including economic integration, globalization, and financial crises.
Of particular interest is understanding whether the temporal evolution of a
real economic network is in a (quasi-)stationary equilibrium, i.e.
characterized by smooth structural changes rather than abrupt transitions.
Smooth changes in quasi-equilibrium networks can be generally controlled for,
and largely predicted, via an appropriate rescaling of structural quantities,
while this is generally not possible for abrupt transitions in non-stationary
networks. Here we study whether real economic networks are in or out of
equilibrium by checking their consistency with quasi-equilibrium
maximum-entropy ensembles of graphs. As illustrative examples, we consider the
International Trade Network (ITN) and the Dutch Interbank Network (DIN). We
show that, despite the globalization process, the ITN is an almost perfect
example of quasi-equilibrium network, while the DIN is clearly an
out-of-equilibrium network undergoing major structural changes and displaying
non-stationary dynamics. Among the out-of-equilibrium properties of the DIN, we
find striking early-warning signals of the interbank crisis of 2008.

The combination of the network theoretic approach with recently available
abundant economic data leads to the development of novel analytic and
computational tools for modelling and forecasting key economic indicators. The
main idea is to introduce a topological component into the analysis, taking
into account consistently all higher-order interactions. We present three basic
methodologies to demonstrate different approaches to harness the resulting
network gain. First, a multiple linear regression optimisation algorithm is
used to generate a relational network between individual components of national
balance of payment accounts. This model describes annual statistics with a high
accuracy and delivers good forecasts for the majority of indicators. Second, an
early-warning mechanism for global financial crises is presented, which
combines network measures with standard economic indicators. From the analysis
of the cross-border portfolio investment network of long-term debt securities,
the proliferation of a wide range of over-the-counter-traded financial
derivative products, such as credit default swaps, can be described in terms of
gross-market values and notional outstanding amounts, which are associated with
increased levels of market interdependence and systemic risk. Third,
considering the flow-network of goods traded between G-20 economies, network
statistics provide better proxies for key economic measures than conventional
indicators. For example, it is shown that a country's gate-keeping potential,
as a measure for local power, projects its annual change of GDP generally far
better than the volume of its imports or exports.

Economic integration, globalization and financial crises represent examples
of processes whose understanding requires the analysis of the underlying
network structure. Of particular interest is establishing whether a real
economic network is in a state of (quasi)stationary equilibrium, i.e.
characterized by smooth structural changes rather than abrupt transitions.
While in the former case the behaviour of the system can be reasonably
controlled and predicted, in the latter case this is generally impossible.
Here, we propose a method to assess whether a real economic network is in a
quasi-stationary state by checking the consistency of its structural evolution
with appropriate quasi-equilibrium maximum-entropy ensembles of graphs. As
illustrative examples, we consider the International Trade Network (ITN) and
the Dutch Interbank Network (DIN). We find that the ITN is an almost perfect
example of quasi-equilibrium network, while the DIN is clearly
out-of-equilibrium. In the latter, the entity of the deviation from
quasi-stationarity contains precious information that allows us to identify
remarkable early warning signals of the interbank crisis of 2008. These early
warning signals involve certain dyadic and triadic topological properties,
including dangerous 'debt loops' with different levels of interbank
reciprocity.

How do we assign value to economic transactions? To answer this question, we
must consider whether the value of objects is inherent, is a product of social
interaction, or involves other mechanisms. Economic theory predicts that there
is an optimal price for any market transaction, and can be observed during
auctions or other bidding processes. However, there are also social, cultural,
and cognitive components to the assignation of value, which can be observed in
both human and non-human Primate societies. While behaviors related to these
factors are embedded in market interactions, they also involve a biological
substrate for the assignation of value (valuation). To synthesize this
diversity of perspectives, we will propose that the process of valuation can be
modeled computationally and conceived of as a set of interrelated cultural
evolutionary, cognitive, and neural processes. To do this, contextual geometric
structures (CGS) will be placed in an agent-based context (minimal and
compositional markets). Objects in the form of computational propositions can
be acquired and exchanged, which will determine the value of both singletons
and linked propositions. Expected results of this model will be evaluated in
terms of their contribution to understanding human economic phenomena. The
paper will focus on computational representations and how they correspond to
real-world concepts. The implications for evolutionary economics and our
contemporary understanding of valuation and market dynamics will also be
discussed.

Statistical evaluations of the economic mobility of a society are more
difficult than measurements of the income distribution, because they require to
follow the evolution of the individuals' income for at least one or two
generations. In micro-to-macro theoretical models of economic exchanges based
on kinetic equations, the income distribution depends only on the asymptotic
equilibrium solutions, while mobility estimates also involve the detailed
structure of the transition probabilities of the model, and are thus an
important tool for assessing its validity. Empirical data show a remarkably
general negative correlation between economic inequality and mobility, whose
explanation is still unclear. It is therefore particularly interesting to study
this correlation in analytical models. In previous work we investigated the
behavior of the Gini inequality index in kinetic models in dependence on
several parameters which define the binary interactions and the taxation and
redistribution processes: saving propensity, taxation rates gap, tax evasion
rate, welfare means-testing etc. Here, we check the correlation of mobility
with inequality by analyzing the mobility dependence from the same parameters.
According to several numerical solutions, the correlation is confirmed to be
negative.

This paper analyzes some economic and demographic features of Italians living
in cities containing a Saint name in their appellation (hagiotoponyms).
Demographic data come from the surveys done in the 15th (2011) Italian Census,
while the economic wealth of such cities is explored through their recent
[2007-2011] aggregated tax income (ATI). This cultural problem is treated from
various points of view. First, the exact list of hagiotoponyms is obtained
through linguistic and religiosity criteria. Next, it is examined how such
cities are distributed in the Italian regions. Demographic and economic
perspectives are also offered at the Saint level, i.e. calculating the
cumulated values of the number of inhabitants and the ATI, "per Saint", as well
as the corresponding relative values taking into account the Saint popularity.
On one hand, frequency-size plots and cumulative distribution function plots,
and on the other hand, scatter plots and rank-size plots between the various
quantities are shown and discussed in order to find the importance of
correlations between the variables. It is concluded that rank-rank correlations
point to a strong Saint effect, which explains what actually Saint-based
toponyms imply in terms of comparing economic and demographic data.

We propose a novel approach to generate chaotic business cycles in a
deterministic setting. Rather than producing chaos endogenously, we consider
aggregate economic models with limit cycles and equilibriums, subject them to
chaotic exogenous shocks and obtain chaotic cyclical motions. Thus, we
emphasize that chaotic cycles, which are inevitable in economics, are not only
interior properties of economic models, but also can be considered as a result
of interaction of several economical systems. This provides a comprehension of
chaos (unpredictability, lack of forecasting) and control of chaos as a global
economic phenomenon from the deterministic point of view.
  We suppose that the results of our paper are contribution to the mixed
exogenous-endogenous theories of business cycles in classification by P.A.
Samuelson [76]. Moreover, they demonstrate that the irregularity of the
extended chaos can be structured, and this distinguishes them from the
generalized synchronization. The advantage of the knowledge of the structure is
that by applying instruments, which already have been developed for
deterministic chaos one can control the chaos, emphasizing a parameter or a
type of motion. For the globalization of cyclic chaos phenomenon we utilize new
mechanisms such that entrainment by chaos, attraction of chaotic cycles by
equilibriums and bifurcation of chaotic cycles developed in our earlier papers.

Evaluating the economies of countries and their relations with products in
the global market is a central problem in economics, with far-reaching
implications to our theoretical understanding of the international trade as
well as to practical applications, such as policy making and financial
investment planning. The recent Economic Complexity approach aims to quantify
the competitiveness of countries and the quality of the exported products based
on the empirical observation that the most competitive countries have
diversified exports, whereas developing countries only export few low quality
products -- typically those exported by many other countries. Two different
metrics, Fitness-Complexity and the Method of Reflections, have been proposed
to measure country and product score in the Economic Complexity framework. We
use international trade data and a recent ranking evaluation measure to
quantitatively compare the ability of the two metrics to rank countries and
products according to their importance in the network. The results show that
the Fitness-Complexity metric outperforms the Method of Reflections in both the
ranking of products and the ranking of countries. We also investigate a
Generalization of the Fitness-Complexity metric and show that it can produce
improved rankings provided that the input data are reliable.

Currently, the increase in financial returns from economic operations is
constrained in view of the lack of a single efficiency criterion, which allows
uniquely identify the business operation by their main feature - the
possibility of obtaining the maximum value added (profit). One of the main
scientific steps on the way to obtaining the formula of efficiency is
developing the "resource intensity" indicator. The development of this
indicator was based on the model of the deployed operation and determination of
the time of the actual completion of the target operation, which does not
coincide with the traditional notion of the time of completion of economic
operations. For processes with distributed parameters, an expression for
determining the resource intensity using numerical methods was derived. For
economic operations, which can be reduced to simple operations, an analytical
expression of resource intensity was obtained. Using mathematical modeling
methods it was revealed that in the case of a fixed value of expert (cost)
estimate of output products of the operation, the minimum resource intensity of
the operation indicates a maximum efficiency operation with respect to the
target product of the operation. Development of the resource intensity of
economic operations is the final step towards the development of cybernetic
(interdisciplinary) efficiency indicator that allows to maximize the value
added (profit) of economic operations.

The Economic Complexity Index (ECI; Hidalgo & Hausmann, 2009) measures the
complexity of national economies in terms of product groups. Analogously to
ECI, a Patent Complexity Index (PatCI) can be developed on the basis of a
matrix of nations versus patent classes. Using linear algebra, the three
dimensions: countries, product groups, and patent classes can be combined into
a measure of "Triple Helix" complexity (THCI) including the trilateral
interaction terms between knowledge production, wealth generation, and
(national) control. THCI can be expected to capture the extent of systems
integration between the global dynamics of markets (ECI) and technologies
(PatCI) in each national system of innovation. We measure ECI, PatCI, and THCI
during the period 2000-2014 for the 34 OECD member states, the BRICS countries,
and a group of emerging and affiliated economies (Argentina, Hong Kong,
Indonesia, Malaysia, Romania, and Singapore). The three complexity indicators
are correlated between themselves; but the correlations with GDP per capita are
virtually absent. Of the world's major economies, Japan scores highest on all
three indicators, while China has been increasingly successful in combining
economic and technological complexity. We could not reproduce the correlation
between ECI and average income that has been central to the argument about the
fruitfulness of the economic complexity approach.

The so-called great divergence in the income per capita is described in the
Unified Growth Theory as the mind-boggling and unresolved mystery about the
growth process. This mystery has now been solved: the great divergence never
happened. It was created by the manipulation of data. Economic growth in
various regions is at different levels of development but it follows similar,
non-divergent trajectories. Unified Growth Theory is shown yet again to be
incorrect and scientifically unacceptable. It promotes incorrect and even
potentially dangerous concepts. The distorted presentation of data supporting
the concept of the great divergence shows that economic growth is now
developing along moderately-increasing trajectories but mathematical analysis
of the same data and even their undistorted presentation shows that these
trajectories are now increasing approximately vertically with time. So, while
the distorted presentation of data used in the Unified Growth Theory suggests
generally sustainable and secure economic growth, the undistorted presentation
of data demonstrates that the growth is unsustainable and insecure. The concept
of takeoffs from stagnation to the sustained-growth regime promoted in the
Unified Growth Theory is also dangerously misleading because it suggests a
sustainable and prosperous future while the mathematical analysis of data shows
that the current economic growth is insecure and unsustainable.

Macroeconomic theories of growth and wealth distribution have an outsized
influence on national and international social and economic policies. Yet, due
to a relative lack of reliable, system wide data, many such theories remain, at
best, unvalidated and, at worst, misleading. In this paper, we introduce a
novel economic observatory and framework enabling high resolution comparisons
and assessments of the distributional impact of economic development through
the remote sensing of planet earth's surface. Striking visual and empirical
validation is observed for a broad, global macroeconomic sigma-convergence in
the period immediately following the end of the Cold War. What is more, we
observe strong empirical evidence that the mechanisms driving sigma-convergence
failed immediately after the financial crisis and the start of the Great
Recession. Nevertheless, analysis of both cross-country and cross-state samples
indicates that, globally, disproportionately high growth levels and excessively
high decay levels have become rarer over time. We also see that urban areas,
especially concentrated within short distances of major capital cities were
more likely than rural or suburban areas to see relatively high growth in the
aftermath of the financial crisis. Observed changes in growth polarity can be
attributed plausibly to post-crisis government intervention and subsidy
policies introduced around the world. Overall, the data and techniques we
present here make economic evidence for the rise of China, the decline of U.S.
manufacturing, the euro crisis, the Arab Spring, and various, recent, Middle
East conflicts visually evident for the first time.

Recent advances in the urban science make broad use of the notion of scaling.
We focus here on the important scaling relationship between the gross
metropolitan product (GMP) of a city and its population (pop). It has been
demonstrated that GMP $\propto$ Y pop $^{\beta}$ with $\beta$ always greater
than 1 and close to 1.2. This fundamental finding highlights a universal rule
that holds across countries and cultures and might explain the very nature of
cities. However, in an increasingly connected world, the hypothesis that the
economy of a city solely depends on its population might be questionable. Using
data for 248 cities in the European Union between 2005 and 2010, we found a
double GMP/pop scaling regime. For West EU cities, $\beta$ = 1 over the whole
the period, while for post-communist cities $\beta >$1 and increases from
$\sim$1.2 to $\sim$1.4. The evolution of the scaling exponent describes the
convergence of post-communist European cities to open and liberal economies. We
propose a simple model of economic convergence in which, under stable political
conditions, a linear GMP/pop scaling is expected for all cities. The results
suggest that the GMP/pop super-linear scaling represents a phase of economic
growth rather than a steady, universal urban feature. The results also suggest
that relationships between cities are embedded in their political and economic
context and cannot be neglected in explanations of cities, urbanization and
urban economics.

Labor market institutions are central for modern economies, and their polices
can directly affect unemployment rates and economic growth. At the individual
level, unemployment often has a detrimental impact on people's well-being and
health. At the national level, high employment is one of the central goals of
any economic policy, due to its close association with national prosperity. The
main goal of this thesis is to highlight the need for frameworks that take into
account the complex structure of labor market interactions. In particular, we
explore the benefits of leveraging tools from computational social science,
network science, and data-driven theories to measure the flow of opportunities
and information in the context of the labor market. First, we investigate our
key hypothesis, which is that opportunity/information flow through weak ties,
and this is a key determinant of the length of unemployment. We then extend the
idea of opportunity/information flow to clusters of other economic activities,
where we expect the flow within clusters of related activities to be higher
than within isolated activities. This captures the intuition that within
related activities there are more "capitals" involved and that such activities
require similar "capabilities." Therefore, more extensive clusters of economic
activities should generate greater growth through exploiting the greater flow
of opportunities and information. We quantify the opportunity/information flow
using a complexity measure of two economic activities (i.e. jobs and exports).

Objective: The Expected Value of Sample Information (EVSI) quantifies the
economic benefit of reducing uncertainty in a health economic model by
collecting additional information. This has the potential to improve the
allocation of research budgets. Despite this, practical EVSI evaluations are
limited, partly due to the computational cost of estimating this value using
the "gold-standard" nested simulation methods. Recently, however, Heath et al
developed an estimation procedure that reduces the number of simulations
required for this "gold-standard" calculation. Up to this point, this new
method has been presented in purely technical terms. Study Design: This study
presents the practical application of this new method to aid its
implementation. We use a worked example to illustrate the key steps of the EVSI
estimation procedure before discussing its optimal implementation using a
practical health economic model. Methods: The worked example is based on a
three parameter linear health economic model. The more realistic model
evaluates the cost-effectiveness of a new chemotherapy treatment which aims to
reduce the number of side effects experienced by patients. We use a Markov
Model structure to evaluate the health economic profile of experiencing side
effects. Results: This EVSI estimation method offers accurate estimation within
a feasible computation time, seconds compared to days, even for more complex
model structures. The EVSI estimation is more accurate if a greater number of
nested samples are used, even for a fixed computational cost. Conclusions: This
new method reduces the computational cost of estimating the EVSI by nested
simulation.

This note is a contribution to the debate about the optimal algorithm for
Economic Complexity that recently appeared on ArXiv [1, 2] . The authors of [2]
eventually agree that the ECI+ algorithm [1] consists just in a renaming of the
Fitness algorithm we introduced in 2012, as we explicitly showed in [3].
However, they omit any comment on the fact that their extensive numerical tests
claimed to demonstrate that the same algorithm works well if they name it ECI+,
but not if its name is Fitness. They should realize that this eliminates any
credibility to their numerical methods and therefore also to their new
analysis, in which they consider many algorithms [2]. Since by their own
admission the best algorithm is the Fitness one, their new claim became that
the search for the best algorithm is pointless and all algorithms are alike.
This is exactly the opposite of what they claimed a few days ago and it does
not deserve much comments. After these clarifications we also present a
constructive analysis of the status of Economic Complexity, its algorithms, its
successes and its perspectives. For us the discussion closes here, we will not
reply to further comments.

Background: The Expected Value of Sample Information (EVSI) determines the
economic value of any future study with a specific design aimed at reducing
uncertainty in a health economic model. This has potential as a tool for trial
design; the cost and value of different designs could be compared to find the
trial with the greatest net benefit. However, despite recent developments, EVSI
analysis can be slow especially when optimising over a large number of
different designs. Methods: This paper develops a method to reduce the
computation time required to calculate the EVSI across different sample sizes.
Our method extends the moment matching approach to EVSI estimation to optimise
over different sample sizes for the underlying trial with a similar
computational cost to a single EVSI estimate. This extension calculates
posterior variances across the alternative sample sizes and then uses Bayesian
non-linear regression to calculate the EVSI. Results: A health economic model
developed to assess the cost-effectiveness of interventions for chronic pain
demonstrates that this EVSI calculation method is fast and accurate for
realistic models. This example also highlights how different trial designs can
be compared using the EVSI. Conclusion: The proposed estimation method is fast
and accurate when calculating the EVSI across different sample sizes. This will
allow researchers to realise the potential of using the EVSI to determine an
economically optimal trial design for reducing uncertainty in health economic
models. Limitations: Our method relies on some additional simulation, which can
be expensive in models with very large computational cost.

Being able to assess the impact of government-led investment onto
socio-economic indicators in cities has long been an important target of urban
planning. However, due to the lack of large-scale data with a fine
spatio-temporal resolution, there have been limitations in terms of how
planners can track the impact and measure the effectiveness of cultural
investment in small urban areas. Taking advantage of nearly 4 million
transition records for three years in London from a popular location-based
social network service, Foursquare, we study how the socio-economic impact of
government cultural expenditure can be detected and predicted. Our analysis
shows that network indicators such as average clustering coefficient or
centrality can be exploited to estimate the likelihood of local growth in
response to cultural investment. We subsequently integrate these features in
supervised learning models to infer socio-economic deprivation changes for
London's neighbourhoods. This research presents how geo-social and mobile
services can be used as a proxy to track and predict socio-economic deprivation
changes as government financial effort is put in developing urban areas and
thus gives evidence and suggestions for further policy-making and investment
optimisation.

High fuel consumption cost results in drivers' economic burden. Plug-In
Hybrid Electric Vehicles (PHEVs) consume two fuel sources (i.e., gasoline and
electricity energy sources) with floating prices. To reduce drivers' total fuel
cost, recommending economical routes to them becomes one of the effective
methods. In this paper, we present a novel economical path-planning framework
called Eco-Route, which consists of two phases. In the first phase, we build a
driving route cost model (DRCM) for each PHEV (and driver) under the energy
management strategy, based on driving condition and vehicles' parameters. In
the second phase, with the real-time traffic information collected via the
mobile crowdsensing manner, we are able to estimate and compare the driving
cost among the shortest and the fastest routes for a given PHEV, and then
recommend the driver with the more economical one. We evaluate the two-phase
framework using 8 different PHEVs simulated in Matlab/Simulink, and the
real-world datasets consisting of the road network, POI and GPS trajectory data
generated by 559 taxis in seven days in Beijing, China. Experimental results
demonstrate that the proposed model achieves good accuracy, with a mean cost
error of less 8% when paths length is longer than 5 km. Moreover, users could
save about 9% driving cost on average if driving along suggested routes in our
case studies.

Decentralization, in the form of mesh networking and blockchain, two
promising technologies, is coming to the telecommunications industry. Mesh
networking allows wider low cost Internet access with infrastructures built
from routers contributed by diverse owners, while blockchain enables
transparency and accountability for investments, revenue or other forms of
economic compensations from sharing of network traffic, content and services.
Crowdsourcing network coverage, combined with crowdfunding costs, can create
economically sustainable yet decentralized Internet access. This means every
participant can invest in resources, and pay or be paid for usage to recover
the costs of network devices and maintenance. While mesh networks and mesh
routing protocols enable self-organized networks that expand organically,
cryptocurrencies and smart contracts enable the economic coordination among
network providers and consumers. We explore and evaluate two existing
blockchain software stacks, Hyperledger Fabric (HLF) and Ethereum geth with
Proof of Authority (PoA) intended as a local lightweight distributed ledger,
deployed in a real city-wide production mesh network and also in laboratory
network. We quantify the performance, bottlenecks and identify the current
limitations and opportunities for improvement to serve locally the needs of
wireless mesh networks, without the privacy and economic cost of relying on
public blockchains.

Existing recommendation algorithms mostly focus on optimizing traditional
recommendation measures, such as the accuracy of rating prediction in terms of
RMSE or the quality of top-$k$ recommendation lists in terms of precision,
recall, MAP, etc. However, an important expectation for commercial
recommendation systems is to improve the final revenue/profit of the system.
Traditional recommendation targets such as rating prediction and top-$k$
recommendation are not directly related to this goal. In this work, we blend
the fundamental concepts in online advertising and micro-economics into
personalized recommendation for profit maximization. Specifically, we propose
value-aware recommendation based on reinforcement learning, which directly
optimizes the economic value of candidate items to generate the recommendation
list. In particular, we generalize the basic concept of click conversion rate
(CVR) in computational advertising into the conversation rate of an arbitrary
user action (XVR) in E-commerce, where the user actions can be clicking, adding
to cart, adding to wishlist, etc. In this way, each type of user action is
mapped to its monetized economic value. Economic values of different user
actions are further integrated as the reward of a ranking list, and
reinforcement learning is used to optimize the recommendation list for the
maximum total value. Experimental results in both offline benchmarks and online
commercial systems verified the improved performance of our framework, in terms
of both traditional top-$k$ ranking tasks and the economic profits of the
system.

Health economic evaluations face the issues of non-compliance and missing
data. Here, non-compliance is defined as non-adherence to a specific treatment,
and occurs within randomised controlled trials (RCTs) when participants depart
from their random assignment. Missing data arises if, for example, there is
loss to follow-up, survey non-response, or the information available from
routine data sources is incomplete. Appropriate statistical methods for
handling non-compliance and missing data have been developed, but they have
rarely been applied in health economics studies. Here, we illustrate the issues
and outline some of the appropriate methods to handle these with an application
to a health economic evaluation that uses data from an RCT.
  In an RCT the random assignment can be used as an instrument for treatment
receipt, to obtain consistent estimates of the complier average causal effect,
provided the underlying assumptions are met. Instrumental variable methods can
accommodate essential features of the health economic context such as the
correlation between individuals' costs and outcomes in cost-effectiveness
studies. Methodological guidance for handling missing data encourages
approaches such as multiple imputation or inverse probability weighting, that
assume the data are Missing At Random, but also sensitivity analyses that
recognise the data may be missing according to the true, unobserved values,
that is, Missing Not at Random.
  Future studies should subject the assumptions behind methods for handling
non-compliance and missing data to thorough sensitivity analyses. Modern
machine learning methods can help reduce reliance on correct model
specification. Further research is required to develop flexible methods for
handling more complex forms of non-compliance and missing data.

Existing research argues that countries increase their production basket by
adding products which require similar capabilities to those they already
produce, a process referred to as path dependency. Green economic growth is a
global movement that seeks to achieve economic expansion while at the same time
mitigating environmental risks. We postulate that countries engaging in green
economic growth are motivated to invest strategically to develop new
capabilities that will help them transition to a green economy. As a result,
they could potentially increase their production baskets not only by a path
dependent process but also by the non path dependent process we term, high
investment structural jumps. The main objective of this research is to
determine whether countries increase their green production basket mainly by a
process of path dependency, or alternatively, by a process of structural jumps.
We analyze data from 65 countries and over a period from years 2007 to 2017. We
focus on China as our main case study. The results of this research show that
countries not only increase their green production baskets based on their
available capabilities, following path dependency, but also expand to products
that path dependency does not predict by investing in innovating and developing
new environmental related technologies.

The enterprise of trying to explain different social and economic phenomena
using concepts and ideas drawn from physics has a long history. Statistical
mechanics, in particular, has been often seen as most likely to provide the
means to achieve this, because it provides a lucid and concrete framework for
describing the collective behavior of systems comprising large numbers of
interacting entities. Several physicists have, in recent years, attempted to
use such tools to throw light on the mechanisms underlying a plethora of
socio-economic phenomena. These endeavors have led them to develop a community
identity - with their academic enterprise being dubbed as "econophysics" by
some. However, the emergence of this field has also exposed several academic
fault-lines. Social scientists often regard physics-inspired models, such as
those involving spins coupled to each other, as over-simplifications of
empirical phenomena. At the same time, while models of rational agents who
strategically make choices based on complete information so as to maximize
their utility are commonly used in economics, many physicists consider them to
be caricatures of reality. We show here that while these contrasting approaches
may seem irreconcilable there are in fact many parallels and analogies between
them. In addition, we suggest that a new formulation of statistical mechanics
may be necessary to permit a complete mapping of the game-theoretic formalism
to a statistical physics framework. This may indeed turn out to be the most
significant contribution of econophysics.

Probabilistic projections of baseline (with no additional mitigation
policies) future carbon emissions are important for sound climate risk
assessments. Deep uncertainty surrounds many drivers of projected emissions.
Here we use a simple integrated assessment model, calibrated to century-scale
data and expert assessments of baseline emissions, global economic growth, and
population growth, to make probabilistic projections of carbon emissions
through 2100. Under a variety of assumptions about fossil fuel resource levels
and decarbonization rates, our projections largely agree with several emissions
projections under current policy conditions. Our global sensitivity analysis
identifies several key economic drivers of uncertainty in future emissions and
shows important higher-level interactions between economic and technological
parameters, while population uncertainties are less important. Our analysis
also projects relatively low global economic growth rates over the remainder of
the century. This illustrates the importance of additional research into
economic growth dynamics for climate risk assessment, especially if pledged and
future climate mitigation policies are weakened or have delayed
implementations. These results showcase the power of using a simple,
transparent, and calibrated model. While the simple model structure has several
advantages, it also creates caveats for our results which are related to
important areas for further research.

Disruptions to transportation networks are inevitable. Currently, most
mandated development-related transportation planning is intended to prepare for
frequently occurring and observable disruptions while low probability events
that have not yet materialized attract less attention. When road networks are
not resilient, these unpredictable events can cause significant delays that may
not be proportional to the extent of the disruption. Enhancing resilience can
help in mitigating consequences of disruptions but requires financial
investment that is difficult to justify given that low probability event may
not have materialized. This paper highlights economic implications of
unmitigated random disruptions in urban road systems and makes the case for
investment in transportation network resilience. We utilized a model of urban
transportation network performance that quantifies resilience and demonstrated
its integration with microeconomic transportation planning model REMI
TranSight. The model was applied to 10 cities in the USA to calculate several
economic indicators under baseline scenario where economic impact was assumed
to be proportional to the magnitude of disruptive events and under a test
scenario where the magnitude of disruption was used to calculate additional
delays in transportation networks and these additional delays were explicitly
integrated in REMI model. Results show that GDP losses suffered as a result of
disruption may be far more significant in the case scenario and economic output
is not necessarily back to normal in the year the following disruptive event.
We thus conclude that support for investment decisions on network efficiency
and resilience management should be based on a framework that utilizes
resilience, quantified in terms that are compatible with standard practice, and
scenarios to test the implications of topological attributes.

A recent paper by Hausmann and collaborators (1) reaches the important
conclusion that Complexity-weighted diversification is the essential element to
predict country growth. We like this result because Complexity-weighted
diversification is precisely the first equation of the Fitness algorithm that
we introduced in 2012 (2,3). However, contrary to what is claimed in (1), it is
incorrect to say that diversification is contained also in the ECI algorithm
(4). We discuss the origin of this misunderstanding and show that the ECI
algorithm contains exactly zero diversification. This is actually one of the
reasons for the poor performances of ECI which leads to completely unrealistic
results, as for instance, the derivation that Qatar or Saudi Arabia are
industrially more competitive than China (5,6). Another important element of
our new approach is the representation of the economic dynamics of countries as
trajectories in the GDPpc-Fitness space (7-10). In some way also this has been
rediscovered by Hausmann and collaborators and renamed as "Stream plots", but,
given their weaker metrics and methods, they propose it to use it only for a
qualitative insight, while ours led to quantitative and successful forecasting.
The Fitness approach has paved the way to a robust and testable framework for
Economic Complexity resulting in a highly competitive scheme for growth
forecasting (7-10). According to a recent report by Bloomberg (9): The new
Fitness method, "systematically outperforms standard methods, despite requiring
much less data".

Social cost of carbon (SCC) is estimated by integrated assessment models
(IAM) and is widely used by government agencies to value climate policy
impacts. While there is an ongoing debate about obtained numerical estimates
and related uncertainties, little attention has been paid so far to the SCC
calculation method itself.
  This work attempts to fill the gap by providing theoretical background and
economic interpretation of the SCC calculation approach implemented in the
open-source IAM DICE (Dynamic Integrated model of Climate and the Economy). Our
analysis indicates that the present calculation method provides an
approximation that might work pretty well in some cases, while in the other
cases the estimated value substantially (by the factor of four) deviates from
the "true" value. This deviation stems from the inability of the present
calculation method to catch the linkages between two key IAM's components --
complex interconnected systems -- climate and economy, both influenced by
emission abatement policies. Within the modeling framework of DICE, the
presently estimated SCC valuates policy-uncontrolled emissions against
economically unjustified consumption, which makes it irrelevant for application
in climate-economic policies and, therefore, calls for a replacement by a more
appropriate indicator.
  An apparent SCC alternative, which can be employed for policy formulation is
the direct output of the DICE model -- the socially optimal marginal abatement
cost (SMAC), which corresponds to technological possibilities at optimal level
of carbon emissions abatement. In policy making, because of the previously
employed implicit approximation, great attention needs to be paid to the use of
SCC estimates obtained earlier.

We consider the model of economic growth with time delayed investment
function. Assuming the investment is time distributed we can use the linear
chain trick technique to transform delay differential equation system to
equivalent system of ordinary differential system (ODE). The time delay
parameter is a mean time delay of gamma distribution. We reduce the system with
distribution delay to both three and four-dimensional ODEs. We study the Hopf
bifurcation in these systems with respect to two parameters: the time delay
parameter and the rate of growth parameter. We derive the results from the
analytical as well as numerical investigations. From the former we obtain the
sufficient criteria on the existence and stability of a limit cycle solution
through the Hopf bifurcation. In numerical studies with the Dana and Malgrange
investment function we found two Hopf bifurcations with respect to the rate
growth parameter and detect the existence of stable long-period cycles in the
economy. We find that depending on the time delay and adjustment speed
parameters the range of admissible values of the rate of growth parameter
breaks down into three intervals. First we have stable focus, then the limit
cycle and again the stable solution with two Hopf bifurcations. Such behaviour
appears for some middle interval of admissible range of values of the rate of
growth parameter.

Integrated Assessment Models (IAMs) of the climate and economy aim to analyze
the impact and efficacy of policies that aim to control climate change, such as
carbon taxes and subsidies. A major characteristic of IAMs is that their
geophysical sector determines the mean surface temperature increase over the
preindustrial level, which in turn determines the damage function. Most of the
existing IAMs are perfect-foresight forward-looking models, assuming that we
know all of the future information. However, there are significant
uncertainties in the climate and economic system, including parameter
uncertainty, model uncertainty, climate tipping risks, economic risks, and
ambiguity. For example, climate damages are uncertain: some researchers assume
that climate damages are proportional to instantaneous output, while others
assume that climate damages have a more persistent impact on economic growth.
Climate tipping risks represent (nearly) irreversible climate events that may
lead to significant changes in the climate system, such as the Greenland ice
sheet collapse, while the conditions, probability of tipping, duration, and
associated damage are also uncertain. Technological progress in carbon capture
and storage, adaptation, renewable energy, and energy efficiency are uncertain
too. In the face of these uncertainties, policymakers have to provide a
decision that considers important factors such as risk aversion, inequality
aversion, and sustainability of the economy and ecosystem. Solving this problem
may require richer and more realistic models than standard IAMs, and advanced
computational methods. The recent literature has shown that these uncertainties
can be incorporated into IAMs and may change optimal climate policies
significantly.

The COVID-19 outbreak forced governments worldwide to impose lockdowns and
quarantines to prevent virus transmission. As a consequence, there are
disruptions in human and economic activities all over the globe. The recovery
process is also expected to be rough. Economic activities impact social
behaviors, which leave signatures in satellite images that can be automatically
detected and classified. Satellite imagery can support the decision-making of
analysts and policymakers by providing a different kind of visibility into the
unfolding economic changes. In this work, we use a deep learning approach that
combines strategic location sampling and an ensemble of lightweight
convolutional neural networks (CNNs) to recognize specific elements in
satellite images that could be used to compute economic indicators based on it,
automatically. This CNN ensemble framework ranked third place in the US
Department of Defense xView challenge, the most advanced benchmark for object
detection in satellite images. We show the potential of our framework for
temporal analysis using the US IARPA Function Map of the World (fMoW) dataset.
We also show results on real examples of different sites before and after the
COVID-19 outbreak to illustrate different measurable indicators. Our code and
annotated high-resolution aerial scenes before and after the outbreak are
available on GitHub (https://github.com/maups/covid19-satellite-analysis).

In the initial wave of the COVID-19 pandemic we observed great discrepancies
in both infection and mortality rates between countries. Besides the biological
and epidemiological factors, a multitude of social and economic criteria also
influence the extent to which these discrepancies appear. Consequently, there
is an active debate regarding the critical socio-economic and health factors
that correlate with the infection and mortality rates outcome of the pandemic.
Here, we leverage Bayesian model averaging techniques and country level data to
investigate the potential of 28 variables, describing a diverse set of health
and socio-economic characteristics, in being correlates of the final number of
infections and deaths during the first wave of the coronavirus pandemic. We
show that only few variables are able to robustly correlate with these
outcomes. To understand the relationship between the potential correlates in
explaining the infection and death rates, we create a Jointness Space. Using
this space, we conclude that the extent to which each variable is able to
provide a credible explanation for the COVID-19 infections/mortality outcome
varies between countries because of their heterogeneous features.

Distress propagation occurs in connected networks, its rate and extent being
dependent on network topology. To study this, we choose economic production
networks as a paradigm. An economic network can be examined at many levels:
linkages among individual agents (microscopic), among firms/sectors
(mesoscopic) or among countries (macroscopic). New emergent dynamical
properties appear at every level, so the granularity matters. For viral
epidemics, even an individual node may act as an epicenter of distress and
potentially affect the entire network. Economic networks, however, are known to
be immune at the micro-levels and more prone to failure in the
meso/macro-levels. We propose a dynamical interaction model to characterize the
mechanism of distress propagation, across different modules of a network,
initiated at different epicenters. Vulnerable modules often lead to large
degrees of destabilization. We demonstrate our methodology using a unique
empirical data-set of input-output linkages across 0.14 million firms in one
administrative state of India, a developing economy. The network has multiple
hub-and-spoke structures that exhibits moderate disassortativity, which varies
with the level of coarse-graining. The novelty lies in characterizing the
production network at different levels of granularity or modularity, and
finding `too-big-to-fail' modules supersede `too-central-to-fail' modules in
distress propagation.

The behavior of complex systems is one of the most intriguing phenomena
investigated by recent science; natural and artificial systems offer a wide
opportunity for this kind of analysis. The energy conversion is both a process
based on important physical laws and one of the most important economic
sectors; the interaction between these two aspects of energy production
suggests the possibility to apply some of the approaches of the dynamic
systems' analysis. In particular, a phase plot, which is one of the methods to
detect a correlation between quantities in a complex system, provides a good
way to establish qualitative analogies between the ecological systems and the
economic ones and may shed light on the processes governing the evolution of
the system. The aim of this paper is to highlight the analogies between some
peculiar characteristics of the oil production vs. price and show in which way
such characteristics are similar to some behavioral mechanisms found in Nature.

In this paper, we consider the problem of periodic optimal control of
nonlinear systems subject to online changing and periodically time-varying
economic performance measures using model predictive control (MPC). The
proposed economic MPC scheme uses an online optimized artificial periodic orbit
to ensure recursive feasibility and constraint satisfaction despite
unpredictable changes in the economic performance index. We demonstrate that
the direct extension of existing methods to periodic orbits does not
necessarily yield the desirable closed-loop economic performance. Instead, we
carefully revise the constraints on the artificial trajectory, which ensures
that the closed-loop average performance is no worse than a locally optimal
periodic orbit. In the special case that the prediction horizon is set to zero,
the proposed scheme is a modified version of recent publications using
periodicity constraints, with the important difference that the resulting
closed loop has more degrees of freedom which are vital to ensure convergence
to an optimal periodic orbit. In addition, we detail a tailored offline
computation of suitable terminal ingredients, which are both theoretically and
practically beneficial for closed-loop performance improvement. Finally, we
demonstrate the practicality and performance improvements of the proposed
approach on benchmark examples.

Climate change has become intertwined with the global economy. Here, we
describe the importance of inertia to continued growth in energy consumption.
Drawing from thermodynamic arguments, and using 38 years of available
statistics between 1980 to 2017, we find a persistent time-independent scaling
between the historical time integral $W$ of world inflation-adjusted economic
production $Y$, or $W\left(t\right) = \int_0^t Y\left(t'\right)dt'$, and
current rates of world primary energy consumption $\mathcal E$, such that
$\lambda = \mathcal{E}/W = 5.9\pm0.1$ Gigawatts per trillion 2010 US dollars.
This empirical result implies that population expansion is a symptom rather
than a cause of the current exponential rise in $\mathcal E$ and carbon dioxide
emissions $C$, and that it is past innovation of economic production efficiency
$Y/\mathcal{E}$ that has been the primary driver of growth, at predicted rates
that agree well with data. Options for stabilizing $C$ are then limited to
rapid decarbonization of $\mathcal E$ through sustained implementation of over
one Gigawatt of renewable or nuclear power capacity per day. Alternatively,
assuming continued reliance on fossil fuels, civilization could shift to a
steady-state economy that devotes economic production exclusively to
maintenance rather than expansion. If this were instituted immediately,
continual energy consumption would still be required, so atmospheric carbon
dioxide concentrations would not balance natural sinks until concentrations
exceeded 500 ppmv, and double pre-industrial levels if the steady-state was
attained by 2030.

One of the widely used models for studying economics of climate change is the
Dynamic Integrated model of Climate and Economy (DICE), which has been
developed by Professor William Nordhaus, one of the laureates of the 2018 Nobel
Memorial Prize in Economic Sciences. Originally a single-objective optimal
control problem has been defined on DICE dynamics, which is aimed to maximize
the social welfare. In this paper, a bi-objective optimal control problem
defined on DICE model, objectives of which are maximizing social welfare and
minimizing the temperature deviation of atmosphere. This multi-objective
optimal control problem solved using Non-Dominated Sorting Genetic Algorithm II
(NSGA-II) also it is compared to previous works on single-objective version of
the problem. The resulting Pareto front rediscovers the previous results and
generalizes to a wide range of non-dominant solutions to minimize the global
temperature deviation while optimizing the economic welfare. The previously
used single-objective approach is unable to create such a variety of
possibilities, hence, its offered solution is limited in vision and reachable
performance. Beside this, resulting Pareto-optimal set reveals the fact that
temperature deviation cannot go below a certain lower limit, unless we have
significant technology advancement or positive change in global conditions.

The International Monetary Fund (IMF) provides financial assistance to its
member-countries in economic turmoil, but requires at the same time that these
countries reform their public policies. In several contexts, these reforms are
at odds with population health. While researchers have empirically analyzed the
consequences of these reforms on health, no analysis exist on identifying fair
tradeoffs between consequences on population health and economic outcomes. Our
article analyzes and identifies the principles governing these tradeoffs.
First, this article reviews existing policy-evaluation studies, which show, on
balance, that IMF policies frequently cause adverse effects on child health and
material standards in the pursuit of macroeconmic improvement. Second, this
article discusses four theories in distributive ethics (maximization,
egalitarianianism, prioritarianiasm, and sufficientarianism) to identify which
is the most compatible with the core mission of the IMF, that is, improved
macroeconomics (Articles of Agreement) while at the same time balancing
consequences on health. Using a distributive-ethics analyses of IMF polices, we
argue that sufficientarianism is the most compatible theory. Third, this
article offer a qualitative rearticulation of the Articles of Agreement, and
formalize sufficientarian principles in the language of causal inference. We
also offer a framework on how to empirically measure, from observational data,
the extent that IMF policies trade off fairly between population health and
economic outcomes. We conclude with policy recommendations and suggestions for
future research.

Public transit is central to cultivating equitable communities. Meanwhile,
the novel coronavirus disease COVID-19 and associated social restrictions has
radically transformed ridership behavior in urban areas. Perhaps the most
concerning aspect of the COVID-19 pandemic is that low-income and historically
marginalized groups are not only the most susceptible to economic shifts but
are also most reliant on public transportation. As revenue decreases, transit
agencies are tasked with providing adequate public transportation services in
an increasingly hostile economic environment. Transit agencies therefore have
two primary concerns. First, how has COVID-19 impacted ridership and what is
the new post-COVID normal? Second, how has ridership varied spatio-temporally
and between socio-economic groups? In this work we provide a data-driven
analysis of COVID-19's affect on public transit operations and identify
temporal variation in ridership change. We then combine spatial distributions
of ridership decline with local economic data to identify variation between
socio-economic groups. We find that in Nashville and Chattanooga, TN,
fixed-line bus ridership dropped by 66.9% and 65.1% from 2019 baselines before
stabilizing at 48.4% and 42.8% declines respectively. The largest declines were
during morning and evening commute time. Additionally, there was a significant
difference in ridership decline between the highest-income areas and
lowest-income areas (77% vs 58%) in Nashville.

The demand for electricity is undergoing considerable spatial and temporal
change. With the uptake of efficient technologies and increased
electrification, a better understanding of how potential changes in demand
patterns can affect network reliability is necessary. We quantify the
macro-economic impacts of potential future changes in demand profiles for an
electricity network undergoing generation shortages. Applied to Great Britain,
potential savings or losses are assessed for changes to peak demands under four
different load profiles: (i) current day, (ii) widespread uptake of efficient
appliances, (iii) deployment of heat pumps, and (iv) moving to an idealised
fully balanced load profile. Considerable variation in economic disruption is
observed both between different demand profiles and across Great Britain. When
the networks generation capacity is severely disrupted, we estimate hourly
macro-economic impacts to increase by up to {\pounds}1.23 million per
additional GW of national electricity demand. A similar reduction in impacts
can be achieved if peak demands are reduced by demand side management. We
conclude that risk-related economic impacts are directly linked to the temporal
pattern of energy demands and need to be included in the decision-making around
demand side measures. We find that decarbonisation strategies without
accompanying demand side management may lead to increased economic losses
without supply side interventions.

The dynamics of wealth distribution plays a critical role in the economic
market, hence an understanding of its nonequilibrium statistical mechanics is
of great importance to human society. For this aim, a simple and efficient
one-dimensional (1D) lattice gas automaton (LGA) is presented for wealth
distribution of agents with or without saving propensity. The LGA comprises two
stages, i.e., random propagation and economic transaction. During the former
phase, an agent either remains motionless or travels to one of its neighboring
empty sites with a certain probability. In the subsequent procedure, an
economic transaction takes place between a pair of neighboring agents randomly.
It requires at least 4 neighbors to present correct simulation results. The LGA
reduces to the simplest model with only random economic transaction if all
agents are neighbors and no empty sites exist. The 1D-LGA has a higher
computational efficiency than the 2D-LGA and the famous Chakraborti-Chakrabarti
economic model. Finally, the LGA is validated with two benchmarks, i.e., the
wealth distributions of individual agents and dual-earner families. With the
increasing saving fraction, both the Gini coefficient and Kolkata index (for
individual agents or two-earner families) reduce, while the deviation degree
(defined to measure the difference between the probability distributions with
and without saving propensities) increases. It is demonstrated that the wealth
distribution is changed significantly by the saving propensity which alleviates
wealth inequality.

This paper provides an economic perspective on the predictive maintenance of
filtration units. The rise of predictive maintenance is possible due to the
growing trend of industry 4.0 and the availability of inexpensive sensors.
However, the adoption rate for predictive maintenance by companies remains low.
The majority of companies are sticking to corrective and preventive
maintenance. This is not due to a lack of information on the technical
implementation of predictive maintenance, with an abundance of research papers
on state-of-the-art machine learning algorithms that can be used effectively.
The main issue is that most upper management has not yet been fully convinced
of the idea of predictive maintenance. The economic value of the implementation
has to be linked to the predictive maintenance program for better justification
by the management. In this study, three machine learning models were trained to
demonstrate the economic value of predictive maintenance. Data was collected
from a testbed located at the Singapore University of Technology and Design.
The testbed closely resembles a real-world water treatment plant. A
cost-benefit analysis coupled with Monte Carlo simulation was proposed. It
provided a structured approach to document potential costs and savings by
implementing a predictive maintenance program. The simulation incorporated
real-world risk into a financial model. Financial figures were adapted from
CITIC Envirotech Ltd, a leading membrane-based integrated environmental
solutions provider. Two scenarios were used to elaborate on the economic values
of predictive maintenance. Overall, this study seeks to bridge the gap between
technical and business domains of predictive maintenance.

This paper examines the evolution of business and consumer uncertainty amid
the coronavirus pandemic in 32 European countries and the European Union
(EU).Since uncertainty is not directly observable, we approximate it using the
geometric discrepancy indicator of Claveria et al. (2019).This approach allows
us quantifying the proportion of disagreement in business and consumer
expectations of 32 countries.We have used information from all monthly
forward-looking questions contained in Joint Harmonised Programme of Business
and Consumer Surveys conducted by the European Commission (the industry survey,
the service survey, the retail trade survey, the building survey and the
consumer survey).First, we have calculated a discrepancy indicator for each of
the 17 survey questions analysed, which allows us to approximate the proportion
of uncertainty about different aspects of economic activity, both form the
demand and the supply sides of the economy.We then use these indicators to
calculate disagreement indices at the sector level.We graphic the evolution of
the degree of uncertainty in the main economic sectors of the analysed
economies up to June 2020.We observe marked differences, both across variables,
sectors and countries since the inception of the COVID-19 crisis.Finally, by
adding the sectoral indicators, an indicator of business uncertainty is
calculated and compared with that of consumers.Again, we find substantial
differences in the evolution of uncertainty between managers and consumers.This
analysis seeks to offer a global overview of the degree of economic uncertainty
in the midst of the coronavirus crisis at the sectoral level.

Understanding the microeconomic details of technological catch-up processes
offers great potential for informing both innovation economics and development
policy. We study the economic transition of the PR China from an agrarian
country to a high-tech economy as one example for such a case. It is clear from
past literature that rapidly rising productivity levels played a crucial role.
However, the distribution of labor productivity in Chinese firms has not been
comprehensively investigated and it remains an open question if this can be
used to guide economic development. We analyze labor productivity and the
dynamic change of labor productivity in firm-level data for the years 1998-2013
from the Chinese Industrial Enterprise Database. We demonstrate that both
variables are conveniently modeled as L\'evy alpha-stable distributions,
provide parameter estimates and analyze dynamic changes to this distribution.
We find that the productivity gains were not due to super-star firms, but due
to a systematic shift of the entire distribution with otherwise mostly
unchanged characteristics. We also found an emerging right-skew in the
distribution of labor productivity change. While there are significant
differences between the 31 provinces and autonomous regions of the P.R. China,
we also show that there are systematic relations between micro-level and
province-level variables. We conclude with some implications of these findings
for development policy.

This paper describes a study designed to investigate the current and emergent
impacts of Covid-19 and Brexit on UK horticultural businesses. Various
characteristics of UK horticultural production, notably labour reliance and
import dependence, make it an important sector for policymakers concerned to
understand the effects of these disruptive events as we move from 2020 into
2021. The study design prioritised timeliness, using a rapid survey to gather
information from a relatively small (n = 19) but indicative group of producers.
The main novelty of the results is to suggest that a very substantial majority
of producers either plan to scale back production in 2021 (47%) or have been
unable to make plans for 2021 because of uncertainty (37%). The results also
add to broader evidence that the sector has experienced profound labour supply
challenges, with implications for labour cost and quality. The study discusses
the implications of these insights from producers in terms of productivity and
automation, as well as in terms of broader economic implications. Although
automation is generally recognised as the long-term future for the industry
(89%), it appeared in the study as the second most referred short-term option
(32%) only after changes to labour schemes and policies (58%). Currently,
automation plays a limited role in contributing to the UK's horticultural
workforce shortage due to economic and socio-political uncertainties. The
conclusion highlights policy recommendations and future investigative
intentions, as well as suggesting methodological and other discussion points
for the research community.

Economic shocks due to Covid-19 were exceptional in their severity,
suddenness and heterogeneity across industries. To study the upstream and
downstream propagation of these industry-specific demand and supply shocks, we
build a dynamic input-output model inspired by previous work on the economic
response to natural disasters. We argue that standard production functions, at
least in their most parsimonious parametrizations, are not adequate to model
input substitutability in the context of Covid-19 shocks. We use a survey of
industry analysts to evaluate, for each industry, which inputs were absolutely
necessary for production over a short time period. We calibrate our model on
the UK economy and study the economic effects of the lockdown that was imposed
at the end of March and gradually released in May. Looking back at predictions
that we released in May, we show that the model predicted aggregate dynamics
very well, and sectoral dynamics to a large extent. We discuss the relative
extent to which the model's dynamics and performance was due to the choice of
the production function or the choice of an exogenous shock scenario. To
further explore the behavior of the model, we use simpler scenarios with only
demand or supply shocks, and find that popular metrics used to predict a priori
the impact of shocks, such as output multipliers, are only mildly useful.

Automated market makers (AMMs) are one of the most prominent decentralized
finance (DeFi) applications. AMMs allow users to trade different types of
crypto-tokens, without the need to find a counter-party. There are several
implementations and models for AMMs, featuring a variety of sophisticated
economic mechanisms. We present a theory of AMMs. The core of our theory is an
abstract operational model of the interactions between users and AMMs, which
can be concretised by instantiating the economic mechanisms. We exploit our
theory to formally prove a set of fundamental properties of AMMs,
characterizing both structural and economic aspects. We do this by abstracting
from the actual economic mechanisms used in implementations, and identifying
sufficient conditions which ensure the relevant properties. Notably, we devise
a general solution to the arbitrage problem, the main game-theoretic foundation
behind the economic mechanisms of AMMs.

Advances in unmanned aerial vehicle (UAV) design have opened up applications
as varied as surveillance, firefighting, cellular networks, and delivery
applications. Additionally, due to decreases in cost, systems employing fleets
of UAVs have become popular. The uniqueness of UAVs in systems creates a novel
set of trajectory or path planning and coordination problems. Environments
include many more points of interest (POIs) than UAVs, with obstacles and
no-fly zones. We introduce REPlanner, a novel multi-agent reinforcement
learning algorithm inspired by economic transactions to distribute tasks
between UAVs. This system revolves around an economic theory, in particular an
auction mechanism where UAVs trade assigned POIs. We formulate the path
planning problem as a multi-agent economic game, where agents can cooperate and
compete for resources. We then translate the problem into a Partially
Observable Markov decision process (POMDP), which is solved using a
reinforcement learning (RL) model deployed on each agent. As the system
computes task distributions via UAV cooperation, it is highly resilient to any
change in the swarm size. Our proposed network and economic game architecture
can effectively coordinate the swarm as an emergent phenomenon while
maintaining the swarm's operation. Evaluation results prove that REPlanner
efficiently outperforms conventional RL-based trajectory search.

We develop a method suitable for detecting whether racial homophily is on the
rise and also whether the economic divide (i.e., the gap between individuals
with different education levels and thereby with different abilities to
generate income) is growing in a society. We identify these changes with the
changing aggregate marital preferences over the partners' race and education
level through their effects on the share of inter-racial couples and the share
of educationally homogamous couples. These shares are shaped not only by
preferences, but also by the distributions of marriageable men and women by
traits. The method proposed is designed to control for changes in the trait
distributions from one generation to another. By applying the method, we find
the economic divide in the US to display a U-curve pattern between 1960 and
2010 followed by its slightly negative trend between 2010 and 2015. The
identified trend of racial homophily suggests that the American society has
become more and more permissive towards racial intermarriages since 1970.
Finally, we refute the aggregate version of the status-cast exchange hypothesis
based on the joint dynamics of the economic divide and the racial homophily.

Production networks are integral to economic dynamics, yet dis-aggregated
network data on inter-firm trade is rarely collected and often proprietary.
Here we situate company-level production networks among networks from other
domains according to their local connectivity structure. Through this lens, we
study a regional and a national network of inferred trade relationships
reconstructed from Dutch national economic statistics and re-interpret prior
empirical findings. We find that company-level production networks have
so-called functional structure, as previously identified in protein-protein
interaction (PPI) networks. Functional networks are distinctive in their
over-representation of closed squares, which we quantify using an existing
measure called spectral bipartivity. Shared local connectivity structure lets
us ferry insights between domains. PPI networks are shaped by complementarity,
rather than homophily, and we use multi-layer directed configuration models to
show that this principle explains the emergence of functional structure in
production networks. Companies are especially similar to their close
competitors, not to their trading partners. Our findings have practical
implications for the analysis of production networks and a thorough
understanding of their local connectivity structure will help us better reason
about the micro-economic mechanisms behind their routine function, failure, and
growth.

A polycentric approach to ecosystem service (ES) governance that combines
individual incentives for interdependent ES providers with collective action is
a promising lever to overcome the decline in ES and generate win-win solutions
in agricultural landscapes. In this study, we explored the effectiveness of
such an approach by focusing on incentives for managed pollination targeting
either beekeepers or farmers who were either in communication with each other
or not. We used a stylized bioeconomic model to simulate (i) the mutual
interdependency through pollination in intensive agricultural landscapes and
(ii) the economic and ecological impacts of introducing two beekeeping
subsidies and one pesticide tax. The findings showed that incentives generated
a spillover effect, affecting not only targeted stakeholders but non-targeted
stakeholders as well as the landscape, and that this effect was amplified by
communication. However, none of the simulated types of polycentric ES
governance proved sustainable overall: subsidies showed excellent economic but
low environmental performance, while the tax led to economic losses but was
beneficial for the landscape. Based on these results, we identified three
conditions for sustainable ES governance based on communication between
stakeholders and incentives: (i) strong mutual interdependency (i.e. few
alternatives exist for stakeholders), (ii) the benefits of communication
outweigh the costs, and (iii) the incentivized ES drivers are not detrimental
to other ES. Further research is needed to systematize which combination of
individual payments and collaboration are sustainable in which conditions.

The European Union Emission Trading Scheme (EU ETS) is a cornerstone of the
EU's strategy to fight climate change and an important device for plummeting
greenhouse gas (GHG) emissions in an economically efficient manner. The power
industry has switched to an auction-based allocation system at the onset of
Phase III of the EU ETS to bring economic efficiency by negating windfall
profits that have been resulted from grandfathered allocation of allowances in
the previous phases. In this work, we analyze and simulate the interaction of
oligopolistic generators in an electricity market with a game-theoretical
framework where the electricity and the emissions markets interact in a
two-stage electricity market. For analytical simplicity, we assume a single
futures market where the electricity is committed at the futures price, and the
emissions allowance is contracted in advance, prior to a spot market where the
energy and allowances delivery takes place. Moreover, a coherent risk measure
is applied (Conditional Value at Risk) to model both risk averse and risk
neutral generators and a two-stage stochastic optimization setting is
introduced to deal with the uncertainty of renewable capacity, demand,
generation, and emission costs. The performance of the proposed equilibrium
model and its main properties are examined through realistic numerical
simulations. Our results show that renewable generators are surging and
substituting conventional generators without compromising social welfare.
Hence, both renewable deployment and emission allowance auctioning are
effectively reducing GHG emissions and promoting low-carbon economic path.

With the costs of renewable energy technologies declining, new forms of urban
energy systems are emerging that can be established in a cost-effective way.
The SolarEV City concept has been proposed that uses rooftop Photovoltaics (PV)
to its maximum extent, combined with Electric Vehicle (EV) with bi-directional
charging for energy storage. Urban environments consist of various areas, such
as residential and commercial districts, with different energy consumption
patterns, building structures, and car parks. The cost effectiveness and
decarbonization potentials of PV + EV and PV (+ battery) systems vary across
these different urban environments and change over time as cost structures
gradually shift. To evaluate these characteristics, we performed
techno-economic analyses of PV, battery, and EV technologies for a residential
area in Shinchi, Fukushima and the central commercial district of Kyoto, Japan
between 2020 and 2040. We found that PV + EV and PV only systems in 2020 are
already cost competitive relative to existing energy systems (grid electricity
and gasoline car). In particular, the PV + EV system rapidly increases its
economic advantage over time, particularly in the residential district which
has larger PV capacity and EV battery storage relative to the size of energy
demand. Electricity exchanges between neighbors (e.g., peer-to-peer or
microgrid) further enhanced the economic value (net present value) and
decarbonization potential of PV + EV systems up to 23 percent and 7 percent in
2030, respectively. These outcomes have important strategic implications for
urban decarbonization over the coming decades.

Involving residential actors in the energy transition is crucial for its
success. Local energy generation, consumption and trading are identified as
desirable forms of involvement, especially in energy communities. The
potentials for energy communities in the residential building stock are high
but are largely untapped in multi-family buildings. In many countries, rapidly
evolving legal frameworks aim at overcoming related barriers, e.g. ownership
structures, principal-agent problems and system complexity. But academic
literature is scarce regarding the techno-economic and environmental
implications of such complex frameworks. This paper develops a mixed-integer
linear program (MILP) optimisation model for assessing the implementation of
multi-energy systems in an energy community in multi-family buildings with a
special distinction between investor and user. The model is applied to the
German Tenant Electricity Law. Based on hourly demands from appliances, heating
and electric vehicles, the optimal energy system layout and dispatch are
determined. The results contain a rich set of performance indicators that
demonstrate how the legal framework affects the technologies' interdependencies
and economic viability of multi-energy system energy communities. Certain
economic technology combinations may fail to support national emissions
mitigation goals and lead to lock-ins in Europe's largest residential building
stock. The subsidies do not lead to the utilisation of a battery storage.
Despite this, self-sufficiency ratios of more than 90% are observable for
systems with combined heat and power plants and heat pumps. Public CO2
mitigation costs range between 147.5-272.8 EUR/tCO2. Finally, the results show
the strong influence of the heat demand on the system layout.

Global concern regarding ultrafine particles (UFPs), which are particulate
matter (PM) with a diameter of less than 100nm, is increasing. These
particles-with more serious health effects than PM less than 2.5 micrometers
(PM2.5)-are difficult to measure using the current methods because their
characteristics are different from those of other air pollutants. Therefore, a
new monitoring system is required to obtain accurate UFPs information, which
will raise the financial burden of the government and people. In this study, we
estimated the economic value of UFPs information by evaluating the
willingness-to-pay (WTP) for the UFPs monitoring and reporting system. We used
the contingent valuation method (CVM) and the one-and-one-half-bounded
dichotomous choice (OOHBDC) spike model. We analyzed how the respondents'
socio-economic variables, as well as their cognition level of PM, affected
their WTP. Therefore, we collected WTP data of 1,040 Korean respondents through
an online survey. The estimated mean WTP for building a UFPs monitoring and
reporting system is KRW 6,958.55-7,222.55 (USD 6.22-6.45) per household per
year. We found that people satisfied with the current air pollutant
information, and generally possessing relatively greater knowledge of UFPs,
have higher WTP for a UFPs monitoring and reporting system. The results can be
used to establish new policies response to PM including UFPs.

While the potential for peer-to-peer electricity trading, where households
trade surplus electricity with peers in a local energy market, is rapidly
growing, the drivers of participation in this trading scheme have been
understudied so far. In particular, there is a dearth of research on the role
of non-monetary incentives for trading surplus electricity, despite their
potentially important role. This paper presents the first discrete choice
experiment conducted with prosumers (i.e. proactive households actively
managing their electricity production and consumption) in the Netherlands.
Electricity trading preferences are analyzed regarding economic, environmental,
social and technological parameters, based on survey data (N = 74). The
dimensions most valued by prosumers are the environmental and, to a lesser
extent, economic dimensions, highlighting the key motivating roles of
environmental factors. Furthermore, a majority of prosumers stated they would
provide surplus electricity for free or for non-monetary compensations,
especially to energy-poor households. These observed trends were more
pronounced among members of energy cooperatives. This suggests that
peer-to-peer energy trading can advance a socially just energy transition.
Regarding policy recommendations, these findings point to the need for
communicating environmental and economic benefits when marketing P2P
electricity trading platforms and for technical designs enabling effortless and
customizable transactions

Economic transformation -- change in what an economy produces -- is
foundational to development and rising standards of living. Our understanding
of this process has been propelled recently by two branches of work in the
field of economic complexity, one studying how economies diversify, the other
how the complexity of an economy is expressed in the makeup of its output.
However, the connection between these branches is not well understood, nor how
they relate to a classic understanding of structural transformation. Here, we
present a simple dynamical modeling framework that unifies these areas of work,
based on the widespread observation that economies diversify preferentially
into activities that are related to ones they do already. We show how stylized
facts of long-run structural change, as well as complexity metrics, can both
emerge naturally from this one observation. However, complexity metrics take on
new meanings, as descriptions of the long-term changes an economy experiences
rather than measures of complexity per se. This suggests relatedness and
complexity metrics are connected, in a hitherto overlooked way: Both describe
structural change, on different time scales. Whereas relatedness probes
transformation on short time scales, complexity metrics capture long-term
change.

Since the onset of the the COVID-19 pandemic, many countries across the world
have implemented various non-pharmaceutical interventions (NPIs) to contain the
spread of virus, as well as economic support policies (ESPs) to save their
economies. The pandemic and the associated NPIs have triggered unprecedented
waves of economic shocks to the financial markets, including the foreign
exchange (FX) markets. Although there are some studies exploring the impact of
the NPIs and ESPs on FX markets, the relative impact of individual NPIs or ESPs
has not been studied in a combined framework. In this work, we investigate the
relative impact of NPIs and ESPs with Explainable AI (XAI) techniques.
Experiments over exchange rate data of G10 currencies during the period from
January 1, 2020 to January 13, 2021 suggest strong impacts on exchange rate
markets by all measures of the strict lockdown, such as stay at home
requirements, workplace closing, international travel control, and restrictions
on internal movement. Yet, the impact of individual NPI and ESP can vary across
different currencies. To the best of our knowledge, this is the first work that
uses XAI techniques to study the relative impact of NPIs and ESPs on the FX
market. The derived insights can guide governments and policymakers to make
informed decisions when facing with the ongoing pandemic and a similar
situation in the near future.

The COVID-19 pandemic and subsequent public health restrictions led to a
significant slump in economic activities around the globe. This slump has met
by various policy actions to cushion the detrimental socio-economic
consequences of the COVID-19 crisis and eventually bring the economy back on
track. We provide an ex-ante evaluation of the effectiveness of a massive
increase in research and innovation (R&I) funding in Finland to stimulate
post-crisis recovery growth through an increase in R&I activities of Finnish
firms. We make use of the fact that novel R&I grants for firms in disruptive
circumstances granted in 2020 were allocated through established R&I policy
channels. This allows us to estimate the structural link between R&I funding
and economic growth for Finnish NUTS-3 regions using pre-COVID-19 data.
Estimates are then used to forecast regional recovery growth out of sample and
to quantify the growth contribution of R&I funding. Depending on the chosen
scenario, our forecasts point to a mean recovery growth rate of GDP between
2-4% in 2021 after a decline of up to -2.5% in 2020. R&I funding constitutes a
significant pillar of the recovery process with mean contributions in terms of
GDP growth of between 0.4% and 1%.

Simulation models, in particular agent-based models, are gaining popularity
in economics. The considerable flexibility they offer, as well as their
capacity to reproduce a variety of empirically observed behaviours of complex
systems, give them broad appeal, and the increasing availability of cheap
computing power has made their use feasible. Yet a widespread adoption in
real-world modelling and decision-making scenarios has been hindered by the
difficulty of performing parameter estimation for such models. In general,
simulation models lack a tractable likelihood function, which precludes a
straightforward application of standard statistical inference techniques.
Several recent works have sought to address this problem through the
application of likelihood-free inference techniques, in which parameter
estimates are determined by performing some form of comparison between the
observed data and simulation output. However, these approaches are (a) founded
on restrictive assumptions, and/or (b) typically require many hundreds of
thousands of simulations. These qualities make them unsuitable for large-scale
simulations in economics and can cast doubt on the validity of these inference
methods in such scenarios. In this paper, we investigate the efficacy of two
classes of black-box approximate Bayesian inference methods that have recently
drawn significant attention within the probabilistic machine learning
community: neural posterior estimation and neural density ratio estimation. We
present benchmarking experiments in which we demonstrate that neural network
based black-box methods provide state of the art parameter inference for
economic simulation models, and crucially are compatible with generic
multivariate time-series data. In addition, we suggest appropriate assessment
criteria for future benchmarking of approximate Bayesian inference procedures
for economic simulation models.

This paper studies the role of social networks in spatial mobility across
India. Using aggregated and de-identified data from the world's largest online
social network, we (i) document new descriptive findings on the structure of
social networks and spatial mobility in India; (ii) quantify the effects of
social networks on annual migration choice; and (iii) embed these estimates in
a spatial equilibrium model to study the wage implications of increasing social
connectedness. Across millions of individuals, we find that multiple measures
of social capital are concentrated among the rich and educated and among
migrants. Across destinations, both mobility patterns and social networks are
concentrated toward richer areas. A model of migration suggests individuals are
indifferent between a 10% increase in destination wages and a 12-16% increase
in destination social networks. Accounting for networks reduces the
migration-distance relationship by 19%. In equilibrium, equalizing social
networks across locations improves average wages by 3% (24% for the bottom
wage-quartile), a larger impact than removing the marginal cost of distance. We
find evidence of an economic support mechanism, with destination economic
improvements reducing the migration-network elasticity. We also find suggestive
evidence for an emotional support mechanism from qualitative surveys among
Facebook users. Difference-in-difference estimates suggest college attendance
delivers a 20% increase in network size and diversity. Taken together, our data
suggest that - by reducing effective moving costs - increasing social
connectedness across space may have considerable economic gains.

The international financial system is currently not yet prepared to face a
foreseeable crisis mainly motivated by the dichotomy between the real economy
and the virtual economy. Skepticism is widespread even when it comes to
investments in sustainable economy. The concentration of capital in a few
persons is one of the greatest risks for the possible reiteration of economic
crises. The benevolent sentences of the courts to some of the fraudsters do not
contribute to dispelling the ghost of fraud nor to the disappearance of tax
havens. From the diachronic perspective, it is observed that economic crises
are increasingly frequent and incidents always in the financial field; which
forces us to rethink an economic model on an international scale in which there
is a greater weight of the economic policy of governments over the power of
multinational companies in the context of globalization. In the context of
Corporate Social Responsibility, Corporate Governance is listed as one of the
fundamental levers to curb large business fraud, but its efficiency seems
insufficient due to the lack of international regulations and the ignorance of
hidden forces in what has been known as fiscal and financial engineering. The
application of liberal policies in an unorthodox way is causing real social
gaps in the distribution of income and is undermining the current capitalist
system. The need to implement corporate governments is recommended as one of
the essential formulas for sustaining the international economic system.

In this work, we study economic model predictive control (MPC) in situations
where the optimal operating behavior is periodic. In such a setting, the
performance of a standard economic MPC scheme without terminal conditions can
generally be far from optimal even with arbitrarily long prediction horizons.
Whereas there are modified economic MPC schemes that guarantee optimal
performance, all of them are based on prior knowledge of the optimal period
length or of the optimal periodic orbit itself. In contrast to these
approaches, we propose to achieve optimality by multiplying the stage cost by a
linear discount factor. This modification is not only easy to implement but
also independent of any system- or cost-specific properties, making the scheme
robust against online changes therein. Under standard dissipativity and
controllability assumptions, we can prove that the resulting linearly
discounted economic MPC without terminal conditions achieves optimal asymptotic
average performance up to an error that vanishes with growing prediction
horizons. Moreover, we can guarantee practical asymptotic stability of the
optimal periodic orbit under the additional technical assumption that
dissipativity holds with a continuous storage function. We complement these
qualitative guarantees with a quantitative analysis of the transient and
asymptotic average performance of the linearly discounted MPC scheme in a
numerical simulation study.

In transmission expansion planning, situations can arise in which an
expansion plan that is optimal for the system as a whole is detrimental to a
specific country in terms of its expected economic welfare. If this country is
one of the countries hosting the planned capacity expansion, it has the power
to veto the plan and thus, undermine the system-wide social optimum. To solve
this issue, welfare compensation mechanisms may be constructed that compensate
suffering countries and make them willing to participate in the expansion plan.
In the literature, welfare compensation mechanisms have been developed that
work in expectation. However, in a stochastic setting, even if the welfare
effect after compensation is positive in expectation, countries might still be
hesitant to accept the risk that the actual, realized welfare effect may be
negative in some scenarios.
  In this paper we analyze welfare compensation mechanisms in a stochastic
setting. We consider two existing mechanisms, lump-sum payments and purchase
power agreements, and we develop two novel mechanisms, based on the flow
through the new transmission line and its economic value. Using a case study of
the Northern European power market, we investigate how well these mechanisms
succeed in mitigating risk for the countries involved. Using a theoretically
ideal model-based mechanism, we show that there is a significant potential for
mitigating risk through welfare compensation mechanisms. Out of the four
practical mechanisms we consider, our results indicate that a mechanism based
on the economic value of the new transmission line is most promising.

The evolution of economic and innovation systems at the national scale is
shaped by a complex dynamics, the footprint of which is the nested structure of
the activities in which different countries are competitive. Nestedness is a
persistent feature across multiple kinds (layers) of activities related to the
production of knowledge and goods: scientific research, technological
innovation, industrial production and trade. We observe that in the layers of
innovation and trade the competitiveness of countries correlates unambiguously
with their diversification, while the science layer displays some peculiar
feature. The evolution of scientific domains leads to an increasingly modular
structure, in which the most developed nations become less competitive in the
less advanced scientific domains, where they are replaced by the emerging
countries. This observation is in line with a capability-based view of the
evolution of economic systems, but with a slight twist. Indeed, while the
accumulation of specific know-how and skills is a fundamental step towards
development, resource constraints force countries to acquire competitiveness in
the more complex research fields at the price of losing ground in more basic,
albeit less visible (or more crowded), fields. This tendency towards a
relatively specialized basket of capabilities leads to a trade-off between the
need to diversify in order to evolve and the need to allocate resources
efficiently. Collaborative patterns among developed nations reduce the
necessity to be competitive in the less sophisticated fields, freeing resources
for the more complex domains.

In recent studies, moving target defense (MTD) has been applied to detect
false data injection (FDI) attacks using distributed flexible AC transmission
system (D-FACTS) devices. However, the inherent conflict between the security
goals of MTD (i.e., detecting FDI attacks) and the economic goals of D-FACTS
devices (i.e., reducing power losses) would impede the application of MTD in
real systems. Moreover, the detection capabilities of existing MTDs are often
insufficient. This paper proposes a multi-stage MTD (MMTD) approach to resolve
these two issues by adding a group of designed security-oriented schemes before
D-FACTS' economic-oriented scheme to detect FDI attacks. We keep these
security-oriented schemes for a very short time interval and then revert to the
economic-oriented scheme for the remaining time to ensure the economic
requirements. We prove that a designed MMTD can significantly improve the
detection capability compared to existing one-stage MTDs. We find the supremum
of MMTD's detection capability and study its relationship with system topology
and D-FACTS deployment. Meanwhile, a greedy algorithm is proposed to search the
MMTD strategy to reach this supremum. Simulation results show that the proposed
MMTD can achieve the supremum against FDI attacks while outperforming current
MTD strategies on economic indicators.

In our time cybersecurity has grown to be a topic of massive proportion at
the national and enterprise levels. Our thesis is that the economic perspective
and investment decision-making are vital factors in determining the outcome of
the struggle. To build our economic framework, we borrow from the pioneering
work of Gordon and Loeb in which the Defender optimally trades-off investments
for lower likelihood of its system breach. Our two-sided model additionally has
an Attacker, assumed to be rational and also guided by economic considerations
in its decision-making, to which the Defender responds. Our model is a
simplified adaptation of a model proposed during the Cold War for weapons
deployment in the US. Our model may also be viewed as a Stackelberg game and,
from an analytic perspective, as a Max-Min problem, the analysis of which is
known to have to contend with discontinuous behavior. The complexity of our
simple model is rooted in its inherent nonlinearity and, more consequentially,
non-convexity of the objective function in the optimization. The possibilities
of the Attacker's actions add substantially to the risk to the Defender, and
the Defender's rational, risk-neutral optimal investments in general
substantially exceed the optimal investments predicted by the one-sided
Gordon-Loeb model. We obtain a succinct set of three decision types that
categorize all of the Defender's optimal investment decisions. Also, the
Defender's optimal decisions exhibit discontinuous behavior as the initial
vulnerability of its system is varied. The analysis is supplemented by
extensive numerical illustrations. The results from our model open several
major avenues for future work.

Publication selection bias undermines the systematic accumulation of
evidence. To assess the extent of this problem, we survey over 68,000
meta-analyses containing over 700,000 effect size estimates from medicine
(67,386/597,699), environmental sciences (199/12,707), psychology (605/23,563),
and economics (327/91,421). Our results indicate that meta-analyses in
economics are the most severely contaminated by publication selection bias,
closely followed by meta-analyses in environmental sciences and psychology,
whereas meta-analyses in medicine are contaminated the least. After adjusting
for publication selection bias, the median probability of the presence of an
effect decreased from 99.9% to 29.7% in economics, from 98.9% to 55.7% in
psychology, from 99.8% to 70.7% in environmental sciences, and from 38.0% to
29.7% in medicine. The median absolute effect sizes (in terms of standardized
mean differences) decreased from d = 0.20 to d = 0.07 in economics, from d =
0.37 to d = 0.26 in psychology, from d = 0.62 to d = 0.43 in environmental
sciences, and from d = 0.24 to d = 0.13 in medicine.

An economic zone requires continuous monitoring and controlling by an
autonomous surveillance system for heightening its production competency and
security. Wireless sensor network (WSN) has swiftly grown popularity over the
world for uninterruptedly monitoring and controlling a system. Sensor devices,
the main elements of WSN, are given limited amount of energy, which leads the
network to limited lifespan. Therefore, the most significant challenge is to
increase the lifespan of a WSN system. Topology control mechanism (TCM) is a
renowned method to enhance the lifespan of WSN. This paper proposes an approach
to extend the lifetime of WSN for an economic area, targeting an economic zone
in Bangladesh. Observations are made on the performance of the network lifetime
considering the individual combinations of the TCM protocols and comparative
investigation between the time and energy triggering strategy of TCM protocols.
Results reveal the network makes a better performance in the case of A3
protocol while using the topology maintenance protocols with both time and
energy triggering methods. Moreover, the performance of the A3 and DGETRec is
superior to the other combinations of TCM protocols. Hence, the WSN system can
be able to serve better connectivity coverage in the target economic zone.

We consider a zonal international power market and investigate potential
economic incentives for short-term reductions of transmission capacities on
existing interconnectors by the responsible transmission system operators
(TSOs). We show that if a TSO aims to maximize domestic total welfare, it often
has an incentive to reduce the capacity on the interconnectors to neighboring
countries.
  In contrast with the (limited) literature on this subject, which focuses on
incentives through the avoidance of future balancing costs, we show that
incentives can exist even if one ignores balancing and focuses solely on
welfare gains in the day-ahead market itself. Our analysis consists of two
parts. In the first part, we develop an analytical framework that explains why
these incentives exist. In particular, we distinguish two mechanisms: one based
on price differences with neighboring countries and one based on the domestic
electricity price. In the second part, we perform numerical experiments using a
model of the Northern-European power system, focusing on the Danish TSO. In 97%
of the historical hours tested, we indeed observe economic incentives for
capacity reductions, leading to significant welfare gains for Denmark and
welfare losses for the system as a whole. We show that the potential for
welfare gains greatly depends on the ability of the TSO to adapt interconnector
capacities to short-term market conditions. Finally, we explore the extent to
which the recently introduced European "70%-rule" can mitigate the incentives
for capacity reductions and their welfare effects.

In a 3-2 split verdict, the Supreme Court approved the exclusion of India's
socially and economically backward classes from its affirmative action measures
to address economic deprivation. Dissenting justices, including the Chief
Justice of India, protested the majority opinion for sanctioning ``an avowedly
exclusionary and discriminatory principle.'' In order to justify their
controversial decision, majority justices rely on technical arguments which are
categorically false. The confusion of the majority justices is due to a
combination of two related but subtle technical aspects of the affirmative
action system in India. The first aspect is the significance of overlaps
between members of various protected groups, and the second one is the
significance of the processing sequence of protected groups in the presence of
such overlaps. Conventionally, protected classes were determined by the caste
system, which meant they did not overlap. Addition of a new protected class
defined by economic criteria alters this structure, unless it is artificially
enforced. The majority justices failed to appreciate the significance of these
changes in the system, and inaccurately argued that the controversial exclusion
is a technical necessity to provide benefits to previously-unprotected members
of a new class. We show that this case could have been resolved with three
competing policies that each avoids the controversial exclusion. One of these
policies is in line with the core arguments in the majority opinion, whereas a
second one is in line with those in the dissenting opinion.

This paper uncovers the evolution of cities and Islamist insurgencies, so
called jihad, in the process of the reversal of fortune over the centuries. In
West Africa, water access in ancient periods predicts the locations of the core
cities of inland trade routes -- the trans-Saharan caravan routes -- founded up
to the 1800s, when historical Islamic states played significant economic roles
before European colonization. In contrast, ancient water access does not have a
persistent influence on contemporary city formation and economic activities.
After European colonization and the invention of modern trading technologies,
along with the constant shrinking of water sources, landlocked pre-colonial
core cities contracted or became extinct. Employing an instrumental variable
strategy, we show that these deserted locations have today been replaced by
battlefields for jihadist organizations. We argue that the power relations
between Islamic states and the European military during the 19th century
colonial era shaped the persistence of jihadist ideology as a legacy of
colonization. Investigations into religious ideology related to jihadism, using
individual-level surveys from Muslims, support this mechanism. Moreover, the
concentration of jihadist violence in "past-core-and-present-periphery" areas
in West Africa is consistent with a global-scale phenomenon. Finally,
spillovers of violent events beyond these stylized locations are partly
explained by organizational heterogeneity among competing factions (Al Qaeda
and the Islamic State) over time.

In recent decades, trade between nations has constituted an important
component of global Gross Domestic Product (GDP), with official estimates
showing that it likely accounted for a quarter of total global production.
While evidence of association already exists in macro-economic data between
trade volume and GDP growth, there is considerably less work on whether, at the
level of individual granular sectors (such as vehicles or minerals),
associations exist between the complexity of trading networks and global GDP.
In this paper, we explore this question by using publicly available data from
the Atlas of Economic Complexity project to rigorously construct global trade
networks between nations across multiple sectors, and studying the correlation
between network-theoretic measures computed on these networks (such as average
clustering coefficient and density) and global GDP. We find that there is
indeed significant association between trade networks' complexity and global
GDP across almost every sector, and that network metrics also correlate with
business cycle phenomena such as the Great Recession of 2007-2008. Our results
show that trade volume alone cannot explain global GDP growth, and that network
science may prove to be a valuable empirical avenue for studying complexity in
macro-economic phenomena such as trade.

This paper evaluates Machine Learning (ML) in establishing ratemaking for new
insurance schemes. To make the evaluation feasible, we established expected
indemnities as premiums. Then, we use ML to forecast indemnities using a
minimum set of variables. The analysis simulates the introduction of an income
insurance scheme, the so-called Income Stabilization Tool (IST), in Italy as a
case study using farm-level data from the FADN from 2008-2018. We predicted the
expected IST indemnities using three ML tools, LASSO, Elastic Net, and
Boosting, that perform variable selection, comparing with the Generalized
Linear Model (baseline) usually adopted in insurance investigations.
Furthermore, Tweedie distribution is implemented to consider the peculiarity
shape of the indemnities function, characterized by zero-inflated, no-negative
value, and asymmetric fat-tail. The robustness of the results was evaluated by
comparing the econometric and economic performance of the models. Specifically,
ML has obtained the best goodness-of-fit than baseline, using a small and
stable selection of regressors and significantly reducing the gathering cost of
information. However, Boosting enabled it to obtain the best economic
performance, balancing the most and most minor risky subjects optimally and
achieving good economic sustainability. These findings suggest how machine
learning can be successfully applied in agricultural insurance.This study
represents one of the first to use ML and Tweedie distribution in agricultural
insurance, demonstrating its potential to overcome multiple issues.

This paper evaluates the impact of the pandemic and enforcement at the US and
Mexican borders on the emigration of Guatemalans during 2017-2020. During this
period, the number of crossings from Guatemala fell by 10%, according to the
Survey of Migration to the Southern Border of Mexico. Yet, there was a rise of
nearly 30% in the number of emigration crossings of male adults travelling with
their children. This new trend was partly driven by the recent reduction in the
number of children deported from the US. For a one-point reduction in the
number of children deported from the US to Guatemalan municipalities, there was
an increase of nearly 14 in the number of crossings made by adult males leaving
from Guatemala for Mexico; and nearly 0.5 additional crossings made by male
adults travelling with their children. However, the surge of emigrants
travelling with their children was also driven by the acute economic shock that
Guatemala experienced during the pandemic. During this period, air pollution in
the analysed Guatemalan municipalities fell by 4%, night light per capita fell
by 15%, and homicide rates fell by 40%. Unlike in previous years, emigrants are
fleeing poverty rather than violence. Our findings suggest that a reduction in
violence alone will not be sufficient to reduce emigration flows from Central
America, but that economic recovery is needed.

Greece constitutes a coastal country with a lot of geomorphologic, climatic,
cultural and historic peculiarities favoring the development of many aspects of
tourism. Within this framework, this article examines what are the effects of
tourism in Greece and how determinative these effects are, by applying a
macroscopic analysis on empirical data for the estimation of the contribution
of tourism in the Greek Economy. The available data regard records of the
Balance of Payments in Greece and of the major components of the Balance of the
Invisible Revenues, where a measurable aspect of tourism, the Travel or Tourism
Exchange, is included. At the time period of the available data (2000-2012) two
events of the recent Greek history are distinguished as the most significant
(the Olympic Games in the year 2004 and the economic crisis initiated in the
year 2009) and their impact on the diachronic evolution in the tourism is
discussed. Under an overall assessment, the analysis illustrated that tourism
is a sector of the Greek economy, which is described by a significant
resilience, but it seems that it has not yet been submitted to an effective
developmental plan exploiting the endogenous tourism dynamics of the country,
suggesting currently a promising investment of low risk for the economic growth
of country and the exit of the economic crisis

The network economical sharing economy, with direct exchange as a core
characteristic, is implemented both, on a commons and platform economical
basis. This is due to a gain in importance of trust, collaborative consumption
and democratic management as well as technological progress, in the form of
near zero marginal costs, open source contributions and digital transformation.
Concurrent to these commons-based drivers, the grey area between commerce and
private exchange is used to exploit work, safety and tax regulations by central
platform economists. Instead of central intermediators, the blockchain
technology makes decentralized consensus finding, using Proof-of-Work (PoW)
within a self-sustaining Peer-to-Peer network, possible. Therefore, a
blockchain-based open source mediation seems to offer a commons-compatible
implementation of the sharing economy. This thesis is investigated through a
qualitative case study of Sardex and Interlace with their blockchain
application, based on expert interviews and a structured content analysis. To
detect the most commons-compatible implementation, the different implementation
options through conventional platform intermediators, an open source blockchain
with PoW as well as Interlaces' permissioned blockchain approach, are compared.
The following confrontation is based on deductive criteria, which illustrates
the inherent characteristics of a commons-based sharing economy.

Economic parameters are identified for an in-space industry where the capital
is made on one planet, it is transported to and teleoperated on a second
planet, and the product is transported off the second planet for consumption.
This framework is used to model the long-run cost of lunar propellant
production to help answer whether it is commercially competitive against
propellant launched from Earth. The prior techno-economic analyses (TEAs) of
lunar propellant production had disagreed over this. The "gear ratio on cost"
for capital transport, G, and the production mass ratio of the capital, phi,
are identified as the most important factors determining competitiveness. The
prior TEAs are examined for how they handled these two metrics. This identifies
crucial mistakes in some of the TEAs: choosing transportation architectures
with high G, and neglecting to make choices for the capital that could achieve
adequate phi. The tent sublimation technology has a value of phi that is an
order of magnitude better than the threshold for competitiveness even in low
Earth orbit (LEO). The strip mining technology is closer to the threshold, but
technological improvements plus several years of operating experience will
improve its competitiveness, according to the model. Objections from members of
the aerospace community are discussed, especially the question whether the
technology can attain adequate reliability in the lunar environment. The
results suggest that lunar propellant production will be commercially viable
and that it should lower the cost of doing everything else in space.

Contemporary deep learning based solution methods used to compute approximate
equilibria of high-dimensional dynamic stochastic economic models are often
faced with two pain points. The first problem is that the loss function
typically encodes a diverse set of equilibrium conditions, such as market
clearing and households' or firms' optimality conditions. Hence the training
algorithm trades off errors between those -- potentially very different --
equilibrium conditions. This renders the interpretation of the remaining errors
challenging. The second problem is that portfolio choice in models with
multiple assets is only pinned down for low errors in the corresponding
equilibrium conditions. In the beginning of training, this can lead to
fluctuating policies for different assets, which hampers the training process.
To alleviate these issues, we propose two complementary innovations. First, we
introduce Market Clearing Layers, a neural network architecture that
automatically enforces all the market clearing conditions and borrowing
constraints in the economy. Encoding economic constraints into the neural
network architecture reduces the number of terms in the loss function and
enhances the interpretability of the remaining equilibrium errors. Furthermore,
we present a homotopy algorithm for solving portfolio choice problems with
multiple assets, which ameliorates numerical instabilities arising in the
context of deep learning. To illustrate our method we solve an overlapping
generations model with two permanent risk aversion types, three distinct
assets, and aggregate shocks.

Building energy retrofits have been identified as key to realizing climate
mitigation goals in Canada. This study aims to provide a roadmap for existing
mid-rise building retrofits in order to understand the required capital
investment, energy savings, energy cost savings, and carbon footprint for
mid-rise residential buildings in Canada. This study employed EnergyPlus to
examine the energy performance of 11 energy retrofit measures for a typical
multi-unit residential building (MURB) in Metro Vancouver, British Columbia,
Canada. The author employed the energy simulation software (EnergyPlus) to
evaluate the pre-and post-retrofit operational energy performance of the
selected MURB. Two base building models powered by natural gas (NG-building)
and electricity (E-building) were created by SketchUP. The energy simulation
results were combined with cost and emission impact data to evaluate the
economic and environmental performance of the selected energy retrofit
measures. The results indicated that the NG-building can produce significant
GHG emission reductions (from 27.64 tCO2e to 3.77 tCO2e) by implementing these
energy retrofit measures. In terms of energy savings, solar PV, ASHP, water
heater HP, and HRV enhancement have great energy saving potential compared to
other energy retrofit measures. In addition, temperature setback, lighting, and
airtightness enhancement present the best economic performance from a life
cycle perspective. However, windows, ASHP, and solar PV, are not economical
choices because of higher life cycle costs. While ASHP can increase life cycle
costs for the NG-building, with the financial incentives provided by the
governments, ASHP could be the best choice to reduce GHG emissions when
stakeholders make decisions on implementing energy retrofits.

March 2020 confinement has shot Portuguese savings to historic levels,
reaching 13.4% of gross disposable income in early 2021 (INE, 2023). To find
similar savings figures we need to go back to 1999. With consumption reduced to
a bare minimum, the Portuguese were forced to save. Households reduced spending
more because of a lack of alternatives to consumption than for any other
reason. The relationship between consumption, savings, and income has occupied
an important role in economic thought [(Keynes, 1936; 1937); (Friedman, 1957)].
Traditionally, high levels of savings have been associated with benefits to the
economy, since financing capacity is enhanced (Singh, 2010). However, the
effects here can be twofold. On the one hand, it seems that Portugal faced the
so-called Savings Paradox (Keynes, 1936). If consumers decide to save a
considerable part of their income, there will be less demand for the goods
produced. Lower demand will lead to lower supply, production, income, and,
paradoxically, fewer savings. On the other hand, after having accumulated
savings at the peak of the pandemic, the Portuguese are now using them to carry
out postponed consumption and, hopefully, to better resist the escalating
inflation. This study aims to examine Portuguese households' savings evolution
during the most critical period of the pandemic, between March 2020 and April
2022. The methodology analyses the correlation between savings, consumption,
and GDP as well as GDP's decomposition into its various components and
concluded that these suddenly forced savings do not fit traditional economic
theories of savings.

This study investigates the role of editorial board members as gatekeepers in
science, creating and utilizing a database of 1,516 active economics journals
in 2019, which includes more than 44,000 scholars from over 6,000 institutions
and 142 countries. The composition of these editorial boards is explored in
terms of geographic affiliation, institutional affiliation, and gender. Results
highlight that the academic publishing environment is primarily governed by men
affiliated with elite universities in the United States. The study further
explores social similarities among journals using a network analysis
perspective based on interlocking editorship. Comparison of networks generated
by all scholars, editorial leaders, and non-editorial leaders reveals
significant structural similarities and associations among clusters of
journals. These results indicate that links between pairs of journals tend to
be redundant, and this can be interpreted in terms of social and intellectual
homophily within each board, and between boards of journals belonging to the
same cluster. Finally, the analysis of the most central journals and scholars
in the networks suggests that journals probably adopt 'strategic decisions' in
the selection of the editorial board members. The documented high concentration
of editorial power poses a serious risk to innovative research in economics.

Damage functions in integrated assessment models (IAMs) map changes in
climate to economic impacts and form the basis for most of estimates of the
social cost of carbon. Implicit in these functions lies an unwarranted
assumption that restricts the spatial variation (Svar) and temporal variability
(Tvar) of changes in climate to be null. This could bias damage estimates and
the climate policy advice from IAMs. While the effects of Tvar have been
studied in the literature, those of Svar and their interactions with Tvar have
not. Here we present estimates of the economic costs of climate change that
account for both Tvar and Svar, as well as for the seasonality of damages
across sectors. Contrary to the results of recent studies which show little
effect that of Tvar on expected losses, we reveal that ignoring Svar produces
large downward biases, as warming is highly heterogeneous over space. Using a
conservative calibration for the damage function, we show that previous
estimates are biased downwards by about 23-36%, which represents additional
losses of about US$1,400-US$2,300 billion by 2050 and US$17-US$28 trillion by
the end of the century, under a high emissions scenario. The present value of
losses during the period 2020-2100 would be larger than reported in previous
studies by $47-$66 trillion or about 1/2 to 3/4 of annual global GDP in 2020.
Our results imply that using global mean temperature change in IAMs as a
summary measure of warming is not adequate for estimating the costs of climate
change. Instead, IAMs should include a more complete description of climate
conditions.

Identifying macroeconomic events that are responsible for dramatic changes of
economy is of particular relevance to understand the overall economic dynamics.
We introduce an open-source available efficient Python implementation of a
Bayesian multi-trend change point analysis which solves significant memory and
computing time limitations to extract crisis information from a correlation
metric. Therefore, we focus on the recently investigated S&P500 mean market
correlation in a period of roughly 20 years that includes the dot-com bubble,
the global financial crisis and the Euro crisis. The analysis is performed
two-fold: first, in retrospect on the whole dataset and second, in an on-line
adaptive manner in pre-crisis segments. The on-line sensitivity horizon is
roughly determined to be 80 up to 100 trading days after a crisis onset. A
detailed comparison to global economic events supports the interpretation of
the mean market correlation as an informative macroeconomic measure by a rather
good agreement of change point distributions and major crisis events.
Furthermore, the results hint to the importance of the U.S. housing bubble as
trigger of the global financial crisis, provide new evidence for the general
reasoning of locally (meta)stable economic states and could work as a
comparative impact rating of specific economic events.

This paper presents a pioneering approach for simulation of economic
activity, policy implementation, and pricing of goods in token economies. The
paper proposes a formal analysis framework for wealth distribution analysis and
simulation of interactions between economic participants in an economy. Using
this framework, we define a mechanism for identifying prices that achieve the
desired wealth distribution according to some metric, and stability of economic
dynamics.
  The motivation to study tokenomics theory is the increasing use of
tokenization, specifically in financial infrastructures, where designing token
economies is in the forefront. Tokenomics theory establishes a quantitative
framework for wealth distribution amongst economic participants and implements
the algorithmic regulatory controls mechanism that reacts to changes in
economic conditions.
  In our framework, we introduce a concept of tokenomic taxonomy where agents
in the economy are categorized into agent types and interactions between them.
This novel approach is motivated by having a generalized model of the
macroeconomy with controls being implemented through interactions and policies.
The existence of such controls allows us to measure and readjust the wealth
dynamics in the economy to suit the desired objectives.

This study presents the first comprehensive analysis of the social and
economic effects of solar mini-grids in rural African settings, specifically in
Kenya and Nigeria. A group of 2,658 household heads and business owners
connected to mini-grids over the last five years were interviewed both before
and one year after their connection. These interviews focused on changes in
gender equality, productivity, health, safety, and economic activity. The
results show notable improvements in all areas. Economic activities and
productivity increased significantly among the connected households and
businesses. The median income of rural Kenyan community members quadrupled.
Gender equality also improved, with women gaining more opportunities in
decision making and business. Health and safety enhancements were linked to
reduced use of hazardous energy sources like kerosene lamps. The introduction
of solar mini-grids not only transformed the energy landscape but also led to
broad socioeconomic benefits in these rural areas. The research highlights the
substantial impact of decentralized renewable energy on the social and economic
development of rural African communities. Its findings are crucial for
policymakers, development agencies, and stakeholders focused on promoting
sustainable energy and development in Africa.

This paper examines whether there is a premium in country size. We study
whether there are significant gains from being a small or a large country in
terms of certain socioeconomic indicators and how large this premium is. Using
panel data for 200 countries over 50 years, we estimate premia for various
sizes of nations across a variety of key economic and socioeconomic performance
indicators. We find that smaller countries are richer, have larger governments,
and are more prudent in terms of fiscal policies than larger ones. On the other
hand, smaller countries seem to be subject to higher absolute and per capita
costs for the provision of essential public goods, which may lower their
socioeconomic performance in terms of health and education. In terms of
economic performance, small countries seem to do better than large countries,
compensating for smallness by relying on foreign trade and foreign direct
investment. The latter comes at the cost of higher vulnerability to external
shocks, resulting in higher volatility of growth rates. This paper's findings
offer essential guidance to policymakers, international organizations, and
business researchers, especially those assessing a country's economic or
socioeconomic performance or potential. The study implies that comparisons with
medium-sized or large countries may be of little utility in predicting the
performance of small countries.

In this paper, we employ spatial econometric methods to analyze panel data
from German NUTS 3 regions. Our goal is to gain a deeper understanding of the
significance and interdependence of industry clusters in shaping the dynamics
of GDP. To achieve a more nuanced spatial differentiation, we introduce
indicator matrices for each industry sector which allows for extending the
spatial Durbin model to a new version of it. This approach is essential due to
both the economic importance of these sectors and the potential issue of
omitted variables. Failing to account for industry sectors can lead to omitted
variable bias and estimation problems. To assess the effects of the major
industry sectors, we incorporate eight distinct branches of industry into our
analysis. According to prevailing economic theory, these clusters should have a
positive impact on the regions they are associated with. Our findings indeed
reveal highly significant impacts, which can be either positive or negative, of
specific sectors on local GDP growth. Spatially, we observe that direct and
indirect effects can exhibit opposite signs, indicative of heightened
competitiveness within and between industry sectors. Therefore, we recommend
that industry sectors should be taken into consideration when conducting
spatial analysis of GDP. Doing so allows for a more comprehensive understanding
of the economic dynamics at play.

Pandemics involve the high transmission of a disease that impacts global and
local health and economic patterns. The impact of a pandemic can be minimized
by enforcing certain restrictions on a community. However, while minimizing
infection and death rates, these restrictions can also lead to economic crises.
Epidemiological models help propose pandemic control strategies based on
non-pharmaceutical interventions such as social distancing, curfews, and
lockdowns, reducing the economic impact of these restrictions. However,
designing manual control strategies while considering disease spread and
economic status is non-trivial. Optimal strategies can be designed through
multi-objective reinforcement learning (MORL) models, which demonstrate how
restrictions can be used to optimize the outcome of a pandemic. In this
research, we utilized an epidemiological Susceptible, Exposed, Infected,
Recovered, Deceased (SEIRD) model: a compartmental model for virtually
simulating a pandemic day by day. We combined the SEIRD model with a deep
double recurrent Q-network to train a reinforcement learning agent to enforce
the optimal restriction on the SEIRD simulation based on a reward function. We
tested two agents with unique reward functions and pandemic goals to obtain two
strategies. The first agent placed long lockdowns to reduce the initial spread
of the disease, followed by cyclical and shorter lockdowns to mitigate the
resurgence of the disease. The second agent provided similar infection rates
but an improved economy by implementing a 10-day lockdown and 20-day
no-restriction cycle. This use of reinforcement learning and epidemiological
modeling allowed for both economic and infection mitigation in multiple
pandemic scenarios.

The rapid development of the digital economy has had a profound impact on the
implementation of the rural revitalization strategy. Based on this, this study
takes Hainan Province as the research object to deeply explore the impact of
digital economic development on rural revitalization. The study collected panel
data from 2003 to 2022 to construct an evaluation index system for the digital
economy and rural revitalization and used panel regression analysis and other
methods to explore the promotion effect of the digital economy on rural
revitalization. Research results show that the digital economy has a
significant positive impact on rural revitalization, and this impact increases
as the level of fiscal expenditure increases. The issuance of digital RMB has
further exerted a regulatory effect and promoted the development of the digital
economy and the process of rural revitalization. At the same time, the
establishment of the Hainan Free Trade Port has also played a positive role in
promoting the development of the digital economy and rural revitalization. In
the prediction of the optimal strategy for rural revitalization based on the
development levels of the primary, secondary, and tertiary industries (Rate1,
Rate2, and Rate3), it was found that rate1 can encourage Hainan Province to
implement digital economic innovation, encourage rate3 to implement promotion
behaviors, and increase rate2 can At the level of sustainable development when
rate3 promotes rate2's digital economic innovation behavior, it can standardize
rate2's production behavior to the greatest extent, accelerate the faster
application of the digital economy to the rural revitalization industry, and
promote the technological advancement of enterprises.

Agricultural products play a critical role in human development. With
economic globalization and the financialization of agricultural products
continuing to advance, the interconnections between different agricultural
futures have become closer. We utilize a TVP-VAR-DY model combined with the
quantile method to measure the risk spillover between 11 agricultural futures
on the futures exchanges of US and China from July 9,2014, to December 31,2022.
This study yielded several significant findings. Firstly, CBOT corn, soybean,
and wheat were identified as the primary risk transmitters, with DCE corn and
soybean as the main risk receivers. Secondly, sudden events or increased
economic uncertainty can increase the overall risk spillovers. Thirdly, there
is an aggregation of risk spillovers amongst agricultural futures based on the
dynamic directional spillover results. Lastly, the central agricultural futures
under the conditional mean are CBOT corn and soybean, while CZCE hard wheat and
long-grained rice are the two risk spillover centers in extreme cases, as per
the results of the spillover network and minimum spanning tree. Based on these
results, decision-makers are advised to safeguard against the price risk of
agricultural futures under sudden economic events, and investors can utilize
the results to construct a superior investment portfolio by taking different
agricultural product futures as risk-leading indicators according to various
situations.

This work investigates the effects of complex networks on the collective
behavior of a three-state opinion formation model in economic systems. Our
model considers two distinct types of investors in financial markets: noise
traders and fundamentalists. Financial states evolve via probabilistic dynamics
that include economic strategies with local and global influences. The local
majoritarian opinion drives noise traders' market behavior, while the market
index influences the financial decisions of fundamentalist agents. We introduce
a level of market anxiety $q$ present in the decision-making process that
influences financial action. In our investigation, nodes of a complex network
represent market agents, whereas the links represent their financial
interactions. We investigate the stochastic dynamics of the model on three
distinct network topologies, including scale-free networks, small-world
networks and Erd{\"o}s-R\'enyi random graphs. Our model mirrors various traits
observed in real-world financial return series, such as heavy-tailed return
distributions, volatility clustering, and short-term memory correlation of
returns. The histograms of returns are fitted by coupled Gaussian
distributions, quantitatively revealing transitions from a leptokurtic to a
mesokurtic regime under specific economic heterogeneity. We show that the
market dynamics depend mainly on the average agent connectivity, anxiety level,
and market composition rather than on specific features of network topology.

We propose a Statistical-Mechanics inspired framework for modeling economic
systems. Each agent composing the economic system is characterized by a few
variables of distinct nature (e.g. saving ratio, expectations, etc.). The
agents interact locally by their individual variables: for example, people
working in the same office may influence their peers' expectations
(optimism/pessimism are contagious), while people living in the same
neighborhood may influence their peers' saving patterns (stinginess/largeness
are contagious). Thus, for each type of variable there exists a different
underlying social network, which we refer to as a ``layer''. Each layer
connects the same set of agents by a different set of links defining a
different topology. In different layers, the nature of the variables and their
dynamics may be different (Ising, Heisenberg, matrix models, etc). The
different variables belonging to the same agent interact (the level of optimism
of an agent may influence its saving level), thus coupling the various layers.
We present a simple instance of such a network, where a one-dimensional Ising
chain (representing the interaction between the optimist-pessimist
expectations) is coupled through a random site-to-site mapping to a
one-dimensional generalized Blume-Capel chain (representing the dynamics of the
agents' saving ratios). In the absence of coupling between the layers, the
one-dimensional systems describing respectively the expectations and the saving
ratios do not feature any ordered phase (herding). Yet, such a herding phase
emerges in the coupled system, highlighting the non-trivial nature of the
present framework.

The initial theoretical connections between Leontief input-output models and
Markov chains were established back in 1950s. However, considering the wide
variety of mathematical properties of Markov chains, there has not been a full
investigation of evolving world economic networks with Markov chain formalism.
Using the recently available world input-output database, we modeled the
evolution of the world economic network from 1995 to 2011 through analysis of a
series of finite Markov chains. We assessed different aspects of this evolving
system via different properties of the Markov chains such as mixing time,
Kemeny constant, steady state probabilities and perturbation analysis of the
transition matrices. First, we showed how the time series of mixing times and
Kemeny constants could be used as an aggregate index of globalization. Next, we
focused on the steady state probabilities as a measure of structural power of
the economies that are comparable to GDP shares of economies as the traditional
index of economies. Further, we introduced two measures of systemic risk,
called systemic influence and systemic fragility, where the former is the ratio
of number of influenced nodes to the total number of nodes, caused by a shock
in the activity of a node and the latter is based on the number of times a
specific economic node is affected by a shock in the activity of any of the
other nodes. Finally, focusing on Kemeny constant as a global indicator of
monetary flow across the network, we showed that there is a paradoxical effect
of a change in activity levels of economic nodes on the overall flow of the
network. While the economic slowdown of the majority of nodes with high
structural power results to a slower average monetary flow over the network,
there are some nodes, where their slowdowns improve the overall quality of the
network in terms of connectivity and the average monetary flow.

Recently we uploaded to the arxiv a paper entitled: Improving the Economic
Complexity Index. There, we compared three metrics of the knowledge intensity
of an economy, the original metric we published in 2009 (the Economic
Complexity Index or ECI), a variation of the metric proposed in 2012, and a
variation we called ECI+. It was brought to our attention that the definition
of ECI+ was equivalent to the variation of the metric proposed in 2012. We have
verified this claim, and found that while the equations are not exactly the
same, they are similar enough to be our own oversight. More importantly, we now
ask: how many variations of the original ECI work? In this paper we provide a
simple unifying framework to explore multiple variations of ECI, including both
the original 2009 ECI and the 2012 variation. We found that a large fraction of
variations have a similar predictive power, indicating that the chance of
finding a variation of ECI that works, after the seminal 2009 measure, are
surprisingly high. In fact, more than 28 percent of these variations have a
predictive power that is within 90 percent of the maximum for any variation.
These findings show that, once the idea of measuring economic complexity was
out, creating a variation with a similar predictive power (like the ones
proposed in 2012) was trivial (a 1 in 3 shot). More importantly, the result
show that using exports data to measure the knowledge intensity of an economy
is a robust phenomenon that works for multiple functional forms. Moreover, the
fact that multiple variations of the 2009 ECI perform close to the maximum,
tells us that no variation of ECI will have a performance that is substantially
better. This suggests that research efforts should focus on uncovering the
mechanisms that contribute to the diffusion and accumulation of productive
knowledge instead of on exploring small variations to existing measures.

Economic growth and the growth of human population in the past 2,000,000
years are extensively examined. Data are found to be in a clear contradiction
of the currently accepted explanations of the mechanism of growth, which
revolve around two fundamental but incorrect doctrines: (1) the doctrine of
stagnation (inappropriately labelled also as Malthusian stagnation, because
Malthus never claimed that his positive checks would cause a long-lasting and
wide-spread stagnation) and (2) the doctrine of explosion described also as a
takeoff, sprint, spike or by other similar attributes. These doctrines and
other related postulates are contradicted even by precisely the same data,
which are used in the economic research and by the research results published
in a prestigious scientific journal as early as in 1960. The generally accepted
explanations are not based on a rigorous analysis of data but on impressions
created by the easily misleading features of hyperbolic distributions. Two
leading theories: the Demographic Transitions Theory (or Model) and the Unified
Growth Theory are fundamentally incorrect. Descriptions of the past
socio-economic conditions are not questioned. They might have been harsh,
difficult and primitive but they are not reflected in the growth trajectories.
They did not create stagnation in the economic growth and in the growth of
population. Likewise, impacts of the Industrial Revolution on many aspects of
life are not questioned. It is only demonstrated that this event had absolutely
no impact on shaping growth trajectories. A general law of growth is formulated
and used to explain the mechanism of growth of human population and of economic
growth. The growth was predominantly hyperbolic. Such a growth is described by
exceptionally simple mathematical function and the explanation of the mechanism
of growth turns out to be also simple.

Domestic and foreign scholars have already done much research on regional
disparity and its evolution in China, but there is a big difference in
conclusions. What is the reason for this? We think it is mainly due to
different analytic approaches, perspectives, spatial units, statistical
indicators and different periods for studies. On the basis of previous analyses
and findings, we have done some further quantitative computation and empirical
study, and revealed the inter-provincial disparity and regional disparity of
economic development and their evolution trends from 1952-2000. The results
shows that (a) Regional disparity in economic development in China, including
the inter-provincial disparity, inter-regional disparity and intra-regional
disparity, has existed for years; (b) Gini coefficient and Theil coefficient
have revealed a similar dynamic trend for comparative disparity in economic
development between provinces in China. From 1952 to 1978, except for the
"Great Leap Forward" period, comparative disparity basically assumes a upward
trend and it assumed a slowly downward trend from 1979 to1990. Afterwards from
1991 to 2000 the disparity assumed a slowly upward trend again; (c) A
comparison between Shanghai and Guizhou shows that absolute inter-provincial
disparity has been quite big for years; and (d) The Hurst exponent (H=0.5) in
the period of 1966-1978 indicates that the comparative inter-provincial
disparity of economic development showed a random characteristic, and in the
Hurst exponent (H>0.5) in period of 1979-2000 indicates that in this period the
evolution of the comparative inter-provincial disparity of economic development
in China has a long-enduring characteristic.

Decentralized Ledger Technology, popularized by the Bitcoin network, aims to
keep track of a ledger of valid transactions between agents of a virtual
economy without a central institution for coordination. In order to keep track
of a faithful and accurate list of transactions, the ledger is broadcast and
replicated across machines in a peer-to-peer network. To enforce validity of
transactions in the ledger (i.e., no negative balance or double spending), the
network as a whole coordinates to accept or reject new transactions based on a
set of rules aiming to detect and block operations of malicious agents (i.e.,
Byzantine attacks). Consensus protocols are particularly important to
coordinate operation of the network, since they are used to reconcile
potentially conflicting versions of the ledger. Regardless of architecture and
consensus mechanism used, resulting economic networks remain largely similar,
with economic agents driven by incentives under a set of rules. Due to the
intense activity in this area, proper mathematical frameworks to model and
analyze behavior of blockchain-enabled systems are essential. In this paper, we
address this need and provide the following contributions: (i) we establish a
formal framework, with tools from dynamical systems theory, to mathematically
describe core concepts in blockchain-enabled networks, (ii) we apply this
framework to the Bitcoin network and recover its key properties, and (iii) we
connect our modeling framework with powerful tools from control engineering,
such as Lyapunov-like functions, to properly engineer economic systems with
provable properties. Apart from the aforementioned contributions, the
mathematical framework herein proposed lays a foundation for engineering more
general economic systems built on emerging Turing complete networks, such as
the Ethereum network, through which complex alternative economic models are
explored.

The ongoing rapid urbanization phenomena make the understanding of the
evolution of urban environments of utmost importance to improve the well-being
and steer societies towards better futures. Many studies have focused on the
emerging properties of cities, leading to the discovery of scaling laws
mirroring, for instance, the dependence of socio-economic indicators on city
sizes. Though scaling laws allow for the definition of city-size independent
socio-economic indicators, only a few efforts have been devoted to the modeling
of the dynamical evolution of cities as mirrored through socio-economic
variables and their mutual influence. In this work, we propose a Maximum
Entropy (ME), non-linear, generative model of cities. We write in particular a
Hamiltonian function in terms of a few macro-economic variables, whose coupling
parameters we infer from real data corresponding to French towns. We first
discover that non-linear dependencies among different indicators are needed for
a complete statistical description of the non-Gaussian correlations among them.
Furthermore, though the dynamics of individual cities are far from being
stationary, we show that the coupling parameters corresponding to different
years turn out to be quite robust. The quasi time-invariance of the Hamiltonian
model allows proposing an analytic model for the evolution in time of the
macro-economic variables, based on the Langevin equation. Despite no temporal
information about the evolution of cities has been used to derive this model,
its forecast accuracy of the temporal evolution of the system is compatible to
that of a model inferred using explicitly such information.

CO2 capture and storage (CCS) technology is likely to be widely deployed in
coming decades in response to major climate and economics drivers: CCS is part
of every clean energy pathway that limits global warming to 2C or less and
receives significant CO2 tax credits in the United States. These drivers are
likely to stimulate capture, transport, and storage of hundreds of millions or
billions of tonnes of CO2 annually. A key part of the CCS puzzle will be
identifying and characterizing suitable storage sites for vast amounts of CO2.
We introduce a new software tool called SCO2T (Sequestration of CO2 Tool,
pronounced "Scott") to rapidly characterizing saline storage reservoirs. The
tool is designed to rapidly screen hundreds of thousands of reservoirs, perform
sensitivity and uncertainty analyses, and link sequestration engineering
(injection rates, reservoir capacities, plume dimensions) to sequestration
economics (costs constructed from around 70 separate economic inputs). We
describe the novel science developments supporting SCO2T including a new
approach to estimating CO2 injection rates and CO2 plume dimensions as well as
key advances linking sequestration engineering with economics. Next, we perform
a sensitivity and uncertainty analysis of geology combinations (including
formation depth, thickness, permeability, porosity, and temperature) to
understand the impact on carbon sequestration. Through the sensitivity analysis
we show that increasing depth and permeability both can lead to increased CO2
injection rates, increased storage potential, and reduced costs, while
increasing porosity reduces costs without impacting the injection rate (CO2 is
injected at a constant pressure in all cases) by increasing the reservoir
capacity.

The COVID-19 Pandemic has been described as the global challenge of our time,
an enormous human tragedy with dramatic economic impacts. This paper describes
the response and expected recovery process for Western Australia, where a rapid
and effective response was implemented. This has enabled an early transition
into an expected recovery both in health and economic terms. The positive
lessons learned from this experience are documented as they emerge in order to
support other states and nations as they address this issue globally in the
near-term and consider enduring improvements for the longer term. While the
authors have personal experience in the WA context, wider observations across
Australia and selected international benchmarks are also included. Key lessons
include the importance of good health advice in Australia's interest; timely,
synchronized and aligned action at all levels of government; a program of well
communicated, aligned health and economic measures which support all in society
allowing a very high level of appropriate community behaviour, ensuring the
health system was not overloaded; innovation in telehealth, testing, pandemic
modelling, and integrated operations which also allowed essential industries to
continue; and strong border and travel controls with highly effective isolation
preventing community spread, ultimately enabling rapid elimination of the
disease from the hospital system. In combination, these demonstrate that in the
case of Western Australia the result of first eliminating the disease from the
community, and then reopening the economy progressively at a strong pace, has
enabled a world leading outcome in both in health and economic terms. The
lessons from this experience are widely applicable, shareable both as
supporting service to other regions and through knowledge transfer.

We develop methodology for estimation and inference using machine learning to
enrich economic models. Our framework takes a standard economic model and
recasts the parameters as fully flexible nonparametric functions, to capture
the rich heterogeneity based on potentially high dimensional or complex
observable characteristics. These "parameter functions" retain the
interpretability, economic meaning, and discipline of classical parameters.
Deep learning is particularly well-suited to structured modeling of
heterogeneity in economics. We show how to design the network architecture to
match the structure of the economic model, delivering novel methodology that
moves deep learning beyond prediction. We prove convergence rates for the
estimated parameter functions. These functions are the key inputs into the
finite-dimensional parameter of inferential interest. We obtain inference based
on a novel influence function calculation that covers any second-stage
parameter and any machine-learning-enriched model that uses a smooth
per-observation loss function. No additional derivations are required. The
score can be taken directly to data, using automatic differentiation if needed.
The researcher need only define the original model and define the parameter of
interest. A key insight is that we need not write down the influence function
in order to evaluate it on the data. Our framework gives new results for a host
of contexts, covering such diverse examples as price elasticities,
willingness-to-pay, and surplus measures in binary or multinomial choice
models, effects of continuous treatment variables, fractional outcome models,
count data, heterogeneous production functions, and more. We apply our
methodology to a large scale advertising experiment for short-term loans. We
show how economically meaningful estimates and inferences can be made that
would be unavailable without our results.

I develop a novel macroeconomic epidemiological agent-based model to study
the impact of the COVID-19 pandemic under varying policy scenarios. Agents
differ with regard to their profession, family status and age and interact with
other agents at home, work or during leisure activities. The model allows to
implement and test actually used or counterfactual policies such as closing
schools or the leisure industry explicitly in the model in order to explore
their impact on the spread of the virus, and their economic consequences. The
model is calibrated with German statistical data on time use, demography,
households, firm demography, employment, company profits and wages. I set up a
baseline scenario based on the German containment policies and fit the
epidemiological parameters of the simulation to the observed German death curve
and an estimated infection curve of the first COVID-19 wave. My model suggests
that by acting one week later, the death toll of the first wave in Germany
would have been 180% higher, whereas it would have been 60% lower, if the
policies had been enacted a week earlier. I finally discuss two stylized fiscal
policy scenarios: procyclical (zero-deficit) and anticyclical fiscal policy. In
the zero-deficit scenario a vicious circle emerges, in which the economic
recession spreads from the high-interaction leisure industry to the rest of the
economy. Even after eliminating the virus and lifting the restrictions, the
economic recovery is incomplete. Anticyclical fiscal policy on the other hand
limits the economic losses and allows for a V-shaped recovery, but does not
increase the number of deaths. These results suggest that an optimal response
to the pandemic aiming at containment or holding out for a vaccine combines
early introduction of containment measures to keep the number of infected low
with expansionary fiscal policy to keep output in lower risk sectors high.

Various measures have been taken in different countries to mitigate the
Covid-19 epidemic. But, throughout the world, many citizens don't understand
well how these measures are taken and even question the decisions taken by
their government. Should the measures be more (or less) restrictive? Are they
taken for a too long (or too short) period of time? To provide some
quantitative elements of response to these questions, we consider the
well-known SEIR model for the Covid-19 epidemic propagation and propose a
pragmatic model of the government decision-making operation. Although simple
and obviously improvable, the proposed model allows us to study the tradeoff
between health and economic aspects in a pragmatic and insightful way. Assuming
a given number of phases for the epidemic and a desired tradeoff between health
and economic aspects, it is then possible to determine the optimal duration of
each phase and the optimal severity level for each of them. The numerical
analysis is performed for the case of France but the adopted approach can be
applied to any country. One of the takeaway messages of this analysis is that
being able to implement the optimal 4-phase epidemic management strategy in
France would have led to 1.05 million infected people and a GDP loss of 231
billion euro instead of 6.88 million of infected and a loss of 241 billion
euro. This indicates that, seen from the proposed model perspective, the
effectively implemented epidemic management strategy is good economically,
whereas substantial improvements might have been obtained in terms of health
impact. Our analysis indicates that the lockdown/severe phase should have been
more severe but shorter, and the adjustment phase occurred earlier. Due to the
natural tendency of people to deviate from the official rules, updating
measures every month over the whole epidemic episode seems to be more
appropriate.

State governments in the U.S. have been facing difficult decisions involving
tradeoffs between economic and health-related outcomes during the COVID-19
pandemic. Despite evidence of the effectiveness of government-mandated
restrictions mitigating the spread of contagion, these orders are stigmatized
due to undesirable economic consequences. This tradeoff resulted in state
governments employing mandates in widely different ways. We compare the
different policies states implemented during periods of restriction (lockdown)
and reopening with indicators of COVID-19 spread and consumer card spending at
each state during the first wave of the pandemic in the U.S. between March and
August 2020. We find that while some states enacted reopening decisions when
the incidence rate of COVID-19 was minimal or sustained in its relative
decline, other states relaxed socioeconomic restrictions near their highest
incidence and prevalence rates experienced so far. Nevertheless, all states
experienced similar trends in consumer card spending recovery, which was
strongly correlated with reopening policies following the lockdowns and
relatively independent from COVID-19 incidence rates at the time. Our findings
suggest that consumer card spending patterns can be attributed to government
mandates rather than COVID-19 incidence in the states. We estimate the recovery
in states that reopened in late April was more than the recovery in states that
did not reopen in the same period - 15% for consumer card spending and 18% for
spending by high income households. This result highlights the important role
of state policies in minimizing health impacts while promoting economic
recovery and helps planning effective interventions in subsequent waves and
immunization efforts.

We recall the historically admitted prerequisites of Economic Freedom (EF).
We have examined 908 data points for the Economic Freedom of the World (EFW)
index and 1884 points for the Index of Economic Freedom (IEF); the studied
periods are 2000-2006 and 1997-2007, respectively, thereby following the Berlin
wall collapse, and including Sept. 11, 2001. After discussing EFW index and
IEF, in order to compare the indices, one needs to study their overlap in time
and space. That leaves 138 countries to be examined over a period extending
from 2000 to 2006, thus 2 sets of 862 data points. The data analysis pertains
to the rank-size law technique. It is examined whether the distributions obey
an exponential or a power law. A correlation with the country Gross Domestic
Product (GDP), an admittedly major determinant of EF, follows, distinguishing
regional aspects, i.e. defining 6 continents. Semi-log plots show that the
EFW-rank relationship is exponential for countries of high rank ($\ge 20$);
overall the log-log plots point to a behaviour close to a power law. In
contrast, for the IEF, the overall ranking has an exponential behaviour; but
the log-log plots point to the existence of a transitional point between two
different power laws, i.e., near rank 10. Moreover, log-log plots of the EFW
index relationship to country GDP is characterised by a power law, with a
rather stable exponent ($\gamma \simeq 0.674$) as a function of time. In
contrast, log-log plots of the IEF relationship with the country's gross
domestic product point to a downward evolutive power law as a function of time.
Markedly the two studied indices provide different aspects of EF.

An information entropy statistical methodology was used to evaluate the
growth of the UK economy over the period 2000 to 2019, with an emphasis on the
impact of labour productivity on gross domestic product (GDP) per capita and
the average growth in real wages, during this time period. The growth of the UK
economy over the period 2000 to 2019 can be described in terms of three
distinct phases: 1) 2000 to 2007 - strong sustained economic growth 2) 2008 to
2013 - the impact of the international financial crisis, its immediate
aftermath, and period of recovery 3) 2014 to 2019 - weak sustained economic
growth The key determinant of the UK economic performance over this period
would appear to the annual rate of growth in labour productivity. It was
closely related to the annual rate of growth in GDP per capita, and it was
significantly weaker in the period 2014 to 2019 compared to the period 2000 to
2007. This also corresponded with a weaker rate of growth in annual average
real wages over the period 2014 to 2019 compared to the period 2000 to 2007.
Throughout the period 2000 to 2019, UK CPI was maintained, on average, at
approximately 2.1% per annum. More rapid UK economic growth would be expected
to be achieved by sustained investment in measures that enhance labour
productivity, with the further expectation that a sustained improvement in
labour productivity would increase the annual rate of growth of UK GDP per
capita and average real wages. While the results given in this paper are
specific to the UK over the time period 2000 to 2019, the expectation is that
the methodology and approach adopted can be applied to quantifying the dynamics
of any developed economy over any time period.

Interactions between power and gas systems, which are both large and complex,
have been gradually intensified during the last decades, predominantly due to
the propagation of large fleet natural gas-fired power units (GPUs) and the
technological developments of power-to-gas (P2G) facilities. These interactions
not only bring significant economic benefits to society but also provide
additional operating flexibilities, which are essential to handle fluctuations
of the large-scale renewable power generation (RPG) and power system
contingencies. Moreover, neglecting these interactions in power system
operation may not only result in infeasible operation status in the gas systems
but also increase the decision-making operation costs of both systems. previous
studies suffered from two significant drawbacks, namely (1) they assumed the
existence of only one utility that has full control authority over the power
system and gas system; (2) the economic interactions between power systems and
gas systems have been neglected, which goes against the current industrial
practice. This research revisits the day-ahead resilient and economic
operations of power systems considering the economic and physical interactions
with gas systems, which are characterized by the modeling of bilateral energy
purchase contracts and operational constraints of gas systems. This thesis
provides a novel perspective and solution for the resilient and economically
coordinated robust operation of integrated electric-gas systems (IEGSs) under
uncertainties. The proposed robust scheduling decision frameworks are
practically compatible with the existing industrial operations of the IEGSs.

The economic shocks that followed the COVID-19 pandemic have brought to light
the difficulty, both for academics and policy makers, of describing and
predicting the dynamics of inflation. This paper offers an alternative
modelling approach. We study the 2020-2023 period within the well-studied
Mark-0 Agent-Based Model, in which economic agents act and react according to
plausible behavioural rules. We include a mechanism through which trust of
economic agents in the Central Bank can de-anchor. We investigate the influence
of regulatory policies on inflationary dynamics resulting from three exogenous
shocks, calibrated on those that followed the COVID-19 pandemic: a
production/consumption shock due to COVID-related lockdowns, a supply-chain
shock, and an energy price shock exacerbated by the Russian invasion of
Ukraine. By exploring the impact of these shocks under different assumptions
about monetary policy efficacy and transmission channels, we review various
explanations for the resurgence of inflation in the United States, including
demand-pull, cost-push, and profit-driven factors. Our main results are
four-fold: (i) without appropriate fiscal policy, the shocked economy can take
years to recover, or even tip over into a deep recession; {(ii) the success of
monetary policy in curbing inflation is primarily due to expectation anchoring,
rather than to the direct economic impact of interest rate hikes; (iii)
however, strong inflation anchoring is detrimental to consumption and
unemployment, leading to a narrow window of ``optimal'' policy responses due to
the trade-off between inflation and unemployment;} (iv) the two most sensitive
model parameters are those describing wage and price indexation. The results of
our study have implications for Central Bank decision-making, and offers an
easy-to-use tool that may help anticipate the consequences of different
monetary and fiscal policies.

Recent years have shown a rapid adoption of residential solar PV with
increased self-consumption and self-sufficiency levels in Europe. A major
driver for their economic viability is the electricity tax exemption for the
consumption of self-produced electricity. This leads to large residential PV
capacities and partially overburdened distribution grids. Furthermore, the tax
exemption that benefits wealthy households that can afford capital-intense
investments in solar panels in particular has sparked discussions about energy
equity and the appropriate taxation level for self-consumption. This study
investigates the implementation of uniform electricity taxes on all
consumption, irrespective of the origin of the production, by means of a case
study of 155,000 hypothetical Danish prosumers. The results show that the new
taxation policy redistributes costs progressively across household sizes. As
more consumption is taxed, the tax level can be reduced by 38%, leading to 61%
of all households seeing net savings of up to 23% off their yearly tax bill.
High-occupancy houses save an average of 116 Euro per year at the expense of
single households living in large dwellings who pay 55 Euro per year more.
Implementing a uniform electricity tax in combination with a reduced overall
tax level can (a) maintain overall tax revenues and (b) increase the
interaction of batteries with the grid at the expense of behind-the-meter
operations. In the end, the implicit cross-subsidy is removed by taxing
self-consumption uniformly, leading to a cost redistribution supporting
occupant-dense households and encouraging the flexible behavior of prosumers.
This policy measure improves economic efficiency and greater use of technology
with positive system-wide impacts.

Classical economics has developed an arsenal of methods, based on the idea of
representative agents, to come up with precise numbers for next year's GDP,
inflation and exchange rates, among (many) other things. Few, however, will
disagree with the fact that the economy is a complex system, with a large
number of strongly heterogeneous, interacting units of different types (firms,
banks, households, public institutions) and different sizes.
  Now, the main issue in economics is precisely the emergent organization,
cooperation and coordination of such a motley crowd of micro-units. Treating
them as a unique ``representative'' firm or household clearly risks throwing
the baby with the bathwater. As we have learnt from statistical physics,
understanding and characterizing such emergent properties can be difficult.
Because of feedback loops of different signs, heterogeneities and
non-linearities, the macro-properties are often hard to anticipate. In
particular, these situations generically lead to a very large number of
possible equilibria, or even the lack thereof.
  Spin-glasses and other disordered systems give a concrete example of such
difficulties. In order to tackle these complex situations, new theoretical and
numerical tools have been invented in the last 50 years, including of course
the replica method and replica symmetry breaking, and the cavity method, both
static and dynamic. In this chapter we review the application of such ideas and
methods in economics and social sciences. Of particular interest are the
proliferation (and fragility) of equilibria, the analogue of satisfiability
phase transitions in games and random economies, and condensation (or
concentration) effects in opinion, wealth, etc

We are developing an economic model to explore multiple topics in Australian
youth mental health policy. We want that model to be readily transferable to
other jurisdictions. We developed a software framework for authoring
transparent, reusable and updatable Computational Health Economic Models
(CHEMs) (the software files that implement health economic models). We
specified framework user requirements of a template CHEM module that
facilitates modular model implementations, a simple programming syntax and
tools for authoring new CHEM modules, supplying CHEMs with data, reporting
reproducible CHEM analyses, searching for CHEM modules and maintaining a CHEM
project website. We implemented the framework as six development version code
libraries in the programming language R that integrate with online services for
software development and research data archiving. We used the framework to
author five development version R libraries of CHEM modules focused on utility
mapping in youth mental health. These modules provide tools for variable
validation, dataset description, multi-attribute instrument scoring,
construction of mapping models, reporting of mapping studies and making out of
sample predictions. We assessed these CHEM module libraries as mostly meeting
transparency, reusability and updatability criteria that we have previously
developed, but requiring more detailed documentation and unit testing of
individual modules. Our software framework has potential value as a prototype
for future tools to support the development of transferable CHEMs.

The global surge in AI applications is transforming industries, leading to
displacement and complementation of existing jobs, while also giving rise to
new employment opportunities. Human oversight of AI is an emerging task in
which human workers interact with an AI model to improve its performance,
safety, and compliance with normative principles. Data annotation, encompassing
the labelling of images or annotating of texts, serves as a critical human
oversight process, as the quality of a dataset directly influences the quality
of AI models trained on it. Therefore, the efficiency of human oversight work
stands as an important competitive advantage for AI developers. This paper
delves into the foundational economics of human oversight, with a specific
focus on the impact of norm design and monetary incentives on data quality and
costs. An experimental study involving 307 data annotators examines six groups
with varying task instructions (norms) and monetary incentives. Results reveal
that annotators provided with clear rules exhibit higher accuracy rates,
outperforming those with vague standards by 14%. Similarly, annotators
receiving an additional monetary incentive perform significantly better, with
the highest accuracy rate recorded in the group working with both clear rules
and incentives (87.5% accuracy). However, both groups require more time to
complete tasks, with a 31% increase in average task completion time compared to
those working with standards and no incentives. These empirical findings
underscore the trade-off between data quality and efficiency in data curation,
shedding light on the nuanced impact of norm design and incentives on the
economics of AI development. The paper contributes experimental insights to
discussions on the economical, ethical, and legal considerations of AI
technologies.

We consider an ecological system governed by Lotka-Volterra dynamics and an
example of an economic system as a mesomarket with perfect competition. We
propose a mechanism for cooperative self-regulation that enables the system
under consideration to respond properly to changes in the environment. This
mechanism is based on (1) active individual behavior of the system elements at
each hierarchical level and (2) self-processing of information caused by the
hierarchical organization. It is shown how the proposed mechanism suppresses
nonlocal interaction of elements belonging to a particular level as mediated by
higher levels.

I find a topological arrangement of stocks traded in a financial market which
has associated a meaningful economic taxonomy. The topological space is a graph
connecting the stocks of the portfolio analyzed. The graph is obtained starting
from the matrix of correlation coefficient computed between all pairs of stocks
of the portfolio by considering the synchronous time evolution of the
difference of the logarithm of daily stock price. The hierarchical tree of the
subdominant ultrametric space associated with the graph provides information
useful to investigate the number and nature of the common economic factors
affecting the time evolution of logarithm of price of well defined groups of
stocks.

We propose a general scenario to analyze social and economic changes in
modern environments. We illustrate the ideas with a model that incorporating
the main trends is simple enough to extract analytical results and, at the same
time, sufficiently complex to display a rich dynamic behavior. Our study shows
that there exists a macroscopic observable that is maximized in a regime where
the system is critical, in the sense that the distribution of events follow
power-laws. Computer simulations show that, in addition, the system always
self-organizes to achieve the optimal performance in the stationary state.

Using a metric related to the returns correlation, a method is proposed to
reconstruct an economic space from the market data. A reduced subspace,
associated to the systematic structure of the market, is identified and its
dimension related to the number of terms in factor models. Example were worked
out involving sets of companies from the DJIA and S&P500 indexes. Having a
metric defined in the space of companies, network topology coefficients may be
used to extract further information from the data. A notion of "continuous
clustering" is defined and empirically related to the occurrence of market
shocks.

The risk of a credit portfolio depends crucially on correlations between the
probability of default (PD) in different economic sectors. Often, PD
correlations have to be estimated from relatively short time series of default
rates, and the resulting estimation error hinders the detection of a signal. We
present statistical evidence that PD correlations are well described by a
(one-)factorial model. We suggest a method of parameter estimation which avoids
in a controlled way the underestimation of correlation risk. Empirical evidence
is presented that, in the framework of the CreditRisk+ model with integrated
correlations, this method leads to an increased reliability of the economic
capital estimate.

The American economy can be thought of as a highly connected random network
in terms of both its technological and informational connections. The
cumulative size of economic recessions, the fall in output from peak to trough,
is analysed for the US economy 1900-2002. A least squares fit of an exponential
relationship between size and the rank of size gives a good description of most
of the data. But the observation for the Great Depression of the 1930s stands
out as a very distinct outlier. In other words, we observe a bimodal
relationship of the type predicted by theory.

In the present work we extend the Bak-Sneppen model for biological evolution
by introducing local interactions between species. This ``environmental''
perturbation can modify the intrinsic fitness of each element of the ecology,
leading to higher survival probability, even for the less fit. While the system
still self-organizes toward a critical state, the distribution of fitness
broadens, losing the classical step-function shape. A possible application in
economics is discussed, where firms are represented like evolving species
linked by mutual interests.

In the past few years, the discoveries of small-world and scale-free
properties of many natural and artificial complex networks have stimulated
significant advances in better understanding the relationship between the
topology and the collective dynamics of complex networks. This paper reports
recent progresses in the literature of synchronization of complex dynamical
networks including stability criteria, network synchronizability and uniform
synchronous criticality in different topologies, and the connection between
control and synchronization of complex networks as well. The economic-cycle
synchronous phenomenon in the World Trade Web, a scale-free type of social
economic networks, is used to illustrate an application of the network
synchronization mechanism.

Any economic agent constituting the monetary economy maintains the activity
of monetary flow equilibration for fulfilling the condition of monetary flow
continuity in the record, except at the central bank. At the same time,
monetary flow equilibration at one economic agent constantly induces at other
agents in the economy further flow disequilibrium to be eliminated
subsequently. We propose the rate of monetary flow disequilibration as a figure
measuring the progressive movement of the economy. The rate of disequilibration
was read out of both the Japanese and the United States monetary economy
recorded over the last fifty years.

After the 1991 economic policy made a shift from a closed economic model to a
market-oriented model. The government invited private sector to participate in
reforming its telecom sector. However, the government took a half-hearted
approach in overhauling the legal and regulatory regime, suitable for
competitive regime or in framing the 1994 Telecom Policy.
  Competition was allowed in cellular and basis services. The ministry and the
incumbent (DOT) issued licenses to their competitors. Lack of transparency in
issuing licenses and unrealistic license fee derailed the reforms process and
led to wasteful litigation. The courts did not support the regulator and
virtually made its role redundant.

This research investigates the relationship between telecommunications
infrastructure, economic conditions, and federal and state policies and
initiatives. It presents a detailed look at the telecommunications environment
of the Appalachian region, particularly focusing on broadband technologies. A
strong, positive association exists between telecommunications infrastructure
and economic status. The effects of federal and state universal service
policies are examined, as well as some of the ways states have leveraged their
own infrastructure to improve telecommunications capabilities in their region.
Other state and local telecommunications-related programs are noted.

Computing economics are changing. Today there is rough price parity between
(1) one database access, (2) ten bytes of network traffic, (3) 100,000
instructions, (4) 10 bytes of disk storage, and (5) a megabyte of disk
bandwidth. This has implications for how one structures Internet-scale
distributed computing: one puts computing as close to the data as possible in
order to avoid expensive network traffic.

Existing attempts at utility computing revolve around two approaches. The
first consists of proprietary solutions involving renting time on dedicated
utility computing machines. The second requires the use of heavy, monolithic
applications that are difficult to deploy, maintain, and use.
  We propose a distributed, community-oriented approach to utility computing.
Our approach provides an infrastructure built on Web Services in which modular
components are combined to create a seemingly simple, yet powerful system. The
community-oriented nature generates an economic environment which results in
fair transactions between consumers and providers of computing cycles while
simultaneously encouraging improvements in the infrastructure of the
computational grid itself.

There has been a remarkable increase in work at the interface of computer
science and game theory in the past decade. In this article I survey some of
the main themes of work in the area, with a focus on the work in computer
science. Given the length constraints, I make no attempt at being
comprehensive, especially since other surveys are also available, and a
comprehensive survey book will appear shortly.

Wonderland, a compact, integrated economic, demographic and environmental
model is investigated using methods developed for studying critical phenomena.
Simulation results show the parameter space separates into two phases, one of
which contains the property of long term, sustainable development. By employing
information contain in the phase diagram, an optimal strategy involving
pollution taxes is developed as a means of moving a system initially in a
unsustainable region of the phase diagram into a region of sustainability while
ensuring minimal regret with respect to long term economic growth.

In this paper, we investigate the economic mobility in some money transfer
models which have been applied into the research on wealth distribution. We
demonstrate the mobility by recording the time series of agents' ranks and
observing their volatility. We also compare the mobility quantitatively by
employing an index, "the per capita aggregate change in log-income", raised by
economists. Like the shape of distribution, the character of mobility is also
decided by the trading rule in these transfer models. It is worth noting that
even though different models have the same type of distribution, their mobility
characters may be quite different.

Gambling is one of the basic economic activities that humans indulge in. An
investigation of gambling activities provides deep insights into the economic
actions of people and sheds lights on the study of econophysics. In this paper
we present an analysis of the distribution of the final odds of the races
organized by the Japan Racing Association. The distribution of the final odds
$P_o(x)$ indicates a clear power law $P_o(x)\propto 1/x$, where $x$ represents
the final odds. This power law can be explained on the basis of the assumption
that that every bettor bets his money on the horse that appears to be the
strongest in a race.

This article is a response to the recent Worrying Trends in Econophysics
critique written by four respected theoretical economists. Two of the four have
written books and papers that provide very useful critical analyses of the
shortcomings of the standard textbook economic model, neo-classical economic
theory and have even endorsed my book. Largely, their new paper reflects
criticism that I have long made and that our group as a whole has more recently
made. But I differ with the authors on some of their criticism, and partly with
their proposed remedy.

We review some statistical many-agent models of economic and social systems
inspired by microscopic molecular models and discuss their stochastic
interpretation. We apply these models to wealth exchange in economics and study
how the relaxation process depends on the parameters of the system, in
particular on the saving propensities that define and diversify the agent
profiles.

The steady-state and nonequilibrium properties of the model of
environmental-economic interactions are studied. The interacting heterogeneous
agents are simulated on the platform of the emission dynamics of cellular
automaton. The model possess the discontinuous transition between the safe and
catastrophic ecology. Right at the critical line, the broad-scale power-law
distributions of emission rates have been identified. Their relationship to
Zipf's law and models of self-organized criticality is discussed.

A simple model for simulating tug of war game as varying the player number in
a team is discussed to identify the slow pace of fast change. This model shows
that a large number of information sources leads slow change for the system.
Also, we introduce an opinion diffusion model including the effect of a high
degree of clustering. This model shows that the de facto standard and lock-in
effect, well-known phenomena in economics and business management, can be
explained by the network clusters.

We derive the optimal N to M phase-covariant quantum cloning for equatorial
states in dimension d with M=kd+N, k integer. The cloning maps are optimal for
both global and single-qudit fidelity. The map is achieved by an ``economical''
cloning machine, which works without ancilla.

We describe a unified framework of phase covariant multi user quantum
transformations for d-dimensional quantum systems. We derive the optimal phase
covariant cloning and transposition tranformations for multi phase states. We
show that for some particular relations between the input and output number of
copies they correspond to economical tranformations, which can be achieved
without the need of auxiliary systems. We prove a relation between the optimal
phase covariant cloning and transposition maps, and optimal estimation of
multiple phases for equatorial states.

The research area "Virtual Manufacturing (VM)'' is the use of information
technology and computer simulation to model real world manufacturing processes
for the purpose of analysing and understanding them. As automation technologies
such as CAD/CAM have substantially shortened the time required to design
products, Virtual Manufacturing will have a similar effect on the manufacturing
phase thanks to the modelling, simulation and optimisation of the product and
the processes involved in its fabrication. After a description of Virtual
Manufacturing (definitions and scope), we present some socio-economic factors
of VM and finaly some "hot topics'' for the future are proposed.

The aim of this paper is to construct a natural Riemann-Lagrange differential
geometry on 1-jet spaces, in the sense of nonlinear connections, generalized
Cartan connections, d-torsions, d-curvatures, jet electromagnetic fields and
jet Yang-Mills energies, starting from some given non-linear evolution DEs
systems modelling economic phenomena, like the Kaldor model of the bussines
cycle or the Tobin-Benhabib-Miyao model regarding the role of money on economic
growth.

Besides an indicator of the GDP, the Central Bank of Venezuela generates the
so called Monthly Economic Activity General Indicator. The a priori knowledge
of this indicator, which represents and sometimes even anticipates the
economy's fluctuations, could be helpful in developing public policies and in
investment decision making. The purpose of this study is forecasting the IGAEM
through non parametric methods, an approach that has proven effective in a wide
variety of problems in economics and finance.

This paper intends to explain Venezuela's country spread behavior through the
Neural Networks analysis of a monthly economic activity general index of
economic indicators constructed by the Central Bank of Venezuela, a measure of
the shocks affecting country risk of emerging markets and the U.S. short term
interest rate. The use of non parametric methods allowed the finding of non
linear relationship between these inputs and the country risk. The networks
performance was evaluated using the method of excess predictability.

We analyze the effect of flavor changing neutral currents within 331 models.
In particular, we concentrate in the so-called "economical" models, which have
a minimal scalar sector. Taking into account the experimental measurements of
observables related to neutral K and B meson mixing, we study the resulting
bounds for angles and phases in the mixing matrix for the down quark sector,
and the mass and mixing parameters related to the new Z' gauge boson.

A new approach to stochastic integration is described, which is based on an
a.s. pathwise approximation of the integrator by simple, symmetric random
walks. Hopefully, this method is didactically more advantageous, more
transparent, and technically less demanding than other existing ones. In a
large part of the theory one has a.s. uniform convergence on compacts. In
particular, it gives a.s. convergence for the stochastic integral of a finite
variation function of the integrator, which is not c\`adl\`ag in general.

We show that a simple model of a spatially resolved evolving economic system,
which has a steady state under simultaneous updating, shows stable oscillations
in price when updated asynchronously. The oscillations arise from a gradual
decline of the mean price due to competition among sellers competing for the
same resource. This lowers profitability and hence population but is followed
by a sharp rise as speculative sellers invade the large un-inhabited areas.
This cycle then begins again.

Econophysics embodies the recent upsurge of interest by physicists into
financial economics, driven by the availability of large amount of data, job
shortage in physics and the possibility of applying many-body techniques
developed in statistical and theoretical physics to the understanding of the
self-organizing economy. This brief historical survey emphasizes that
Econophysics has many historical precursors, and is in fact rooted in a
continuous cross-fertilization between economics and physics that has been
active in the last centuries.

Many economic models include random shocks imposed on a large number
(continuum) of economic agents with individual risk. In this context, an exact
law of large numbers and its converse is presented in Sun (2006) to
characterize the cancelation of individual risk via aggregation. However, it is
well known that the Lebesgue unit interval is not suitable for modeling a
continuum of agents in the particular setting. The purpose of this paper is to
show that an extension of the Lebesgue unit interval does work well as an agent
space with various desirable properties associated with individual risk.

Fermion superpartners - neutralinos and charginos in the supersymmetric
economical 3-3-1 model are studied. By imposition R parity, their masses and
eigenstates are derived.
  Assuming that Bino-like is dark matter, its mass density is calculated. The
cosmological dark matter density gives a bound on mass of LSP neutralino in the
range of 20 - 100 GeV, while the bound on mass of the lightest slepton is in
the range of 60 - 130 GeV

We propose a fast optically induced two-qubit \textsc{c-phase} gate between
two resident spins in a pair of coupled quantum dots. An excited bound state
which extends over the two dots provides an effective electron-electron
exchange interaction. The gate is made possible by the electron-hole exchange
interaction, which isolates a single transition in the system. When combined
with appropriate single qubit rotations, this gate generates an entangled state
of the two spins.

We revisit a recently introduced agent model[ACS {\bf 11}, 99 (2008)], where
economic growth is a consequence of education (human capital formation) and
innovation, and investigate the influence of the agents' social network, both
on an agent's decision to pursue education and on the output of new ideas.
Regular and random networks are considered. The results are compared with the
predictions of a mean field (representative agent) model.

Labor productivity in developed countries is analyzed and modeled. Modeling
is based on our previous finding that the rate of labor force participation is
a unique function of GDP per capita. Therefore, labor productivity is fully
determined by the rate of economic growth, and thus, is a secondary economic
variable. Initially, we assess a model for the U.S. and then test it using data
for Japan, France, the UK, Italy, and Canada. Results obtained for these
countries validate those for the U.S. The evolution of labor force productivity
is predictable at least at an 11-year horizon

We prove a necessary optimality condition for isoperimetric problems under
nabla-differentiable curves. As a consequence, the recent results of [M.R.
Caputo, A unified view of ostensibly disparate isoperimetric variational
problems, Appl. Math. Lett. (2008), doi:10.1016/j.aml.2008.04.004], that put
together seemingly dissimilar optimal control problems in economics and
physics, are extended to a generic time scale. We end with an illustrative
example of application of our main result to a dynamic optimization problem
from economics.

We present several new characterizations of correlated equilibria in games
with continuous utility functions. These have the advantage of being more
computationally and analytically tractable than the standard definition in
terms of departure functions. We use these characterizations to construct
effective algorithms for approximating a single correlated equilibrium or the
entire set of correlated equilibria of a game with polynomial utility
functions.

This paper presents a novel study on gas-like models for economic systems.
The interacting agents and the amount of exchanged money at each trade are
selected with different levels of randomness, from a purely random way to a
more chaotic one. Depending on the interaction rules, these statistical models
can present different asymptotic distributions of money in a community of
individuals with a closed economy.

In the given work the first attempt to generalize quantum uncertainty
relation on macro objects is made. Business company as one of economical
process participants was chosen by the authors for this purpose. The analogies
between quantum micro objects and the structures which from the first sight do
not have anything in common with physics are given. The proof of generalized
uncertainty relation is produced. With the help of generalized uncertainty
relation the authors wanted to elaborate a new non-traditional approach to the
description of companies' business activity and their developing and try to
formulate some advice for them. Thus, our work makes the base of quantum theory
of econimics

I use the recently proposed framework of ontological models [Harrigan et al.,
arXiv:0709.1149v2] to obtain economical models for results of tomographically
complete sets of measurements on finite-dimensional quantum systems. I describe
a procedure that simplifies the models by decreasing the number of necessary
ontic states, and present an explicit model with just 33 ontic states for a
qutrit.

The main goal of the present work is to analyze the activity of the French
companies from Romania related to the increasing use of the information and
communication technology in the productive field. The convergent assembly of
information and communication technologies and the process of economic
globalization have led to a profound transformation of the economic activity.
The present paper is part of a series of studies made on the French firms from
Romania between 2007 and 2008.

The correctness of Harrods model in the differential form is studied. The
inadequacy of exponential growth of economy is shown; an alternative result is
obtained. By example of Phillips model, an approach to correction of
macroeconomic models (in terms of initial prerequisites) is generalized. A
methodology based on balance relations for modelling of economic dynamics,
including obtaining forecast estimates, is developed. The problems thus
considered are reduced to the solution of Volterra and Fredholm integral
equations of the second kind.

In this paper we investigate a stochastic model for an economic game. To
describe this model we have used a Wiener process, as the noise has a
stabilization effect. The dynamics are studied in terms of stochastic stability
in the stationary state, by constructing the Lyapunov exponent, depending on
the parameters that describe the model. Also, the Lyapunov function is
determined in order to analyze the mean square stability. The numerical
simulation that we did justifies the theoretical results.

In this introduction the editors showcase the papers by way of a structured
project and seek to clarify the two key concepts cited in the title. We
consider the history of the idea that knowledge is an economic factor, and
discuss the question of whether regions provide the relevant system of
reference for knowledge-based economic development. Current transformations in
university-industry-government relations at various levels can be considered as
a metamorphosis in industry organization. The concept of constructed advantage
will be elaborated. The various papers arising from a conference on this
subject hosted by Memorial University, Newfoundland, Canada are approached from
this perspective.

In this paper, we present the principal components of an economic scenario
generator (ESG), both for the theoretical design and for practical
implementation. The choice of these components should be linked to the ultimate
vocation of the economic scenario generator, which can be either a tool for
pricing financial products or a tool for projection and risk management. We
then develop a study on some performance measure indicators of the ESG as an
input for the decision-making process, namely the indicators of stability and
bias absence. Finally, a numerical application illustrates the main ideas of
the paper.

In this paper we present several novel efficient techniques and
multidimensional data structures which can improve the decision making process
in many domains. We consider online range aggregation, range selection and
range weighted median queries; for most of them, the presented data structures
and techniques can provide answers in polylogarithmic time. The presented
results have applications in many business and economic scenarios, some of
which are described in detail in the paper.

In composite models with colored preons leptogluons (l_(8)) has a same status
with leptoquarks, excited leptons and quarks etc. We analyze resonant
production of color octet electron (e_(8)) at QCD Explorer stage of the Large
Hadron electron Collider (LHeC). It is shown that the e_(8) discovery at the
LHeC will simultaneously determine the compositeness scale.

The concept of absence of opportunities for free lunches is one of the
pillars in the economic theory of financial markets. This natural assumption
has proved very fruitful and has lead to great mathematical, as well as
economical, insights in Quantitative Finance. Formulating rigorously the exact
definition of absence of opportunities for riskless profit turned out to be a
highly non-trivial fact that troubled mathematicians and economists for at
least two decades. The purpose of this note is to give a quick (and,
necessarily, incomplete) account of the recent work aimed at providing a simple
and intuitive no-free-lunch assumption that would suffice in formulating a
version of the celebrated Fundamental Theorem of Asset Pricing.

The correctness of Harrods model in the differential form is studied. The
inadequacy of exponential growth of economy is shown; an alternative result is
obtained. By example of Phillips model, an approach to correction of
macroeconomic models (in terms of initial prerequisites) is generalized. A
methodology based on balance relations for modelling of economic dynamics,
including obtaining forecast estimates, is developed. The problems thus
considered are reduced to the solution of Volterra and Fredholm integral
equations of the second kind.

The goal of this article is to understand some interesting features of
sequences of arbitrage operations, which look relevant to various processes in
Economics and Finances. In the second part of the paper, analysis of sequences
of arbitrages is reformulated in the linear algebra terms. This admits an
elegant geometric interpretation of the problems under consideration linked to
the asynchronous systems theory. We feel that this interpretation will be
useful in understanding more complicated, and more realistic, mathematical
models in economics.

The main points of the first section of the article written by S.I.
Chernyshov, A.V. Voronin and S.A. Razumovsky arXiv:1003.4382), which deals with
the fundamental bases of the macroeconomic theory, have been analyzed. An
incorrectness of the Harrod's model of the economical growth in its generally
accepted interpretation was specifically considered. The inevitability of the
economic crisis has been shown to follow directly from the premises of this
model. At the same time there is an opportunity to realize the damping
strategies.

We present an efficient and economic scheme for five-party quantum state
sharing of an arbitrary m-qubit state with $2m$ three-particle
Greenberger-Horne-Zeilinger (GHZ) states and three-particle GHZ-state
measurements. It is more convenient than other schemes as it only resorts to
three-particle GHZ states and three-particle joint measurement, not
five-particle entanglements and five-particle joint measurements. Moreover,
this symmetric scheme is in principle secure even though the number of the
dishonest agents is more than one. Its total efficiency approaches the maximal
value.

In this work, the time chart of Dow Jones Industrial Average (DJIA) index is
analyzed and approach of recession time term is predicted, which may be
hallmark of a worldwide economic crisis. However, the methods used for the
prediction will be disclosed a few years from now. On the other hand, this work
will be updated by the author frequently once in every few months where no
revisions will be made on the previous uploads and a timestamp will designate
each part. Thus, the time evolution of the crisis can be followed and the
prediction may be verified by the readers in time.

Existence of the fourth family follows from the basics of the Standard Model
and the actual mass spectrum of the third family fermions. We discuss possible
manifestations of the fourth SM family at existing and future colliders. The
LHC and Tevatron potentials to discover the fourth SM family have been
compared. The scenario with dominance of the anomalous decay modes of the
fourth family quarks has been considered in details.

Experimental economics has repeatedly demonstrated that the Nash equilibrium
makes inaccurate predictions for a vast set of games. Instead, several
alternative theoretical concepts predict behavior that is much more in tune
with observed data, with the quantal response equilibrium as the most prominent
example. However, here we show that this equilibrium notion itself, like any
other concept that varies smoothly with the payoffs, is necessarily subject to
framing effects: If the same economic problem is represented in a different but
equivalent way, the predicted results will differ. As a consequence, we argue
that tools and methods that are successful in explaining human behavior in
laboratory experiments may be unsuitable for doing theory.

An interesting toy model has recently been proposed on Schumpeterian economic
dynamics by Thurner {\it et al.} following the idea of economist Joseph
Schumpeter. Punctuated equilibrium dynamics is shown to emerge from this model
and some detail analyses of the time series indicate SOC kind of behaviours.
The focus in the present work is to toss the idea whether the dynamics can
really be like a self organized critical (SOC) type. This study indicates that
it is necessary to incorporate the concepts of 'fitness' and 'selection' in
such a model in the line of the biological evolutionary model by Bak and
Sneppen in order to obtain power law and thus SOC behaviour.

In this paper are made some considerations of the application of
phenomenological thermodynamics in risk analysis for the transaction on
financial markets, using the concept of economic entropy and the macrostate
parameter introduced by us in a previous works [15,16]. The investment risk
diagrams for a number of Romanian listed companies in 2008 and 2009 years were
calculed. Also, the evolution of the macrostate parameter during financial and
economic crisis in Romania are studied.

We have reconsidered the Higgs boson search via the "golden mode" for
Tevatron. It is shown that this mode will give opportunity to observe the Higgs
boson with mass up to 300 GeV before Tevatron shutdown.

Simulation serves as a third way of doing science, in contrast to both
induction and deduction. The web based modeling may considerably facilitate the
execution of simulations by other people. We present examples of agent-based
and stochastic models of competition and business processes in economics. We
start from as simple as possible models, which have microscopic, agent-based,
versions and macroscopic treatment in behavior. Microscopic and macroscopic
versions of herding model proposed by Kirman and Bass diffusion of new products
are considered in this contribution as two basic ideas.

This paper combines ideas from classical economics and modern finance with
the general Lotka-Volterra models of Levy & Solomon to provide straightforward
explanations of wealth and income distributions. Using a simple and realistic
economic formulation, the distributions of both wealth and income are fully
explained. Both the power tail and the log-normal like body are fully captured.
It is of note that the full distribution, including the power law tail, is
created via the use of absolutely identical agents. It is further demonstrated
that a simple scheme of compulsory saving could eliminate poverty at little
cost to the taxpayer.

In this paper, we explore what \emph{network economics} is all about,
focusing on the interesting topics brought about by the Internet. Our intent is
make this a brief survey, useful as an outline for a course on this topic, with
an extended list of references. We try to make it as intuitive and readable as
possible. We also deliberately try to be critical at times, and hope our
interpretation of the topic will lead to interests for further discussions by
those doing research in the same field.

We provide a detailed characterization of the optimal consumption stream for
the additive habit-forming utility maximization problem, in a framework of
general discrete-time incomplete markets and random endowments. This
characterization allows us to derive the monotonicity and concavity of the
optimal consumption as a function of wealth, for several important classes of
incomplete markets and preferences. These results yield a deeper understanding
of the fine structure of the optimal consumption and provide a further
theoretical support for the classical conjectures of Keynes (1936).

We consider a Markovian queue subject to Poisson generated catastrophes.
Whenever a catastrophe occurs, all customers are forced to abandon the system,
the server is rendered inoperative and an exponential repair time is set on. We
assume that the arriving customers decide whether to join the system or balk,
based on a natural reward-cost structure. We study the balking behavior of the
customers and derive the corresponding Nash equilibrium strategies.

The existence of optimal strategy in robust utility maximization is addressed
when the utility function is finite on the entire real line. A delicate problem
in this case is to find a "good definition" of admissible strategies, so that
an optimizer is obtained. Under suitable assumptions, especially a
time-consistency property of the set of probabilities which describes the model
uncertainty, we show that an optimal strategy is obtained in the class of
strategies whose wealths are supermartingales under all local martingale
measures having a finite generalized entropy with at least one of candidate
models (probabilities).

We show that the economical 3-3-1 model has a dark mater candidate. It is a
real scalar H_1^0 in which main part is bilepton (with lepton number 2) and its
mass is in the range of some TeVs. We calculate the relic abundance of H_1^0
dark matter by using micrOMEGAs 2.4 and figure out parameter space satisfying
the WMAP constraints. Direct and indirect searches are studied for a special
choice of parameters in the WMAP - allowed region.

The aim of this paper is to analyze the processes of polarization and
agglomeration, to explain the mechanisms and causes of these phenomena in order
to identify similarities and differences. As the main implication of this study
should be noted that both process pretend to explain the concentration of
economic activity and population in certain places, through cumulative
phenomena, but with different perspectives, in other words, the polarization
with a view of economic development and agglomeration with a perspective of
space.

This paper illustrates successful implementation of three evolutionary
algorithms, namely- Particle Swarm Optimization(PSO), Artificial Bee Colony
(ABC) and Bacterial Foraging Optimization (BFO) algorithms to economic load
dispatch problem (ELD). Power output of each generating unit and optimum fuel
cost obtained using all three algorithms have been compared. The results
obtained show that ABC and BFO algorithms converge to optimal fuel cost with
reduced computational time when compared to PSO for the two example problems
considered.

We consider a Markovian clearing queueing system, where the customers are
accumulated according to a Poisson arrival process and the server removes all
present customers at the completion epochs of exponential service cycles. This
system may represent the visits of a transportation facility with unlimited
capacity at a certain station. The system evolves in an alternating environment
that influences the arrival and the service rates. We assume that the arriving
customers decide whether to join the system or balk, based on a natural linear
reward-cost structure. We study the balking behavior of the customers and
derive the corresponding Nash equilibrium strategies under various levels of
information.

In this paper, the basic ideal of the Event Space Theory and Analyzing Events
are expatiated on. Then it is suggested that how to set up event base library
in developing application software. Based above the designing principle of
facing methodology. Finally, in order to explain how to apply the Event Space
Theory in developing economic evaluation software, the software of "sewage
treatment CAD" in a national "8th-Five Year Plan Research Project" of PRC is
used as an example. This software concerns economic effectiveness evaluation
for construction projects.

We propose an Economic - Probabilistic analogy: the category of cost is
analogous to the category of Probability. The proposed analogy permits
construction of an informal theory of nonlinear non-convex Gaussian Utility and
Cost, which describes the real economic processes more adequately than a theory
based on a linear and convex models. Based on the proposed analogy, we build a
nonlinear non-convex planning model with a Gaussian optimality criterion -
Gaussian Programming Model. We also describe a corresponding model of
Generalized Piecewise-Linear Programming that can be used to approximate a
Gaussian Programming model, and vice verse. Proposed constructions are
illustrated on a numerical example.

We develop a model for the dynamic evolution of default-free and defaultable
interest rates in a LIBOR framework. Utilizing the class of affine processes,
this model produces positive LIBOR rates and spreads, while the dynamics are
analytically tractable under defaultable forward measures. This leads to
explicit formulas for CDS spreads, while semi-analytical formulas are derived
for other credit derivatives. Finally, we give an application to counterparty
risk.

Knowing and modelling the migration phenomena and especially the social and
economic consequences have a theoretical and practical importance, being
related to their consequences for development, economic progress (or as
appropriate, regression), environmental influences etc. One of the causes of
migration, especially of the interregional and why not intercontinental, is
that resources are unevenly distributed, and from the human perspective there
are differences in culture, education, mentality, collective aspirations etc.
This study proposes a new econophysics model for the migration phenomena.

We present examples of agent-based and stochastic models of competition and
business processes in economics and finance. We start from as simple as
possible models, which have microscopic, agent-based, versions and macroscopic
treatment in behavior. Microscopic and macroscopic versions of herding model
proposed by Kirman and Bass diffusion of new products are considered in this
contribution as two basic ideas. Further we demonstrate that general herding
behavior can be considered as a background of nonlinear stochastic model of
financial fluctuations.

The notion that economies should normally be in equilibrium is by now
well-established; equally well-established is that economies are almost never
precisely in equilibrium. Using a very general formulation, we show that under
dynamics that are second-order in time a price system can remain away from
equilibrium with permanent and repeating opportunities for arbitrage, even when
a damping term drives the system towards equilibrium. We also argue that
second-order dynamic equations emerge naturally when there are heterogeneous
economic actors, some behaving as active and knowledgeable arbitrageurs, and
others using heuristics. The essential mechanism is that active arbitrageurs
are able to repeatedly benefit from the suboptimal heuristics that govern most
economic behavior.

This paper investigates the impact of the Great Japan Earthquake (and
subsequent tsunami turmoil) on socio-economic activities by using data on hotel
opportunities collected from an electronic hotel booking service. A method to
estimate both primary and secondary regional effects of a natural disaster on
human behavior is proposed. It is confirmed that temporal variation in the
regional share of available hotels before and after a natural disaster may be
an indicator to measure the socio-economic impact at each district.

It is shown that a family of analytically solvable pulses can be used to
obtain high fidelity quantum phase gates with surprising robustness against
imperfections in the system or pulse parameters. Phase gates are important
because they can implement the necessary operations for universal quantum
computing. They are particularly suited for systems such as self-assembled
quantum dots, trapped ions, and defects in solids, as these are typically
manipulated by the transient excitation of a state outside the qubit subspace.

Among the strategic choices made by today's economic actors are choices about
algorithms and computational resources. Different access to computational
resources may result in a kind of economic asymmetry analogous to information
asymmetry. In order to represent strategic computational choices within a game
theoretic framework, we propose a new game specification, Strategic Bayesian
Networks (SBN). In an SBN, random variables are represented as nodes in a
graph, with edges indicating probabilistic dependence. For some nodes, players
can choose conditional probability distributions as a strategic choice. Using
SBN, we present two games that demonstrate computational asymmetry. These games
are symmetric except for the computational limitations of the actors. We show
that the better computationally endowed player receives greater payoff.

When imperfect quality products are produced in a production process, rework
may be performed to make them become serviceable. In an inventory system, items
may deteriorate. Selling deteriorated items to customers will create negative
impact on corporate image. In this paper, two economic production quantity
(EPQ) models are proposed for deteriorating items with rework process. A single
production-rework plant system and a system consists of $n$ production plants
and one rework plant are considered. Approximated analytic results are obtained
and numerical examples are provided to illustrate the solution procedure.

Quasi-equilibrium models for aggregate variables are widely-used throughout
finance and economics. The validity of such models depends crucially upon
assuming that the systems' participants behave both independently and in a
Markovian fashion.
  We present a simplified market model to demonstrate that herding effects
between agents can cause a transition to boom-and-bust dynamics at realistic
parameter values. The model can also be viewed as a novel stochastic particle
system with switching and reinjection.

The "social-networking revolution" of late (e.g., with the advent of social
media, Facebook, and the like) has been propelling the crusade to elucidate the
embedded networks that underlie economic activity. An unexampled synthesis of
network science and economics uncovers how the web of human interactions
spurred by familiarity and similarity could potentially induce the ups and
downs ever so common to our economy. Zeroing in on the million-strong global
industry known as multi-level marketing, this study finds that such a
socially-powered enterprise can only work stably through discrimination about
who to make entrepreneurial connections with.

In this paper, a general model of a pure exchange differential information
economy is studied. In this economic model, the space of states of nature is a
complete probability measure space, the space of agents is a measure space with
a finite measure, and the commodity space is the Euclidean space. Under
appropriate and standard assumptions on agents' characteristics, results on
continuity and measurability of the aggregate preferred correspondence in the
sense of Aumann in [4] are established. These results together with other
techniques are then employed to prove the existence of a maximin rational
expectations equilibrium (maximin REE) of the economic model.

The Lebesgue property (order-continuity) of a monotone convex function on a
solid vector space of measurable functions is characterized in terms of (1) the
weak inf-compactness of the conjugate function on the order-continuous dual
space, (2) the attainment of the supremum in the dual representation by
order-continuous linear functionals. This generalizes and unifies several
recent results obtained in the context of convex risk measures.

Escalation is the fact that in a game (for instance an auction), the agents
play forever. It is not necessary to consider complex examples to establish its
rationality. In particular, the $0,1$-game is an extremely simple infinite game
in which escalation arises naturally and rationally. In some sense, it can be
considered as the paradigm of escalation. Through an example of economic games,
we show the benefit economics can take of coinduction.

In this paper, we pay our attention to geometric parameters and their
applications in economics and finance. We discuss the multiplicative models in
which a geometric mean and a geometric standard deviation are more natural than
arithmetic ones. We give two examples from Warsaw Stock Exchange in 1995--2009
and from a bid of 52-week treasury bills in 1992--2009 in Poland as an
illustrative example. For distributions having applications in finance and
insurance we give their multiplicative parameters as well as their estimations.
We consider, among others, heavy-tailed distributions such as lognormal and
Pareto distribution, applied to modelling of large losses.

This paper studies convex problems of Bolza in the conjugate duality
framework of Rockafellar. We parameterize the problem by a general Borel
measure which has direct economic interpretation in problems of financial
economics. We derive a dual representation for the optimal value function in
terms of continuous dual arcs and we give conditions for the existence of
solutions. Combined with well-known results on problems of Bolza over
absolutely continuous arcs, we obtain optimality conditions in terms of
extended Hamiltonian conditions.

Collective behavior of the complex socio-economic systems is heavily
influenced by the herding, group, behavior of individuals. The importance of
the herding behavior may enable the control of the collective behavior of the
individuals. In this contribution we consider a simple agent-based herding
model modified to include agents with controlled state. We show that in certain
case even the smallest fixed number of the controlled agents might be enough to
control the behavior of a very large system.

The Internet and Social Media change our way of decision-making. We are no
longer the independent decision makers we used to be. Instead, we have become
networked minds, social decision-makers, more than ever before. This has
several fundamental implications. First of all, our economic theories must
change, and second, our economic institutions must be adapted to support the
social decision-maker, the "homo socialis", rather than tailored to the perfect
egoist, known as "homo economicus".

We study the necessity of interaction between individuals for obtaining
approximately efficient allocations. The role of interaction in markets has
received significant attention in economic thinking, e.g. in Hayek's 1945
classic paper.
  We consider this problem in the framework of simultaneous communication
complexity. We analyze the amount of simultaneous communication required for
achieving an approximately efficient allocation. In particular, we consider two
settings: combinatorial auctions with unit demand bidders (bipartite matching)
and combinatorial auctions with subadditive bidders. For both settings we first
show that non-interactive systems have enormous communication costs relative to
interactive ones. On the other hand, we show that limited interaction enables
us to find approximately efficient allocations.

The production function is one of the key concepts of mainstream neoclassical
theories in economics. The study of the shape and properties of the production
possibility frontier is a subject of great interest in economic analysis. In
this respect, Cobb-Douglas and CES production functions with flat graph
hypersurfaces in Euclidean spaces are first studied in [19,20]. Later, more
general studies of production models were given in [4-10, 12, 21]. On the other
hand, from visual-physical experiences, the second and third authors proposed
in [14] to study production models via isotropic geometry. The purpose of this
paper is thus to investigate important production models via isotropic
geometry.

In this paper we present different applications of finite state mean field
games to socio-economic sciences. Examples include paradigm shifts in the
scientific community or the consumer choice behaviour in the free market. The
corresponding finite state mean field game models are hyperbolic systems of
partial differential equations, for which we present and validate different
numerical methods. We illustrate the behaviour of solutions with various
numerical experiments, which show interesting phenomena like shock formation.
Hence we conclude with an investigation of the shock structure in the case of
two-state problems.

This paper analyses the relationship between BitCoin price and supply-demand
fundamentals of BitCoin, global macro-financial indicators and BitCoin
attractiveness for investors. Using daily data for the period 2009-2014 and
applying time-series analytical mechanisms, we find that BitCoin market
fundamentals and BitCoin attractiveness for investors have a significant impact
on BitCoin price. Our estimates do not support previous findings that the
macro-financial developments are driving BitCoin price.

It is assumed that under suitable economic and information-theoretic
conditions, market exchange rates are free from arbitrage. Commodity markets in
which trades occur over a complete graph are shown to be trivial. We therefore
examine the vector space of no-arbitrage exchange rate ensembles over an
arbitrary connected undirected graph. Consideration is given for the minimal
information for determination of an exchange rate ensemble. We conclude with a
topical discussion of exchanges in which our analyses may be relevant,
including the emergent but highly-regulated (and therefore not a complete
graph) market for digital currencies.

The purpose of this paper is to identify a relevant statistical correlation
between rate of default, RD, and loss given default, LGD, in a major Brazilian
financial institution Retail Home Equity exposure rated using the IRB approach,
so that we may find a causal relationship between the two risk parameters.
Therefore, according to Central Bank of Brazil requirements, a methodology is
applied to add conservatism to the estimation of the Loss Given Default
parameter at times of economic decline, reflected as increased rates of
default.

The paper analyzes the socio-economic impacts of dropshipping - a new form of
organization of sales, is rapidly gaining the Russian market. Dropshipping
opens up tremendous prospects not only for ordinary people, but also for the
Russian manufacturers, which if used properly can dropshipping opportunities
with minimal direct access to the world market.

This paper offers a step towards research infrastructure, which makes data
from experimental economics efficiently usable for analysis of web data. We
believe that regularities of human behavior found in experimental data also
emerge in real world web data. A format for data from experiments is suggested,
which enables its publication as open data. Once standardized datasets of
experiments are available on-line, web mining can take advantages from this
data. Further, the questions about the order of causalities arisen from web
data analysis can inspire new experiment setups.

The friendship paradox is revisited by considering both local and global
averages of friends. How the economics of attention affects the recruitment of
friends is examined. Statistical implications of varying individual attentions
are investigated and it is argued that this is one reason why the mean of
friends is higher than the median in social networks. The distribution of
friends skews to the right for two other reasons: (i) the presence of
institutional nodes that increase the mean; and (ii) the dormancy of many of
the nodes. The difference between friends and friends of friends is a measure
of the structural information about the network.

Governments across the globe are facing challenging times to generate more
revenue because of the economic slowdown and to balance their budgets. There is
a growing need to find new ways of revenue generation as spending cuts and
austerity measures dont go well with most sections of the society, especially
during difficult economic times. Internet Today is seen more as a necessary
commodity and nobody can deny the fact that the Internet has improved peoples
life in an unprecedented way than any other technology in the past.The Internet
as a commodity is taxed in many countries, but the Internet usage or the
transactions carried out are something thats not taxed.

Estimating large covariance and precision matrices are fundamental in modern
multivariate analysis. The problems arise from statistical analysis of large
panel economics and finance data. The covariance matrix reveals marginal
correlations between variables, while the precision matrix encodes conditional
correlations between pairs of variables given the remaining variables. In this
paper, we provide a selective review of several recent developments on
estimating large covariance and precision matrices. We focus on two general
approaches: rank based method and factor model based method. Theories and
applications of both approaches are presented. These methods are expected to be
widely applicable to analysis of economic and financial data.

We investigate a pool of international chess title holders born between 1901
and 1943. Using Elo ratings we compute for every player his expected score in a
game with a randomly selected player from the pool. We use this figure as
player's merit. We measure players' fame as the number of Google hits. The
correlation between fame and merit is 0.38. At the same time the correlation
between the logarithm of fame and merit is 0.61. This suggests that fame grows
exponentially with merit.

Is the elasticity of intertemporal substitution (EIS) more or less than one?
This question can be answered by confronting theoretical results of asset
pricing models with investor behaviour during episodes of stock market panic.
If we consider these episodes as periods of high risk aversion, then lower
asset prices are in fact associated with higher risk aversion. However,
according to theoretical models, risky asset price is an increasing function of
the coefficient of risk aversion only if the EIS exceeds unity. It may
therefore be concluded that the EIS must be more than one to reconcile theory
with the observed stock price decline during periods of panic.

We develop a model to study the role of rationality in economics and biology.
The model's agents differ continuously in their ability to make rational
choices. The agents' objective is to ensure their individual survival over time
or, equivalently, to maximize profits. In equilibrium, however, rational agents
who maximize their objective survival probability are, individually and
collectively, eliminated by the forces of competition. Instead of rationality,
there emerges a unique distribution of irrational players who are individually
not fit for the struggle of survival. The selection of irrational players over
rational ones relies on the fact that all rational players coordinate on the
same optimal action, which leaves them collectively undiversified and thus
vulnerable to aggregate risks.

Quality-designed consumer products are easy to recognize. Wouldn't it be
great if the quality of financial products became just as apparent? This paper
is addressed to financial practitioners. It provides an informal introduction
to Quantitative Structuring -- a technology of manufacturing quality financial
products (information derivatives). The presentation is arranged in three
parts: the main text assumes no prior knowledge of the topic; important
detailed discussions are arranged as a set of appendices; finally, a list of
references provides further details including applications beyond product
design: from model risk to economics and statistics.

We study the predictive power of industry-specific economic sentiment
indicators for future macro-economic developments. In addition to the sentiment
of firms towards their own business situation, we study their sentiment with
respect to the banking sector - their main credit providers. The use of
industry-specific sentiment indicators results in a high-dimensional
forecasting problem. To identify the most predictive industries, we present a
bootstrap Granger Causality test based on the Adaptive Lasso. This test is more
powerful than the standard Wald test in such high-dimensional settings.
Forecast accuracy is improved by using only the most predictive industries
rather than all industries.

These lecture notes provide a self-contained introduction to the mathematical
methods required in a Bachelor degree programme in Business, Economics, or
Management. In particular, the topics covered comprise real-valued vector and
matrix algebra, systems of linear algebraic equations, Leontief's stationary
input-output matrix model, linear programming, elementary financial
mathematics, as well as differential and integral calculus of real-valued
functions of one real variable. A special focus is set on applications in
quantitative economical modelling.

Here, we examine a mean-field game (MFG) that models the economic growth of a
population of non-cooperative rational agents. In this MFG, agents are
described by two state variables - the capital and consumer goods they own.
Each agent seeks to maximize their utility by taking into account statistical
data of the total population. The individual actions drive the evolution of the
players, and a market-clearing condition determines the relative price of
capital and consumer goods. We study the existence and uniqueness of optimal
strategies of the agents and develop numerical methods to compute these
strategies and the equilibrium price.

We investigate unification of two systems of identical elements having
different dimensions which may be of interest for both physics and economics.
Characteristic parameters as well as explicit formulae for the temperature (in
economics - capital turnover) and dimension of the united system are obtained
as functions of parameters of the initial systems. Expressions also found for
the entropies of initial and united system. The process of unification is
accompanied by the transfer of particles (money) between the systems and
explicit expression is obtained for the transferred number of particles (size
of the capital). A relation between parameters of the initial systems also
found which defines the regime with zero particle transfer.

This paper presents a dynamic model to study the impact on the economic
outcomes in different societies during the Malthusian Era of individualism
(time spent working alone) and collectivism (complementary time spent working
with others). The model is driven by opposing forces: a greater degree of
collectivism provides a higher safety net for low quality workers but a greater
degree of individualism allows high quality workers to leave larger bequests.
The model suggests that more individualistic societies display smaller
populations, greater per capita income and greater income inequality. Some
(limited) historical evidence is consistent with these predictions.

Taking environmental tax rate as given, is there an optimal allocation of tax
revenues to benefit economic variables? This paper analyzes this issue in an
overlapping-generations model with the pollution-related health damage. It
finds the optimal allocations towards pollution abatement and labor income to
maximize the steady-state lifetime welfare and per-worker output, respectively.
Moreover, a greater shift towards labor income might enhance steady-state
welfare while reducing per-worker output.

This paper studies how international investors' concerns about model
misspecification affect sovereign bond spreads. We develop a general
equilibrium model of sovereign debt with endogenous default wherein investors
fear that the probability model of the underlying state of the borrowing
economy is misspecified. Consequently, investors demand higher returns on their
bond holdings to compensate for the default risk in the context of uncertainty.
In contrast with the existing literature on sovereign default, we match the
bond spreads dynamics observed in the data together with other business cycle
features for Argentina, while preserving the default frequency at historical
low levels.

We show that a steady-state stock-flow consistent macro-economic model can be
represented as a Constraint Satisfaction Problem (CSP).The set of solutions is
a polytope, which volume depends on the constraintsapplied and reveals the
potential fragility of the economic circuit,with no need to study the dynamics.
Several methods to compute the volume are compared, inspired by operations
research methods and theanalysis of metabolic networks, both exact and
approximate.We also introduce a random transaction matrix, and study the
particularcase of linear flows with respect to money stocks.

Data describing historical economic growth are analysed. They demonstrate
convincingly that the takeoffs from stagnation to growth, claimed in the
Unified Growth Theory, never happened. This theory is again contradicted by
data, which were used, but never properly analysed, during its formulation. The
absence of the claimed takeoffs demonstrates also that the postulate of the
differential takeoffs is contradicted by data.

We explore the use of deep learning hierarchical models for problems in
financial prediction and classification. Financial prediction problems -- such
as those presented in designing and pricing securities, constructing
portfolios, and risk management -- often involve large data sets with complex
data interactions that currently are difficult or impossible to specify in a
full economic model. Applying deep learning methods to these problems can
produce more useful results than standard methods in finance. In particular,
deep learning can detect and exploit interactions in the data that are, at
least currently, invisible to any existing financial economic theory.

A production function is a mathematical formalization in economics which
denotes the relations between the output generated by a firm, an industry or an
economy and the inputs that have been used in obtaining it. In this paper, we
study the product production functions of 2 variables in terms of the geometry
of their associated graph surfaces in the isotropic 3-space I^{3}. In
particular, we derive several classification results for the graph surfaces of
product production functions in I^{3} with constant curvature.

We introduce a model for a market based economic system of cyber-risk
valuation to correct fundamental problems of incentives within the information
technology and information processing industries. We assess the makeup of the
current day marketplace, identify incentives, identify economic reasons for
current failings, and explain how a market based risk valuation system could
improve these incentives to form a secure and robust information marketplace
for all consumers by providing visibility into open, consensus based risk
pricing and allowing all parties to make well informed decisions.

Socio-economic inequality is characterized from data using various indices.
The Gini ($g$) index, giving the overall inequality is the most common one,
while the recently introduced Kolkata ($k$) index gives a measure of $1-k$
fraction of population who possess top $k$ fraction of wealth in the society.
Here, we show the relationship between the two indices, using both empirical
data and analytical estimates. The significance of their relationship has been
discussed.

The goal of the paper is to introduce a set of problems which we call mean
field games of timing. We motivate the formulation by a dynamic model of bank
run in a continuous-time setting. We briefly review the economic and game
theoretic contributions at the root of our effort, and we develop a
mathematical theory for continuous-time stochastic games where the strategic
decisions of the players are merely choices of times at which they leave the
game, and the interaction between the strategic players is of a mean field
nature.

A coordinated economic dispatch method for multi-area power systems is
proposed. Choosing boundary phase angles as coupling variables, the proposed
method exploits the structure of critical regions in local problems defined by
active and inactive constraints. For a fixed boundary state given by the
coordinator, local operators compute the coefficients of critical regions
containing the boundary state and of the optimal cost functions then
communicate them to the coordinator who in turn optimizes the boundary state to
minimize the overall cost. By iterating between local operators and the
coordinator, the proposed algorithm converges to the global optimal solution in
finite steps, and it requires limited information sharing.

We present an agent behavior based microscopic model for diffusion price
processes. As such we provide a model not only containing a convenient
framework for describing socio-economic behavior, but also a sophisticated link
to price dynamics. We furthermore establish the circumstances under which the
dynamics converge to diffusion processes in the large market limit. To
demonstrate the applicability of a separation of behavior and price process, we
show how herding behavior of market participants can lead to equilibria
transition and oscillations in diffusion price processes.

We discuss social network analysis from the perspective of economics. We
organize the presentaion around the theme of externalities: the effects that
one's behavior has on others' well-being. Externalities underlie the
interdependencies that make networks interesting. We discuss network formation,
as well as interactions between peoples' behaviors within a given network, and
the implications in a variety of settings. Finally, we highlight some empirical
challenges inherent in the statistical analysis of network-based data.

We present an agent behavior based microscopic model that induces jumps,
spikes and high volatility phases in the price process of a traded asset. We
transfer dynamics of thermally activated jumps of an unexcited/ excited two
state system discussed in the context of quantum mechanics to agent
socio-economic behavior and provide microfoundations. After we link the
endogenous agent behavior to price dynamics we establish the circumstances
under which the dynamics converge to an It\^o-diffusion price processes in the
large market limit.

We study the effects of introducing information inefficiency in a model for a
random linear economy with a representative consumer. This is done by
considering statistical, instead of classical, economic general equilibria.
Employing two different approaches we show that inefficiency increases the
consumption set of a consumer but decreases her expected utility. In this
scenario economic activity grows while welfare shrinks, that is the opposite of
the behavior obtained by considering a rational consumer.

In this paper we introduce the concept of split Nash equilibrium problems
associated with two related noncooperative strategic games. Then we apply the
Fan-KKM theorem to prove the existence of solutions to split Nash equilibrium
problems of related noncooperative strategic games, in which the strategy sets
of the players are nonempty closed and convex subsets in Banach spaces. As
application of this existence to economics, an example is provided to study the
existence of split Nash equilibrium of utilities of two related economies. As
applications, we study the existence of split Nash equilibrium in the dual
extended Bertrant duopoly model of price competition

In this paper we analyze the gravity model in the world passenger
air-transport network. We show that in the standard form the model is
inadequate to correctly describe the relationship between passenger flows and
typical geo-economic variables that characterize connected countries. We
propose a model of transfer flights which allows to exploit these discrepancies
to discover hidden subflows in the network. We illustrate its usefulness by
retrieving the distance coefficient in the gravity model which is one of the
determinants of the globalization process. Finally, we discuss the correctness
of the presented approach by comparing the distance coefficient to several well
known economical events.

We note a simple mechanism that may at least partially resolve several
outstanding economic puzzles, including why the cyclically adjusted price to
earnings ratio of the S&P 500 index has been oddly high for the past two
decades, why gains to capital have outpaced gains to wages, and the persistence
of the equity premium.

Health economic evaluation studies are widely used in public health to assess
health strategies in terms of their cost-effectiveness and inform public
policies. We developed an R package for Markov models implementing most of the
modelling and reporting features described in reference textbooks and
guidelines: deterministic and probabilistic sensitivity analysis, heterogeneity
analysis, time dependency on state-time and model-time (semi-Markov and
non-homogeneous Markov models), etc. In this paper we illustrate the features
of heemod by building and analysing an example Markov model. We then explain
the design and the underlying implementation of the package.

Introduction of market mechanisms in distribution systems is currently
subject to extensive studies. One of the challenges facing Distribution Market
Operators (DMOs) is to implement a fair and economically efficient pricing
mechanism that can incentivize consumers to positively contribute to grid
operations and to improve economic performance of the distribution system. This
paper studies a penalty-based pricing mechanism in distribution markets and
further investigates the interrelationship between the locational marginal
prices (LMPs) at transmission and distribution levels. As a result, a
closed-form relationship between these LMPs is derived. The possibility of
zeroing out the settlement profit is further investigated under the proposed
pricing mechanism.

America's transportation infrastructure is the backbone of our economy. A
strong infrastructure means a strong America - an America that competes
globally, supports local and regional economic development, and creates jobs.
Strategic investments in our transportation infrastructure are vital to our
national security, economic growth, transportation safety and our technology
leadership. This document outlines critical needs for our transportation
infrastructure, identifies new technology drivers and proposes strategic
investments for safe and efficient air, ground, rail and marine mobility of
people and goods.

In this work we compute all relevant contributions stemming from the
economical 3-3-1 model to the muon magnetic moment and the lepton flavor
violation decay $\mu \rightarrow e\gamma$. Using the current bounds on these
phenomena, we derive lower limits on the scale of symmetry breaking of the
model. Moreover, taking into account existing limits from meson and collider
studies we show that there is still room for a possible signal in $\mu
\rightarrow e\gamma$ in the near future.

We investigate the economic outcomes that result under simulated bidder
behavior in a model of the FCC's reverse auction for radio spectrum. In our
simulations, limiting our notion of efficiency to the reverse auction in
isolation, the reverse clock auction achieves very efficient solutions, the
FCC's scoring rule greatly reduces the total payments to TV broadcasters at the
cost of some efficiency, and using a poor feasibility checker can have grave
consequences both in terms of the auction's cost and efficiency.

An important issue within the present economic crisis is understanding the
dynamics of the public debt of a given country, and how the behavior of average
consumers and tax payers in that country affects it. Starting from a model of
the average consumer behavior introduced earlier by the authors, we propose a
simple model to quantitatively address this issue. The model is then studied
and analytically solved under some reasonable simplifying assumptions. In this
way we obtain a condition under which the public debt steadily decreases.

The aim of this paper is to reemphasize the money theory of exchange which is
centered on the function of exchange medium of money, and make a contribution
towards linearization of the quantity equation of exchange. A dynamical
quantity equation is presented and an important balanced path of economic
evolution is derived. To understand the business cycle we propose a hypothesis
of natural cycle and driving cycle concerning the evolution of the balanced
path and plentiful conclusions can be made.

This paper examines signalling when the sender exerts effort and receives
benefits over time. Receivers only observe a noisy public signal about the
effort, which has no intrinsic value.
  The modelling of signalling in a dynamic context gives rise to novel
equilibrium outcomes. In some equilibria, a sender with a higher cost of effort
exerts strictly more effort than his low-cost counterpart. The low-cost type
can compensate later for initial low effort, but this is not worthwhile for a
high-cost type. The interpretation of a given signal switches endogenously over
time, depending on which type the receivers expect to send it.
  JEL classification: D82, D83, C73.
  Keywords: Dynamic games, signalling , incomplete information

Sustainable and economical generation of electrical power is an essential and
mandatory component of infrastructure in today's world. Optimal generation
(generator subset selection) of power requires a careful evaluation of various
factors like type of source, generation, transmission & storage capacities,
congestion among others which makes this a difficult task. We created a grid to
simulate various conditions including stimuli like generator supply, weather
and load demand using Siemens PSS/E software and this data is trained using
deep learning methods and subsequently tested. The results are highly
encouraging. As per our knowledge, this is the first paper to propose a working
and scalable deep learning model for this problem.

Probabilistic forecasts in the form of probability distributions over future
events have become popular in several fields including meteorology, hydrology,
economics, and demography. In typical applications, many alternative
statistical models and data sources can be used to produce probabilistic
forecasts. Hence, evaluating and selecting among competing methods is an
important task. The scoringRules package for R provides functionality for
comparative evaluation of probabilistic models based on proper scoring rules,
covering a wide range of situations in applied work. This paper discusses
implementation and usage details, presents case studies from meteorology and
economics, and points to the relevant background literature.

The Russian wholesale electricity market started to operate in its present
form since 2006. Its unique specificity is the AC-based economic dispatch model
underlying the market applications. Solution method for the day-ahead and
balancing market applications was developed and implemented. Now it is employed
by the commercial operator and the system operator of the Russian energy
system. It is proved to be an adequate and reliable scheduling market
technology

In this paper, we optimize over the control parameter space of our
planar-bipedal robot, RAMone, for stable and energetically economical walking
at various speeds. We formulate this task as an episodic reinforcement learning
problem and use Covariance Matrix Adaptation. The parameters we are interested
in modifying include gains from our Hybrid Zero Dynamics style controller and
from RAMone's low-level motor controllers.

Compositional Game Theory is a new, recently introduced model of economic
games based upon the computer science idea of compositionality. In it, complex
and irregular games can be built up from smaller and simpler games, and the
equilibria of these complex games can be defined recursively from the
equilibria of their simpler subgames. This paper extends the model by providing
a final coalgebra semantics for infinite games. In the course of this, we
introduce a new operator on games to model the economic concept of subgame
perfection.

The overall objective of Requirements Engineering is to specify, in a
systematic way, a system that satisfies the expectations of its stakeholders.
Despite tremendous effort in the field, recent studies demonstrate this is
objective is not always achieved. In this paper, we discuss one particularly
challenging factor to Requirements Engineering projects, namely the change of
requirements. We proposes a rough discussion of how learning and time explain
requirements changes, how it can be introduced as a key variable in the
formulation of the Requirements Engineering Problem, and how this induces costs
for a requirements engineering project. This leads to a new discipline of
requirements economics.

Separate selling of two independent goods is shown to yield at least 62% of
the optimal revenue, and at least 73% when the goods satisfy the Myerson
regularity condition. This improves the 50% result of Hart and Nisan (2017,
originally circulated in 2012).

We discuss Russia's underlying motives for issuing its government-backed
cryptocurrency, CryptoRuble, and the implications thereof and of other
likely-soon-forthcoming government-issued cryptocurrencies to some stakeholders
(populace, governments, economy, finance, etc.), existing decentralized
cryptocurrencies (such as Bitcoin and Ethereum), as well as the future of the
world monetary system (the role of the U.S. therein and a necessity for the
U.S. to issue CryptoDollar), including a future algorithmic universal world
currency that may also emerge. We further provide a comprehensive list of
references on cryptocurrencies.

In this short paper, we study the economic dispatch with adjustable
transformer ratio and phase shifter, both of which, along with the transmission
line, are formulated into a generalized branch model. Resulted nonlinear parts
are thereafter exactly linearized using the piecewise liner technique to make
the derived ED problem computationally tractable. Numerical studies based on
modified IEEE systems demonstrate the effectiveness of the proposed method to
efficiency and flexibility of power system operation.

We propose an axiomatic approach which economically underpins the
representation of dynamic preferences in terms of a stochastic utility
function, sensitive to the information available to the decision maker. Our
construction is iterative and based on inter-temporal preference relations,
whose characterization is inpired by the original intuition given by Debreu's
State Dependent Utilities (1960).

In their seminal paper, Karp, Vazirani and Vazirani (STOC'90) introduce the
online bipartite matching problem, and the RANKING algorithm, which admits a
tight $1-\frac{1}{e}$ competitive ratio. Since its publication, the problem has
received considerable attention, including a sequence of simplified proofs. In
this paper we present a new proof that gives an economic interpretation of the
RANKING algorithm -- further simplifying the proof and avoiding arguments such
as duality. The new proof gives a new perspective on previous proofs.

The aims of this article are two-fold. First, we give a geometric
characterization of the optimal basic solutions of the general linear
programming problem (no compactness assumptions) and provide a simple,
self-contained proof of it together with an economical interpretation. Then, we
turn to considering a dynamic version of the linear programming problem in that
we consider the Kuratowski convergence of polyhedra and study the behaviour of
optimal solutions. Our methods are purely geometric.

Although using non-Gaussian distributions in economic models has become
increasingly popular, currently there is no systematic way for calibrating a
discrete distribution from the data without imposing parametric assumptions.
This paper proposes a simple nonparametric calibration method based on the
Golub-Welsch algorithm for Gaussian quadrature. Application to an optimal
portfolio problem suggests that assuming Gaussian instead of nonparametric
shocks leads to up to 17% overweighting in the stock portfolio because the
investor underestimates the probability of crashes.

There are visible changes in the world organization, environment and health
of national conscience that create a background for discussion on possible
redefinition of global, state and regional management goals. The author applies
the sustainable development criteria to a hierarchical management scheme that
is to lead the world community to non-contradictory growth. Concrete
definitions are discussed in respect of decision-making process representing
the state mostly. With the help of systems analysis it is highlighted how to
understand who would carry the distinctive sign of world leadership in the
nearest future.

This paper examines the history of previous examples of EMU from the
viewpoint that state actors make decisions about whether to participate in a
monetary union based on rational self-interest concerning costs and benefits to
their national economies. Illustrative examples are taken from nineteenth
century German, Italian and Japanese attempts at monetary integration with
early twentieth century ones from the Latin Monetary Union and the Scandinavian
Monetary Union and contemporary ones from the West African Monetary Union and
the European Monetary System. Lessons learned from the historical examples will
be used to identify issues that could arise with the move towards closer EMU in
Europe.

We study a generalization of the model of a dark market due to
Duffie-G\^arleanu- Pedersen [6]. Our market is segmented and involves multiple
assets. We show that this market has a unique asymptotically stable
equilibrium. In order to establish this result, we use a novel approach
inspired by a theory due to McKenzie and Hawkins-Simon. Moreover, we obtain a
closed form solution for the price of each asset at which investors trade at
equilibrium. We conduct a comparative statics analysis which shows, among other
sensitivities, how equilibrium prices respond to the level of interactions
between investors.

In this paper, we implement a stochastic deflator with five economic and
financial risk factors: interest rates, market price of risk, stock prices,
default intensities, and convenience yields. We examine the deflator with
different financial assets, such as stocks, zero-coupon bonds, vanilla options,
and corporate coupon bonds. We find required regularity conditions to implement
our stochastic deflator. Our numerical results show the reliability of the
deflator approach in pricing financial derivatives.

A standard growth model is modified in a straightforward way to incorporate
what Keynes (1936) suggests in the "essence" of his general theory. The
theoretical essence is the idea that exogenous changes in investment cause
changes in employment and unemployment. We implement this idea by assuming the
path for capital growth rate is exogenous in the growth model. The result is a
growth model that can explain both long term trends and fluctuations around the
trend. The modified growth model was tested using the U.S. economic data from
1947 to 2014. The hypothesized inverse relationship between the capital growth
and changes in unemployment was confirmed, and the structurally estimated model
fits fluctuations in unemployment reasonably well.

This paper studies the problem of optimally extracting nonrenewable natural
resources. Taking into account the fact that the market values of the main
natural resources i.e. oil, natural gas, copper,..., etc, fluctuate randomly
following global and seasonal macroeconomic parameters, the prices of natural
resources are modeled using Markov switching L\'evy processes. We formulate
this optimal extraction problem as an infinite-time horizon optimal control
problem. We derive closed-form solutions for the value function as well as the
optimal extraction policy. Numerical examples are presented to illustrate these
results.

This paper considers the use for Value-at-Risk computations of the so-called
Beta-Kotz distribution based on a general family of distributions including the
classical Gaussian model. Actually, this work develops a new method for
estimating the Value-at-Risk, the Conditional Value-at-Risk and the Economic
Capital when the underlying risk factors follow a Beta-Kotz distribution. After
estimating the parameters of the distribution of the loss random variable, both
analytical for some particular values of the parameters and numerical
approaches are provided for computing these mentioned measures. The proposed
risk measures are finally applied for quantifying the potential risk of
economic losses in Credit Risk.

There are many analogies among fortune hunting in business, politics, and
science. The prime task of the gold digger was to go to the Klondikes, find the
right mine and mine the richest veins. This task requires motivation, sense of
purpose and ability. Techniques and equipment must be developed. Fortune
hunting in New England was provided at one time by hunting for whales. One went
to a great whalers' station such as New Bedford and joined the whale hunters.
The hunt in academic research is similar. A single-minded passion is called
for. These notes here are the wrap-up comments containing some terminal
observations of mine on a hunt for a theory money and financial institutions.

This paper examines the question whether information is contained in
forecasts from DSGE models beyond that contained in lagged values, which are
extensively used in the models. Four sets of forecasts are examined. The
results are encouraging for DSGE forecasts of real GDP. The results suggest
that there is information in the DSGE forecasts not contained in forecasts
based only on lagged values and that there is no information in the
lagged-value forecasts not contained in the DSGE forecasts. The opposite is
true for forecasts of the GDP deflator.
  Keywords: DSGE forecasts, Lagged values
  JEL Classification Codes: E10, E17, C53

This empirical research explores the impact of age on nationality bias. World
Cup competition data suggest that judges of professional ski jumping
competitions prefer jumpers of their own nationality and exhibit this
preference by rewarding them with better marks. Furthermore, the current study
reveals that this nationality bias is diminished among younger judges, in
accordance with the reported lower levels of national discrimination among
younger generations. Globalisation and its effect in reducing class-based
thinking may explain this reduced bias in judgment of others.

This paper establishes an interesting link between $k$th price auctions and
Catalan numbers by showing that for distributions that have linear density, the
bid function at any symmetric, increasing equilibrium of a $k$th price auction
with $k\geq 3$ can be represented as a finite series of $k-2$ terms whose
$\ell$th term involves the $\ell$th Catalan number. Using an integral
representation of Catalan numbers, together with some classical combinatorial
identities, we derive the closed form of the unique symmetric, increasing
equilibrium of a $k$th price auction for a non-uniform distribution.

We relate models based on costs of switching beliefs (e.g. due to
inattention) to hypothesis tests. Specifically, for an inference problem with a
penalty for mistakes and for switching the inferred value, a band of inaction
is optimal. We show this band is equivalent to a confidence interval, and
therefore to a two-sided hypothesis test.

The class of $\alpha$-stable distributions received much interest for
modelling impulsive phenomena occur in engineering, economics, insurance, and
physics. The lack of non-analytical form for probability density function is
considered as the main obstacle to modelling data via the class of
$\alpha$-stable distributions. As the central member of this class, the Cauchy
distribution has received many applications in economics, seismology,
theoretical and applied physics. We derive estimators for the parameters of the
Cauchy and mixture of Cauchy distributions through the EM algorithm.
Performance of the EM algorithm is demonstrated through simulations and real
sets of data.

We measure trends in the diffusion of misinformation on Facebook and Twitter
between January 2015 and July 2018. We focus on stories from 570 sites that
have been identified as producers of false stories. Interactions with these
sites on both Facebook and Twitter rose steadily through the end of 2016.
Interactions then fell sharply on Facebook while they continued to rise on
Twitter, with the ratio of Facebook engagements to Twitter shares falling by
approximately 60 percent. We see no similar pattern for other news, business,
or culture sites, where interactions have been relatively stable over time and
have followed similar trends on the two platforms both before and after the
election.

In this study we present evidence that endowment effect can be elicited
merely by assigned ownership. Using Google Customer Survey, we administered a
survey were participants (n=495) were randomly split into 4 groups. Each group
was assigned ownership of either legroom or their ability to recline on an
airline. Using this experiment setup we were able to generate endowment effect,
a 15-20x (at p<0.05) increase between participant's willingness to pay (WTP)
and their willingness to accept (WTA).

The current article unveils and analyzes important shades of meaning for the
widely discussed term talent management. It not only grounds the outlined
perspectives in incremental formulation and elaboration of this construct, but
also is oriented to exploring the underlying reasons for the social actors,
proposing new nuances. Thus, a mind map and a fish-bone diagram are constructed
to depict effectively and efficiently the current state of development for
talent management and make easier the realizations of future research
endeavours in this field.

Proxies for regulatory reforms based on categorical variables are
increasingly used in empirical evaluation models. We surveyed 63 studies that
rely on such indices to analyze the effects of entry liberalization,
privatization, unbundling, and independent regulation of the electricity,
natural gas, and telecommunications sectors. We highlight methodological issues
related to the use of these proxies. Next, taking stock of the literature, we
provide practical advice for the design of the empirical strategy and discuss
the selection of control and instrumental variables to attenuate endogeneity
problems undermining identification of the effects of regulatory reforms.

We provide an exact analytical solution of the Nash equilibrium for $k$-
price auctions. We also introduce a new type of auction and demonstrate that it
has fair solutions other than the second price auctions, therefore paving the
way for replacing second price auctions.

Most of today's products and services are made in global supply chains. As a
result, a consumption of goods and services in one country is associated with
various environmental pressures all over the world due to international trade.
Advances in global multi-region input-output models have allowed researchers to
draw detailed, international supply-chain connections between production and
consumptions activities and associated environmental impacts. Due to a limited
data availability there is little evidence about the more recent trends in
global energy footprint. In order to expand the analytical potential of the
existing WIOD 2016 dataset to a wider range of research themes, this paper
develops energy accounts and presents the global energy footprint trends for
the period 2000-2014.

We study the effect of religion and intense religious experiences on
terrorism by focusing on one of the five pillars of Islam: Ramadan fasting. For
identification, we exploit two facts: First, daily fasting from dawn to sunset
during Ramadan is considered mandatory for most Muslims. Second, the Islamic
calendar is not synchronized with the solar cycle. We find a robust negative
effect of more intense Ramadan fasting on terrorist events within districts and
country-years in predominantly Muslim countries. This effect seems to operate
partly through decreases in public support for terrorism and the operational
capabilities of terrorist groups.

McFadden and Richter (1991) and later McFadden (2005) show that the Axiom of
Revealed Stochastic Preference characterizes rationalizability of choice
probabilities through random utility models on finite universal choice spaces.
This note proves the result in one short, elementary paragraph and extends it
to set valued choice. The latter requires a different axiom than is reported in
McFadden (2005).

In this paper we consider distributed convex optimization over time-varying
undirected graphs. We propose a linearized version of primarily averaged
network dual ascent (PANDA) while requiring less computational costs. The
proposed method, economic primarily averaged network dual ascent (Eco-PANDA),
provably converges at R-linear rate to the optimal point given that the agents'
objective functions are strongly convex and have Lipschitz continuous
gradients. Therefore, the method is competitive, in terms of type of rate, with
both DIGing and PANDA. The proposed method halves the communication costs of
methods like DIGing while still converging R-linearly and having the same per
iterate complexity.

We study the impact of oil price shocks on the U.S. stock market volatility.
We jointly analyze three different structural oil market shocks (i.e.,
aggregate demand, oil supply, and oil-specific demand shocks) and stock market
volatility using a structural vector autoregressive model. Identification is
achieved by assuming that the price of crude oil reacts to stock market
volatility only with delay. This implies that innovations to the price of crude
oil are not strictly exogenous, but predetermined with respect to the stock
market. We show that volatility responds significantly to oil price shocks
caused by unexpected changes in aggregate and oil-specific demand, whereas the
impact of supply-side shocks is negligible.

We consider a stochastic game-theoretic model of an investment market in
continuous time with short-lived assets and study strategies, called survival,
which guarantee that the relative wealth of an investor who uses such a
strategy remains bounded away from zero. The main results consist in obtaining
a sufficient condition for a strategy to be survival and showing that all
survival strategies are asymptotically close to each other. It is also proved
that a survival strategy allows an investor to accumulate wealth in a certain
sense faster than competitors.

We develop an axiomatic theory of information acquisition that captures the
idea of constant marginal costs in information production: the cost of
generating two independent signals is the sum of their costs, and generating a
signal with probability half costs half its original cost. Together with
Blackwell monotonicity and a continuity condition, these axioms determine the
cost of a signal up to a vector of parameters. These parameters have a clear
economic interpretation and determine the difficulty of distinguishing states.

This paper provides new conditions for dynamic optimality in discrete time
and uses them to establish fundamental dynamic programming results for several
commonly used recursive preference specifications. These include Epstein-Zin
preferences, risk-sensitive preferences, narrow framing models and recursive
preferences with sensitivity to ambiguity. The results obtained for these
applications include (i) existence of optimal policies, (ii) uniqueness of
solutions to the Bellman equation, (iii) a complete characterization of optimal
policies via Bellman's principle of optimality, and (iv) a globally convergent
method of computation via value function iteration.

The endogenous adaptation of agents, that may adjust their local contact
network in response to the risk of being infected, can have the perverse effect
of increasing the overall systemic infectiveness of a disease. We study a
dynamical model over two geographically distinct but interacting locations, to
better understand theoretically the mechanism at play. Moreover, we provide
empirical motivation from the Italian National Bovine Database, for the period
2006-2013.

In this study, a method will be developed and applied for estimating
biological migration parameters of the biomass of a fishery resource by means
of a decision analysis of the spatial behavior of the fleet. First, a model of
discrete selection is estimated, together with patch capture function. This
will allow estimating the biomass availability on each patch. In the second
regression, values of biomass are used in order to estimate a model of
biological migration between patches. This method is proven in the Chilean jack
mackerel fishery. This will allow estimating statistically significant
migration parameters, identifying migration patterns.

This study tries to find the relationship among poverty inequality and
growth. The major finding of this study is poverty has reduced significantly
from 2000 to 2016, which is more than 100 percent but in recent time poverty
reduction has slowed down. Slower and unequal household consumption growth
makes sloth the rate of poverty reduction. Average annual consumption fell from
1.8 percent to 1.4 percent from 2010 to 2016 and poorer households experienced
slower consumption growth compared to richer households.

We use insights from epidemiology, namely the SIR model, to study how agents
infect each other with "investment ideas." Once an investment idea "goes
viral," equilibrium prices exhibit the typical "fever peak," which is
characteristic for speculative excesses. Using our model, we identify a time
line of symptoms that indicate whether a boom is in its early or later stages.
Regarding the market's top, we find that prices start to decline while the
number of infected agents, who buy the asset, is still rising. Moreover, the
presence of fully rational agents (i) accelerates booms (ii) lowers peak prices
and (iii) produces broad, drawn-out, market tops.

E-commerce is on the rise in Hungary, with significantly growing numbers of
customers shopping online. This paper aims to identify the direct and indirect
drivers of the double-digit growth rate, including the related macroeconomic
indicators and the Digital Economy and Society Index (DESI). Moreover, this
study provides a deep insight into industry trends and outlooks, including high
industry concentration and top industrial players. It also draws the profile of
the typical online shopper and the dominant characteristics of online
purchases. Development of e-commerce is robust, but there is still plenty of
potential for growth and progress in Hungary.

Technological parasitism is a new theory to explain the evolution of
technology in society. In this context, this study proposes a model to analyze
the interaction between a host technology (system) and a parasitic technology
(subsystem) to explain evolutionary pathways of technologies as complex
systems. The coefficient of evolutionary growth of the model here indicates the
typology of evolution of parasitic technology in relation to host technology:
i.e., underdevelopment, growth and development. This approach is illustrated
with realistic examples using empirical data of product and process
technologies. Overall, then, the theory of technological parasitism can be
useful for bringing a new perspective to explain and generalize the evolution
of technology and predict which innovations are likely to evolve rapidly in
society.

We prove that a subtle but substantial bias exists in a common measure of the
conditional dependence of present outcomes on streaks of past outcomes in
sequential data. The magnitude of this streak selection bias generally
decreases as the sequence gets longer, but increases in streak length, and
remains substantial for a range of sequence lengths often used in empirical
work. We observe that the canonical study in the influential hot hand fallacy
literature, along with replications, are vulnerable to the bias. Upon
correcting for the bias we find that the long-standing conclusions of the
canonical study are reversed.

We study the incentive properties of envy-free mechanisms for the allocation
of rooms and payments of rent among financially constrained roommates. Each
agent reports her values for rooms, her housing earmark (soft budget), and an
index that reflects the difficulty the agent experiences from having to pay
over this amount. Then an envy-free allocation for these reports is
recommended. The complete information non-cooperative outcomes of each of these
mechanisms are exactly the envy-free allocations with respect to true
preferences if and only if the admissible budget violation indices have a
bound.

In this work, we propose the Haar wavelet method for the coupled degenerate
reaction-diffusion PDEs and the ODEs having non-linear a source with Neumann
boundary, applicable in various fields of the natural sciences, engineering,
and economics, for example in gas dynamics, certain biological models, assets
pricing in economics, composite media etc. Convergence analysis of the proposed
numerical scheme has been carried out. We use the GMRES solver to solve the
linear system of equations. Numerical solutions for the model problems of
medical significance have been successfully solved.

Norms are challenging to define and measure, but this paper takes advantage
of text data and the recent development in machine learning to create an
encompassing measure of norms. An LSTM neural network is trained to detect
gendered language. The network functions as a tool to create a measure on how
gender norms changes in relation to the Metoo movement on Swedish Twitter. This
paper shows that gender norms on average are less salient half a year after the
date of the first appearance of the hashtag #Metoo. Previous literature
suggests that gender norms change over generations, but the current result
suggests that norms can change in the short run.

Are there positive or negative externalities in knowledge production? Do
current contributions to knowledge production increase or decrease the future
growth of knowledge? We use a randomized field experiment, which added relevant
content to some pages in Wikipedia while leaving similar pages unchanged. We
find that the addition of content has a negligible impact on the subsequent
long-run growth of content. Our results have implications for information
seeding and incentivizing contributions, implying that additional content does
not generate sizable externalities by inspiring nor discouraging future
contributions.

Regression plays a key role in many research areas and its variable selection
is a classic and major problem. This study emphasizes cost of predictors to be
purchased for future use, when we select a subset of them. Its economic aspect
is naturally formalized by the decision-theoretic approach. In addition, two
Bayesian approaches are proposed to address uncertainty about model parameters
and models: the restricted and extended approaches, which lead us to rethink
about model averaging. From objective, rule-based, or robust Bayes point of
view, the former is preferred. Proposed method is applied to three popular
datasets for illustration.

We reconsider the Cournot duopoly problem in light of the theory for long
memory. We introduce the Caputo fractional-order difference calculus to
classical duopoly theory to propose a fractional-order discrete Cournot duopoly
game model, which allows participants to make decisions while making full use
of their historical information. Then we discuss Nash equilibria and local
stability by using linear approximation. Finally, we detect the chaos of the
model by employing a 0-1 test algorithm.

This paper attempts to outline how the adoption of Internet of Things (IoT)
in healthcare can create real economic value and improve the patient
experience. Thus, getting the maximum benefits requires understanding both the
IoT paradigm and the enabling technologies, and how IoT can be applied in the
field of healthcare. We will mention some open challenging issues to be
addressed by the research community, and not only. Besides the real barriers in
adopting the Internet of Things, there are some advantages regard collecting
and processing patient data, and monitoring the daily health states of
individuals, just to name a few. These aspects could revolutionize the
healthcare industry.

This paper examines the effects of institutional quality on financing choice
of individual using a large dataset of 137,160 people from 131 countries. We
classify borrowing activities into three categories, including formal,
constructive informal, and underground borrowing. Although the result shows
that better institutions aids the uses of formal borrowing, the impact of
institutions on constructive informal and underground borrowing among three
country sub-groups differs. Higher institutional quality improves constructive
informal borrowing in middle-income countries but reduces the use of
underground borrowing in high- and low-income countries.

The application of Markov chains to modelling refugee crises is explored,
focusing on local migration of individuals at the level of cities and days. As
an explicit example we apply the Markov chains migration model developed here
to UNHCR data on the Burundi refugee crisis. We compare our method to a
state-of-the-art `agent-based' model of Burundi refugee movements, and
highlight that Markov chain approaches presented here can improve the match to
data while simultaneously being more algorithmically efficient.

We study Bayesian coordination games where agents receive noisy private
information over the game's payoff structure, and over each others' actions. If
private information over actions is precise, we find that agents can coordinate
on multiple equilibria. If private information over actions is of low quality,
equilibrium uniqueness obtains like in a standard global games setting. The
current model, with its flexible information structure, can thus be used to
study phenomena such as bank-runs, currency crises, recessions, riots, and
revolutions, where agents rely on information over each others' actions.

We consider infinite dimensional optimization problems motivated by the
financial model called Arbitrage Pricing Theory. Using probabilistic and
functional analytic tools, we provide a dual characterization of the
super-replication cost. Then, we show the existence of optimal strategies for
investors maximizing their expected utility and the convergence of their
reservation prices to the super-replication cost as their risk-aversion tends
to infinity.

We aim to determine whether a game-theoretic model between an insurer and a
healthcare practice yields a predictive equilibrium that incentivizes either
player to deviate from a fee-for-service to capitation payment system. Using
United States data from various primary care surveys, we find that non-extreme
equilibria (i.e., shares of patients, or shares of patient visits, seen under a
fee-for-service payment system) can be derived from a Stackelberg game if
insurers award a non-linear bonus to practices based on performance. Overall,
both insurers and practices can be incentivized to embrace capitation payments
somewhat, but potentially at the expense of practice performance.

Research in operations management has traditionally focused on models for
understanding, mostly at a strategic level, how firms should operate. Spurred
by the growing availability of data and recent advances in machine learning and
optimization methodologies, there has been an increasing application of data
analytics to problems in operations management. In this paper, we review recent
applications of data analytics to operations management, in three major areas
-- supply chain management, revenue management and healthcare operations -- and
highlight some exciting directions for the future.

Quality data is a fundamental contributor to success in statistics and
machine learning. If a statistical assessment or machine learning leads to
decisions that create value, data contributors may want a share of that value.
This paper presents methods to assess the value of individual data samples, and
of sets of samples, to apportion value among different data contributors. We
use Shapley values for individual samples and Owen values for combined samples,
and show that these values can be computed in polynomial time in spite of their
definitions having numbers of terms that are exponential in the number of
samples.

The Receiver Operating Characteristic (ROC) curve is a representation of the
statistical information discovered in binary classification problems and is a
key concept in machine learning and data science. This paper studies the
statistical properties of ROC curves and its implication on model selection. We
analyze the implications of different models of incentive heterogeneity and
information asymmetry on the relation between human decisions and the ROC
curves. Our theoretical discussion is illustrated in the context of a large
data set of pregnancy outcomes and doctor diagnosis from the Pre-Pregnancy
Checkups of reproductive age couples in Henan Province provided by the Chinese
Ministry of Health.

We provide conditions for stable equilibrium in Cournot duopoly models with
tax evasion and time delay. We prove that our conditions actually imply
asymptotically stable equilibrium and delay independence. Conditions include
the same marginal cost and equal probability for evading taxes. We give
examples of cost and inverse demand functions satisfying the proposed
conditions. Some economic interpretations of our results are also included.

We present a method for computing the likelihood of a mixed hitting-time
model that specifies durations as the first time a latent L\'evy process
crosses a heterogeneous threshold. This likelihood is not generally known in
closed form, but its Laplace transform is. Our approach to its computation
relies on numerical methods for inverting Laplace transforms that exploit
special properties of the first passage times of L\'evy processes. We use our
method to implement a maximum likelihood estimator of the mixed hitting-time
model in MATLAB. We illustrate the application of this estimator with an
analysis of Kennan's (1985) strike data.

This study examines the role of pawnshops as a risk-coping device in prewar
Japan. Using data on pawnshop loans for more than 250 municipalities and
exploiting the 1918-1920 influenza pandemic as a natural experiment, we find
that the adverse health shock increased the total amount of loans from
pawnshops. This is because those who regularly relied on pawnshops borrowed
more money from them than usual to cope with the adverse health shock, and not
because the number of people who used pawnshops increased.

We study a class of optimal stopping games (Dynkin games) of preemption type,
with uncertainty about the existence of competitors. The set-up is well-suited
to model, for example, real options in the context of investors who do not want
to publicly reveal their interest in a certain business opportunity. We show
that there exists a Nash equilibrium in randomized stopping times which is
described explicitly in terms of the corresponding one-player game.

Auction theories are believed to provide a better selling opportunity for the
resources to be allocated. Various organizations have taken measures to
increase trust among participants towards their auction system, but trust alone
cannot ensure a high level of participation. We propose a new type of auction
system which takes advantage of lucky draw and gambling addictions to increase
the engagement level of candidates in an auction. Our system makes use of
security features present in existing auction systems for ensuring fairness and
maintaining trust among participants.

In the framework of such basic principles as local gauge invariance,
unification of the weak and electromagnetic interactions and spontaneous
symmetry breaking in the Standard Model the most economical and simplest
possibilities are realized. We discuss the problem of neutrino masses from the
point of view of economy and simplicity. It is unlikely that neutrino masses
are of the same SM origin as masses of leptons and quarks. The Weinberg
effective Lagrangian is the simplest and the most economical, beyond the
Standard Model mechanism of the generation of small Majorana neutrino masses.
The resolution of the sterile neutrino anomaly and observation of the
neutrinoless double $\beta$-decay would be crucial tests of this mechanism.

Halfway between the experiment and the focus group, between the quiz and a
game, we have experienced a new format to "focus" on sustainability and the
fundamental laws of thermodynamics and its principles. Concepts as
reversibility, efficiency and entropy, are then "visualized" by the
participants, showing the relations with the economic value, waste, the
energetics budget and raw material costs are explained from a different point
of view, proving the physical limits to the economic growth and the
environmental exploitation.

The average household income is one of the most important indexes for
decision making and the modelling of economic inequity and poverty. In this
work we propose a practical procedure to estimate the average income using
small area methods. We illustrate our proposal using information from a
multipurpose survey and suitable economic and demographic variables such as the
multidimensional poverty and the valorization indexes and the official
population projections. We find that the standard relative errors for the
income average estimates improve substantially when the proposed methodology is
implemented.

This paper is part of the research on the interlinkages between insurers and
their contribution to systemic risk on the insurance market. Its main purpose
is to present the results of the analysis of linkage dynamics and systemic risk
in the European insurance sector which are obtained using correlation networks.
These networks are based on dynamic dependence structures modelled using a
copula. Then, we determine minimum spanning trees (MST). Finally, the linkage
dynamics is described by means of selected topological network measures.

This paper investigates the dynamics of gambling and how they can affect
risk-taking behavior in regions not explored by Kahneman and Tversky's Prospect
Theory. Specifically, it questions why extreme outcomes do not fit the theory
and proposes alternative ways to measure prospects. The paper introduces a
measure of contrast between gambles and conducts an experiment to test the
hypothesis that individuals prospect gambles with nonadditive dynamics
differently. The results suggest a strong bias towards certain options, which
challenges the predictions of Kahneman and Tversky's theory.

We outline a token model for Truebit, a retrofitting, blockchain enhancement
which enables secure, community-based computation. The model addresses the
challenge of stable task pricing, as raised in the Truebit whitepaper, without
appealing to external oracles, exchanges, or hierarchical nodes. The system's
sustainable economics and fair market pricing derive from a mintable token
format which leverages existing tokens for liquidity. Finally, we introduce a
governance layer whose lifecycles culminates with permanent dissolution into
utility tokens, thereby tending the network towards autonomous
decentralization.

I use a panel of higher education clearinghouse data to study the centralized
assignment of applicants to Finnish polytechnics. I show that on a yearly
basis, large numbers of top applicants unnecessarily remain unassigned to any
program. There are programs which rejected applicants would find acceptable,
but the assignment mechanism both discourages applicants from applying, and
stops programs from admitting those who do. A mechanism which would admit each
year's most eligible applicants has the potential to substantially reduce
re-applications, thereby shortening the long queues into Finnish higher
education.

The concept of a blockchain has given way to the development of
cryptocurrencies, enabled smart contracts, and unlocked a plethora of other
disruptive technologies. But, beyond its use case in cryptocurrencies, and in
network coordination and automation, blockchain technology may have serious
sociotechnical implications in the future co-existence of robots and humans.
Motivated by the recent explosion of interest around blockchains, and our
extensive work on open-source blockchain technology and its integration into
robotics - this paper provides insights in ways in which blockchains and other
decentralized technologies can impact our interactions with robot agents and
the social integration of robots into human society.

In this paper, we extend and improve the production chain model introduced by
Kikuchi et al. (2018). Utilizing the theory of monotone concave operators, we
prove the existence, uniqueness, and global stability of equilibrium price,
hence improving their results on production networks with multiple upstream
partners. We propose an algorithm for computing the equilibrium price function
that is more than ten times faster than successive evaluations of the operator.
The model is then generalized to a stochastic setting that offers richer
implications for the distribution of firms in a production network.

Data-based decisionmaking must account for the manipulation of data by agents
who are aware of how decisions are being made and want to affect their
allocations. We study a framework in which, due to such manipulation, data
becomes less informative when decisions depend more strongly on data. We
formalize why and how a decisionmaker should commit to underutilizing data.
Doing so attenuates information loss and thereby improves allocation accuracy.

We show that competitive equilibria in a range of models related to
production networks can be recovered as solutions to dynamic programs. Although
these programs fail to be contractive, we prove that they are tractable. As an
illustration, we treat Coase's theory of the firm, equilibria in production
chains with transaction costs, and equilibria in production networks with
multiple partners. We then show how the same techniques extend to other
equilibrium and decision problems, such as the distribution of management
layers within firms and the spatial distribution of cities.

We develop a model to predict consumer default based on deep learning. We
show that the model consistently outperforms standard credit scoring models,
even though it uses the same data. Our model is interpretable and is able to
provide a score to a larger class of borrowers relative to standard credit
scoring models while accurately tracking variations in systemic risk. We argue
that these properties can provide valuable insights for the design of policies
targeted at reducing consumer default and alleviating its burden on borrowers
and lenders, as well as macroprudential regulation.

A desire to understand the decision of the UK to leave the European Union,
Brexit, in the referendum of June 2016 has continued to occupy academics, the
media and politicians. Using topological data analysis ball mapper we extract
information from multi-dimensional datasets gathered on Brexit voting and
regional socio-economic characteristics. While we find broad patterns
consistent with extant empirical work, we also evidence that support for Leave
drew from a far more homogenous demographic than Remain. Obtaining votes from
this concise set was more straightforward for Leave campaigners than was
Remain's task of mobilising a diverse group to oppose Brexit.

We examine the impact of a new tool for suppressing the expression of
dissent---a daily tax on social media use. Using a synthetic control framework,
we estimate that the tax reduced the number of georeferenced Twitter users in
Uganda by 13 percent. The estimated treatment effects are larger for poorer and
less frequent users. Despite the overall decline in Twitter use, tweets
referencing collective action increased by 31 percent and observed protests
increased by 47 percent. These results suggest that taxing social media use may
not be an effective tool for reducing political dissent.

We study preferences estimated from finite choice experiments and provide
sufficient conditions for convergence to a unique underlying "true" preference.
Our conditions are weak, and therefore valid in a wide range of economic
environments. We develop applications to expected utility theory, choice over
consumption bundles, menu choice and intertemporal consumption. Our framework
unifies the revealed preference tradition with models that allow for errors.

We derive new prox-functions on the simplex from additive random utility
models of discrete choice. They are convex conjugates of the corresponding
surplus functions. In particular, we explicitly derive the convexity parameter
of discrete choice prox-functions associated with generalized extreme value
models, and specifically with generalized nested logit models. Incorporated
into subgradient schemes, discrete choice prox-functions lead to natural
probabilistic interpretations of the iteration steps. As illustration we
discuss an economic application of discrete choice prox-functions in consumer
theory. The dual averaging scheme from convex programming naturally adjusts
demand within a consumption cycle.

This paper studies the forecasting ability of cryptocurrency time series.
This study is about the four most capitalized cryptocurrencies: Bitcoin,
Ethereum, Litecoin and Ripple. Different Bayesian models are compared,
including models with constant and time-varying volatility, such as stochastic
volatility and GARCH. Moreover, some crypto-predictors are included in the
analysis, such as S\&P 500 and Nikkei 225. In this paper the results show that
stochastic volatility is significantly outperforming the benchmark of VAR in
both point and density forecasting. Using a different type of distribution, for
the errors of the stochastic volatility the student-t distribution came out to
be outperforming the standard normal approach.

Arrow's `impossibility' theorem asserts that there are no satisfactory
methods of aggregating individual preferences into collective preferences in
many complex situations. This result has ramifications in economics, politics,
i.e., the theory of voting, and the structure of tournaments. By identifying
the objects of choice with mathematical sets, and preferences with Hausdorff
measures of the distances between sets, it is possible to extend Arrow's
arguments from a sociological to a mathematical setting. One consequence is
that notions of reversibility can be expressed in terms of the relative
configurations of patterns of sets.

We consider the problem of designing a derivatives exchange aiming at
addressing clients needs in terms of listed options and providing suitable
liquidity. We proceed into two steps. First we use a quantization method to
select the options that should be displayed by the exchange. Then, using a
principal-agent approach, we design a make take fees contract between the
exchange and the market maker. The role of this contract is to provide
incentives to the market maker so that he offers small spreads for the whole
range of listed options, hence attracting transactions and meeting the
commercial requirements of the exchange.

We study identification and estimation of causal effects in settings with
panel data. Traditionally researchers follow model-based identification
strategies relying on assumptions governing the relation between the potential
outcomes and the observed and unobserved confounders. We focus on a different,
complementary approach to identification where assumptions are made about the
connection between the treatment assignment and the unobserved confounders.
Such strategies are common in cross-section settings but rarely used with panel
data. We introduce different sets of assumptions that follow the two paths to
identification and develop a doubly robust approach. We propose estimation
methods that build on these identification strategies.

This paper shows that black and Hispanic borrowers are 39% more likely to
experience a debt collection judgment than white borrowers, even after
controlling for credit scores and other relevant credit attributes. The racial
gap in judgments is more pronounced in areas with a high density of payday
lenders, a high share of income-less households, and low levels of tertiary
education. State-level measures of racial discrimination cannot explain the
judgment gap, nor can neighborhood-level differences in the previous share of
contested judgments or cases with attorney representation. A
back-of-the-envelope calculation suggests that closing the racial wealth gap
could significantly reduce the racial disparity in debt collection judgments.

How much should you receive in a week to be indifferent to \$ 100 in six
months? Note that the indifference requires a rule to ensure the similarity
between early and late payments. Assuming that rational individuals have low
accuracy, then the following rule is valid: if the amounts to be paid are much
less than the personal wealth, then the $q$-exponential discounting guarantees
indifference in several periods. Thus, the discounting can be interpolated
between hyperbolic and exponential functions due to the low accuracy to
distinguish time averages when the payments have low impact on personal wealth.
Therefore, there are physical conditions that allow the hyperbolic discounting
regardless psycho-behavioral assumption.

This paper has the following objectives: to understand the concepts of
Environmental Accounting in Brazil; Make criticisms and propositions anchored
in the reality or demand of environmental accounting for Amazonia Paraense. The
methodological strategy was a critical analysis of Ferreira's books (2007);
Ribeiro (2010) and Tinoco and Kraemer (2011) using their correlation with the
scientific production of authors discussing the Paraense Amazon, besides our
experience as researchers of this territory. As a result, we created three
sections: one for understanding the current constructs of environmental
accounting, one for criticism and one for propositions.

Inefficient markets allow investors to consistently outperform the market. To
demonstrate that inefficiencies exist in sports betting markets, we created a
betting algorithm that generates above market returns for the NFL, NBA, NCAAF,
NCAAB, and WNBA betting markets. To formulate our betting strategy, we
collected and examined a novel dataset of bets, and created a non-parametric
win probability model to find positive expected value situations. As the United
States Supreme Court has recently repealed the federal ban on sports betting,
research on sports betting markets is increasingly relevant for the growing
sports betting industry.

In Hopenhayn's (1992) entry-exit model productivity is bounded, implying that
the predicted firm size distribution cannot match the power law tail observable
in the data. In this paper we remove the boundedness assumption and, in this
more general setting, provide an exact characterization of existence of
stationary equilibria, as well as a novel sufficient condition for existence
based on treating production as a Lyapunov function. We also provide new
representations of the rate of entry and aggregate supply. Finally, we prove
that the firm size distribution has a power law tail under a very broad set of
productivity growth specifications.

We demonstrate that the tail dependence should always be taken into account
as a proxy for systematic risk of loss for investments. We provide the clear
statistical evidence of that the structure of investment portfolios on a
regulated market should be adjusted to the price of gold. Our finding suggests
that the active bartering of oil for goods would prevent collapsing the
national market facing international sanctions.

A mathematical model of measurement of the perception of well-being for
groups with increasing incomes, but proportionally unequal is proposed.
Assuming that welfare grows with own income and decreases with relative
inequality (income of the other concerning one's own), possible scenarios for
long-term behavior in welfare functions are concluded. Also, it is proved that
a high relative inequality (parametric definition) always implies the loss of
the self-perception of the well-being of the most disadvantaged group.

In this paper, a novel tube-based economic Model Predictive Control (MPC)
scheme for uncertain systems that uses neither terminal costs nor terminal
constraints is investigated. We show that the results from the undisturbed case
can be extended to systems with bounded disturbances by using similar turnpike
arguments and a properly modified stage cost. We prove robust guarantees on the
closed-loop performance, convergence, and stability under suitable
dissipativity and controllability conditions and discuss them in a numerical
example.

Paul A. Samuelson's (1966) capitulation during the so-called Cambridge
controversy on the re-switching of techniques in capital theory had
implications not only in pointing at supposed internal contradiction of the
marginal theory of production and distribution, but also in preserving vested
interests in the academic and political world. Based on a new non-switching
theorem, the present paper demonstrates that Samuelson's capitulation was
logically groundless from the point of view of the economic theory of
production.

The existence of stylized facts in financial data has been documented in many
studies. In the past decade the modeling of financial markets by agent-based
computational economic market models has become a frequently used modeling
approach. The main purpose of these models is to replicate stylized facts and
to identify sufficient conditions for their creations. In this paper we
introduce the most prominent examples of stylized facts and especially present
stylized facts of financial data. Furthermore, we given an introduction to
agent-based modeling. Here, we not only provide an overview of this topic but
introduce the idea of universal building blocks for agent-based economic market
models.

Many economic activities are embedded in networks: sets of agents and the
(often) rivalrous relationships connecting them to one another. Input sourcing
by firms, interbank lending, scientific research, and job search are four
examples, among many, of networked economic activities. Motivated by the
premise that networks' structures are consequential, this chapter describes
econometric methods for analyzing them. I emphasize (i) dyadic regression
analysis incorporating unobserved agent-specific heterogeneity and supporting
causal inference, (ii) techniques for estimating, and conducting inference on,
summary network parameters (e.g., the degree distribution or transitivity
index); and (iii) empirical models of strategic network formation admitting
interdependencies in preferences. Current research challenges and open
questions are also discussed.

This paper analyzes identification issues of a behavorial New Keynesian model
and estimates it using likelihood-based and limited-information methods with
identification-robust confidence sets. The model presents some of the same
difficulties that exist in simple benchmark DSGE models, but the analytical
solution is able to indicate in what conditions the cognitive discounting
parameter (attention to the future) can be identified and the robust estimation
methods is able to confirm its importance for explaining the proposed
behavioral model.

Digital advertising markets are growing and attracting increased scrutiny.
This paper explores four market inefficiencies that remain poorly understood:
ad effect measurement, frictions between and within advertising channel
members, ad blocking and ad fraud. These topics are not unique to digital
advertising, but each manifests in new ways in markets for digital ads. We
identify relevant findings in the academic literature, recent developments in
practice, and promising topics for future research.

We propose a new method for conducting Bayesian prediction that delivers
accurate predictions without correctly specifying the unknown true data
generating process. A prior is defined over a class of plausible predictive
models. After observing data, we update the prior to a posterior over these
models, via a criterion that captures a user-specified measure of predictive
accuracy. Under regularity, this update yields posterior concentration onto the
element of the predictive class that maximizes the expectation of the accuracy
measure. In a series of simulation experiments and empirical examples we find
notable gains in predictive accuracy relative to conventional likelihood-based
prediction.

This paper employs the survey data of CHFS (2013) to investigate the impact
of housing investment on household stock market participation and portfolio
choice. The results show that larger housing investment encourages the
household participation in the stock market, but reduces the proportion of
their stockholding. The above conclusion remains true even when the endogeneity
problem is controlled with risk attitude classification, Heckman model test and
subsample regression. This study shows that the growth in the housing market
will not lead to stock market development because of lack of household
financial literacy and the low expected yield on stock market.

Public transit disruption is becoming more common across different transit
services, which can have a destructive influence on the resiliency and
reliability of the transportation system. Utilizing a recently collected data
of transit users in the Chicago Metropolitan Area, the current study aims to
analyze how transit users respond to unplanned service disruption and disclose
the factors that affect their behavior.

Malaysia is experiencing ever increasing domestic energy consumption. This
study is an attempt at analyzing the changes in sectoral energy intensities in
Malaysia for the period 1995 to 2011. The study quantifies the sectoral total,
direct, and indirect energy intensities to track the sectors that are
responsible for the increasing energy consumption. The energy input-output
model which is a frontier method for examining resource embodiments in goods
and services on a sectoral scale that is popular among scholars has been
applied in this study.

This study examines the disparities of infrastructure in four states in
Northern Peninsular Malaysia. This study used a primer data which is collected
by using a face to face interview with a structure questionnaire on head of
household at Kedah, Perlis, Penang and Perak. The list of respondents is
provided by the Department of Statistics of Malaysia (DOS). The Department of
Statistics of Malaysia (DOS) uses the population observation in 2010 to
determine the respondents is provided by the Department of Statistics of
Malaysia (DOS).

Robust assessment of the institutionalist account of comparative development
is hampered by problems of omitted variable bias and reverse causation, since
institutional quality is not randomly assigned with respect to geographic and
human capital endowments. A recent series of papers has applied spatial
regression discontinuity designs to estimate the impact of institutions on
incomes at international borders, drawing inference from the abrupt
discontinuity in governance at borders, whereas other determinants of income
vary smoothly across borders. I extend this literature by assessing the
importance of sub-national variation in institutional quality at provincial
borders in China. Employing nighttime lights emissions as a proxy for income,
across multiple specifications I find no evidence in favour of an
institutionalist account of the comparative development of Chinese provinces.

Many researches have discussed the phenomenon and definition of sharing
economy, but an understanding of sharing economy's reconstructions of the world
remains elusive. We illustrate the mechanism of sharing economy's
reconstructions of the world in detail based on big data including the
mechanism of sharing economy's reconstructions of society, time and space,
users, industry, and self-reconstruction in the future, which is very important
for society to make full use of the reconstruction opportunity to upgrade our
world through sharing economy. On the one hand, we established the mechanisms
for sharing economy rebuilding society, industry, space-time, and users through
qualitative analyses, and on the other hand, we demonstrated the rationality of
the mechanisms through quantitative analyses of big data.

We study the cake-cutting problem when agents have single-peaked preferences
over the cake. We show that a recently proposed mechanism by Wang-Wu (2019) to
obtain envy-free allocations can yield large welfare losses. Using a
simplifying assumption, we characterize all Pareto optimal allocations, which
have a simple structure: are peak-preserving and non-wasteful. Finally, we
provide simple alternative mechanisms that Pareto dominate that of Wang-Wu, and
which achieve envy-freeness or Pareto optimality.

Most doctors in the NRMP are matched to one of their most-preferred
internship programs. Since various surveys indicate similarities across
doctors' preferences, this suggests a puzzle. How can nearly everyone get a
position in a highly-desirable program when positions in each program are
scarce? We provide one possible explanation for this puzzle. We show that the
patterns observed in the NRMP data may be an artifact of the interview process
that precedes the match. Our analysis highlights the importance of interactions
occurring outside of a matching clearinghouse for resulting outcomes, and casts
doubts on analysis of clearinghouses that take reported preferences at face
value.

The start up costs in many kinds of generators lead to complex cost
structures, which in turn yield severe market loopholes in the locational
marginal price (LMP) scheme. Convex hull pricing (a.k.a. extended LMP) is
proposed to improve the market efficiency by providing the minimal uplift
payment to the generators. In this letter, we consider a stylized model where
all generators share the same generation capacity. We analyze the generators'
possible strategic behaviors in such a setting, and then propose an index for
market power quantification in the convex hull pricing schemes.

We prove that the consumption functions in optimal savings problems are
asymptotically linear if the marginal utility is regularly varying. We also
analytically characterize the asymptotic marginal propensities to consume
(MPCs) out of wealth. Our results are useful for obtaining good initial guesses
when numerically computing consumption functions, and provide a theoretical
justification for linearly extrapolating consumption functions outside the
grid.

We apply numerical dynamic programming techniques to solve discrete-time
multi-asset dynamic portfolio optimization problems with proportional
transaction costs and shorting/borrowing constraints. Examples include problems
with multiple assets, and many trading periods in a finite horizon problem. We
also solve dynamic stochastic problems, with a portfolio including one
risk-free asset, an option, and its underlying risky asset, under the existence
of transaction costs and constraints. These examples show that it is now
tractable to solve such problems.

This paper investigates the effect of the novel coronavirus and crude oil
prices on the United States (US) economic policy uncertainty (EPU). Using daily
data for the period January 21-March 13, 2020, our Autoregressive Distributed
Lag (ARDL) model shows that the new infection cases reported at global level,
and the death ratio, have no significant effect on the US EPU, whereas the oil
price negative dynamics leads to increased uncertainty. However, analyzing the
situation outside China, we discover that both new case announcements and the
COVID-19 associated death ratio have a positive influence on the US EPU.

A kernel density is an aggregate of kernel functions, which are itself
densities and could be kernel densities. This is used to decompose a kernel
into its constituent parts. Pearson's test for equality of proportions is
applied to quantiles to test whether the component distributions differ from
one another. The proposed methods are illustrated with a meta-analysis of the
social cost of carbon. Different discount rates lead to significantly different
Pigou taxes, but not different growth rates. Estimates have not varied over
time. Different authors have contributed different estimates, but these
differences are insignificant. Kernel decomposition can be applied in many
other fields with discrete explanatory variables.

We study how the quality dimension affects the social optimum in a model of
spatial differentiation where two facilities provide a public service. If
quality enters linearly in the individuals' utility function, a symmetric
configuration, in which both facilities have the same quality and serve groups
of individuals of the same size, does not maximize the social welfare. This is
a surprising result as all individuals are symmetrically identical having the
same quality valuation. We also show that a symmetric configuration of
facilities may maximize the social welfare if the individuals' marginal utility
of quality is decreasing.

We study the welfare consequences of merging Shapley--Scarf markets. Market
integration can lead to large welfare losses and make the vast majority of
agents worse-off, but is on average welfare-enhancing and makes all agents
better off ex-ante. The number of agents harmed by integration is a minority
when all markets are small or agents' preferences are highly correlated.

The mining of bitcoin is modeled using system dynamics, showing that the past
evolution of the network hash rate can be explained to a large extent by an
efficient market hypothesis applied to the mining of blocks. The possibility of
a decrease in the network hash rate from the next halving event (May 2020) is
exposed, implying that the network may be close to 'peak hash', if the price of
bitcoin and the revenues from transaction fees will remain at approximately the
present level.

As the amount of economic and other data generated worldwide increases
vastly, a challenge for future generations of econometricians will be to master
efficient algorithms for inference in empirical models with large information
sets. This Chapter provides a review of popular estimation algorithms for
Bayesian inference in econometrics and surveys alternative algorithms developed
in machine learning and computing science that allow for efficient computation
in high-dimensional settings. The focus is on scalability and parallelizability
of each algorithm, as well as their ability to be adopted in various empirical
settings in economics and finance.

In this study, we make use of empirically observed occupational
stratification patterns, in order to identify the relationship between
education and social mobility of individuals - the latter is approximated by
the social distance of an individual's occupation from his/her household's
traditional niche occupation. Our study draws upon a novel occupational network
construction proposed in Lambert et.al (2018), with slight adjustments, to
empirically identify social stratification patterns using cross sectional
household surveys available in the Indian context. We use IHDS-2 data-set for
the purpose of our study.

The core is a traditional and useful solution concept in economic theory. But
in discrete exchange economies without transfers, when endowments are complex,
the core may be empty. This motivates Balbuzanov and Kotowski (2019) to
interpret endowments as exclusion rights and propose a new concept called
exclusion core. Our contribution is twofold. First, we propose a rectification
of the core to solve its problem under complex endowments. Second, we propose a
refinement of Balbuzanov and Kotowski's exclusion core to improve its
performance. Our two core concepts share a common idea of correcting the
misused altruism of unaffected agents in blocking coalitions. We propose a
mechanism to find allocations in the two cores.

We study the effects of financial shocks on the United States economy by
using a Bayesian structural vector autoregressive (SVAR) model that exploits
the non-normalities in the data. We use this method to uniquely identify the
model and employ inequality constraints to single out financial shocks. The
results point to the existence of two distinct financial shocks that have
opposing effects on inflation, which supports the idea that financial shocks
are transmitted to the real economy through both demand and supply side
channels.

Tie-line scheduling in multi-area power systems in the US largely proceeds
through a market-based mechanism called Coordinated Transaction Scheduling
(CTS). We analyze this market mechanism through a game-theoretic lens. Our
analysis characterizes the effect of market liquidity, market participants'
forecasts about inter-area price spreads, transactions fees and coupling of CTS
markets with up-to-congestion virtual transactions. Using real data, we
empirically verify that CTS bidders can employ simple learning algorithms to
discover Nash equilibria that support the conclusions drawn from equilibrium
analysis.

We explain in a nontechnical fashion why dollar-neutral quant trading
strategies, such as equities Statistical Arbitrage, suffered substantial losses
(drawdowns) during the COVID-19 market selloff. We discuss: (i) why these
strategies work during "normal" times; (ii) the market regimes when they work
best; and (iii) their limitations and the reasons for why they "break" during
extreme market events. An accompanying appendix (with a link to freely
accessible source code) includes backtests for various strategies, which put
flesh on and illustrate the discussion in the main text.

We introduce the social welfare implications of the Zenga index, a recently
proposed index of inequality. Our proposal is derived by following the seminal
book by Son (2011) and the recent working paper by Kakwani and Son (2019). We
compare the Zenga based approach with the classical one, based on the Lorenz
curve and the Gini coefficient, as well as the Bonferroni index. We show that
the social welfare specification based on the Zenga uniformity curve presents
some peculiarities that distinguish it from the other considered indexes. The
social welfare specification presented here provides a deeper understanding of
how the Zenga index evaluates the inequality in a distribution.

Structural models with no solution are incoherent, and those with multiple
solutions are incomplete. We show that models with occasionally binding
constraints are not generically coherent. Coherency requires restrictions on
the parameters or on the support of the distribution of the shocks. In presence
of multiple shocks, the support restrictions cannot be independent from each
other, so the assumption of orthogonality of structural shocks is incompatible
with coherency. Models whose coherency is based on support restrictions are
generically incomplete, admitting a very large number of minimum state variable
solutions.

We provide evidence that many narrative shocks used by prominent literature
are persistent. We show that the two leading methods to estimate impulse
responses to an independently identified shock (local projections and
distributed lag models) treat persistence differently, hence identifying
different objects. We propose corrections to re-establish the equivalence
between local projections and distributed lag models, providing applied
researchers with methods and guidance to estimate their desired object of
interest. We apply these methods to well-known empirical work and find that how
persistence is treated has a sizable impact on the estimates of dynamic
effects.

After a market downturn, especially in an uncertain economic environment such
as the current state, there can be a relatively long period with a sideways
market, where indexes, stocks, etc., move in channels with support and
resistance levels. We discuss option pricing in such scenarios, in both cases
of unattainable as well as attainable boundaries, and obtain closed-form option
pricing formulas. Our results also apply to FX rates in target zones without
interest rate pegging (USD/HKD, digital currencies, etc.).

We consider games in which players search for a hidden prize, and they have
asymmetric information about the prize location. We study the social payoff in
equilibria of these games. We present sufficient conditions for the existence
of an equilibrium that yields the first-best payoff (i.e., the highest social
payoff under any strategy profile), and we characterize the first-best payoff.
The results have interesting implications for innovation contests and R&D
races.

Since 2018 UK firms with at least 250 employees have been mandated to
publicly disclose gender equality indicators. Exploiting variations in this
mandate across firm size and time we show that pay transparency closes 18
percent of the gender pay gap by reducing men's wage growth. The public
availability of the equality indicators seems to influence employers' response
as worse performing firms and industries more exposed to public scrutiny reduce
their gender pay gap the most. Employers are also 9 percent more likely to post
wages in job vacancies, potentially in an effort to improve gender equality at
entry level.

Risk-neutral pricing dictates that the discounted derivative price is a
martingale in a measure equivalent to the economic measure. The residual
ambiguity for incomplete markets is here resolved by minimising the entropy of
the price measure from the economic measure, subject to mark-to-market
constraints, following arguments based on the optimisation of portfolio risk.
The approach accounts for market and funding convexities and incorporates
available price information, interpolating between methodologies based on
expectation and replication.

We investigate the networks of Japanese corporate boards and its influence on
the appointments of female board members. We find that corporate boards with
women show homophily with respect to gender. The corresponding firms often have
above average profitability. We also find that new appointments of women are
more likely at boards which observe female board members at other firms to
which they are tied by either ownership relations or corporate board
interlocks.

What are the chances of an ethical individual rising through the ranks of a
political party or a corporation in the presence of unethical peers? To answer
this question, I consider a four-player two-stage elimination tournament, in
which players are partitioned into those willing to be involved in sabotage
behavior and those who are not. I show that, under certain conditions, the
latter are more likely to win the tournament.

Convex analysis is fundamental to proving inequalities that have a wide
variety of applications in economics and mathematics. In this paper we provide
Jensen-type inequalities for functions that are, intuitively, "very" convex.
These inequalities are simple to apply and can be used to generalize and extend
previous results or to derive new results. We apply our inequalities to
quantify the notion "more risk averse" provided in \cite{pratt1978risk}. We
also apply our results in other applications from different fields, including
risk measures, Poisson approximation, moment generating functions,
log-likelihood functions, and Hermite-Hadamard type inequalities.

In this work we characterize sudden increases in the land price of certain
urban areas, a phenomenon causing gentrification, via an extended Schelling
model. An initial price rise forces some of the disadvantaged inhabitants out
of the area, creating vacancies which other groups find economically
attractive. Intolerance issues forces further displacements, possibly giving
rise to an avalanche. We consider how gradual changes in the economic
environment affect the urban architecture through such avalanche processes,
when agents may enter or leave the city freely. The avalanches are
characterized by power-law histograms, as it is usually the case in
self-organized critical phenomena.

In this online appendix we provide additional information and analyses to
support "The Determinants of Social Connectedness in Europe." We include a
number of case studies illustrating how language, history, and other factors
have shaped European social networks. We also look at the effects of social
connectedness. Our results provide empirical support for theoretical models
that suggest social networks play an important role in individuals' travel
decisions. We study variation in the degree of connectedness of regions to
other European countries, finding a negative correlation between Euroscepticism
and greater levels of international connection.

The COVID-19 pandemic has posed a policy making crisis where efforts to slow
down or end the pandemic conflict with economic priorities. This paper provides
mathematical analysis of optimal disease control policies with idealized
compartmental models for disease propagation and simplistic models of social
and economic costs. Two locally optimal control strategies are found and
categorized as `suppression' and `mitigation' strategies. We analyze how these
strategies change when we incorporate vaccination into the model and find a new
optimal `delay-mitigation' strategy.

We examine a patient player's behavior when he can build reputations in front
of a sequence of myopic opponents. With positive probability, the patient
player is a commitment type who plays his Stackelberg action in every period.
We characterize the patient player's action frequencies in equilibrium. Our
results clarify the extent to which reputations can refine the patient player's
behavior and provide new insights to entry deterrence, business transactions,
and capital taxation. Our proof makes a methodological contribution by
establishing a new concentration inequality.

Using a semi-structural approach, the paper identifies how heterogeneity and
financial frictions affect the transmission of aggregate shocks. Approximating
a heterogeneous agent model around the representative agent allocation can
successfully trace the aggregate and distributional dynamics and can be
consistent with alternative mechanisms. Employing Spanish macroeconomic data as
well as firm and household survey data, the paper finds that frictions on both
consumption and investment have rich interactions with aggregate shocks. The
response of heterogeneity amplifies or dampens these effects depending on the
type of the shock. Both dispersion in consumption shares and the marginal
revenue product of firms, as well as the proportion of investment constrained
firms are key determinants of the fiscal multiplier.

Lockdown procedures have been proven successful in mitigating the spread of
the viruses in this COVID-19 pandemic, but they also have devastating impact on
the economy. We use a modified Susceptible-Infectious-Recovered-Deceased model
with time dependent infection rate to simulate how the infection is spread
under lockdown. The economic cost due to the loss of workforce and incurred
medical expenses is evaluated with a simple model. We find the best strategy,
meaning the smallest economic cost for the entire course of the pandemic, is to
keep the strict lockdown as long as possible.

Cryptocurrency refers to a type of digital asset that uses distributed
ledger, or blockchain, technology to enable a secure transaction. Although the
technology is widely misunderstood, many central banks are considering
launching their own national cryptocurrency. In contrast to most data in
financial economics, detailed data on the history of every transaction in the
cryptocurrency complex are freely available. Furthermore, empirically-oriented
research is only now beginning, presenting an extraordinary research
opportunity for academia. We provide some insights into the mechanics of
cryptocurrencies, describing summary statistics and focusing on potential
future research avenues in financial economics.

We discuss common errors and fallacies when using naive "evidence based"
empiricism and point forecasts for fat-tailed variables, as well as the
insufficiency of using naive first-order scientific methods for tail risk
management. We use the COVID-19 pandemic as the background for the discussion
and as an example of a phenomenon characterized by a multiplicative nature, and
what mitigating policies must result from the statistical properties and
associated risks. In doing so, we also respond to the points raised by
Ioannidis et al. (2020).

This paper derives primitive, easily verifiable sufficient conditions for
existence and uniqueness of (stochastic) recursive utilities for several
important classes of preferences. In order to accommodate models commonly used
in practice, we allow both the state-space and per-period utilities to be
unbounded. For many of the models we study, existence and uniqueness is
established under a single, primitive "thin tail" condition on the distribution
of growth in per-period utilities. We present several applications to robust
preferences, models of ambiguity aversion and learning about hidden states, and
Epstein-Zin preferences.

As the world is becoming more dependent on the internet for information
exchange, some overzealous journalists, hackers, bloggers, individuals and
organizations tend to abuse the gift of free information environment by
polluting it with fake news, disinformation and pretentious content for their
own agenda. Hence, there is the need to address the issue of fake news and
disinformation with utmost seriousness. This paper proposes a methodology for
fake news detection and reporting through a constraint mechanism that utilizes
the combined weighted accuracies of four machine learning algorithms.

Estimating linear regression using least squares and reporting robust
standard errors is very common in financial economics, and indeed, much of the
social sciences and elsewhere. For thick tailed predictors under
heteroskedasticity this recipe for inference performs poorly, sometimes
dramatically so. Here, we develop an alternative approach which delivers an
unbiased, consistent and asymptotically normal estimator so long as the means
of the outcome and predictors are finite. The new method has standard errors
under heteroskedasticity which are easy to reliably estimate and tests which
are close to their nominal size. The procedure works well in simulations and in
an empirical exercise. An extension is given to quantile regression.

I have analyzed the practicality of the Evans Rule in the state based forward
guidance and possible ways to reform it. I examined the biases, measurement
errors, and other limitations extant in the unemployment and the inflation rate
in the Evans Rule. Using time series analysis, I calibrated the thresholds of
ECI wage growth and the employment to population ratio and investigated the
relationship between other labor utilization variables. Then I imposed various
shocks and constructed impulse response functions to contrast the paths of
eight macroeconomic variables under three scenarios. The results suggest that
under the wage growth rate scenario, the federal funds rate lift off earlier
than under the current Evans Rule.

We exploit variation in the timing of decriminalization of same-sex sexual
intercourse across U.S. states to estimate the impact of these law changes on
crime through difference-in-difference and event-study models. We provide the
first evidence that sodomy law repeals led to a decline in the number of
arrests for disorderly conduct, prostitution, and other sex offenses.
Furthermore, we show that these repeals led to a reduction in arrests for drug
and alcohol consumption.

This paper considers how to elicit information from sensitive survey
questions. First we thoroughly evaluate list experiments (LE), a leading method
in the experimental literature on sensitive questions. Our empirical results
demonstrate that the assumptions required to identify sensitive information in
LE are violated for the majority of surveys. Next we propose a novel survey
method, called Multiple Response Technique (MRT), for eliciting information
from sensitive questions. We require all of the respondents to answer three
questions related to the sensitive information. This technique recovers
sensitive information at a disaggregated level while still allowing arbitrary
misreporting in survey responses. An application of the MRT provides novel
empirical evidence on sexual orientation and Lesbian, Gay, Bisexual, and
Transgender (LGBT)-related sentiment.

We propose a conceptual framework for counterfactual and welfare analysis for
approximate models. Our key assumption is that model approximation error is the
same magnitude at new choices as the observed data. Applying the framework to
quasilinear utility, we obtain bounds on quantities at new prices using an
approximate law of demand. We then bound utility differences between bundles
and welfare differences between prices. All bounds are computable as linear
programs. We provide detailed analytical results describing how the data map to
the bounds including shape restrictions that provide a foundation for plug-in
estimation. An application to gasoline demand illustrates the methodology.

The Sturgis Motorcycle Rally that took place from August 7-16 was one of the
largest public gatherings since the start of the COVID-19 outbreak. Over
460,000 visitors from across the United States travelled to Sturgis, South
Dakota to attend the ten day event. Using anonymous cell phone tracking data we
identify the home counties of visitors to the rally and examine the impact of
the rally on the spread of COVID-19. Our baseline estimate suggests a one
standard deviation increase in Sturgis attendance increased COVID-19 case
growth by 1.1pp in the weeks after the rally.

This article concerns the tail probabilities of a light-tailed
Markov-modulated L\'evy process stopped at a state-dependent Poisson rate. The
tails are shown to decay exponentially at rates given by the unique positive
and negative roots of the spectral abscissa of a certain matrix-valued
function. We illustrate the use of our results with an application to the
stationary distribution of wealth in a simple economic model in which agents
with constant absolute risk aversion are subject to random mortality and income
fluctuation.

Consequences from the 2019 anti-extradition protests in Hong Kong have been
studied in many facets, but one topic of interest that has not been explored is
the impact on the immigration of Bangladeshi immigrants into the city. This
paper explores the value add of Bangladeshis to the Hong Kong, how the protests
affected their mentality and consequently their immigration, and potentially
longer-term detrimental effects on the city.

This article explores the challenges for the adoption of scrubbers and low
sulfur fuels on ship manufacturers and shipping companies. Results show that
ship manufacturers, must finance their working capital and operating costs,
which implies an increase in the prices of the ships employing these new
technologies. On the other hand, shipping companies must adopt the most
appropriate technology according to the areas where ships navigate, the scale
economies of trade routes, and the cost-benefit analysis of ship modernization.

Empirical evidence for the Heckscher-Ohlin model has been inconclusive. We
test whether the predictions of the Heckscher-Ohlin Theorem with respect to
labor and capital find support in value-added trade. Defining labor-capital
intensities and endowments as the ratio of hours worked to the nominal capital
stock, we find evidence against Heckscher-Ohlin. However, taking the ratio of
total factor compensations, and thus accounting for differences in
technologies, we find strong support for it. That is, labor-abundant countries
tend to export value-added in goods of labor-intensive industries. Moreover,
differentiating between broad industries, we find support for nine out of
twelve industries.

The world's stock markets display a strikingly suspicious pattern of
overnight and intraday returns. Overnight returns to major stock market indices
over the past few decades have been wildly positive, while intraday returns
have been disturbingly negative. The cause of these astonishingly consistent
return patterns is unknown. We highlight the features of these extraordinary
patterns that have hindered the construction of any plausible innocuous
explanation. We then use those same features to deduce the only plausible
explanation so far advanced for these strikingly suspicious returns.

In the single IV model, current practice relies on the first-stage F
exceeding some threshold (e.g., 10) as a criterion for trusting t-ratio
inferences, even though this yields an anti-conservative test. We show that a
true 5 percent test instead requires an F greater than 104.7. Maintaining 10 as
a threshold requires replacing the critical value 1.96 with 3.43. We re-examine
57 AER papers and find that corrected inference causes half of the initially
presumed statistically significant results to be insignificant. We introduce a
more powerful test, the tF procedure, which provides F-dependent adjusted
t-ratio critical values.

A variety of social, economic, and political interactions have long been
modelled after Blotto games. In this paper, we introduce a general model of
dynamic $n$-player Blotto contests. The players have asymmetric resources, and
the battlefield prizes are not necessarily homogeneous. Each player's
probability of winning the prize in a battlefield is governed by a contest
success function and players' resource allocation on that battlefield. We show
that there exists a subgame perfect equilibrium in which players allocate their
resources proportional to the battlefield prizes for every history. This result
is robust to exogenous resource shocks throughout the game.

I study the heterogeneity of credence goods provision in taxi drivers taking
detours in New York City. First, I document that there is significant detouring
on average by drivers. Second, there is significant heterogeneity in cheating
across individuals, yet each individual's propensity to take detours is stable:
drivers who detour almost always detour, while those who do not detour almost
never do. Drivers who take longer detours on each trip also take such trips
more often. Third, cultural attitudes plausibly explain some of this
heterogeneity in behavior across individuals.

In this investigation we analyze impact of diversification of agriculture on
farmer's income, a study from primitive tribal groups from eastern ghats of
India. We have taken crop diversification index to measure the extent and
regression formalism to analyze the impact, of crop diversification.
Descriptive statistics is employed to know the average income of the farmers,
paired results of crop diversification index. We observed a positive impact on
crop diversification in scheduled areas and investigated reasons where it did
not work.

We develop novel quantitative techno-economic evaluation criteria for power
flow allocation schemes. Such schemes assign which nodes are responsible for
which proportion of power flows on a line in a meshed electricity transmission
network. As this allocation is, as such, indeterminate, the literature has
proposed a number of dedicated schemes. To better understand the implications
of applying different schemes, our criteria comprise their (i) fairness, (ii)
plausibility, (iii) uniqueness, and (iv) stability. We apply, illustrate, and
discuss these criteria for four prominent schemes based on results of a
detailed electricity sector model with linear power flow for a German mid-term
future case.

This research is to assess cryptocurrencies with the conditional beta,
compared with prior studies based on unconditional beta or fixed beta. It is a
new approach to building a pricing model for cryptocurrencies. Therefore, we
expect that the use of conditional beta will increase the explanatory ability
of factors in previous pricing models. Besides, this research is also a pioneer
in placing the uncertainty factor in the cryptocurrency pricing model. Earlier
studies on cryptocurrency pricing have ignored this factor. However, it is a
significant factor in the valuation of cryptocurrencies because uncertainty
leads to investor sentiment and affects prices.

In this paper, I present a visual representation of the relationship between
mean hourly total compensation divided by per-capita GDP, hours worked per
capita, and the labor share, and show the represented labor equilibrium
equation is the definition of the labor share. I also present visual
examination of the productivity horizon and wage compression, and use these to
show the relationship between productivity, available employment per capita,
and minimum wage. From this I argue that wages are measured in relation to
per-capita GDP, and that minimum wage controls income inequality and
productivity growth.

The adoption of a "makeup" strategy is one of the proposals in the ongoing
review of the Fed's monetary policy framework. Another suggestion, to avoid the
zero lower bound, is a more active role for fiscal policy. We put together
these ideas to study monetary-fiscal interactions under price level targeting.
Under price level targeting and a fiscally-led regime, we find that following a
deflationary demand shock: (i) the central bank increases (rather than
decreases) the policy rate; (ii) the central bank, thus, avoids the zero lower
bound; (iii) price level targeting is generally welfare improving if compared
to inflation targeting.

Mobile phone-based sports betting has exploded in popularity in many African
countries. Commentators worry that low-ability gamblers will not learn from
experience, and may rely on debt to gamble. Using data on financial
transactions for over 50 000 Kenyan smartphone users, we find that gamblers do
learn from experience. Gamblers are less likely to bet following poor results
and more likely to bet following good results. The reaction to positive and
negative feedback is of equal magnitude and is consistent with a model of
Bayesian updating. Using an instrumental variables strategy, we find no
evidence that increased gambling leads to increased debt.

Do firm dynamics matter for the transmission of monetary policy? Empirically,
the startup rate declines following a monetary contraction, while the exit rate
increases, both of which reduce aggregate employment. I present a model that
combines firm dynamics in the spirit of Hopenhayn (1992) with New-Keynesian
frictions and calibrate it to match cross-sectional evidence. The model can
qualitatively account for the responses of entry and exit rates to a monetary
policy shock. However, the responses of macroeconomic variables closely
resemble those in a representative-firm model. I discuss the equilibrium forces
underlying this approximate equivalence, and what may overturn this result.

COVID-19 has had a much larger impact on the financial markets compared to
previous epidemics because the news information is transferred over the social
networks at a speed of light. Using Twitter's API, we compiled a unique dataset
with more than 26 million COVID-19 related Tweets collected from February 2nd
until May 1st, 2020. We find that more frequent use of the word "stock" in
daily Tweets is associated with a substantial decline in log returns of three
key US indices - Dow Jones Industrial Average, S&P500, and NASDAQ. The results
remain virtually unchanged in multiple robustness checks.

Students of Computer Science often wonder when, exactly, one can apply a
greedy algorithm to a problem, and when one must use the more complicated and
time-consuming techniques of dynamic programming. This paper argues that the
existing pedagogical literature does not offer clear guidance on this issue. We
suggest improving computer science pedagogy by importing a concept economists
use in their own implementations of dynamic programming. That economic concept
is "opportunity cost," and we explain how it can aid students in
differentiating "greedy problems" from problems requiring dynamic programming

There is a growing interest in the integration of energy infrastructures to
increase systems' flexibility and reduce operational costs. The most studied
case is the synergy between electric and heating networks. Even though
integrated heat and power markets can be described by a convex optimization
problem, prices derived from dual values do not guarantee cost recovery. In
this work, a two-step approach is presented for the calculation of the optimal
energy dispatch and prices. The proposed methodology guarantees cost-recovery
for each of the energy vectors and revenue-adequacy for the integrated market.

The COVID-19 crisis has led to the sharpest collapse in the Spanish trade of
goods and services in recent decades. The containment measures adopted to
arrest the spread of the virus have caused an especially intense fall of trade
in services. Spain's export specialization in transport equipment, capital and
outdoor goods, and services that rely on the movement of people has made the
COVID-19 trade crisis more intense in Spain than in the rest of the European
Union. However, the nature of the collapse suggests that trade in goods can
recover swiftly when the health crisis ends. On the other hand, COVID-19 may
have a long-term negative impact on the trade of services that rely on the
movement of people.

This paper shows that modelling comovement in the asymmetry of the predictive
distributions of GDP growth and a timely related series improves nowcasting
uncertainty when it matters most : in times of severe economic downturn. Rather
than using many predictors to nowcast GDP, I show that it is possible to
extract more information than we currently do from series closely related to
economic growth such as employment data. The proposed methodology relies on
score driven techniques and provides an alternative approach for nowcasting
besides dynamic factor models and MIDAS regression where dynamic asymmetry (or
skewness) parameters have not yet been explored.

A minimal central bank credibility, with a non-zero probability of not
renegning his commitment ("quasi-commitment"), is a necessary condition for
anchoring inflation expectations and stabilizing inflation dynamics. By
contrast, a complete lack of credibility, with the certainty that the policy
maker will renege his commitment ("optimal discretion"), leads to the local
instability of inflation dynamics. In the textbook example of the new-Keynesian
Phillips curve, the response of the policy instrument to inflation gaps for
optimal policy under quasi-commitment has an opposite sign than in optimal
discretion, which explains this bifurcation.

Literature about the scholarly impact of scientific research offers very few
contributions on private sector research, and the comparison with public
sector. In this work, we try to fill this gap examining the citation-based
impact of Italian 2010-2017 publications distinguishing authorship by the
private sector from the public sector. In particular, we investigate the
relation between different forms of collaboration and impact: how intra-sector
private publications compare to public, and how private-public joint
publications compare to intra-sector extramural collaborations. Finally, we
assess the different effect of international collaboration on private and
public research impact, and whether there occur differences across research
fields.

We analyse the Bihar assembly elections of 2020, and find that poverty was
the key driving factor, over and above female voters as determinants. The
results show that the poor were more likely to support the NDA. The relevance
of this result for an election held in the midst of a pandemic, is very
crucial, given that the poor were the hardest hit. Secondly, in contrast to
conventional commentary, the empirical results show that the AIMIM-factor and
the LJP-factor hurt the NDA while benefitting the MGB, with their presence in
these elections. The methodological novelty in this paper is combining
elections data with wealth index data to study the effect of poverty on
elections outcomes.

We consider a variant of Cournot competition, where multiple firms allocate
the same amount of resource across multiple markets. We prove that the game has
a unique pure-strategy Nash equilibrium (NE), which is symmetric and is
characterized by the maximal point of a "potential function". The NE is
globally asymptotically stable under the gradient adjustment process, and is
not socially optimal in general. An application is in transportation, where
drivers allocate time over a street network.

This article is a response to a question many economists ask: how can I
improve my first draft? The first section addresses a common approach to doing
this: treating problems visible on the surface. This paper presents six such
symptoms along with treatments for them. The second section addresses another
approach, one that often turns out to be more effective: looking deeper for the
underlying malady that causes several symptoms to show up on the surface and
treating it. This paper presents five common maladies that matter for eventual
outcomes, such as publishing and hiring.

We examine how Green governments influence environmental, macroeconomic, and
education outcomes. We exploit that the Fukushima nuclear disaster in Japan
gave rise to an unanticipated change in government in the German state
Baden-Wuerttemberg in 2011. Using the synthetic control method, we find no
evidence that the Green government influenced CO2 emissions or increased
renewable energy usage overall. The share of wind power usage even decreased.
Intra-ecological conflicts prevented the Green government from implementing
drastic changes in environmental policies. The results do not suggest that the
Green government influenced macroeconomic outcomes. Inclusive education
policies caused comprehensive schools to become larger.

In 2014 the Patient Protection and Affordable Care Act (ACA) introduced the
expansion of Medicaid where states can opt to expand the eligibility for those
in need of free health insurance. In this paper, we attempt to assess the
effectiveness of Medicaid expansion on health outcomes of state populations
using Difference-in-Difference (DD) regressions to seek for causal impacts of
expanding Medicaid on health outcomes in 49 states. We find that in the time
frame of 2013 to 2016, Medicaid expansion seems to have had no significant
impact on the health outcomes of states that have chosen to expand.

This paper serves as a brief technical examination of Net Neutrality and the
Internet fundamentals relevant to the discussion. This document seeks to
provide sufficient technical perspective that it may inform the political and
economic debate surrounding the issue in the United States. Further, this
research demonstrates that existing Internet economics are based strictly on
usage, and that this model can account for all uses. Finally, I will argue that
there should be some legislation and regulation of ISPs with regard to Net
Neutrality in the U.S.

Labor displacement off-or nearshore is a performance improvement instrument
that currently sparks a lot of interest in the service sector. This article
proposes a model to understand the consequences of such a decision on
management consulting firms. Its calibration on the market of consulting
services for the German transportation industry highlights that, under
realistic assumptions, labor displacement translates in price decrease by-0.5%
on average per year and that for MC practices to remain competitive/profitable
they have to at least increase the amount of work they off/nears shore by +0.7%
a year.

The topic of my research is "Learning and Upgrading in Global Value Chains:
An Analysis of India's Manufacturing Sector". To analyse India's learning and
upgrading through position, functions, specialisation & value addition of
manufacturing GVCs, it is required to quantify the extent, drivers, and impacts
of India's Manufacturing links in GVCs. I have transformed this overall broad
objective into three fundamental questions: (1) What is the extent of India's
Manufacturing Links in GVCs? (2) What are the determinants of India's
Manufacturing Links in GVCs? (3) What are the impacts of India's Manufacturing
Links in GVCs? These three objectives represent my three chapters in my PhD
thesis.

We argue that uncertainty network structures extracted from option prices
contain valuable information for business cycles. Classifying U.S. industries
according to their contribution to system-related uncertainty across business
cycles, we uncover an uncertainty hub role for the communications, industrials
and information technology sectors, while shocks to materials, real estate and
utilities do not create strong linkages in the network. Moreover, we find that
this ex-ante network of uncertainty is a useful predictor of business cycles,
especially when it is based on uncertainty hubs. The industry uncertainty
network behaves counter-cyclically in that a tighter network tends to associate
with future business cycle contractions.

We study the impact of fiscal revenue shocks on local fiscal policy. We focus
on the very volatile revenues from the immovable property gains tax in the
canton of Zurich, Switzerland, and analyze fiscal behavior following large and
rare positive and negative revenue shocks. We apply causal machine learning
strategies and implement the post-double-selection LASSO estimator to identify
the causal effect of revenue shocks on public finances. We show that local
policymakers overall predominantly smooth fiscal shocks. However, we also find
some patterns consistent with fiscal conservatism, where positive shocks are
smoothed, while negative ones are mitigated by spending cuts.

This study examines the influence of learning in a female teacher homeroom
class in elementary school on pupils' voting behavior later in life, using
independently collected individual-level data. Further, we evaluate its effect
on preference for women's participation in the workplace in adulthood. Our
study found that having a female teacher in the first year of school makes
individuals more likely to vote for female candidates, and to prefer policy for
female labor participation in adulthood. However, the effect is only observed
among males, and not female pupils. These findings offer new evidence for the
female socialization hypothesis.

Limited-information inference on New Keynesian Phillips Curves (NKPCs) and
other single-equation macroeconomic relations is characterised by weak and
high-dimensional instrumental variables (IVs). Beyond the efficiency concerns
previously raised in the literature, I show by simulation that ad-hoc selection
procedures can lead to substantial biases in post-selection inference. I
propose a Sup Score test that remains valid under dependent data, arbitrarily
weak identification, and a number of IVs that increases exponentially with the
sample size. Conducting inference on a standard NKPC with 359 IVs and 179
observations, I find substantially wider confidence sets than those commonly
found.

This paper studies optimal bundling of products with non-additive values.
Under monotonic preferences and single-peaked profits, I show a monopolist
finds pure bundling optimal if and only if the optimal sales volume for the
grand bundle is larger than the optimal sales volume for any smaller bundle. I
then (i) detail how my analysis relates to "ratio monotonicity" results on
bundling; and (ii) describe the implications for non-linear pricing.

We use a five percent sample of Americans' credit bureau data, combined with
a regression discontinuity approach, to estimate the effect of universal health
insurance at age 65-when most Americans become eligible for Medicare-at the
national, state, and local level. We find a 30 percent reduction in debt
collections-and a two-thirds reduction in the geographic variation in
collections-with limited effects on other financial outcomes. The areas that
experienced larger reductions in collections debt at age 65 were concentrated
in the Southern United States, and had higher shares of black residents, people
with disabilities, and for-profit hospitals.

We propose a multivariate extension of Yaari's dual theory of choice under
risk. We show that a decision maker with a preference relation on
multidimensional prospects that preserves first order stochastic dominance and
satisfies comonotonic independence behaves as if evaluating prospects using a
weighted sum of quantiles. Both the notions of quantiles and of comonotonicity
are extended to the multivariate framework using optimal transportation maps.
Finally, risk averse decision makers are characterized within this framework
and their local utility functions are derived. Applications to the measurement
of multi-attribute inequality are also discussed.

Does the ability to pledge an asset as collateral, after purchase, affect its
price? This paper identifies the impact of collateral service flows on house
prices, exploiting a plausibly exogenous constitutional amendment in Texas
which legalized home equity loans in 1998. The law change increased Texas house
prices 4%; this is price-based evidence that households are credit-constrained
and value home equity loans to facilitate consumption smoothing. Prices rose
more in locations with inelastic supply, higher pre-law house prices, higher
income, and lower unemployment. These estimates reveal that richer households
value the option to pledge their home as collateral more strongly.

I provide evidence that, when income-contingent loans are available, student
enrolment in university courses is not significantly affected by large
increases in the price of those courses. I use publicly available domestic
enrolment data from Australia. I study whether large increases in the price of
higher education for selected disciplines in Australia in 2009 and in 2012 was
associated with changes in their enrolment growth. I find that large increases
in the price of a course did not lead to significant changes in their enrolment
growth for that course.

We propose a multivariate extension of a well-known characterization by S.
Kusuoka of regular and coherent risk measures as maximal correlation
functionals. This involves an extension of the notion of comonotonicity to
random vectors through generalized quantile functions. Moreover, we propose to
replace the current law invariance, subadditivity and comonotonicity axioms by
an equivalent property we call strong coherence and that we argue has more
natural economic interpretation. Finally, we reformulate the computation of
regular and coherent risk measures as an optimal transportation problem, for
which we provide an algorithm and implementation.

In Japan, the increase in the consumption tax rate, a measure of balanced
public finance, reduces the inequality of fiscal burden between the present and
future generations. This study estimates the effect of grandchildren on an
older person's view of consumption tax, using independently collected data. The
results show that having grandchildren is positively associated with supporting
an increase in consumption tax. Further, this association is observed strongly
between granddaughters and grandparents. However, the association between
grandsons and grandparents depends on the sub-sample. This implies that people
of the old generation are likely to accept the tax burden to reduce the burden
on their grandchildren, especially granddaughters. In other words, grandparents
show intergenerational altruism.

Machine learning models are increasingly used in a wide variety of financial
settings. The difficulty of understanding the inner workings of these systems,
combined with their wide applicability, has the potential to lead to
significant new risks for users; these risks need to be understood and
quantified. In this sub-chapter, we will focus on a well studied application of
machine learning techniques, to pricing and hedging of financial options. Our
aim will be to highlight the various sources of risk that the introduction of
machine learning emphasises or de-emphasises, and the possible risk mitigation
and management strategies that are available.

Experimental studies regularly show that third-party punishment (TPP)
substantially exists in various settings. This study further investigates the
robustness of TPP under an environment where context effects are involved. In
our experiment, we offer a third party an additional but unattractive risky
investment option. We find that, when the dominated investment option
irrelevant to prosocial behavior is available, the demand for punishment
decreases, whereas the demand for investment increases. These findings support
our hypothesis that the seemingly unrelated and dominated investment option may
work as a compromise and suggest the fragility of TPP in this setting.

We propose a new measure of deviations from expected utility theory. For any
positive number~$e$, we give a characterization of the datasets with a
rationalization that is within~$e$ (in beliefs, utility, or perceived prices)
of expected utility theory. The number~$e$ can then be used as a measure of how
far the data is to expected utility theory. We apply our methodology to data
from three large-scale experiments. Many subjects in those experiments are
consistent with utility maximization, but not with expected utility
maximization. Our measure of distance to expected utility is correlated with
subjects' demographic characteristics.

This paper investigates various ways in which a pandemic such as the novel
coronavirus, could be predicted using different mathematical models. It also
studies the various ways in which these models could be depicted using various
visualization techniques. This paper aims to present various statistical
techniques suggested by the Centres for Disease Control and Prevention in order
to represent the epidemiological data. The main focus of this paper is to
analyse how epidemiological data or contagious diseases are theorized using any
available information and later may be presented wrongly by not following the
guidelines, leading to inaccurate representation and interpretations of the
current scenario of the pandemic; with a special reference to the Indian
Subcontinent.

Which and how many attributes are relevant for the sorting of agents in a
matching market? This paper addresses these questions by constructing indices
of mutual attractiveness that aggregate information about agents' attributes.
The first k indices for agents on each side of the market provide the best
approximation of the matching surplus by a k-dimensional model. The methodology
is applied on a unique Dutch households survey containing information about
education, height, BMI, health, attitude toward risk and personality traits of
spouses.

In an infinitely repeated pricing game, pricing algorithms based on
artificial intelligence (Q-learning) may consistently learn to charge
supra-competitive prices even without communication. Although concerns on
algorithmic collusion have arisen, little is known on underlying factors. In
this work, we experimentally analyze the dynamics of algorithms with three
variants of experience replay. Algorithmic collusion still has roots in human
preferences. Randomizing experience yields prices close to the static Bertrand
equilibrium and higher prices are easily restored by favoring the latest
experience. Moreover, relative performance concerns also stabilize the
collusion. Finally, we investigate the scenarios with heterogeneous agents and
test robustness on various factors.

Decentralised government levels are often entrusted with the management of
public works and required to ensure well-timed infrastructure delivery to their
communities. We investigate whether monitoring the activity of local procuring
authorities during the execution phase of the works they manage may expedite
the infrastructure delivery process. Focussing on an Italian regional law which
imposes monitoring by the regional government on "strategic" works carried out
by local buyers, we draw causal claims using a regression-discontinuity
approach, made unusual by the presence of multiple assignment variables.
Estimation is performed through discrete-time survival analysis techniques.
Results show that monitoring does expedite infrastructure delivery.

Does the ability to protect an asset from unsecured creditors affect its
price? This paper identifies the impact of bankruptcy protection on house
prices using 139 changes in homestead exemptions. Large increases in the
homestead exemption raised house prices 3% before 2005. Smaller exemption
increases, to adjust for inflation, did not affect house prices. The effect
disappeared after BAPCPA, a 2005 federal law designed to prevent bankruptcy
abuse. The effect was bigger in inelastic locations.

This study examines the influence of grandchildren's gender on grandparents'
voting behavior using independently collected individual-level data. The survey
was conducted immediately after the House of Councilors election in Japan. I
observed that individuals with a granddaughter were more likely to vote for
female candidates by around 10 % than those without. However, having a daughter
did not affect the parents' voting behavior. Furthermore, having a son or a
grandson did not influence grandparents' voting behavior. This implies that
grandparents voted for their granddaughter's future benefit because
granddaughters may be too young vote in a male-dominated and aging society.

This paper presents a model addressing welfare optimal policies of demand
responsive transportation service, where passengers cause external travel time
costs for other passengers due to the route changes. Optimal pricing and trip
production policies are modelled both on the aggregate level and on the network
level. The aggregate model is an extension from Jokinen (2016) with flat
pricing model, but occupancy rate is now modelled as an endogenous variable
depending on demand and capacity levels. The network model enables to describe
differences between routes from the viewpoint of occupancy rate and efficient
trip combining. Moreover, the model defines the optimal differentiated pricing
for routes.

This paper considers the use of instruments to identify and estimate private
and social returns to education within a model of employer learning. What an
instrument identifies depends on whether it is hidden from, or transparent
(i.e., observed) to, the employers. A hidden instrument identifies private
returns to education, and a transparent instrument identifies social returns to
education. We use variation in compulsory schooling laws across non-central and
central municipalities in Norway to, respectively, construct hidden and
transparent instruments. We estimate a private return of 7.9%, of which 70% is
due to increased productivity and the remaining 30% is due to signaling.

Identifying the real causes of democracy is an ongoing debate. We contribute
to the literature by examining the robustness of a comprehensive list of 42
potential determinants of democracy. We take a step forward and employ
Instrumental Variable Bayesian Model Averaging (IVBMA) method to tackle
endogeneity explicitly. Using the data of 111 countries, our IVBMA results mark
arable land as the most persistent predictor of democracy with a posterior
inclusion probability (PIP) of 0.961. Youth population (PIP: 0.893), life
expectancy (PIP: 0.839), and GDP per capita (PIP: 0.758) are the next critical
independent variables. In a subsample of 80 developing countries, in addition
to arable land (PIP: 0.919), state fragility proves to be a significant
determinant of democracy (PIP: 0.779).

An important area of anti-crisis public administration is the development of
small businesses. They are an important part of the economy of developed and
developing countries, provide employment for a significant part of the
population and tax revenues to budgets, and contribute to increased competition
and the development of entrepreneurial abilities of citizens. Therefore, the
primary task of the state Federal and regional policy is to reduce
administrative barriers and risks, time and resources spent on opening and
developing small businesses, problems with small businesses ' access to Bank
capital [8], etc. Despite the loud statements of officials, administrative
barriers to the development of small businesses in trade and public catering
are constantly increasing, including during the 2014-2016 crisis.

We present an innovative framework, Crowdsourcing Autonomous Traffic
Simulation (CATS) framework, in order to safely implement and realize orderly
traffic flows. We firstly provide a semantic description of the CATS framework
using theories of economics to construct coupling constraints among drivers, in
which drivers monitor each other by making use of transportation resources and
driving credit. We then introduce an emotion-based traffic simulation, which
utilizes the Weber-Fechner law to integrate economic factors into drivers'
behaviors. Simulation results show that the CATS framework can significantly
reduce traffic accidents and improve urban traffic conditions.

We investigate the sources of variability in agricultural production and
their relative importance in the context of weather index insurance for
smallholder farmers in India. Using parcel-level panel data, multilevel
modeling, and Bayesian methods we measure how large a role seasonal variation
in weather plays in explaining yield variance. Seasonal variation in weather
accounts for 19-20 percent of total variance in crop yields. Motivated by this
result, we derive pricing and payout schedules for actuarially fair index
insurance. These calculations shed light on the low uptake rates of index
insurance and provide direction for designing more suitable index insurance.

I show that the Zero Lower Bound (ZLB) on interest rates can be used to
identify the causal effects of monetary policy. Identification depends on the
extent to which the ZLB limits the efficacy of monetary policy. I propose a
simple way to test the efficacy of unconventional policies, modelled via a
`shadow rate'. I apply this method to U.S. monetary policy using a
three-equation SVAR model of inflation, unemployment and the federal funds
rate. I reject the null hypothesis that unconventional monetary policy has no
effect at the ZLB, but find some evidence that it is not as effective as
conventional monetary policy.

Difference-in-differences estimation is a widely used method of program
evaluation. When treatment is implemented in different places at different
times, researchers often use two-way fixed effects to control for
location-specific and period-specific shocks. Such estimates can be severely
biased when treatment effects change over time within treated units. I review
the sources of this bias and propose several simple diagnostics for assessing
its likely severity. I illustrate these tools through a case study of free
primary education in Sub-Saharan Africa.

In this work, a class of parabolic economic optimal control problems is
considered. These problems are characterized by pointwise state constraints
regularized by a parameter, which transforms the pure state constraints in
mixed control-state ones. However, the convergence of classical (semismooth)
Newton methods deteriorates for decreasing values of the regularization
parameter. To tackle this problem, a nonlinear preconditioner is introduced.
This is based on an overlapping optimized waveform-relaxation method
characterized by Robin transmission conditions. Numerical experiments show that
appropriate choices of the overlap and of the Robin parameter lead to a
preconditioned Newton method with a robust convergence against the state
constraints regularization parameter.

Carlo Cipolla's stupidity quadrant and his five laws of stupidity were
proposed for the first time in 1976. Exposed in a humorous mood by the author,
these concepts nevertheless describe very serious features of the interactions
among human beings. Here, we propose a new interpretation of Cipolla's ideas in
a biophysical framework, using the well-known predator-prey or "Lotka-Volterra"
model. We find that there is indeed a correspondence between Cipolla's
approach, based on economics, and biophysical economics. On the basis of this
examination, we propose a sixth law of stupidity, additional to the five
proposed by Cipolla. The law states that humans are the stupidest species in
the ecosystem.

We model social media as collections of users producing and consuming
content. Users value consuming content, but doing so uses up their scarce
attention, and hence they prefer content produced by more able users. Users
also value receiving attention, creating the incentive to attract an audience
by producing valuable content, but also through attention bartering -- users
agree to become each others' audience. Attention bartering can profoundly
affect the patterns of production and consumption on social media, explains key
features of social media behavior and platform decision-making, and yields
sharp predictions that are consistent with data we collect from EconTwitter.

We propose a general methodology to measure labour market dynamics, inspired
by the search and matching framework, based on the estimate of the transition
rates between labour market states. We show how to estimate instantaneous
transition rates starting from discrete time observations provided in
longitudinal datasets, allowing for any number of states. We illustrate the
potential of such methodology using Italian labour market data. First, we
decompose the unemployment rate fluctuations into inflow and outflow driven
components; then, we evaluate the impact of the implementation of a labour
market reform, which substantially changed the regulations of temporary
contracts.

A mean-field like stochastic evolution equation with growth and reset terms
(LGGR model) is used to model wealth distribution in modern societies. The
stationary solution of the model leads to an analytical form for the density
function that is successful in describing the observed data for all wealth
categories. In the limit of high wealth values the proposed density function
has the accepted Tsallis-Pareto shape. Our results are in agreement with the
predictions of an earlier approach based on a mean-field like wealth exchange
process.

We live in an age of consumption with an ever-increasing demand of already
scarce resources and equally fast growing problems of waste generation and
climate change. To tackle these difficult issues, we must learn from mother
nature. Just like waste does not exist in nature, we must strive to create
circular ecosystems where waste is minimized and energy is conserved. This
paper focuses on how public procurement can help us transition to a more
circular economy, while navigating international trade laws that govern it.

We study a class of deterministic mean field games on finite and infinite
time horizons arising in models of optimal exploitation of exhaustible
resources. The main characteristic of our game is an absorption constraint on
the players' state process. As a result of the state constraint the optimal
time of absorption becomes part of the equilibrium. This requires a novel
approach when applying Pontyagin's maximum principle. We prove the existence
and uniqueness of equilibria and solve the infinite horizon models in closed
form. As players may drop out of the game over time, equilibrium production
rates need not be monotone nor smooth.

In the wake of COVID-19, every government huddles to find the best
interventions that will reduce the number of infection cases while minimizing
the economic impact. However, with many intervention policies available, how
should one decide which policy is the best course of action? In this work, we
describe an integer programming approach to prescribe intervention plans that
optimizes for both the minimal number of daily new cases and economic impact.
We present a method to estimate the impact of intervention plans on the number
of cases based on historical data. Finally, we demonstrate visualizations and
summaries of our empirical analyses on the performance of our model with
varying parameters compared to two sets of heuristics.

Centralized monetary policy, leading to persistent inflation, is often
inconsistent, untrustworthy, and unpredictable. Algorithmic stablecoins enabled
by blockchain technology are promising in solving this problem. Algorithmic
stablecoins utilize a monetary policy that is entirely rule-based. However,
there is little understanding of how to optimize the rule. We propose a model
that trade-off the price for supply stability. We further study the comparative
statics by varying several design features. Finally, we discuss the empirical
implications for designing stablecoins by the private sector and Central Bank
Digital Currency (CBDC) by the public sector.

This study presents longitudinal evidence on the dissension of Management and
Business Research (MBR) in Latin America and the Caribbean (LAC). It looks
after intellectual bridges linking clusters among such dissension. It was
implemented a coword network analysis to a sample of 12,000+ articles published
by authors from LAC during 1998-2017. Structural network scores showed an
increasing number of keywords and mean degree but decreasing modularity and
density. The intellectual bridges were those of the cluster formed by
disciplines/fields that tend toward consensus (e.g., mathematical models) and
not by core MBR subjects (e.g., strategic planning).

How does an entrepreneur's social capital improve small informal business
productivity? Although studies have investigated this relationship, we still
know little about the underlying theoretical mechanisms driving these findings.
Using a unique Zambian Business Survey of 1,971 entrepreneurs administered by
the World Bank, we find an entrepreneur's social capital facilitates small
business productivity through the mediating channels of firm financing and
customer relationships. Our findings identify specific mechanisms that channel
social capital toward an informal business' productivity, which prior studies
have overlooked.

Economists often estimate economic models on data and use the point estimates
as a stand-in for the truth when studying the model's implications for optimal
decision-making. This practice ignores model ambiguity, exposes the decision
problem to misspecification, and ultimately leads to post-decision
disappointment. Using statistical decision theory, we develop a framework to
explore, evaluate, and optimize robust decision rules that explicitly account
for estimation uncertainty. We show how to operationalize our analysis by
studying robust decisions in a stochastic dynamic investment model in which a
decision-maker directly accounts for uncertainty in the model's transition
dynamics.

Decentralization is a centerpiece in Cameroonian's government institutions'
design. This chapter elaborates a simple hierarchy model for the analysis of
the effects of power devolution. The model predicts overall positive effects of
decentralization with larger effects when the local authority processes useful
information on how to better allocate the resources. The estimation of the
effects of the 2010's power devolution to municipalities in Cameroon suggests a
positive impact of decentralization on early human capital accumulation. The
value added by decentralization is the same for Anglophone and Francophone
municipalities; the effects of decentralization are larger for advanced levels
of primary school.

What are the necessary and sufficient conditions for a proposition to be
called a requirement? In Requirements Engineering research, a proposition is a
requirement if and only if specific grammatical and/or communication conditions
hold. I offer an alternative, that a proposition is a requirement if and only
if specific contractual, economic, and engineering relationships hold. I
introduce and define the concept of "Requirements Contract" which defines these
conditions. I argue that seeing requirements as propositions governed by
specific types of contracts leads to new and interesting questions for the
field, and relates requirements engineering to such topics as economic
incentives, interest alignment, principal agent problem, and decision-making
with incomplete information.

We provide the first direct test of how the credibility of an auction format
affects bidding behavior and final outcomes. To do so, we conduct a series of
laboratory experiments where the role of the seller is played by a human
subject who receives the revenue from the auction and who (depending on the
treatment) has agency to determine the outcome of the auction. Contrary to
theoretical predictions, we find that the non-credible second-price auction
fails to converge to the first-price auction. We provide a behavioral
explanation for our results based on sellers' aversion to rule-breaking, which
is confirmed by an additional experiment.

Black markets can reduce the effects of distortionary regulations by
reallocating scarce resources toward consumers who value them most. The illegal
nature of black markets, however, creates transaction costs that reduce the
gains from trade. We take a partial identification approach to infer gains from
trade and transaction costs in the black market for Beijing car license plates,
which emerged following their recent rationing. We find that at least 11% of
emitted license plates are illegally traded. The estimated transaction costs
suggest severe market frictions: between 61% and 82% of the realized gains from
trade are lost to transaction costs.

A decision maker's utility depends on her action $a\in A \subset
\mathbb{R}^d$ and the payoff relevant state of the world $\theta\in \Theta$.
One can define the value of acquiring new information as the difference between
the maximum expected utility pre- and post information acquisition. In this
paper, I find asymptotic results on the expected value of information as $d \to
\infty$, by using tools from the theory of (sub)-Guassian processes and generic
chaining.

It is well known that rightly applied reverse auctions offer big commercial
potential to procurement departments. However, the sheer number of auction
types often overwhelms users in practice. And since the implications of a
wrongly chosen auction type are equally well known, the overall usage of
reverse auctions lacks its potential significantly. In this paper, a novel
method is being proposed that guides the user in selecting the right
combination of basic auction forms for single lot events, considering both
market-, as well as supplier-related, bijective criteria.

The COVID-19 pandemic forced almost all professional and amateur sports to be
played without attending crowds. Thus, it induced a large-scale natural
experiment on the impact of social pressure on decision making and behavior in
sports fields. Using a data set of 1027 rugby union matches from 11 tournaments
in 10 countries, we find that home teams have won less matches and their points
difference decreased during the pandemics, shedding light on the impact of
crowd attendance on the {\em home advantage} of sports teams.

In this paper we discuss the diffusion of serious games and present reasons
for why Rogers traditional approach is limited in this context. We present an
alternative overview through the characteristics of relative advantage,
compatibility, complexity, trialability, and observability, that reflect on the
adoption decision and contributes on the commercialization of serious games.

A meta-analysis of published estimates shows that the social cost of carbon
has increased as knowledge about climate change accumulates. Correcting for
inflation and emission year and controlling for the discount rate, kernel
density decomposition reveals a non-stationary distribution. In the last 10
years, estimates of the social cost of carbon have increased from $33/tC to
$146/tC for a high discount rate and from $446/tC to $1925/tC for a low
discount rate. Actual carbon prices are almost everywhere below its estimated
value and should therefore go up.

This paper describes a basic model of a gift economy in the shape of a Giving
Game and reveals the fundamental structure of such a game. Main result is that
the game shows a community effect in that a small subgroup of players
eventually keeps all circulating goods for themselves. Example applications are
where computers are sharing processing power for complex calculations, or when
commodity traders are making transactions in some professional community. The
Giving Game may equally well be viewed as a basic model of clientelism or
corruption. Keywords in this paper are giving, gift economy, community effect,
stabilization, computational complexity, corruption, micro-economics, game
theory, stock trading, distributed computing, crypto currency, blockchain.

Based on the Global Entrepreneurship Monitor (GEM) surveys and conducting a
panel data estimation to test our hypothesis, this paper examines whether
corruption perceptions might sand or grease the wheels for entrepreneurship
inside companies or intrapreneurship in a sample of 92 countries for the period
2012 to 2019. Our results find that the corruption perception sands the wheel
for intrapreneurship. There is evidence of a quadratic relation, but this
relation is only clear for the less developed countries, which sort of moderate
the very negative effect of corruption for these countries. The results also
confirm that corruption influences differently on intrapreneurship depending on
the level of development of the country.

Agriculture is arguably the most climate-sensitive sector of the economy.
Growing concerns about anthropogenic climate change have increased research
interest in assessing its potential impact on the sector and in identifying
policies and adaptation strategies to help the sector cope with a changing
climate. This chapter provides an overview of recent advancements in the
analysis of climate change impacts and adaptation in agriculture with an
emphasis on methods. The chapter provides an overview of recent research
efforts addressing key conceptual and empirical challenges. The chapter also
discusses practical matters about conducting research in this area and provides
reproducible R code to perform common tasks of data preparation and model
estimation in this literature. The chapter provides a hands-on introduction to
new researchers in this area.

This paper aims to identify the robust determinants of corruption after
integrating out the effects of spatial spillovers in corruption levels between
countries. In other words, we want to specify which variables play the most
critical role in determining the corruption levels after accounting for the
effects that neighbouring countries have on each other. We collected the annual
data of 115 countries over the 1985-2015 period and used the averaged values to
conduct our empirical analysis. Among 39 predictors of corruption, our spatial
BMA models identify Rule of Law as the most persistent determinant of
corruption.

We benchmark a multi-dimensional child nutrition intervention against an
unconditional cash transfer of equal cost. Randomized variation in transfer
amounts allows us to estimate impacts of cash transfers at expenditure levels
equivalent to the in-kind program, as well as to estimate the return to
increasing cash transfer values. While neither the in-kind program nor a
cost-equivalent transfer costing \$124 per household moves core child outcomes
within a year, cash transfers create significantly greater consumption than the
in-kind alternative. A larger cash transfer costing \$517 substantially
improves consumption and investment outcomes and drives modest improvements in
dietary diversity and child growth.

Stratifying commercial product portfolios into multiple classes of decreasing
priority, ABCD analysis, is a common supply chain tool. Key planning parameters
that drive strategic and execution priorities are tied to the resulting
segmentation. These priorities in turn drive supply chain performance. For
large product assortments, manual segmentation is infeasible so an automated
algorithm is needed. We therefore advocate that careful attention be paid to
the design of such an ABCD algorithm and present three key features that can be
incorporated into such a calculation to improve its quality and commercial
utility.

We use gradient boosting machines and logistic regression to predict academic
throughput at a South African university. The results highlight the significant
influence of socio-economic factors and field of study as predictors of
throughput. We further find that socio-economic factors become less of a
predictor relative to the field of study as the time to completion increases.
We provide recommendations on interventions to counteract the identified
effects, which include academic, psychosocial and financial support.

This paper studies Bayesian games with general action spaces, correlated
types and interdependent payoffs. We introduce the condition of ``decomposable
coarser payoff-relevant information'', and show that this condition is both
sufficient and necessary for the existence of pure-strategy equilibria and
purification from behavioral strategies. As a consequence of our purification
method, a new existence result on pure-strategy equilibria is also obtained for
discontinuous Bayesian games. Illustrative applications of our results to
oligopolistic competitions and all-pay auctions are provided.

In this study, empirical moments and the cointegration for all the liquid
commodity futures traded in the Chinese futures markets are analyzed for the
periods before and after Covid-19, which is important for trading strategies
such as pairs trading. The results show that the positive change in the average
returns of the products such as soybean, corn, corn starch, and iron ore
futures are significantly stronger than other products in the post Covid-19
era, whereas the volatility increased most for silver, petroleum asphalt and
egg futures after the pandemic started. The number of cointegrated pairs are
reduced after the pandemic indicating the differentiation in returns due to the
structural changes caused in the demand and supply conditions across
commodities.

We analyze a series of trials that randomly assigned Wikipedia users in
Germany to different web banners soliciting donations. The trials varied
framing or content of social information about how many other users are
donating. Framing a given number of donors in a negative way increased donation
rates. Variations in the communicated social information had no detectable
effects. The findings are consistent with the results from a survey experiment.
In line with donations being strategic substitutes, the survey documents that
the negative framing lowers beliefs about others' donations. Varying the social
information, in contrast, is ineffective in changing average beliefs.

This paper provides a general overview of different perspectives and studies
on trust, offers a definition of trust, and provides factors that play a
substantial role in developing social trust, and shows from which perspectives
it can be fostered. The results showed that trust is playing an important role
in success for organizations involved in cross-national strategic partnerships.
Trust can reduce transaction costs, promotes inter-organizational
relationships, and improve subordinate relationships between managers.

Sovereign wealth funds are created in those countries whose budget is highly
dependent on market factors, usually world commodity prices. At the same time,
these funds are large institutional investors. An analysis of the nature of
investments by the State Pension Fund Global of Norway showed that investments
of the Fund are based on a seven-level model of diversifying its investments.
This model can also be applied to the investments of the National Wealth Fund
of Russia to increase its profitability.

This paper provides a model to analyze and identify a decision maker's (DM's)
hypothetical reasoning. Using this model, I show that a DM's propensity to
engage in hypothetical thinking is captured exactly by her ability to recognize
implications (i.e., to identify that one hypothesis implies another) and that
this later relation is encoded by a DM's observable behavior. Thus, this
characterization both provides a concrete definition of (flawed) hypothetical
reasoning and, importantly, yields a methodology to identify these judgments
from standard economic data.

Recent trends in academics show an increase in enrollment levels in higher
education Predominantly in Doctoral programmes where individual scholars
institutes and supervisors play the key roles The human factor at receiving end
of academic excellence is the scholar having a supervisor at the facilitating
end In this paper I try to establish the role of different factors and
availability of information about them in forming the basic choice set in a
scholars mind After studying three different groups of individuals who were
subjected to substitutive choices we found that scholars prefer an
approachable, moderately intervening and frequently interacting professor as
their guide

The authors of the article analyze the impact of the global COVID-19 pandemic
on the transport and logistics sector. The research is interdisciplinary in
nature. The purpose of the study is to identify and briefly characterize new
trends in the field of transport and cargo transportation in post-COVID
conditions.

Previous research has shown a relationship between voter characteristics and
voter support for tax bonds. These findings, however, are difficult to
interpret because of the high degree of collinearity across the measures. From
13 demographic measures of voters in a library bond election, seven independent
principal components were extracted which accounted for 95 percent of the
variance. Whereas the direct demographic measures showed inconsistent
relationships with voting, the principal components of low SES, college
experience, female and service job were related to affirmative voting, while
high home value was related to negative voting.

Blockchains are still perceived chiefly as a new technology. But each
blockchain is also a community and a social experiment, built around social
consensus. Here I discuss three examples showing how collective intelligence
can help, threat or capitalize on blockchain-based ecosystems. They concern the
immutability of smart contracts, code transparency and new forms of property.
The examples show that more research, new norms and, eventually, laws are
needed to manage the interaction between collective behaviour and the
blockchain technology. Insights from researchers in collective intelligence can
help society rise up to the challenge.

To mitigate inefficiencies in manual contact tracing processes, Digital
Contact Tracing and Exposure Notifications Systems were developed for use as
public-interest technologies during the SARS-CoV-2 global pandemic. Effective
implementation of these tools requires alignment across several factors,
including local regulations and policies and trust in government and public
health officials. Careful consideration should also be made to minimize any
potential conflicts with existing processes in public health which has
demonstrated effectiveness. Four unique cases-of Ireland, Guayaquil, Haiti, and
the Philippines-detailed in this paper will highlight the importance of
upholding the principles of Scientific Validity, Necessity, Time Boundedness,
and Proportionality.

A tailor-made internet survey experiment provides individuals with
information on their income positions to examine their effects on subjective
well-being. In the first survey, respondents were asked about their household
income and subjective well-being. Based on the data collected, three different
respondents' income positions within the residential locality, within a group
of the same educational background, and cohort were obtained. In the follow-up
survey for the treatment group, respondents are informed of their income
positions and then asked for subjective well-being. Key findings are that,
after obtaining information, a higher individual's income position improves
their subjective well-being. The effects varied according to individual
characteristics and proxies.

This paper studies reputation in the online market for illegal drugs in which
no legal institutions exist to alleviate uncertainty. Trade takes place on
platforms that offer rating systems for sellers, thereby providing an
observable measure of reputation. The analysis exploits the fact that one of
the two dominant platforms unexpectedly disappeared. Re-entering sellers reset
their rating. The results show that on average prices decreased by up to 9% and
that a 1% increase in rating causes a price increase of 1%. Ratings and prices
recover after about three months. We calculate that identified good types earn
1,650 USD more per week.

Using three rounds of NSS datasets, the present paper attempts to understand
the relationship between income inequality and intergenerational income
mobility (IGIM) by segregating generations into social and income classes. The
originality of the paper lies in assessing the IGIM using different approaches,
which we expect to contribute to the existing literature. We conclude that the
country has low-income mobility and high inequality which is no longer
associated with a particular social class in India. Also, both may have a
negative or positive relationship, hence needs to be studied at a regional
level.

The MobilityCoin is a new, all-encompassing currency for the management of
the multimodal urban transportation system. MobilityCoins includes and replaces
various existing transport policy instruments while also incentivizing a shift
to more sustainable modes as well as empowering the public to vote for
infrastructure measures.

The 2017 crackdown on Rakhine Rohingyas by the Myanmar army (Tatmadaw) pushed
more than 600,000 refugees into Bangladesh. Both Western and Islamic countries
denounced Aung Sang Suu Kyis government, but both Asian giants, China and
India, supported Myanmars actions. Both also have high stakes in Myanmar given
their long-term geopolitics and geoeconomic South and Southeast Asian plans. In
spite of Myanmar-based commonalities, Chinas and Indias approaches differ
significantly, predicting equally dissimilar outcomes. This chapter examines
their foreign policy and stakes in Myanmar in order to draw a sketch of the
future of Rakhine Rohingyas stuck in Bangladesh.

Each individual in society experiences an evolution of their income during
their lifetime. Macroscopically, this dynamics creates a statistical
relationship between age and income for each society. In this study, we
investigate income distribution and its relationship with age and identify a
stable joint distribution function for age and income within the United Kingdom
and the United States. We demonstrate a flexible calibration methodology using
panel and population surveys and capture the characteristic differences between
the UK and the US populations. The model here presented can be utilised for
forecasting income and planning pensions.

Mastering semiconductor technology is essential to insert any country into
the trends of the future, such as smart cities, internet of things, space
exploration, etc. In this paper we present the growing annual revenue of the
semiconductor industry in the last 20 years and comment on the importance of
mastering semiconductor production technology and its implications for the
development of a nation.

Why is the U.S. industry-level productivity dispersion countercyclical?
Theoretically, we build a duopoly model in which heterogeneous R&D costs
determine firms' optimal behaviors and the equilibrium technology gap after a
negative profit shock. Quantitatively, we calibrate a parameterized model,
simulate firms' post--shock responses and predict that productivity dispersion
is due to the low-cost firm increasing R&D efforts and the high-cost firm doing
the opposite. Empirically, we construct an index of negative profit shocks and
provide two reduced-form tests for this mechanism.

The story of the Chemical Industry in India is one of outperformance and
promise. A consistent value creator, the chemical industry remains an
attractive hub of opportunities, even in an environment of global uncertainty.
This paper aims to analyze the various driving factors, the performance of the
key players over fundamental analysis, and the various trends that would shape
the performance of the industry due to the various geopolitical and
macroeconomic trends in the post-pandemic world.

We highlight the tension between stability and equality in non transferable
utility matching. We consider many to one matchings and refer to the two sides
of the market as students and schools. The latter have aligned preferences,
which in this context means that a school's utility is the sum of its students'
utilities. We show that the unique stable allocation displays extreme
inequality between matched pairs.

The study compares the competitiveness of three Korean groups raised in
different institutional environments: South Korea, North Korea, and China.
Laboratory experiments reveal that North Korean refugees are less likely to
participate in competitive tournaments than South Koreans and Korean-Chinese
immigrants. Analysis using a choice model with probability weighting suggests
that lower cognitive ability may lead to lower expected performance, more
pessimistic beliefs, and greater aversion to competition.

Rapid rise in income inequality in India is a serious concern. While the
emphasis is on inclusive growth, it seems difficult to tackle the problem
without looking at the intricacies of the problem. The Social Mobility Index is
an important tool that focuses on bringing long-term equality by identifying
priority policy areas in the country. The PCA technique is employed in
computation of the index. Overall, the Union Territory of Delhi ranks first,
with the highest social mobility and the least social mobility is in
Chhattisgarh. In addition, health and education access, quality and equity are
key priority areas that can help improve social mobility in India. Thus, we
conclude that human capital is of great importance in promoting social mobility
and development in the present times.

Corrections among colleagues are an integral part of group work, but people
may take corrections as personal criticism, especially corrections by women. I
study whether people dislike collaborating with someone who corrects them and
more so when that person is a woman. People, including those with high
productivity, are less willing to collaborate with a person who has corrected
them even if the correction improves group performance. Yet, people respond to
corrections by women as negatively as by men. These findings suggest that
although women do not face a higher hurdle, correcting colleagues is costly and
reduces group efficiency.

This study reports on the current state-of-affairs in the funding of
entrepreneurship and innovations in China and provides a broad survey of
academic findings on the subject. We also discuss the implications of these
findings for public policies governing the Chinese financial system,
particularly regulations governing the initial public offering (IPO) process.
We also identify and discuss promising areas for future research.

We introduce systematic tests exploiting robust statistical and behavioral
patterns in trading to detect fake transactions on 29 cryptocurrency exchanges.
Regulated exchanges feature patterns consistently observed in financial markets
and nature; abnormal first-significant-digit distributions, size rounding, and
transaction tail distributions on unregulated exchanges reveal rampant
manipulations unlikely driven by strategy or exchange heterogeneity. We
quantify the wash trading on each unregulated exchange, which averaged over 70%
of the reported volume. We further document how these fabricated volumes
(trillions of dollars annually) improve exchange ranking, temporarily distort
prices, and relate to exchange characteristics (e.g., age and userbase), market
conditions, and regulation.

We provide new identification results for panel data models with peer effects
operating through unobserved individual heterogeneity. The results apply for
general network structures governing peer interactions and allow for correlated
effects. Identification hinges on a conditional mean restriction requiring
exogenous mobility of individuals between groups over time. We apply our method
to surgeon-hospital-year data to study take-up of keyhole surgery for cancer,
finding a positive effect of the average individual heterogeneity of other
surgeons practicing in the same hospital

We fully solve a sorting problem with heterogeneous firms and multiple
heterogeneous workers whose skills are imperfect substitutes. We show that
optimal sorting, which we call mixed and countermonotonic, is comprised of two
regions. In the first region, mediocre firms sort with mediocre workers and
coworkers such that the output losses are equal across all these teams
(mixing). In the second region, a high skill worker sorts with low skill
coworkers and a high productivity firm (countermonotonicity). We characterize
the equilibrium wages and firm values. Quantitatively, our model can generate
the dispersion of earnings within and across US firms.

The study examines the relationship between mobile financial services and
individual financial behavior in India wherein a sizeable population is yet to
be financially included. Addressing the endogeneity associated with the use of
mobile financial services using an instrumental variable method, the study
finds that the use of mobile financial services increases the likelihood of
investment, having insurance and borrowing from formal financial institutions.
Further, the analysis highlights that access to mobile financial services have
the potential to bridge the gender divide in financial inclusion. Fastening the
pace of access to mobile financial services may partially alter pandemic
induced poverty.

Using data from World Bank Enterprises Survey 2014, we find that having a
female owner in India increases firm innovation probability using both input
and output indicators of innovation. We account for possible endogeneity of
female owner variable using a two stage instrumental variable probit model. We
find that the positive effect of female owner variable is observed in the
sub-samples of firms with more access to internal funding, young firms and
firms located in regions with no or less crime This study highlights the need
to promote female entrepreneurship as a potential channel for promoting firm
innovation in India.

This paper studies the content of central bank speech communication from 1997
through 2020 and asks the following questions: (i) What global topics do
central banks talk about? (ii) How do these topics evolve over time? I turn to
natural language processing, and more specifically Dynamic Topic Models, to
answer these questions. The analysis consists of an aggregate study of nine
major central banks and a case study of the Federal Reserve, which allows for
region specific control variables. I show that: (i) Central banks address a
broad range of topics. (ii) The topics are well captured by Dynamic Topic
Models. (iii) The global topics exhibit strong and significant autoregressive
properties not easily explained by financial control variables.

Comparing the results for preference attainment, self-perceived influence and
reputational influence, this paper analyzes the relationship between financial
resources and lobbying influence. The empirical analysis builds on data from an
original survey with 312 Swiss energy policy stakeholders combined with
document data from multiple policy consultation submission processes. The
results show that the distribution of influence varies substantially depending
on the measure. While financial resources for political purposes predict
influence across all measures, the relationship is positive only for some. An
analysis of indirect effects sheds light on the potential mechanisms that
translate financial resources into influence.

How and to what extent will new activities spread through social ties? Here,
we develop a more sophisticated framework than the standard mean-field approach
to describe the diffusion dynamics of multiple activities on complex networks.
We show that the diffusion of multiple activities follows a saddle path and can
be highly unstable. In particular, when the two activities are sufficiently
substitutable, either of them would dominate the other by chance even if they
are equally attractive ex ante. When such symmetry-breaking occurs, any
average-based approach cannot correctly calculate the Nash equilibrium - the
steady state of an actual diffusion process.

The assessing resources dynamics problem, in the context of an economic
system with Gaussian consumption and deterministic productivity, is considered
in this paper. Basically it is presented a discrete time recursive equation
that supports the recourse to the Ornstein-Uhlenbeck diffusion process. Some
assumptions on the regeneration of the process are made, in order to observe
the system equilibrium in what concerns the resources depreciation or
accumulation. The objective of this work is to present a result on the sign of
a ratio that can be used to evaluate harvest procedures in this context.

Health behaviors are plagued by self-control problems, and commitment devices
are frequently proposed as a solution. We show that a simple alternative works
even better: appointments. We randomly offer HIV testing appointments and
financial commitment devices to high-risk men in Malawi. Appointments are much
more effective than financial commitment devices, more than doubling testing
rates. In contrast, most men who take up financial commitment devices lose
their investments. Appointments address procrastination without the potential
drawback of commitment failure, and also address limited memory problems.
Appointments have the potential to increase demand for healthcare in the
developing world.

We discuss how to build ETF risk models. Our approach anchors on i) first
building a multilevel (non-)binary classification/taxonomy for ETFs, which is
utilized in order to define the risk factors, and ii) then building the risk
models based on these risk factors by utilizing the heterotic risk model
construction of https://ssrn.com/abstract=2600798 (for binary classifications)
or general risk model construction of https://ssrn.com/abstract=2722093 (for
non-binary classifications). We discuss how to build an ETF taxonomy using ETF
constituent data. A multilevel ETF taxonomy can also be constructed by
appropriately augmenting and expanding well-built and granular third-party
single-level ETF groupings.

This paper considers how sanctions affected the Iranian economy using a novel
measure of sanctions intensity based on daily newspaper coverage. It finds
sanctions to have significant effects on exchange rates, inflation, and output
growth, with the Iranian rial over-reacting to sanctions, followed up with a
rise in inflation and a fall in output. In absence of sanctions, Iran's average
annual growth could have been around 4-5 per cent, as compared to the 3 per
cent realized. Sanctions are also found to have adverse effects on employment,
labor force participation, secondary and high-school education, with such
effects amplified for females.

I establish a translation invariance property of the Blackwell order over
experiments, show that garbling experiments bring them closer together, and use
these facts to define a cardinal measure of informativeness. Experiment $A$ is
inf-norm more informative (INMI) than experiment $B$ if the infinity norm of
the difference between a perfectly informative structure and $A$ is less than
the corresponding difference for $B$. The better experiment is "closer" to the
fully revealing experiment; distance from the identity matrix is interpreted as
a measure of informativeness. This measure coincides with Blackwell's order
whenever possible, is complete, order invariant, and prior-independent, making
it an attractive and computationally simple extension of the Blackwell order to
economic contexts.

This paper proposes a new way to model behavioral agents in dynamic
macro-financial environments. Agents are described as neural networks and learn
policies from idiosyncratic past experiences. I investigate the feedback
between irrationality and past outcomes in an economy with heterogeneous shocks
similar to Aiyagari (1994). In the model, the rational expectations assumption
is seriously violated because learning of a decision rule for savings is
unstable. Agents who fall into learning traps save either excessively or save
nothing, which provides a candidate explanation for several empirical puzzles
about wealth distribution. Neural network agents have a higher average MPC and
exhibit excess sensitivity of consumption. Learning can negatively affect
intergenerational mobility.

Reverse causality is a common causal misperception that distorts the
evaluation of private actions and public policies. This paper explores the
implications of this error when a decision maker acts on it and therefore
affects the very statistical regularities from which he draws faulty
inferences. Using a quadratic-normal parameterization and applying the
Bayesian-network approach of Spiegler (2016), I demonstrate the subtle
equilibrium effects of a certain class of reverse-causality errors, with
illustrations in diverse areas: development psychology, social policy, monetary
economics and IO. In particular, the decision context may protect the decision
maker from his own reverse-causality causal error. That is, the cost of
reverse-causality errors can be lower for everyday decision makers than for an
outside observer who evaluates their choices.

Classical measures of inequality use the mean as the benchmark of economic
dispersion. They are not sensitive to inequality at the left tail of the
distribution, where it would matter most. This paper presents a new inequality
measurement tool that gives more weight to inequality at the lower end of the
distribution, it is based on the comparison of all value pairs and synthesizes
the dispersion of the whole distribution. The differences that sum to the Gini
coefficient are scaled by angular differences between observations. The
resulting index possesses a set of desirable properties, including
normalization, scale invariance, population invariance, transfer sensitivity,
and weak decomposability.

I study how political bias and audience costs impose domestic institutional
constraints that affect states' capacity to reach peaceful agreements during
crises. With a mechanism design approach, I show that the existence of peaceful
agreements hinges crucially on whether the resource being divided can appease
two sides of the highest type (i.e. the maximum war capacity). The derivation
has two major implications. On the one hand, if war must be averted, then
political leaders are not incentivized by audience costs to communicate private
information; they will pool on the strategy that induces the maximum bargaining
gains. On the other hand, political bias matters for the scope of peace because
it alters a state's expected war payoff.

We provide a generalized revealed preference test for quasilinear
preferences. The test applies to nonlinear budget sets and non-convex
preferences as those found in taxation and nonlinear pricing contexts. We study
the prevalence of quasilinear preferences in a laboratory real-effort task
experiment with nonlinear wages. The experiment demonstrates the empirical
relevance of our test. We find support for either convex (non-separable)
preferences or quasilinear preferences but weak support for the hypothesis of
both quasilinear and convex preferences.

We study the effect of globalization of world economy between 1980 and 2010
by using network analysis technics on trade and GDP data of 71 countries in the
world. We draw results distinguishing relatively developing and relatively
developed countries during this period of time and point out the standing out
economies among the BRICS countries during the years of globalization: within
our context of study, China and Russia are the countries that already exhibit
developed economy characters, India is next in line but have some unusual
features, while Brazil and South Africa still have erratic behaviors

Fairness in vaccination is not only important from a social justice point of
view, but experience has shown that a fair distribution of vaccine proves more
effective in public immunization by preventing highly-concentrated infected
areas to form among the population. In this paper, we address fairness from two
simultaneous points of view: equity and accessibility. Equity in our setting
means that as far as possible, each demand zone should receive a fair-share of
the total doses available. On the other hand, accessibility means that as far
as possible, each demand zone should have equal travel distance to access their
assigned vaccination site.

This paper demonstrates the additive and multiplicative version of a long-run
law of unexpected shocks for any economic variable. We derive these long-run
laws by the martingale theory without relying on the stationary and ergodic
conditions. We apply these long-run laws to asset return, risk-adjusted asset
return, and the pricing kernel process and derive new asset pricing
implications. Moreover, we introduce several dynamic long-term measures on the
pricing kernel process, which relies on the sample data of asset return.
Finally, we use these long-term measures to diagnose leading asset pricing
models.

This paper introduces a transparent framework to identify the informational
content of FOMC announcements. We do so by modelling the expectations of the
FOMC and private sector agents using state of the art computational linguistic
tools on both FOMC statements and New York Times articles. We identify the
informational content of FOMC announcements as the projection of high frequency
movements in financial assets onto differences in expectations. Our recovered
series is intuitively reasonable and shows that information disclosure has a
significant impact on the yields of short-term government bonds.

We study Bayesian coordination games where agents receive noisy private
information over the game's payoffs, and over each others' actions. If private
information over actions is of low quality, equilibrium uniqueness obtains in a
manner similar to a global games setting. On the contrary, if private
information over actions (and thus over the game's payoff coefficient) is
precise, agents can coordinate on multiple equilibria. We argue that our
results apply to phenomena such as bank-runs, currency crises, recessions, or
riots and revolutions, where agents monitor each other closely.

We extend the mathematical model proposed by Ottaviano-Tabuchi-Thisse (2002)
to a multi-regional case and investigate the stability of the homogeneous
stationary solution of the model in a one-dimensional periodic space. When the
number of regions is two and three, the homogeneous stationary solution is
stable under sufficiently high transport cost. On the other hand, when the
number of regions is a multiple of four, the homogeneous stationary solution is
unstable under any values of the transport cost.

This work was partially supported by the Program of Fundamental Research of
the Department of Physics and Astronomy of the National Academy of Sciences of
Ukraine "Mathematical models of non equilibrium processes in open systems" N
0120U100857.

The objective of this paper is to find the existence of a relationship
between stock market prices and the fundamental macroeconomic indicators. We
build a Vector Auto Regression (VAR) model comprising of nine major
macroeconomic indicators (interest rate, inflation, exchange rate, money
supply, gdp, fdi, trade-gdp ratio, oil prices, gold prices) and then try to
forecast them for next 5 years. Finally we calculate cross-correlation of these
forecasted values with the BSE Sensex closing price for each of those years. We
find very high correlation of the closing price with exchange rate and money
supply in the Indian economy.

Correlation and cluster analyses (k-Means, Gaussian Mixture Models) were
performed on Generation Z engagement surveys at the workplace. The clustering
indicates relations between various factors that describe the engagement of
employees. The most noticeable factors are a clear statement about the
responsibilities at work, and challenging work. These factors are essential in
practice. The results of this paper can be used in preparing better
motivational systems aimed at Generation Z employees.

We introduce \emph{informational punishment} to the design of mechanisms that
compete with an exogenous status quo mechanism: Players can send garbled public
messages with some delay, and others cannot commit to ignoring them. Optimal
informational punishment ensures that full participation is without loss, even
if any single player can publicly enforce the status quo mechanism.
Informational punishment permits using a standard revelation principle, is
independent of the mechanism designer's objective, and operates exclusively off
the equilibrium path. It is robust to refinements and applies in
informed-principal settings. We provide conditions that make it robust to
opportunistic signal designers.

This paper examines the effect of the political network of Chinese municipal
leaders on the pricing of municipal corporate bonds. Using municipal leaders'
working experience to measure the political network, we find that this network
reduces the bond issuance yield spreads by improving the credit ratings of the
issuer, the local government financing vehicle. The relationship between
political networks and issuance yield spreads is strengthened in areas where
financial markets and legal systems are less developed.

The paper focuses on the bribery network emphasizing harassment bribery. A
bribery network ends with the police officer whose utility from the bribe is
positive and the approving officer in the network. The persistent nature of
corruption is due to colluding behavior of the bribery networks. The
probability of detection of bribery incidents will help in improving
controlling corruption in society. The asymmetric form of punishment and award
equivalent to the amount of punishment to the network can enhance the
probability of detection of harassment bribery $(p_{h})$ and thus increasing
the probability of detection of overall bribery $(p_{h} \in p)$.

We analyse the money-financed fiscal stimulus implemented in Venice during
the famine and plague of 1629--31, which was equivalent to a 'net-worth
helicopter money' strategy -- a monetary expansion generating losses to the
issuer. We argue that the strategy aimed at reconciling the need to subsidize
inhabitants suffering from containment policies with the desire to prevent an
increase in long-term government debt, but it generated much monetary
instability and had to be quickly reversed. This episode highlights the
redistributive implications of the design of macroeconomic policies and the
role of political economy factors in determining such designs.

We exploit the new country-by-country reporting data of multinational
corporations, with unparalleled country coverage, to reveal the distributional
consequences of profit shifting. We estimate that multinational corporations
worldwide shifted over \$850 billion in profits in 2017, primarily to countries
with effective tax rates below 10\%. Countries with lower incomes lose a larger
share of their total tax revenue due to profit shifting. We further show that a
logarithmic function is better suited for capturing the non-linear relationship
between profits and tax rates than linear or quadratic functions. Our findings
highlight effective tax rates' importance for profit shifting and tax reforms.

We show that public firm profit rates fell by half since 1980. Inferred as
the residual from the rise of US corporate profit rates in aggregate data,
private firm profit rates doubled since 1980. Public firm financial returns
matched their fall in profit rates, while public firm representativeness
increased from 30% to 60% of the US capital stock. These results imply that
time-varying selection biases in extrapolating public firms to the aggregate
economy can be severe.

This study focuses on the impact of digital finance on households. While
digital finance has brought financial inclusion, it has also increased the risk
of households falling into a debt trap. We provide evidence that supports this
notion and explain the channel through which digital finance increases the
likelihood of financial distress. Our results show that the widespread use of
digital finance increases credit market participation. The broadened access to
credit markets increases household consumption by changing the marginal
propensity to consume. However, the easier access to credit markets also
increases the risk of households falling into a debt trap.

Extraordinary fiscal and monetary interventions in response to the COVID-19
pandemic have revived concerns about zombie prevalence in advanced economies.
Within a sample of publicly listed U.S. companies, we find zombie prevalence
and zombie-lending not to be a widespread phenomenon per se. Nevertheless, our
results reveal negative spillovers of zombie-lending on productivity,
capital-growth, and employment-growth of non-zombies as well as on overall
business dynamism. It is predominantly the class of healthy small- and
medium-sized companies that is sensitive to zombie-lending activities, with
financial constraints further amplifying these effects.

The COVID-19 pandemic has been a scourge upon humanity, claiming the lives of
more than 5.1 million people worldwide; the global economy contracted by 3.5%
in 2020. This paper presents a COVID-19 calculator, synthesizing existing
published calculators and data points, to measure the positive U.S.
socio-economic impact of a COVID-19 AI/ML pre-screening solution (algorithm &
application).

We propose a novel framework to analyse the velocity of money in terms of the
contribution (MicroVelocity) of each individual agent, and to uncover the
distributional determinants of aggregate velocity. Leveraging on complete
publicly available transactions data stored in blockchains from four
cryptocurrencies, we empirically find that MicroVelocity i) is very
heterogeneously distributed and ii) strongly correlates with agents' wealth. We
further document the emergence of high-velocity intermediaries, thereby
challenging the idea that these systems are fully decentralised. Further, our
framework and results provide policy insights for the development and analysis
of digital currencies.

We study how Chapter 11 bankruptcies affect local legal labor markets. We
document that bankruptcy shocks increase county legal employment and
corroborate this finding by exploiting a stipulation of the law known as Forum
Shopping during the Court Competition Era (1991-1996). We quantify losses to
local communities from firms forum shopping away from their local area as
follows. First, we calculate the unrealized potential employment gains implied
by our reduced-form results. Second, we structurally estimate a model of legal
labor markets and quantify welfare losses. We uncover meaningful costs to local
communities from lax bankruptcy venue laws.

In this paper, we use deep learning to estimate living conditions in India.
We use both census and surveys to train the models. Our procedure achieves
comparable results to those found in the literature, but for a wide range of
outcomes.

We argue that contemporary stock market designs are, due to traders'
inability to fully express their preferences over the execution times of their
orders, prone to latency arbitrage. In turn, we propose a new order type which
allows traders to specify the time at which their orders are executed after
reaching the exchange. Using this order type, traders can synchronize order
executions across different exchanges, such that high-frequency traders, even
if they operate at the speed of light, can no-longer engage in latency
arbitrage.

To develop public health intervention models using microsimulations,
extensive personal information about inhabitants is needed, such as
socio-demographic, economic and health figures. Data confidentiality is an
essential characteristic of such data, while the data should support realistic
scenarios. Collection of such data is possible only in secured environments and
not directly available for external micro-simulation models. The aim of this
paper is to illustrate a method for construction of synthetic data by
predicting individual features through models based on confidential data on
health and socio-economic determinants of the entire Dutch population.

In the recent times of global Covid pandemic, the Federal Reserve has raised
the concerns of upsurges in prices. Given the complexity of interaction between
inflation and inequality, we examine whether the impact of inflation on
inequality differs among distinct levels of income inequality across the US
states. Results reveal that there is a negative contemporaneous effect of
inflation on the inequality which becomes stronger with higher levels of income
inequality. However, over a one year period, we find higher inflation rate to
further increase income inequality only when income inequality is initially
relatively low.

Machine learning is pervasive. It powers recommender systems such as Spotify,
Instagram and YouTube, and health-care systems via models that predict sleep
patterns, or the risk of disease. Individuals contribute data to these models
and benefit from them. Are these contributions (outflows of influence) and
benefits (inflows of influence) reciprocal? We propose measures of outflows,
inflows and reciprocity building on previously proposed measures of training
data influence. Our initial theoretical and empirical results indicate that
under certain distributional assumptions, some classes of models are
approximately reciprocal. We conclude with several open directions.

This study uses a randomized control trial to evaluate a new program for
increased labor market integration of refugees. The program introduces highly
intensive assistance immediately after the residence permit is granted. The
early intervention strategy contrasts previous integration policies, which
typically constitute low-intensive help over long periods of time. We find
positive effects on employment of the program. The magnitude of the effect is
substantial, corresponding to around 15 percentage points. Our cost estimates
suggest that the new policy is less expensive than comparable labor market
programs used in the past.

Putin's Ukraine war has caused gas prices to skyrocket. Because of Europe's
dependence on Russian gas supplies, we all pay significantly more for heating,
involuntarily helping to fund Russia's war against Ukraine. Based on an
analysis of real-time gas price data, we present a calculation that estimates
every household's financial contribution for heating paid to Russian gas
suppliers daily at current prices - six euros per household and day. We show
ways everyone can save energy and help reduce the dependency on Russian gas
supply.

This paper aims to clarify the relationship between monetary policy shocks
and wage inequality. We emphasize the relevance of within and between wage
group inequalities in explaining total wage inequality in the United States.
Relying on the quarterly data for the period 2000-2020, our analysis shows that
racial disparities explain 12\% of observed total wage inequality.
Subsequently, we examine the role of monetary policy in wage inequality. We do
not find compelling evidence that shows that monetary policy plays a role in
exacerbating the racial wage gap. However, there is evidence that accommodative
monetary policy plays a role in magnifying between group wage inequalities but
the impact occurs after 2008.

This paper examines experimentally how reputational uncertainty and the rate
of change of the social environment determine cooperation. Reputational
uncertainty significantly decreases cooperation, while a fast-changing social
environment only causes a second-order qualitative increase in cooperation. At
the individual level, reputational uncertainty induces more leniency and
forgiveness in imposing network punishment through the link proposal and
removal processes, inhibiting the formation of cooperative clusters. However,
this effect is significant only in the fast-changing environment and not in the
slow-changing environment. A substitution pattern between network punishment
and action punishment (retaliatory defection) explains this discrepancy across
the two social environments.

This study aims to identify the differences in SAPA user interest due to each
route, traffic volumes, and after COVID-19 by the daily feedback from them and
to help develop SAPA plans. Food was the most common opinion. However, for the
route, some showed interest in other options. For the traffic volume, the
difference of interest was also shown in some heavy traffic areas, On the other
hand, the changes in customer needs after the COVID-19 disaster were less
changed.

Several key actors -- police, prosecutors, judges -- can alter the course of
individuals passing through the multi-staged criminal justice system. I use
linked arrest-sentencing data for federal courts from 1994-2010 to examine the
role that earlier stages play when estimating Black-white sentencing gaps. I
find no evidence of sample selection at play in the federal setting, suggesting
federal judges are largely responsible for racial sentencing disparities. In
contrast, I document substantial sample selection bias in two different state
courts systems. Estimates of racial and ethnic sentencing gaps that ignore
selection underestimate the true disparities by 15% and 13% respectively.

The high power RF system will be a significant budgetary driver for any
future collider. An order-of-magnitude improvement in cost/capability is
needed, and as a result, a robust R&D program in next-generation, economical RF
sources is essential. In this paper, we discuss the challenges and
opportunities that arise from advancing the state of the art in these devices.
Specifically, research initiatives in new circuit topologies, advanced
manufacturing techniques, and novel alternatives to conventional RF source
components are discussed.

We aim to contribute to the literature on product space and diversification
by proposing a number of extensions of the current literature: (1) we propose
that the alternative but related idea of a country space also has empirical and
theoretical appeal; (2) we argue that the loss of comparative advantage should
be an integral part of (testing the empirical relevance of) the product space
idea; (3) we propose several new indicators for measuring relatedness in
product space; and (4) we propose a non-parametric statistical test based on
bootstrapping to test the empirical relevance of the product space idea.

This review seeks to present a comprehensive picture of recent discussions in
the social sciences of the anticipated impact of AI on the world of work.
Issues covered include technological unemployment, algorithmic management,
platform work an the politics of AI work. The review identifies the major
disciplinary and methodological perspectives on AI's impact on work, and the
obstacles they face in making predictions. Two parameters influencing the
development and deployment of AI in the economy are highlighted, the capitalist
imperative and nationalistic pressures.

As the widely applied method for measuring matching assortativeness in a
transferable utility matching game, a matching maximum score estimation is
proposed by \cite{fox2010qe}. This article reveals that combining unmatched
agents, transfers, and individual rationality conditions with sufficiently
large penalty terms makes it possible to identify the coefficient parameter of
a single common constant, i.e., matching costs in the market.

We study the allocation of and compensation for occupational COVID-19 risk at
Auburn University, a large public university in the U.S. In Spring 2021,
approximately half of the face-to-face classes had enrollments above the legal
capacity allowed by a public health order, which followed CDC social distancing
guidelines. We find lower-ranked graduate student teaching assistants and
adjunct instructors were systematically recruited to deliver riskier classes.
Using an IV strategy in which teaching risk is shifted by classroom features
(geometry and furniture), we show instructors who taught at least one risky
class earned $7,400 more than those who did not.

Data collected through the National Expert Survey (NES) of the Global
Entrepreneurship Monitor (GEM) are widely used to assess the quality and impact
of national entrepreneurial ecosystems. By focusing on the measurement of the
National Entrepreneurship Context Index (NECI), we argue and show that the
subjective nature of the responses of the national experts precludes meaningful
cross-country analyses and cross-country rankings. Moreover, we show that the
limited precision of the NECI severely constraints the longitudinal assessment
of within-country trends. We provide recommendations for the current use of
NECI data and suggestions for future NES data collections.

This paper highlights the hidden dependence of the basic pricing equation of
a multi-period consumption-based asset pricing model on price and payoff
autocorrelations. We obtain the approximations of the basic pricing equation
that describe the mean price "to-day," mean payoff "next-day," price and payoff
volatilities, and price and payoff autocorrelations. The deep conjunction of
the consumption-based model with other versions of asset pricing, such as
ICAPM, APM, etc. (Cochrane, 2001), emphasizes that our results are valid for
other pricing models.

How does the politician's reputation concern affect information provision
when the information is endogenously provided by a biased lobbyist? I develop a
model to study this problem and show that the answer depends on the
transparency design. When the lobbyist's preference is publicly known, the
politician's reputation concern induces the lobbyist to provide more
information. When the lobbyist's preference is unknown, the politician's
reputation concern may induce the lobbyist to provide less information. One
implication of the result is that given transparent preferences, the
transparency of decision consequences can impede information provision by
moderating the politician's reputational incentive.

What would you do if you were asked to "add" knowledge? Would you say that
"one plus one knowledge" is two "knowledges"? Less than that? More? Or
something in between? Adding knowledge sounds strange, but it brings to the
forefront questions that are as fundamental as they are eclectic. These are
questions about the nature of knowledge and about the use of mathematics to
model reality. In this chapter, I explore the mathematics of adding knowledge
starting from what I believe is an overlooked but key observation: the idea
that knowledge is non-fungible.

Using detailed Norwegian data on earnings and education histories, we
estimate a dynamic structural model of schooling and work decisions that
captures our data's rich patterns over the life-cycle. We validate the model
against variation in schooling choices induced by a compulsory schooling
reform. Our approach allows us to estimate the ex-ante returns to different
schooling tracks at different stages of the life-cycle and quantify the
contribution of option values. We find substantial heterogeneity in returns and
establish crucial roles for option values and re-enrollment in determining
schooling choices and the impact of schooling policies.

Two strategies for boreal forestry with goodwill in estate capitalization are
introduced. A strategy focusing on Real Estate (RE) is financially superior to
Timber Sales (TS). The feasibility of the RE requires the presence of forest
land end users in the real estate market, like insurance companies or
investment trusts, and the periodic boundary condition does not apply.
Commercial thinnings do not enter the RE strategy in a stand-level discussion.
However, they may appear in estates with a variable age structure and enable an
extension of stand rotation times.

This paper empirically evaluates whether adopting a common currency has
changed the level of consumption smoothing of euro area member states. We
construct a counterfactual dataset of macroeconomic variables through the
synthetic control method. We then use the output variance decomposition of
Asdrubali, Sorensen and Yosha (1996) on both the actual and the synthetic data
to study if there has been a change in risk sharing and through which channels.
We find that the euro adoption has reduced risk sharing and consumption
smoothing. We further show that this reduction is mainly driven by the
periphery countries of the euro area who have experienced a decrease in risk
sharing through private credit.

Firms' innovation potential depends on their position in the R&D network. But
details on this relation remain unclear because measures to quantify network
embeddedness have been controversially discussed. We propose and validate a new
measure, coreness, obtained from the weighted k-core decomposition of the R&D
network. Using data on R&D alliances, we analyse the change of coreness for
14,000 firms over 25 years and patenting activity. A regression analysis
demonstrates that coreness explains firms' R&D output by predicting future
patenting.

We elicit incomplete preferences over monetary gambles with subjective
uncertainty. Subjects rank gambles, and these rankings are used to estimate
preferences; payments are based on estimated preferences. About 40\% of
subjects express incompleteness, but they do so infrequently. Incompleteness is
similar for individuals with precise and imprecise beliefs, and in an
environment with objective uncertainty, suggesting that it results from
imprecise tastes more than imprecise beliefs. When we force subjects to choose,
we observe more inconsistencies and preference reversals. Evidence suggests
there is incompleteness that is indirectly revealed -- in up to 98\% of
subjects -- in addition to what we directly measure.

Rejected job applicants seldom receive explanations from employers.
Techniques from Explainable AI (XAI) could provide explanations at scale.
Although XAI researchers have developed many different types of explanations,
we know little about the type of explanations job applicants want. We use a
survey of recent job applicants to fill this gap. Our survey generates three
main insights. First, the current norm of, at most, generic feedback frustrates
applicants. Second, applicants feel the employer has an obligation to provide
an explanation. Third, job applicants want to know why they were unsuccessful
and how to improve.

With the rapid increase in the use of social media in the last decade, the
conspicuous consumption lifestyle within society has been now transferred to
the social media. Along with the changing culture of consumption, the consumer
who witnesses such portrayals on social media aspires to and desires the same
products and services. Having regard to this situation, this study examines the
impact of the conspicuous consumption trend in social media on purchasing
intentions. Accordingly, the study aims to discover whether social media is
being used as a conspicuous consumption channel and whether these conspicuous
portrayals affect purchasing intentions

Accurately estimating income Pareto exponents is challenging due to
limitations in data availability and the applicability of statistical methods.
Using tabulated summaries of incomes from tax authorities and a recent
estimation method, we estimate income Pareto exponents in U.S. for 1916-2019.
We find that during the past three decades, the capital and labor income Pareto
exponents have been stable at around 1.2 and 2. Our findings suggest that the
top tail income and wealth inequality is higher and wealthy agents have twice
as large an impact on the aggregate economy than previously thought but there
is no clear trend post-1985.

This paper constructs a third-step second-order numerical approach for
solving a mathematical model on the dynamic of corruption and poverty. The
stability and error estimates of the proposed technique are analyzed using the
$L^{2}$-norm. The developed algorithm is at less zero-stable and second-order
accurate. Furthermore, the new method is explicit, fast and more efficient than
a large class of numerical schemes applied to nonlinear systems of ordinary
differential equations and can serve as a robust tool for integrating general
systems of initial-value problems. Some numerical examples confirm the theory
and also consider the corruption and poverty in Cameroon.

We review the literature on models that try to explain human behavior in
social interactions described by normal-form games with monetary payoffs. We
start by covering social and moral preferences. We then focus on the growing
body of research showing that people react to the language in which actions are
described, especially when it activates moral concerns. We conclude by arguing
that behavioral economics is in the midst of a paradigm shift towards
language-based preferences, which will require an exploration of new models and
experimental setups.

In this paper we propose a methodology suitable for a comprehensive analysis
of the global embodied energy flow trough a complex network approach. To this
end, we extend the existing literature, providing a multilayer framework based
on the environmentally extended input-output analysis. The multilayer
structure, with respect to the traditional approach, allows us to unveil the
different role of sectors and economies in the system. In order to identify key
sectors and economies, we make use of hub and authority scores, by adapting to
our framework an extension of the Kleinberg algorithm, called Multi-Dimensional
HITS (MD-HITS). A numerical analysis based on multi-region input-output tables
shows how the proposed approach provides meaningful insights.

We reveal a geometric structure underlying both hedging and investment
products. The structure follows from a simple formula expressing investment
risks in terms of returns. This informs optimal product designs. Optimal pure
hedging (including cost-optimal products) and hybrid hedging (where a partial
hedge is built into an optimal investment product) are considered. Duality
between hedging and investment is demonstrated with applications to optimal
risk recycling. A geometric interpretation of rationality is presented.

We propose a methodology to assess transportation accessibility inequity in
metropolitan areas. The methodology is based on the classic analysis tools of
Lorenz curves and Gini indices, but the novelty resides in the fact that it can
be easily applied in an automated way to several cities around the World, with
no need for customized data treatment. Indeed, our equity metrics can be
computed solely relying on open data, publicly available in standardized form.
We showcase our method and study transp

We consider collective decision making when the society consists of groups
endowed with voting weights. Each group chooses an internal rule that specifies
the allocation of its weight to the alternatives as a function of its members'
preferences. Under fairly general conditions, we show that the winner-take-all
rule is a dominant strategy, while the equilibrium is Pareto dominated,
highlighting the dilemma structure between optimality for each group and for
the whole society. We also develop a technique for asymptotic analysis and show
Pareto dominance of the proportional rule. Our numerical computation for the US
Electoral College verifies its sensibility.

Can mobile phone data improve program targeting? By combining rich survey
data from a "big push" anti-poverty program in Afghanistan with detailed mobile
phone logs from program beneficiaries, we study the extent to which machine
learning methods can accurately differentiate ultra-poor households eligible
for program benefits from ineligible households. We show that machine learning
methods leveraging mobile phone data can identify ultra-poor households nearly
as accurately as survey-based measures of consumption and wealth; and that
combining survey-based measures with mobile phone data produces classifications
more accurate than those based on a single data source.

In this paper, I revisit Phillips, Wu and Yu's seminal 2011 paper on testing
for the dot-com bubble. I apply recent advancements of their methods to
individual Nasdaq stocks and use a novel specification for fundamentals. To
address a divide in the literature, I generate a detailed sectoral breakdown of
the dot-com bubble. I find that it comprised multiple overlapping episodes of
exuberance and that there were indeed two starting dates for internet
exuberance.

We analyze the current trends in higher education and discuss its impact on
physics enrollment in US institutions. The pandemic, lockdowns, unemployment,
and healthcare problems have led to unique social and economic conditions.
These conditions have modified the latest trends in education. COVID-19 has had
great impact on the academic culture due to online teaching and learning
methods. We identify some of the key factors including economic problems,
changes in job market, modifications in family obligations, physical and mental
health conditions, and overall insecurity and uncertainty in life. These key
factors are causing a shift in educational preferences. A few recommendations
are made to get out of the current dilemma. We use all the data collected by
the American Physical Society statistics department [1].

Most organizations rely on managers to identify talented workers. However,
managers who are evaluated on team performance have an incentive to hoard
workers. This study provides the first empirical evidence of talent hoarding
using personnel records and survey evidence from a large manufacturing firm.
Talent hoarding is reported by three-fourths of managers, is detectable in
managerial decisions, and occurs more frequently when hoarding incentives are
stronger. Using quasi-random variation in exposure to talent hoarding, I
demonstrate that hoarding deters workers from applying to new positions,
inhibiting worker career progression and altering the allocation of talent in
the firm.

Higher education and advanced scientific research lead to social, economic,
and political development of any country. All developed societies like the
current 2022 G7 countries: Canada, France, Germany, Italy, Japan, the UK, and
the US have all not only heavily invested in higher education but also in
advanced scientific research in their respective countries. Similarly, for
African countries to develop socially, economically, and politically, they must
follow suit by massively investing in higher education and local scientific
research.

Debt aversion can have severe adverse effects on financial decision-making.
We propose a model of debt aversion, and design an experiment involving real
debt and saving contracts, to elicit and jointly estimate debt aversion with
preferences over time, risk and losses. Structural estimations reveal that the
vast majority of participants (89%) are debt averse, and that this has a strong
impact on choice. We estimate the "borrowing premium" - the compensation a debt
averse person would require to accept getting into debt - to be around 16% of
the principal for our average participant.

The purpose of this research was to identify commonly adopted SAPs and their
adoption among Kentucky farmers. The specific objectives were to explore
farmers' Perceptions about farm and farming practice sustainability, to
identify predictors of SAPs adoption using farm attributes, farmers' attitudes
and behaviors, socioeconomic and demographic factors, and knowledge, and to
evaluate adoption barriers of SAPs among Kentucky Farmers. Farmers generally
perceive that their farm and farming activities attain the objectives of
sustainable agriculture. Inadequate knowledge, perceived difficulty of
implementation, lack of market, negative attitude about technologies, and lack
of technologies were major adoption barriers of SAPs in Kentucky.

The purpose of this paper is to investigate if a country quality of
governance moderates the effect of natural disasters on startup activity within
that country. We test our hypotheses using a panel of 95 countries from 2006 to
2016. Our findings suggest that natural disasters discourage startup activity
in countries that have low quality governance but encourage startup activity in
countries that have high quality governance. Moreover, our estimates reveal
that natural disasters effects on startup activity persist for the short term
(1-3 years) but not the long term. Our findings provide new insights into how
natural disasters affect entrepreneurship activity and highlight the importance
of country governance during these events.

How accurately can behavioral scientists predict behavior? To answer this
question, we analyzed data from five studies in which 640 professional
behavioral scientists predicted the results of one or more behavioral science
experiments. We compared the behavioral scientists' predictions to random
chance, linear models, and simple heuristics like "behavioral interventions
have no effect" and "all published psychology research is false." We find that
behavioral scientists are consistently no better than - and often worse than -
these simple heuristics and models. Behavioral scientists' predictions are not
only noisy but also biased. They systematically overestimate how well
behavioral science "works": overestimating the effectiveness of behavioral
interventions, the impact of psychological phenomena like time discounting, and
the replicability of published psychology research.

The paper contains the online supplementary materials for "Data-Driven
Prediction and Evaluation on Future Impact of Energy Transition Policies in
Smart Regions". We review the renewable energy development and policies in the
three metropolitan cities/regions over recent decades. Depending on the
geographic variations in the types and quantities of renewable energy resources
and the levels of policymakers' commitment to carbon neutrality, we classify
Singapore, London, and California as case studies at the primary, intermediate,
and advanced stages of the renewable energy transition, respectively.

We measure bond and stock conditional return volatility as a function of
changes in sentiment, proxied by six indicators from the Tel Aviv Stock
Exchange. We find that changes in sentiment affect conditional volatilities at
different magnitudes and often in an opposite manner in the two markets,
subject to market states. We are the first to measure bonds conditional
volatility of retail investors sentiment thanks to a unique dataset of
corporate bond returns from a limit-order-book with highly active retail
traders. This market structure differs from the prevalent OTC platforms, where
institutional investors are active yet less prone to sentiment.

Standard rational expectations models with an occasionally binding zero lower
bound constraint either admit no solutions (incoherence) or multiple solutions
(incompleteness). This paper shows that deviations from full-information
rational expectations mitigate concerns about incoherence and incompleteness.
Models with no rational expectations equilibria admit self-confirming
equilibria involving the use of simple mis-specified forecasting models.
Completeness and coherence is restored if expectations are adaptive or if
agents are less forward-looking due to some information or behavioral friction.
In the case of incompleteness, the E-stability criterion selects an
equilibrium.

Selection on moral hazard represents the tendency to select a specific health
insurance coverage depending on the heterogeneity in utilisation ''slopes''. I
use data from the Swiss Household Panel and from publicly available regulatory
data to explore the extent of selection on slopes in the Swiss managed
competition system. I estimate responses in terms of (log) doctor visits to
lowest and highest deductible levels using Roy-type models, identifying
marginal treatment effects with local instrumental variables. The response to
high coverage plans (i.e. plans with the lowest deductible level) among high
moral hazard types is 25-35 percent higher than average.

An integrated and widespread road system, like the one built during the Roman
Empire in Italy, plays an important role today in facilitating the construction
of new infrastructure. This paper investigates the historical path of Roman
roads as main determinant of both motorways and railways in the country. The
empirical analysis shows how the modern Italian transport infrastructure
followed the path traced in ancient times by the Romans in constructing their
roads. Being paved and connecting Italy from North to South, consular
trajectories lasted in time, representing the starting physical capital for
developing the new transport networks.

This paper examines the interplay between desegregation, institutional bias,
and individual behavior in education. Using a game-theoretic model that
considers race-heterogeneous social incentives, the study investigates the
effects of between-school desegregation on within-school disparities in
coursework. The analysis incorporates a segregation measure based on entropy
and proposes an optimization-based approach to evaluate the impact of student
reassignment policies. The results highlight that Black and Hispanic students
in predominantly White schools, despite receiving less encouragement to apply
to college, exhibit higher enrollment in college-prep coursework due to
stronger social incentives from their classmates' coursework decisions.

The temperature targets in the Paris Agreement cannot be met without very
rapid reduction of greenhouse gas emissions and removal of carbon dioxide from
the atmosphere. The latter requires large, perhaps prohibitively large
subsidies. The central estimate of the costs of climate policy, unrealistically
assuming least-cost implementation, is 3.8-5.6\% of GDP in 2100. The central
estimate of the benefits of climate policy, unrealistically assuming constant
vulnerability, is 2.8-3.2\% of GDP. The uncertainty about the benefits is
larger than the uncertainty about the costs. The Paris targets do not pass the
cost-benefit test unless risk aversion is high and discount rate low.

We study the influence of social messages that promote a digital public good,
a COVID-19 tracing app. We vary whether subjects receive a digital message from
another subject, and, if so, at what cost it came. Observed maximum willingness
to invest in sending varies, from 1 cent up to 20 euros. Does this affect
receivers' sending behavior? Willingness to invest in sending increases when
previously receiving the message. Yet, cost signals have no impact. Thus,
grassroots movements can be started at virtually no cost. App-support matters
normatively as non-supporters are supposed to be punished in triage.

In the backdrop of growing power transitions and rise of strategic high tide
in the world, the Indo Pacific Region (IPR) has emanated as an area which is
likely to witness increased development & enhanced security cooperation through
militarization. With China trying to be at the seat of the Global leadership,
US & its allies in the Indo pacific are aiming at working together, finding
mutually beneficial areas of functional cooperation and redefining the canvas
of security. This purpose of this paper is to analyze an informal alliance,
Quadrilateral Security dialogue (QUAD) in its present form with the
geostrategic landscape in Indo pacific and present recommendations to
strengthen collaboration & response capability.

The August 2022 Alaska Special Election for US House contained many
interesting features from the perspective of social choice theory. This
election used instant runoff voting (often referred to as ranked choice voting)
to elect a winner, and many of the weaknesses of this voting method were on
display in this election. For example, the Condorcet winner is different from
the instant runoff winner, and the election demonstrated a monotonicity
paradox. The election also demonstrated a no show paradox; as far as we are
aware, this election represents the first document American ranked choice
election to demonstrate this paradox.

I show that a class of Linear DSGE models with one endogenous state variable
can be represented as a three-state Markov chain. I develop a new analytical
solution method based on this representation, which amounts to solving for a
vector of Markov states and one transition probability. These two objects
constitute sufficient statistics to compute in closed form objects that have
routinely been computed numerically: impulse response function, cumulative sum,
present discount value multiplier. I apply the method to a standard New
Keynesian model that features optimal monetary policy with commitment.

Either saying that the market is structured to promote workforce exploitation
in some sections or the people who participate there benefit from the existing
structure and exploit, exploitation happens - systematically or
opportunistically. This research presents a perspective on how workforce
exploitation may occur when vulnerable groups voluntarily seek employment in
the labor market.

We present results of an experiment benchmarking a workforce training program
against cash transfers for underemployed young adults in Rwanda. 3.5 years
after treatment, the training program enhances productive time use and asset
investment, while the cash transfers drive productive assets, livestock values,
savings, and subjective well-being. Both interventions have powerful effects on
entrepreneurship. But while labor, sales, and profits all go up, the implied
wage rate in these businesses is low. Our results suggest that credit is a
major barrier to self-employment, but deeper reforms may be required to enable
entrepreneurship to provide a transformative pathway out of poverty.

We propose a game-theoretic model to investigate how non-superpowers with
heterogenous preferences and endowments shape the superpower competition for a
sphere of influence. Two superpowers play a Stackelberg game by providing club
goods. Their utility depends on non-superpowers who form coalitions to join a
club in the presence of externality. The coalition formation, which depends on
the characteristics of non-superpowers, influences the behavior of superpowers
and thus the size of their clubs. Our data-based simulations of the subgame
perfect equilbirum capture how the US-China competition depends on other
countries.

We experimentally study a game in which success requires a sufficient total
contribution by members of a group. There are significant uncertainties
surrounding the chance and the total effort required for success. A theoretical
model with max-min preferences towards ambiguity predicts higher contributions
under ambiguity than under risk. However, in a large representative sample of
the Spanish population (1,500 participants) we find that the ATE of ambiguity
on contributions is zero. The main significant interaction with the personal
characteristics of the participants is with risk attitudes, and it increases
contributions. This suggests that policymakers concerned with ambiguous
problems (like climate change) do not need to worry excessively about
ambiguity.

Environmental degradation, global pandemic and severing natural resource
related problems cater to increase demand resulting from migration is nightmare
for all of us. Huge flocks of people are rushing towards to earn, to live and
to lead a better life. This they do for their own development often ignoring
the environmental cost. With existing model, this paper looks at out migration
(interstate) within India focusing on the various proximate and fundamental
causes relating to migration. The author deploys OLS to see those fundamental
causes. Obviously, these are not exhaustive cause, but definitely plays a role
in migration decision of individual. Finally, this paper advocates for some
policy prescription to cope with this problem.

The need to understand the factors that come to bear in the financial
inclusion on the indigenous peoples in Nigeria necessitated the study. The need
is pressing because scholars have established that the financial inclusion is
crucial to the socio-cultural and economic development of the indigenous
peoples.

Considering collaborative patent development, we provide micro-level evidence
for innovation through exchanges of differentiated knowledge. Knowledge
embodied in a patent is proxied by word pairs appearing in its abstract, while
novelty is measured by the frequency with which these word pairs have appeared
in past patents. Inventors are assumed to possess the knowledge associated with
patents in which they have previously participated. We find that collaboration
by inventors with more mutually differentiated knowledge sets is likely to
result in patents with higher novelty.

In peer mechanisms, the competitors for a prize also determine who wins. Each
competitor may be asked to rank, grade, or nominate peers for the prize. Since
the prize can be valuable, such as financial aid, course grades, or an award at
a conference, competitors may be tempted to manipulate the mechanism. We survey
approaches to prevent or discourage the manipulation of peer mechanisms. We
conclude our survey by identifying several important research challenges.

In this article we survey the main research topics of our group at the
University of Essex. Our research interests lie at the intersection of
theoretical computer science, artificial intelligence, and economic theory. In
particular, we focus on the design and analysis of mechanisms for systems
involving multiple strategic agents, both from a theoretical and an applied
perspective. We present an overview of our group's activities, as well as its
members, and then discuss in detail past, present, and future work in
multi-agent systems.

We show that capital flow (CF) volatility exerts an adverse effect on
exchange rate (FX) volatility, regardless of whether capital controls have been
put in place. However, this effect can be significantly moderated by certain
macroeconomic fundamentals that reflect trade openness, foreign assets
holdings, monetary policy easing, fiscal sustainability, and financial
development. Passing the threshold levels of these macroeconomic fundamentals,
the adverse effect of CF volatility may be negligible. We further construct an
intuitive FX resilience measure, which provides an assessment of the strength
of a country's exchange rates.

Can proximity make friendships more diverse? To address this question, we
propose a learning-driven friendship formation model to study how proximity and
similarity influence the likelihood of forming social connections. The model
predicts that proximity affects more friendships between dissimilar than
similar individuals, in opposition to a preference-driven version of the model.
We use an experiment at selective boarding schools in Peru that generates
random variation in the physical proximity between students to test these
predictions. The empirical evidence is consistent with the learning model:
while social networks exhibit homophily by academic achievement and poverty,
proximity generates more diverse social connections.

Recent economic developments of countries like Japan, Korea, and Singapore,
as a result of improvement in the quality of their education, show that having
a high-quality education may lead to economic growth. In this article, using
some statistical methods, we argue that high quality education can change the
economy towards higher growth. Therefore, for the development of the country,
one should think about how to improve its education. One of the effective ways
to improve the quality of education is to increase the efficiency of teachers
and attract talented people to teaching positions. Research shows that raising
teachers' salaries, along with a proper quality improvement program, can help
facilitate this process.

A simple model is introduced to study the cooperative behavior of nations
regarding solar geoengineering. The results of this model are explored through
numerical methods. A general finding is that cooperation and coordination
between nations on solar geoengineering is very much incentivized. Furthermore,
the stability of solar geoengineering agreements between nations crucially
depends on the perceived riskiness of solar geoengineering. If solar
geoengineering is perceived as riskier, the stability of the most stable solar
geoengineering agreements is reduced. However, the stability of agreements is
completely independent of countries preferences.

We discuss the relationships between the outcome of the COVID-19 pandemic in
Brazil at the municipal level and different health, social, demographic, and
economic indices. We obtain significant correlations between the data gathered
for each municipalitiy and the proportion of cases and deaths by COVID-19 and
the results by municipality of the 2018 Brazilian presidential election. We
obtain different estimates for the number of deaths caused by central
government denialism of scientific facts and measures for mitigation of the
pandemic and its the historical, economic, and social roots.

The fixed-event forecasting setup is common in economic policy. It involves a
sequence of forecasts of the same (`fixed') predictand, so that the difficulty
of the forecasting problem decreases over time. Fixed-event point forecasts are
typically published without a quantitative measure of uncertainty. To construct
such a measure, we consider forecast postprocessing techniques tailored to the
fixed-event case. We develop regression methods that impose constraints
motivated by the problem at hand, and use these methods to construct prediction
intervals for gross domestic product (GDP) growth in Germany and the US.

In the prospect theory, value function is typically concave for gains,
commonly convex for losses, with losses usually having a steeper slope than
gains. The neural system largely differs from the loss and gains sides. Five
new studies on neurons related to this issue have examined neuronal responses
to losses, gains, and reference points. This study investigates a new concept
of the value function. A value function with a neuronal cusp may show
variations and behavior cusps with catastrophe where a trader closes one's
position.

Consumers are sensitive to medical prices when consuming care, but delays in
price information may distort moral hazard. We study how medical bills affect
household spillover spending following utilization, leveraging variation in
insurer claim processing times. Households increase spending by 22\% after a
scheduled service, but then reduce spending by 11\% after the bill arrives.
Observed bill effects are consistent with resolving price uncertainty; bill
effects are strongest when pricing information is particularly salient. A model
of demand for healthcare with delayed pricing information suggests households
misperceive pricing signals prior to bills, and that correcting these
perceptions reduce average (median) spending by 16\% (7\%) annually.

Recent efforts have been very successful in accurately mapping welfare in
datasparse regions of the world using satellite imagery and other
non-traditional data sources. However, the literature to date has focused on
predicting a particular class of welfare measures, asset indices, which are
relatively insensitive to short term fluctuations in well-being. We suggest
that predicting more volatile welfare measures, such as consumption
expenditure, substantially benefits from the incorporation of data sources with
high temporal resolution. By incorporating daily weather data into training and
prediction, we improve consumption prediction accuracy significantly compared
to models that only utilize satellite imagery.

We develop an experimentally validated, short and easy-to-use survey module
for measuring individual debt aversion. To this end, we first estimate debt
aversion on an individual level, using choice data from Meissner and Albrecht
(2022). This data also contains responses to a large set of debt aversion
survey items, consisting of existing items from the literature and novel items
developed for this study. Out of these, we identify a survey module comprising
two qualitative survey items to best predict debt aversion in the incentivized
experiment.

This gives two existence results of alpha-core solutions by introducing
P-open conditions and strong P-open conditions into games without ordered
preferences. The existence of alpha-core solutions is obtained for games with
infinite-players. Secondly, it provides a short proof of Kajii's (Journal of
Economic Theory 56, 194-205, 1992) existence theorem for alpha-core solutions,
further, the Kajii's theorem is equivalent to the Browder fixed point theorem.
In addition, the obtained existence results can include many typical results
for alpha-core solutions and some recent existence results as special cases.

This paper studies the influences of a high-frequency trader (HFT) on a large
trader whose future trading is predicted by the former. We conclude that HFT
always front-runs and the large trader is benefited when: (1) there is
sufficient high-speed noise trading; (2) HFT's prediction is vague enough.
Besides, we find surprisingly that (1) making HFT's prediction less accurate
might decrease large trader's profit; (2) when there is little high-speed noise
trading, although HFT nearly does nothing, the large trader is still hurt.

This paper proposes a brand-new measure of energy efficiency at household
level and explores how it is affected by access to credit. We calculate the
energy and carbon intensity of the related sectors, which experience a
substantial decline from 2005 to 2019. Although there is still high inequality
in energy use and carbon emissions among Chinese households, the energy
efficiency appears to be improved in long run. Our research further maps the
relationship between financial market and energy. The results suggest that
broadened access to credit encourages households to improve energy efficiency,
with higher energy use and carbon emission.

With the development of Internet technology, the issue of privacy leakage has
attracted more and more attention from the public. In our daily life, mobile
phone applications and identity documents that we use may bring the risk of
privacy leakage, which had increasingly aroused public concern. The path of
privacy protection in the digital age remains to be explored. To explore the
source of this risk and how it can be reduced, we conducted this study by using
personal experience, collecting data and applying the theory.

Due to increasing environmental and economic constraints, optimization of ion
beam transport and equipment design becomes essential. The future should be
equipped with planet-friendly facilities, that is, solutions that reduce
environmental impact and improve economic competitiveness. The tendency to
increase the intensity of the current and the power of the beams obliges us and
brings us to new challenges. Installations tend to have larger dimensions with
increased areas, volumes, weights and costs. A new ion beam transport prototype
was developed and used as a test bed to identify key issues to reduce beam
losses and preserve transverse phase-space distributions with large acceptance
conditions.

This paper shows that a non-price intervention which increased the prevalence
of a new technology facilitated its further adoption. The BlueLA program put
Electric Vehicles (EVs) for public use in many heavily trafficked areas,
primarily (but not exclusively) aimed at low-to-middle income households. We
show, using data on subsidies for these households and a difference-in
differences strategy, that BlueLA is associated with a 33\% increase of new EV
adoptions, justifying a substantial portion of public investment. While the
program provides a substitute to car ownership, our findings are consistent
with the hypothesis that increasing familiarity with EVs could facilitate
adoption.

The article examines how institutions, automation, unemployment and income
distribution interact in the context of a neoclassical growth model where
profits are interpreted as a surplus over costs of production. Adjusting the
model to the experience of the US economy, I show that joint variations in
labor institutions and technology are required to provide reasonable
explanations for the behavior of income shares, capital returns, unemployment,
and the big ratios in macroeconomics. The model offers new perspectives on
recent trends by showing that they can be analyzed by the interrelation between
the profit-making capacity of capitalist economies and the political
environment determining labor institutions.

The existence of a (partial) market equilibrium price is proved in a
complete, continuous time finite-agent market setting. The economic agents act
as price takers in a fully competitive setting and maximize exponential utility
from terminal wealth. As the number $N$ of economic agents goes to infinity,
the BSDE system of $N$ equations characterizing the equilibrium asset price
dynamics decouples. Due to the system's symmetry, the influence of the mean
field of the agents, conditionally on the common noise, becomes deterministic.

Our main contribution is that we are using AI to discern the key drivers of
variation of ESG mentions in the corporate filings. With AI, we are able to
separate "dimensions" along which the corporate management presents their ESG
policies to the world. These dimensions are 1) diversity, 2) hazardous
materials, and 3) greenhouse gasses. We are also able to identify separate
"background" dimensions of unofficial ESG activity in the firms, which provide
more color into the firms and their shareholders' thinking about their ESG
processes. We then measure investors' response to the ESG activity "factors".
The AI techniques presented can assist in building better, more reliable and
useful ESG ratings systems.

Inflation is painful, for firms, customers, employees, and society. But
careful study of periods of hyperinflation point to ways that firms can adapt.
In particular, companies need to think about how to change prices regularly and
cheaply, because constant price changes can ultimately be very, very expensive.
And they should consider how to communicate those price changes to customers.
Providing clarity and predictability can increase consumer trust and help firms
in the long run.

Although it has been suggested that the shift from on-site work to telework
will change the city structure, the mechanism of this change is not clear. This
study clarifies how the location of firms changes when the cost of teleworking
decreases and how this affects the urban economy. The two main results obtained
are as follows. (i) The expansion of teleworking causes firms to be located
closer to urban centers or closer to urban fringes. (ii) Teleworking makes
urban production more efficient and cities more compact. This is the first
paper to show that two empirical studies can be represented in a unified
theoretical model and that existing studies obtained by simulation can be
explained analytically.

I study a two-sided marriage market in which agents have incomplete
preferences -- i.e., they find some alternatives incomparable. The strong
(weak) core consists of matchings wherein no coalition wants to form a new
match between themselves, leaving some (all) agents better off without harming
anyone. The strong core may be empty, while the weak core can be too large. I
propose the concept of the "compromise core" -- a nonempty set that sits
between the weak and the strong cores. Similarly, I define the men-(women-)
optimal core and illustrate its benefit in an application to India's
engineering college admissions system.

Permissionless blockchains offer an information environment where users can
interact privately without fear of censorship. Financial services can be
programmatically coded via smart contracts to automate transactions without the
need for human intervention or knowing user identity. This new paradigm is
known as decentralized finance (DeFi). We investigate Compound (a leading DeFi
lending protocol) to show how it works in this novel information environment,
who its users are, and what factors determine their participation. On-chain
transaction data shows that loan durations are short (31 days on average), and
many users borrow to support leveraged investment strategies (yield farming).
We show that systemic risk in DeFi arises from concentration and
interconnection, and how traditional risk management practices can be
challenging for DeFi.

We revisit the results of a recent paper by Equipo Anova, who claim to find
evidence of an improvement in Venezuelan imports of food and medicines
associated with the adoption of U.S. financial sanctions towards Venezuela in
2017. We show that their results are consequence of data coding errors and
questionable methodological choices, including the use an unreasonable
functional form that implies a counterfactual of negative imports in the
absence of sanctions, the omission of data accounting for four-fifths of the
country's food imports at the time of sanctions and incorrect application of
regression discontinuity methods. Once these errors are corrected, the evidence
of a significant improvement in the level and rate of change in imports of
essentials disappears.

This article estimates the impact of violence on emigration crossings from
Guatemala to Mexico as final destination during 2009-2017. To identify causal
effects, we use as instruments the variation in deforestation in Guatemala, and
the seizing of cocaine in Colombia. We argue that criminal organizations
deforest land in Guatemala, fueling violence and leading to emigration,
particularly during exogenous supply shocks to cocaine. A one-point increase in
the homicide rate differential between Guatemalan municipalities and Mexico,
leads to 211 additional emigration crossings made by male adults. This rise in
violence, also leads to 20 extra emigration crossings made by children.

This short paper compiles the big ideas behind some philosophical views,
definitions, and examples of causality. This collection spans the realms of the
four commonly adopted approaches to causality: Humes regularity,
counterfactual, manipulation, and mechanisms. This short review is motivated by
presenting simplified views and definitions and then supplements them with
examples from various fields, including economics, education, medicine,
politics, physics, and engineering. It is the hope that this short review comes
in handy for new and interested readers with little knowledge of causality and
causal inference.

The COVID-19 vaccine reduces infection risk: even if one contracts COVID-19,
the probability of complications like death or hospitalization is lower.
However, vaccination may prompt people to decrease preventive behaviors, such
as staying indoors, handwashing, and wearing a mask. Thereby, if vaccinated
people pursue only their self-interest, the vaccine's effect may be lower than
expected. However, if vaccinated people are pro-social (motivated toward
benefit for the whole society), they might maintain preventive behaviors to
reduce the spread of infection.

We investigate the elasticity of portfolio investment to geographical
distance in a gravity model utilizing a bilateral panel of 86 reporting and 241
counterparty countries/territories for 2007-2017. We find that the elasticity
is more negative for ASEAN than OECD members. The difference is larger if we
exclude Singapore. This indicates that Singapore's behavior is very different
from other ASEAN members. While Singapore tends to invest in faraway OECD
countries, other ASEAN members tend to invest in nearby countries. Our study
also shows the emergence of China as a significant investment destination for
ASEAN members.

Using deep learning techniques, we introduce a novel measure for production
process heterogeneity across industries. For each pair of industries during
1990-2021, we estimate the functional distance between two industries'
production processes via deep neural network. Our estimates uncover the
underlying factors and weights reflected in the multi-stage production decision
tree in each industry. We find that the greater the functional distance between
two industries' production processes, the lower are the number of M&As, deal
completion rates, announcement returns, and post-M&A survival likelihood. Our
results highlight the importance of structural heterogeneity in production
technology to firms' business integration decisions.

Using global wheat trade data and a network model for shock propagation, we
study the impact of the Ukrainian crisis on food security. Depending on the
level of reduction in Ukrainian wheat exports, the number of additional
individuals falling under the minimum dietary energy requirement varies from 1
to 9 millions, and reaches about 4.8 millions for a $50\%$ reduction in
exports. In the most affected countries, supply reductions are mainly related
to indirect trade restrictions.

We examine the effect of auditing on dividends in small private firms. We
hypothesize that auditing can constrain dividends by way of promoting
accounting conservatism. We use register data on private Norwegian firms and
random variation induced by the introduction of a policy allowing small private
firms to forgo the use of an auditor to estimate the effect of auditing on
dividend payout. Identification is obtained by a regression discontinuity
around the arbitrary thresholds for the policy. Propensity score matching is
used to create a balanced synthetic control. We consistently find that forgoing
auditing led to a significant increase in dividends in small private firms.

We analyze the causal impact of positive and negative feedback on
professional performance. We exploit a unique data source in which
quasi-random, naturally occurring variations within subjective ratings serve as
positive and negative feedback. The analysis shows that receiving positive
feedback has a favorable impact on subsequent performance, while negative
feedback does not have an effect. These main results are found in two different
environments and for distinct cultural backgrounds, experiences, and gender of
the feedback recipients. The findings imply that managers should focus on
giving positive motivational feedback.

From the perspective of social choice theory, ranked-choice voting (RCV) is
known to have many flaws. RCV can fail to elect a Condorcet winner and is
susceptible to monotonicity paradoxes and the spoiler effect, for example. We
use a database of 182 American ranked-choice elections for political office
from the years 2004-2022 to investigate empirically how frequently RCV's
deficiencies manifest in practice. Our general finding is that RCV's weaknesses
are rarely observed in real-world elections, with the exception that ballot
exhaustion frequently causes majoritarian failures.

I use the unanticipated and large additional tariffs the US imposed on
European Union products due to the Airbus-Boeing conflict to analyze how
exporters reacted to a change in trade policy. Using firm-level data for Spain
and applying a difference-in-differences methodology, I show that the export
revenue in the US of the firms affected by the tariff hike did not
significantly decrease relative to the one of other Spanish exporters to the
US. I show that Spanish exporters were able to neutralize the increase in
tariffs by substituting Spanish products with products originated in countries
unaffected by tariffs and shifting to varieties not affected by tariffs. My
results show that tariff avoidance is another margin exporters can use to
counteract the effects of a tariff hike.

Extreme events, exacerbated by climate change, pose significant risks to the
energy system and its consumers. However there are natural limits to the degree
of protection that can be delivered from a centralised market architecture.
Distributed energy resources provide resilience to the energy system, but their
value remains inadequately recognized by regulatory frameworks. We propose an
insurance framework to align residual outage risk exposure with locational
incentives for distributed investment. We demonstrate that leveraging this
framework in large-scale electricity systems could improve consumer welfare
outcomes in the face of growing risks from extreme events via investment in
distributed energy.

This paper assesses whether the higher capital maintenance drives up banks
cost of equity. We investigate the hypothesis using fixed effect panel
estimation with the data from a sample of 28 publicly listed commercial banks
over the 2013 to 2019 periods. We find a significant negative relationship
between banks capital and cost of equity. Empirically our baseline estimates
entail that a 10 percent increase in capital would reduce the cost of equity by
4.39 percent.

We consider the problem of how to regulate an oligopoly when firms have
private information about their costs. In the environment, consumers make
discrete choices over goods, and minimal structure is placed on the manner in
which firms compete. In the optimal regulatory policy, the regulator need only
solicit prices from firms, and based on those prices, charge them taxes or give
them subsidies, and impose on each firm a ``yardstick'' price cap that depends
on the posted prices of competing firms.

Regulations and standards in the field of artificial intelligence (AI) are
necessary to minimise risks and maximise benefits, yet some argue that they
stifle innovation. This paper critically examines the idea that regulation
stifles innovation in the field of AI. Current trends in AI regulation,
particularly the proposed European AI Act and the standards supporting its
implementation, are discussed. Arguments in support of the idea that regulation
stifles innovation are analysed and criticised, and an alternative point of
view is offered, showing how regulation and standards can foster innovation in
the field of AI.

We use administrative panel data on the universe of Brazilian formal workers
to investigate the labor market effects of the Venezuelan crisis in Brazil,
focusing on the border state of Roraima. The results using
difference-in-differences show that the monthly wages of Brazilians in Roraima
increased by around 2 percent, which was mostly driven by those working in
sectors and occupations with no refugee involvement. The study finds negligible
job displacement for Brazilians but finds evidence of native workers moving to
occupations without immigrants. We also find that immigrants in the informal
market offset the substitution effects in the formal market.

We study the partial and full set-asides and their implication for changes in
bidding behavior in first-price sealed-bid auctions in the context of United
States Department of Agriculture (USDA) food procurement auctions. Using five
years of bid data on different beef products, we implement weighted least
squares regression models to show that partial set-aside predicts decreases in
both offer prices and winning prices among large and small business bidders.
Full set-aside predicts a small increase in offer prices and winning prices
among small businesses. With these predictions, we infer that net profit of
small businesses is unlikely to increase when set-asides are present.

The economic bubble bursting resulted in a large number of non-performing
loans in Japanese financial institutions, which weakened their functions and
prevented them from extending credit for normal economic activities. However,
cryptocurrency operations are thriving in Japan. In this way, this paper
focuses on non-performing assets and cryptocurrencies. The goal is to use
literature analysis methods to summarise the development process, types of
issuance, mechanisms, evaluation models, application scenarios, and trends in
how cryptocurrencies are supervised.

Silkswap is an automated market maker model designed for efficient stablecoin
trading with minimal price impact. The original purpose of Silkswap is to
facilitate the trading of fiat-pegged stablecoins with the stablecoin Silk, but
it can be applied to any pair of stablecoins. The Silkswap invariant is a
hybrid function that generates an asymmetric price impact curve. We present the
derivation of the Silkswap model and its mathematical properties. We also
compare different numerical methods used to solve the invariant equation.
Finally, we compare our model with the well-known Curve Finance model.

We present a revealed preference framework to study sharing of resources in
households with children. We explicitly model the impact of the presence of
children in the context of stable marriage markets under both potential types
of custody arrangement - joint custody and sole custody. Our models deliver
testable revealed preference conditions and allow for the identification of
intrahousehold allocation of resources. Empirical applications to household
data from the Netherlands (joint custody) and Russia (sole custody) show the
methods' potential to identify intrahousehold allocation.

In this manuscript we consider a class optimal control problem for stochastic
differential delay equations. First, we rewrite the problem in a suitable
infinite-dimensional Hilbert space. Then, using the dynamic programming
approach, we characterize the value function of the problem as the unique
viscosity solution of the associated infinite-dimensional
Hamilton-Jacobi-Bellman equation. Finally, we prove a $C^{1,\alpha}$-partial
regularity of the value function. We apply these results to path dependent
financial and economic problems (Merton-like portfolio problem and optimal
advertising).

This report presents the results of an ex-ante impact assessment of several
scenarios related to the farmer targeting of the input subsidy programme
currently implemented in Senegal. This study has been achieved with the
agricultural household model FSSIM-Dev, calibrated on a sample of 2 278 farm
households from the ESPS-2 survey. The impacts on crop mix, fertilizer
application, farm income and on the government cost are presented and
discussed.

Air traffic control is considered to be a bottleneck in European air traffic
management. As a result, the performance of the air navigation service
providers is critically examined and also used for benchmarking. Using
quantitative methods, we investigate which endogenous and exogenous factors
affect the performance of air traffic control units on different levels. The
methodological discussion is complemented by an empirical analysis. Results may
be used to derive recommendations for operators, airspace users, and
policymakers. We find that efficiency depends significantly on traffic patterns
and the decisions of airspace users, but changes in the airspace structure
could also make a significant contribution to performance improvements.

What are the effects of investing in public infrastructure? We answer this
question with a New Keynesian model. We recast the model as a Markov chain and
develop a general solution method that nests existing ones inside/outside the
zero lower bound as special cases. Our framework delivers a simple expression
for the contribution of public infrastructure. We show that it provides a
unified framework to study the effects of public investment in three scenarios:
$(i)$ normal times $(ii)$ short-lived liquidity trap $(iii)$ long-lived
liquidity trap. We find that calibrations commonly used lead to multipliers
that diverge with the duration of the trap.

Spatially targeted investment grant schemes are a common tool to support
firms in lagging regions. We exploit exogenous variations in Germany's main
regional policy instrument (GRW) arriving from institutional reforms to analyse
local employment effects of investment grants. Findings for reduced-form and IV
regressions point to a significant policy channel running from higher funding
rates to increased firm-level investments and newly created jobs. When we
contrast effects for regions with high but declining funding rates to those
with low but rising rates, we find that GRW reforms led to diminishing
employment increases. Especially small firms responded to changing funding
conditions.

Local Projection is widely used for impulse response estimation, with the
Fixed Effect (FE) estimator being the default for panel data. This paper
highlights the presence of Nickell bias for all regressors in the FE estimator,
even if lagged dependent variables are absent in the regression. This bias is
the consequence of the inherent panel predictive specification. We recommend
using the split-panel jackknife estimator to eliminate the asymptotic bias and
restore the standard statistical inference. Revisiting three macro-finance
studies on the linkage between financial crises and economic contraction, we
find that the FE estimator substantially underestimates the post-crisis
economic losses.

We examine the incremental value of news-based data relative to the FRED-MD
economic indicators for quantile predictions of employment, output, inflation
and consumer sentiment in a high-dimensional setting. Our results suggest that
news data contain valuable information that is not captured by a large set of
economic indicators. We provide empirical evidence that this information can be
exploited to improve tail risk predictions. The added value is largest when
media coverage and sentiment are combined to compute text-based predictors.
Methods that capture quantile-specific non-linearities produce overall superior
forecasts relative to methods that feature linear predictive relationships. The
results are robust along different modeling choices.

Using harmonized administrative data from Scandinavia, we find that
intergenerational rank associations in income have increased uniformly across
Sweden, Denmark, and Norway for cohorts born between 1951 and 1979. Splitting
these trends by gender, we find that father-son mobility has been stable, while
family correlations for mothers and daughters trend upward. Similar patterns
appear in US survey data, albeit with slightly different timing. Finally, based
on evidence from records on occupations and educational attainments, we argue
that the observed decline in intergenerational mobility is consistent with
female skills becoming increasingly valued in the labor market.

We develop a novel methodology for the proxy variable identification of firm
productivity in the presence of productivity-modifying learning and spillovers
which facilitates a unified "internally consistent" analysis of the spillover
effects between firms. Contrary to the popular two-step empirical approach,
ours does not postulate contradictory assumptions about firm productivity
across the estimation steps. Instead, we explicitly accommodate cross-sectional
dependence in productivity induced by spillovers which facilitates
identification of both the productivity and spillover effects therein
simultaneously. We apply our model to study cross-firm spillovers in China's
electric machinery manufacturing, with a particular focus on productivity
effects of inbound FDI.

We conducted regression discontinuity design models in order to evaluate
changes in access to healthcare services and financial protection, using as a
natural experiment the age required to retire in Argentina, the moment in which
people are able to enroll in the free social health insurance called PAMI. The
dependent variables were indicators of the population with health insurance,
out-of-pocket health expenditure, and use of health services. The results show
that PAMI causes a high increase in the population with health insurance and
marginal reductions in health expenditure. No effects on healthcare use were
found.

Path independence is arguably one of the most important choice rule
properties in economic theory. We show that a choice rule is path independent
if and only if it is rationalizable by a utility function satisfying ordinal
concavity, a concept closely related to concavity notions in discrete
mathematics. We also provide a representation result for choice rules that
satisfy path independence and the law of aggregate demand.

We propose four channels through which government guarantees affect banks'
incentives to smooth income. Empirically, we exploit two complementary settings
that represent plausible exogenous changes in government guarantees: the
increase in implicit guarantees following the creation of the Eurozone and the
removal of explicit guarantees granted to the Landesbanken. We show that
increases (decreases) in government guarantees are associated with significant
decreases (increases) in banks' income smoothing. Taken together, our results
largely corroborate the predominance of a tail-risk channel, wherein government
guarantees reduce banks' tail risk, thereby reducing managers' incentives to
engage in income smoothing.

Existing models of rational pure bubble models feature multiple (and often a
continuum of) equilibria, which makes model predictions and policy analysis
non-robust. We show that when the interest rate in the fundamental equilibrium
is below the economic growth rate ($R<G$), a bubbly equilibrium with $R=G$
exists. By injecting dividends that are vanishingly small relative to aggregate
income to the bubble asset, we can eliminate the fundamental steady state and
resolve equilibrium indeterminacy. We show the general applicability of
dividend injection through examples in overlapping generations and
infinite-horizon models with or without production or financial frictions.

The November 2022 ranked choice election for District 4 School Director in
Oakland, CA, was very interesting from the perspective of social choice theory.
The election did not contain a Condorcet winner and exhibited downward and
upward monotonicity paradoxes, for example. Furthermore, an error in the
settings of the ranked choice tabulation software led to the wrong candidate
being declared the winner. This article explores the strange features of this
election and places it in the broader context of ranked choice elections in the
United States.

We propose a new sorting framework: composite sorting. Composite sorting
comprises of (1) distinct worker types assigned to the same occupation, and (2)
a given worker type simultaneously being part of both positive and negative
sorting. Composite sorting arises when fixed investments mitigate variable
costs of mismatch. We completely characterize optimal sorting and additionally
show it is more positive when mismatch costs are less concave. We then
characterize equilibrium wages. Wages have a regional hierarchical structure -
relative wages depend solely on sorting within skill groups. Quantitatively,
composite sorting can generate a sizable portion of within-occupations wage
dispersion in the US.

A common problem in various applications is the additive decomposition of the
output of a function with respect to its input variables. Functions with binary
arguments can be axiomatically decomposed by the famous Shapley value. For the
decomposition of functions with real arguments, a popular method is the
pointwise application of the Shapley value on the domain. However, this
pointwise application largely ignores the overall structure of functions. In
this paper, axioms are developed which fully preserve functional structures and
lead to unique decompositions for all Borel measurable functions.

Several tiers of social organizations with varying economic and social
disparities have been observed, such as bands, tribes, chiefdoms, and kingdoms.
Noting that anthropologists emphasize gifts as drivers of social change, we
introduce a simple model for gift interactions. Numerical results and the
corresponding mean-field theory demonstrate the transition of the above four
socioeconomic phases, characterized by wealth and reputation distribution
following exponential or power-law. A novel mechanism for social evolution is
provided, expanding the scope of econo- and socio-physics.

In regulatory proceedings, few issues are more hotly debated than the cost of
capital. This article formalises the theoretical foundation of cost of capital
estimation for regulatory purposes. Several common regulatory practices lack a
solid foundation in the theory. For example, the common practice of estimating
a single cost of capital for the regulated firm suffers from a circularity
problem, especially in the context of a multi-year regulatory period. In
addition, the relevant cost of debt cannot be estimated using the
yield-to-maturity on a corporate bond. We suggest possible directions for
reform of cost of capital practices in regulatory proceedings.

In 2013, TSOs from the Central European Region complained to the Agency for
the Cooperation of Energy Regulators because of increasing unplanned flows that
were presumed to be caused by a joint German-Austrian bidding zone in the
European electricity market. This paper empirically analyses the effects of the
split of this bidding zone in 2018 on planned and unplanned cross-border flows
between Germany, Austria, Poland, the Czech Republic, Slovakia, and Hungary.
For all bidding zones, apart from the German-Austrian one, planned flows
increased. Further, I find that around the policy intervention between 2017 and
2019, unplanned flows between Germany and Austria as well as for the Czech
Republic and Slovakia decreased. However, for Poland increasing unplanned flows
are found.

With 70 million dead, World War II remains the most devastating conflict in
history. Of the survivors, millions were displaced, returned maimed from the
battlefield, or spent years in captivity. We examine the impact of such wartime
experiences on labor market careers and show that they often become apparent
only at certain life stages. While war injuries reduced employment in old age,
former prisoners of war postponed their retirement. Many displaced workers,
particularly women, never returned to employment. These responses are in line
with standard life-cycle theory and thus likely extend to other conflicts.

The paper describes a potential platform to facilitate academic peer review
with emphasis on early-stage research. This platform aims to make peer review
more accurate and timely by rewarding reviewers on the basis of peer prediction
algorithms. The algorithm uses a variation of Peer Truth Serum for
Crowdsourcing (Radanovic et al., 2016) with human raters competing against a
machine learning benchmark. We explain how our approach addresses two large
productive inefficiencies in science: mismatch between research questions and
publication bias. Better peer review for early research creates additional
incentives for sharing it, which simplifies matching ideas to teams and makes
negative results and p-hacking more visible.

This paper investigates the existence of a G-relaxed optimal control of a
controlled stochastic differential delay equation driven by G-Brownian motion
(G-SDDE in short). First, we show that optimal control of G-SDDE exists for the
finite horizon case. We present as an application of our result an economic
model, which is represented by a G-SDDE, where we studied the optimization of
this model. We connected the corresponding Hamilton Jacobi Bellman equation of
our controlled system to a decoupled G-forward backward stochastic differential
delay equation (G-FBSDDE in short). Finally, we simulate this G-FBSDDE to get
the optimal strategy and cost.

This paper models a two-agent economy with production and appropriation as a
noncooperative dynamic game, and determines its closed-form Markovian Nash
equilibrium. The analysis highlights the para-metric conditions that tip the
economy from a nonaggressive or "co-operative" equilibrium to outright
distributional conflict. The model includes parameters that capture the role of
appropriation technology and destructiveness. The full dynamic implications of
the game are yet to be explored, but the model offers a promising general
framework for thinking about different technological and economic conditions as
more or less conducive to cooperation or distributional conflict.

Using a combination of incentive modeling and empirical meta-analyses, this
paper provides a pointed critique at the incentive systems that drive venture
capital firms to optimize their practices towards activities that increase
General Partner utility yet are disjoint from improving the underlying asset of
startup equity. We propose a "distributed venture firm" powered by software
automations and governed by a set of functional teams called "Pods" that carry
out specific tasks with immediate and long-term payouts given on a deal-by-deal
basis. Avenues are provided for further research to validate this model and
discover likely paths to implementation.

In this paper we explore two intertwined issues. First, using primary data we
examine the impact of asymmetric networks, built on rich relational information
on several spheres of living, on access to workfare employment in rural India.
We find that unidirectional relations, as opposed to reciprocal relations, and
the concentration of such unidirectional relations increase access to workfare
jobs. Further in-depth exploration provides evidence that patron-client
relations are responsible for this differential access to such employment for
rural households. Complementary to our empirical exercises, we construct and
analyse a game-theoretical model supporting our findings.

We show that the knowledge of an agent carrying non-trivial unawareness
violates the standard property of 'necessitation', therefore necessitation
cannot be used to refute the standard state-space model. A revised version of
necessitation preserves non-trivial unawareness and solves the classical
Dekel-Lipman-Rustichini result. We propose a generalised knowledge operator
consistent with the standard state-space model of unawareness, including the
model of infinite state-space.

Non-Fungible Tokens (NFTs) are non-interchangeable assets, usually digital
art, which are stored on the blockchain. Preliminary studies find that female
and darker-skinned NFTs are valued less than their male and lighter-skinned
counterparts. However, these studies analyze only the CryptoPunks collection.
We test the statistical significance of race and gender biases in the prices of
CryptoPunks and present the first study of gender bias in the broader NFT
market. We find evidence of racial bias but not gender bias. Our work also
introduces a dataset of gender-labeled NFT collections to advance the broader
study of social equity in this emerging market.

In this paper, the interaction of geopolitical actors in the production and
sale of military equipment is studied. In section 2 the production of military
equipment is considered as the two person zero-sum game. In such game, the
strategies of the players are defined by the information state of the actors.
The optimal strategy of geopolitical actors is found. In section 3, the
conflict process is considered, the optimal strategy is determined for each
geopolitical actor.

This study provides a practical introduction to high-frequency trading in
blockchain-based currency markets. These types of markets have some specific
characteristics that differentiate them from the stock markets, such as a large
number of trading exchanges (centralized and decentralized), relative
simplicity in moving funds from one exchange to another, and the large number
of new currencies that have very little liquidity. This study analyzes the
possible risks that specifically characterize this type of trading operation,
the potential opportunities, and the algorithms that are mostly used, providing
information that can be useful for practitioners who intend to operate in these
markets by providing (and risking) liquidity.

We consider situations where consumers are aware that a statistical model
determines the price of a product based on their observed behavior. Using a
novel experiment varying the context similarity between participant data and a
product, we find that participants manipulate their responses to a survey about
personal characteristics, and manipulation is more successful when the contexts
are similar. Moreover, participants demand less privacy, and make less optimal
privacy choices when the contexts are less similar. Our findings highlight the
importance of data privacy policies in the age of big data, where behavior in
seemingly unrelated contexts might affect prices.

We study the staggered introduction of a generative AI-based conversational
assistant using data from 5,000 customer support agents. Access to the tool
increases productivity, as measured by issues resolved per hour, by 14 percent
on average, with the greatest impact on novice and low-skilled workers, and
minimal impact on experienced and highly skilled workers. We provide suggestive
evidence that the AI model disseminates the potentially tacit knowledge of more
able workers and helps newer workers move down the experience curve. In
addition, we show that AI assistance improves customer sentiment, reduces
requests for managerial intervention, and improves employee retention.

In railway infrastructure, construction and maintenance is typically procured
using competitive procedures such as auctions. However, these procedures only
fulfill their purpose - using (taxpayers') money efficiently - if bidders do
not collude. Employing a unique dataset of the Swiss Federal Railways, we
present two methods in order to detect potential collusion: First, we apply
machine learning to screen tender databases for suspicious patterns. Second, we
establish a novel category-managers' tool, which allows for sequential and
decentralized screening. To the best of our knowledge, we pioneer illustrating
the adaption and application of machine-learning based price screens to a
railway-infrastructure market.

In dynamic environments, Q-learning is an automaton that (i) provides
estimates (Q-values) of the continuation values associated with each available
action; and (ii) follows the naive policy of almost always choosing the action
with highest Q-value. We consider a family of automata that are based on
Q-values but whose policy may systematically favor some actions over others,
for example through a bias that favors cooperation. In the spirit of Compte and
Postlewaite [2018], we look for equilibrium biases within this family of
Q-based automata. We examine classic games under various monitoring
technologies and find that equilibrium biases may strongly foster collusion.

This papers aims to establish the empirical relationship between income, net
wealth and their joint distribution in a selected group of euro area countries.
I estimate measures of dependence between income and net wealth using a
semiparametric copula approach and calculate a bivariate Gini coefficient. By
combining structural inference from vector autoregressions on the macroeconomic
level with a simulation using microeconomic data, I investigate how
conventional and unconventional monetary policy measures affect the joint
distribution. Results indicate that effects of monetary policy are highly
heterogeneous across different countries, both in terms of the dependence of
income and net wealth on each other, and in terms of inequality in both income
and net wealth.

Nontransitive choices have long been an area of curiosity within economics.
However, determining whether nontransitive choices represent an individual's
preference is a difficult task since choice data is inherently stochastic. This
paper shows that behavior from nontransitive preferences under a monotonicity
assumption is equivalent to a transitive stochastic choice model. In
particular, nontransitive preferences are regularly interpreted as a strength
of preference, so we assume alternatives are chosen proportionally to the
nontransitive preference. One implication of this result is that one cannot
distinguish ``complementarity in attention" and ``complementarity in demand."

Social networks can sustain cooperation by amplifying the consequences of a
single defection through a cascade of relationship losses. Building on Jackson
et al. (2012), we introduce a novel robustness notion to characterize low
cognitive complexity (LCC) networks - a subset of equilibrium networks that
imposes a minimal cognitive burden to calculate and comprehend the consequences
of defection. We test our theory in a laboratory experiment and find that
cooperation is higher in equilibrium than in non-equilibrium networks. Within
equilibrium networks, LCC networks exhibit higher levels of cooperation than
non-LCC networks. Learning is essential for the emergence of equilibrium play.

We examine the effects of an affirmative action policy at an elite Brazilian
university that reserved 45 percent of admission slots for Black and low-income
students. We find that marginally-admitted students who enrolled through the
affirmative action tracks experienced a 14 percent increase in early-career
earnings. But the adoption of affirmative action also caused a large decrease
in earnings for the university's most highly-ranked students. We present
evidence that the negative spillover effects on highly-ranked students'
earnings were driven by both a reduction in human capital accumulation and a
decline in the value of networking.

Take up of microcredit by the poor for investment in businesses or human
capital turned out to be very low. We show that this could be explained by risk
aversion, without relying on fixed costs or other forms of non-convexity in the
technology, if the investment is aimed at increasing the probability of
success. Under this framework, rational risk-averse agents choose corner
solutions, unlike in the case of a risky investment with an exogenous
probability of success. Our online experiment confirms our theoretical
predictions about how agents' choices differ when facing the two types of
investments.

General equilibrium, the cornerstone of modern economics and finance, rests
on assumptions many markets do not meet. Spectrum auctions, electricity
markets, and cap-and-trade programs for resource rights often feature
non-convexities in preferences or production that can cause non-existence of
Walrasian equilibrium and render general equilibrium vacuous. Yquilibrium
complements general equilibrium with an optimization approach to (non-) convex
economies that does not require perfect competition. Yquilibrium coincides with
Walrasian equilibrium when the latter exists. Yquilibrium exists even if
Walrasian equilibrium ceases to and produces optimal allocations subject to
linear, anonymous, and (approximately) utility-clearing prices.

Purely affective interaction allows the welfare of an individual to depend on
her own actions and on the profile of welfare levels of others. Under an
assumption on the structure of mutual affection that we interpret as
"non-explosive mutual affection," we show that equilibria of simultaneous-move
affective interaction are Pareto optimal independently of whether or not an
induced standard game exists. Moreover, if purely affective interaction induces
a standard game, then an equilibrium profile of actions is a Nash equilibrium
of the game, and this Nash equilibrium and Pareto optimal profile of strategies
is locally dominant.

Central Banks interventions are frequent in response to exogenous events with
direct implications on financial market volatility. In this paper, we introduce
the Asymmetric Jump Multiplicative Error Model (AJM), which accounts for a
specific jump component of volatility within an intradaily framework. Taking
the Federal Reserve (Fed) as a reference, we propose a new model-based
classification of monetary announcements based on their impact on the jump
component of volatility. Focusing on a short window following each Fed's
communication, we isolate the impact of monetary announcements from any
contamination carried by relevant events that may occur within the same
announcement day.

This study aimed to investigate how transformational leadership affects team
processes, mediated by change in team members. A self-administered
questionnaire was distributed to construction project team members in Abuja and
Kaduna, and statistical analysis revealed a significant positive relationship
between transformational leadership and team processes, transformational
leadership and change in team members, changes in team members and team
processes, and changes in team members mediating the relationship between
transformational leadership and team processes. Future studies should consider
cultural differences.

This paper develops a dynamic monetary model to study the (in)stability of
the fractional reserve banking system. The model shows that the fractional
reserve banking system can endanger stability in that equilibrium is more prone
to exhibit endogenous cyclic, chaotic, and stochastic dynamics under lower
reserve requirements, although it can increase consumption in the steady-state.
Introducing endogenous unsecured credit to the baseline model does not change
the main results. The calibrated exercise suggests that this channel could be
another source of economic fluctuations. This paper also provides empirical
evidence that is consistent with the prediction of the model.

Linear time series models are the workhorse of structural macroeconometric
analysis. However, economic theory as well as data suggest that nonlinear and
asymmetric effects might be key to understand the potential effects of sudden
economic changes. Taking a dynamical system view, this paper proposes a new
semi-nonparametric approach to construct impulse responses of nonlinear time
series. Estimation of autoregressive models with sieve methods is discussed
under natural physical dependence assumptions, and uniform consistency results
for structural impulse responses are derived. Simulations and two empirical
exercises show that the proposed method performs well and yields new insights
in the dynamic effects of macroeconomic shocks.

Ranked-choice voting anomalies such as monotonicity paradoxes have been
extensively studied through creating hypothetical examples and generating
elections under various models of voter behavior. However, very few real-world
examples of such voting paradoxes have been found and analyzed. We investigate
two single-transferable vote elections from Scotland that demonstrate upward
monotonicity, downward monotonicity, no-show, and committee size paradoxes.
These paradoxes are rarely observed in real-world elections, and this article
is the first case study of such paradoxes in multiwinner elections.

The Supreme Court's federal preemption decisions are notoriously
unpredictable. Traditional left-right voting alignments break down in the face
of competing ideological pulls. The breakdown of predictable voting blocs
leaves the business interests most affected by federal preemption uncertain of
the scope of potential liability to injured third parties and unsure even of
whether state or federal law will be applied to future claims.
  This empirical analysis of the Court's decisions over the last fifteen years
sheds light on the Court's unique voting alignments in obstacle preemption
cases. A surprising anti-obstacle preemption coalition is forming as Justice
Thomas gradually positions himself alongside the Court's liberals to form a
five-justice voting bloc opposing obstacle preemption.

Four rounds of surveys of slum dwellers in Dhaka city during the 2020-21
COVID-19 pandemic raise questions about whether the slum dwellers possess some
form of immunity to the effects of COVID-19? If the working poor of Bangladesh
are practically immune to COVID-19, why has this question not been more
actively investigated? We shed light on some explanations for these pandemic
questions and draw attention to the role of intellectual elites and public
policy, suggesting modifications needed for pandemic research.

This study probes the effects of Japan's traditional alphabetical
surname-based call system on students' experiences and long-term behavior. It
reveals that early listed surnames enhance cognitive and non-cognitive skill
development. The adoption of mixed-gender lists since the 1980s has amplified
this effect, particularly for females. Furthermore, the study uncovers a strong
correlation between childhood surname order and individuals' intention for
COVID-19 revaccination, while changes in adulthood surnames do not exhibit the
same influence. The implications for societal behaviors and policy are
substantial and wide-ranging.

This paper examines how risk and budget limits on investment mandates affect
the bidding strategy in a uniform-price auction for issuing corporate bonds. I
prove the existence of symmetric Bayesian Nash equilibrium and explore how the
risk limits imposed on the mandate may mitigate severe underpricing, as the
symmetric equilibrium's yield positively relates to the risk limit. Investment
mandates with low-risk acceptance inversely affect the equilibrium bid. The
equilibrium bid provides insights into the optimal mechanism for pricing
corporate bonds conveying information about the bond's valuation, market power,
and the number of bidders. These findings contribute to auction theory and have
implications for empirical research in the corporate bond market.

South Korea has become one of the most important economies in Asia. The
largest Korean multinational firms are affiliated with influential family-owned
business groups known as the chaebol. Despite the surging academic popularity
of the chaebol, there is a considerable knowledge gap in the bibliometric
analysis of business groups in Korea. In an attempt to fill this gap, the
article aims to provide a systematic review of the chaebol and the role that
business groups have played in the economy of Korea. Three distinct
bibliometric networks are analyzed, namely the scientific collaboration
network, bibliographic coupling network, and keyword co-occurrence network.

Corporate Social Responsibility (CSR) has become an important topic that is
gaining academic interest. This research paper presents CSREU, a new dataset
with attributes of 115 European companies, which includes several performance
indicators and the respective CSR disclosure scores computed using the Global
Reporting Initiative (GRI) framework. We also examine the correlations between
some of the financial indicators and the CSR disclosure scores of the
companies. According to our results, these correlations are weak and deeper
analysis is required to draw convincing conclusions about the potential impact
of CSR disclosure on financial performance. We hope that the newly created data
and our preliminary results will help and foster research in this field.

We measure the extent of consumption insurance to income shocks accounting
for high-order moments of the income distribution. We derive a nonlinear
consumption function, in which the extent of insurance varies with the sign and
magnitude of income shocks. Using PSID data, we estimate an asymmetric
pass-through of bad versus good permanent shocks -- 17% of a 3 sigma negative
shock transmits to consumption compared to 9% of an equal-sized positive shock
-- and the pass-through increases as the shock worsens. Our results are
consistent with surveys of consumption responses to hypothetical events and
suggest that tail income risk matters substantially for consumption.

In the present paper we propose a new approach on `distributed systems': the
processes are represented through total orders and the communications are
characterized by means of biorders. The resulting distributed systems capture
situations met in various fields (such as computer science, economics and
decision theory). We investigate questions associated to the numerical
representability of order structures, relating concepts of economics and
computing to each other. The concept of `quasi-finite partial orders' is
introduced as a finite family of chains with a communication between them. The
representability of this kind of structure is studied, achieving a construction
method for a finite (continuous) Richter-Peleg multi-utility representation.

Motivated by the idea that lack of experience is a source of errors but that
experience should reduce them, we model agents' behavior using a stochastic
choice model, leaving endogenous the accuracy of their choice. In some games,
increased accuracy is conducive to unstable best-response dynamics. We define
the barrier to learning as the minimum level of noise which keeps the
best-response dynamic stable. Using logit Quantal Response, this defines a
limitQR Equilibrium. We apply the concept to centipede, travelers' dilemma, and
11-20 money-request games and to first-price and all-pay auctions, and discuss
the role of strategy restrictions in reducing or amplifying barriers to
learning.

This article examines the impact of China's delayed retirement announcement
on households' savings behavior using data from China Family Panel Studies
(CFPS). The article finds that treated households, on average, experience an 8%
increase in savings rates as a result of the policy announcement. This
estimation is both significant and robust. Different types of households
exhibit varying degrees of responsiveness to the policy announcement, with
higher-income households showing a greater impact. The increase in household
savings can be attributed to negative perceptions about future pension income.

The digital transformation of the art world has become a revolution for the
sector. Cryptoart, based on non-fungible tokens (NFT), is attracting the
attention of artists, collectors and enthusiasts for its ability to tokenise
any element that can be sold as art in the digital market. That means it is
able to become a scarce resource and an economic asset by encapsulating the
market value of a piece of digital art, which may or may not have a reference
in the real world. This study will delve into the ethical aspects underlying
what is known as the NFT Revolution, particularly impacts related to the abuse
or destruction of cultural heritage, speculation and the generation of economic
bubbles and environmental unsustainability.

A growing body of literature is aimed at designing private mempools in
blockchains. The ultimate goal of this research is addressing several phenomena
broadly classed under MEV with sandwich attacks as the canonical example. The
literature has primarily viewed MEV as a problem arising from oversights in
distributed systems and cryptographic protocol design and has attempted to
address it with the standard tool sets from those disciplines. This paper
argues that the impact of private mempools on markets and agent incentives
renders analyses that do not consider the economic lens incomplete. The paper
presents several observations across blockchains and traditional finance to
justify this argument and highlight specific dynamics for future study.

Counterfactual Regret Minimization(CFR) has shown its success in Texas
Hold'em poker. We apply this algorithm to another popular incomplete
information game, Mahjong. Compared to the poker game, Mahjong is much more
complex with many variants. We study two-player Mahjong by conducting game
theoretical analysis and making a hierarchical abstraction to CFR based on
winning policies. This framework can be generalized to other imperfect
information games.

Cloud computing has revolutionized the way organizations manage their IT
infrastructure, but it has also introduced new challenges, such as managing
cloud costs. This paper explores various techniques for cloud cost
optimization, including cloud pricing, analysis, and strategies for resource
allocation. Real-world case studies of these techniques are presented, along
with a discussion of their effectiveness and key takeaways. The analysis
conducted in this paper reveals that organizations can achieve significant cost
savings by adopting cloud cost optimization techniques. Additionally, future
research directions are proposed to advance the state of the art in this
important field.

Stringent climate policy compatible with the targets of the 2015 Paris
Agreement would pose a substantial fiscal challenge. Reducing carbon dioxide
emissions by 95% or more by 2050 would raise 7% (1-17%) of GDP in carbon tax
revenue, half of current, global tax revenue. Revenues are relatively larger in
poorer regions. Subsidies for carbon dioxide sequestration would amount to 6.6%
(0.3-7.1%) of GDP. These numbers are conservative as they were estimated using
models that assume first-best climate policy implementation and ignore the
costs of raising revenue. The fiscal challenge rapidly shrinks if emission
targets are relaxed.

Decentralized Finance (DeFi) is a new paradigm in the creation, distribution,
and utilization of financial services via the integration of blockchain
technology. Our research conducts a comprehensive introduction and meticulous
classification of various DeFi applications. Beyond that, we thoroughly analyze
these risks from both technical and economic perspectives, spanning multiple
layers. We point out research gaps and revenues, covering technical
advancements, innovative economics, and sociology and ecology optimization.

Cross-Impact Balance Analysis (CIB) is a widely used method to build
scenarios and help researchers to formulate policies in different fields, such
as management sciences and social sciences. During the development of the CIB
method over the years, some derivative methods were developed to expand its
application scope, including a method called dynamic CIB. However, the workflow
of dynamic CIB is relatively complex. In this article, we provide another
approach to extend CIB in multiple timespans based on the concept 'scenario
weight' and simplify the workflow to bring convenience to the policy makers.

Stablecoins have gained significant popularity recently, with their market
cap rising to over $180 billion. However, recent events have raised concerns
about their stability. In this paper, we classify stablecoins into four types
based on the source and management of collateral and investigate the stability
of each type under different conditions. We highlight each type's potential
instabilities and underlying tradeoffs using agent-based simulations. The
results emphasize the importance of carefully evaluating the origin of a
stablecoin's collateral and its collateral management mechanism to ensure
stability and minimize risks. Enhanced understanding of stablecoins should be
informative to regulators, policymakers, and investors alike.

This book presents detailed discussion on the role of higher education in
terms of serving basic knowledge creation, teaching, and doing applied research
for commercialization. The book presents an historical account on how this
challenge was addressed earlier in education history, the cases of successful
academic commercialization, the marriage between basic and applied science and
how universities develop economies of the regions and countries. This book also
discusses cultural and social challenges in research commercialization and
pathways to break the status quo.

We study the parametric description of the city size distribution (CSD) of 70
different countries (developed and developing) using seven models, as follows:
the lognormal (LN), the loglogistic (LL), the double Pareto lognormal (dPLN),
the two-lognormal (2LN), the two-loglogistic (2LL), the three-lognormal (3LN)
and the three-loglogistic (3LL). Our results show that 3LN and 3LL are the best
densities in terms of non-rejections out of standard statistical tests.
Meanwhile, according to the information criteria AIC and BIC, there is no
systematically dominant distribution.

We perform a comparative study for multiple equity indices of different
countries using different models to determine the best fit using the
Kolmogorov-Smirnov statistic, the Anderson-Darling statistic, the Akaike
information criterion and the Bayesian information criteria as goodness-of-fit
measures. We fit models both to daily and to hourly log-returns. The main
result is the excellent performance of a mixture of three Student's $t$
distributions with the numbers of degrees of freedom fixed a priori (3St). In
addition, we find that the different components of the 3St mixture with
small/moderate/high degree of freedom parameter describe the
extreme/moderate/small log-returns of the studied equity indices.

We study the distribution of strike size, which we measure as lost person
days, for a long period in several countries of Europe and America. When we
consider the full samples, the mixtures of two or three lognormals arise as
very convenient models. When restricting to the upper tails, the Pareto power
law becomes almost indistinguishable of the truncated lognormal.

We have studied the parametric description of the distribution of the
log-growth rates of the sizes of cities of France, Germany, Italy, Spain and
the USA. We have considered several parametric distributions well known in the
literature as well as some others recently introduced. There are some models
that provide similar excellent performance, for all studied samples. The normal
distribution is not the one observed empirically.

We study how competitive forces may drive firms to inefficiently acquire
startup talent. In our model, two rival firms have the capacity to acquire and
integrate a startup operating in an orthogonal market. We show that firms may
pursue such acquihires primarily as a preemptive strategy, even when they
appear unprofitable in isolation. Thus, acquihires, even absent traditional
competition-reducing effects, need not be benign, as they can lead to
inefficient talent allocation. Additionally, our analysis underscores that such
talent hoarding can diminish consumer surplus and exacerbate job volatility for
acquihired employees.

In 1979, Weitzman introduced Pandora's box problem as a framework for
sequential search with costly inspections. Recently, there has been a surge of
interest in Pandora's box problem, particularly among researchers working at
the intersection of economics and computation. This survey provides an overview
of the recent literature on Pandora's box problem, including its latest
extensions and applications in areas such as market design, decision theory,
and machine learning.

Despite increasing cognitive demands of jobs, knowledge about the role of
health in retirement has centered on its physical dimensions. This paper
estimates a dynamic programming model of retirement that incorporates multiple
health dimensions, allowing differential effects on labor supply across
occupations. Results show that the effect of cognitive health surges
exponentially after age 65, and it explains a notable share of employment
declines in cognitively demanding occupations. Under pension reforms, physical
constraint mainly impedes manual workers from delaying retirement, whereas
cognitive constraint dampens the response of clerical and professional workers.
Multidimensional health thus unevenly exacerbate welfare losses across
occupations.

This paper examines the long-term gender-specific impacts of parental health
shocks on adult children's employment in China. We build up an inter-temporal
cooperative framework to analyze household work decisions in response to
parental health deterioration. Then employing an event-study approach, we
establish a causal link between parental health shocks and a notable decline in
female employment rates. Male employment, however, remains largely unaffected.
This negative impact shows no abatement up to eight years that are observable
by the sample. These findings indicate the consequence of "growing old before
getting rich" for developing countries.

We introduce a definition of multivariate majorization that is new to the
economics literature. Our majorization technique allows us to generalize Mussa
and Rosen's (1978) "ironing" to a broad class of multivariate principal-agents
problems. Specifically, we consider adverse selection problems in which agents'
types are one dimensional but informational externalities create a
multidimensional ironing problem. Our majorization technique applies to
discrete and continuous type spaces alike and we demonstrate its usefulness for
contract theory and mechanism design. We further show that multivariate
majorization yields a natural extension of second-order stochastic dominance to
multiple dimensions and derive its implications for decision making under
multivariate risk.

This paper studies how to accurately elicit quality for alternatives with
multiple attributes. Two multiple price lists (MPLs) are considered: (i) m-MPL
which asks subjects to compare an alternative to money, and (ii) p-MPL where
subjects are endowed with money and asked whether they would like to buy an
alternative or not. Theoretical results show that m-MPL requires fewer
assumptions for accurate quality elicitation compared to p-MPL. Experimental
evidence from a within-subject experiment using consumer products shows that
switch points between the two MPLs are different, which suggests that quality
measures are sensitive to the elicitation method.

The article develops a general equilibrium model where power relations are
central in the determination of unemployment, profitability, and income
distribution. The paper contributes to the market forces versus institutions
debate by providing a unified model capable of identifying key interrelations
between technical and institutional changes in the economy. Empirically, the
model is used to gauge the relative roles of technology and institutions in the
behavior of the labor share, the unemployment rate, the capital-output ratio,
and business profitability and demonstrates how they complement each other in
providing an adequate narrative to the structural changes of the US economy.

The psychology of science is the least developed member of the family of
science studies. It is growing, however, increasingly into a promising
discipline. After a very brief review of this emerging sub-field of psychology,
we call for it to be invited into the collection of social sciences that
constitute the interdisciplinary field of science policy. Discussing the
classic issue of resource allocation, this paper tries to indicate how prolific
a new psychological conceptualization of this problem would be. Further, from a
psychological perspective, this research will argue in favor of a more
realistic conception of science which would be a complement to the existing one
in science policy.

If checks and balances are aimed at protecting citizens from the government's
abuse of power, why do they sometimes weaken them? We address this question in
a laboratory experiment in which subjects choose between two decision rules:
with and without checks and balances. Voters may prefer an unchecked executive
if that enables a reform that, otherwise, is blocked by the legislature.
Consistent with our predictions, we find that subjects are more likely to
weaken checks and balances when there is political gridlock. However, subjects
weaken the controls not only when the reform is beneficial but also when it is
harmful.

