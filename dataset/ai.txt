AI transparency is a central pillar of responsible AI deployment and
effective human-AI collaboration. A critical approach is communicating
uncertainty, such as displaying AI's confidence level, or its correctness
likelihood (CL), to users. However, these confidence levels are often
uncalibrated, either overestimating or underestimating actual CL, posing risks
and harms to human-AI collaboration. This study examines the effects of
uncalibrated AI confidence on users' trust in AI, AI advice adoption, and
collaboration outcomes. We further examined the impact of increased
transparency, achieved through trust calibration support, on these outcomes.
Our results reveal that uncalibrated AI confidence leads to both the misuse of
overconfident AI and disuse of unconfident AI, thereby hindering outcomes of
human-AI collaboration. Deficiency of trust calibration support exacerbates
this issue by making it harder to detect uncalibrated confidence, promoting
misuse and disuse of AI. Conversely, trust calibration support aids in
recognizing uncalibration and reducing misuse, but it also fosters distrust and
causes disuse of AI. Our findings highlight the importance of AI confidence
calibration for enhancing human-AI collaboration and suggest directions for AI
design and regulation.

This paper focuses on supporting AI/ML Security Workers -- professionals
involved in the development and deployment of secure AI-enabled software
systems. It presents AI/ML Adversarial Techniques, Tools, and Common Knowledge
(AI/ML ATT&CK) framework to enable AI/ML Security Workers intuitively to
explore offensive and defensive tactics.

AI has surpassed humans across a variety of tasks such as image
classification, playing games (e.g., go, "Starcraft" and poker), and protein
structure prediction. However, at the same time, AI is also bearing serious
controversies. Many researchers argue that little substantial progress has been
made for AI in recent decades. In this paper, the author (1) explains why
controversies about AI exist; (2) discriminates two paradigms of AI research,
termed "weak AI" and "strong AI" (a.k.a. artificial general intelligence); (3)
clarifies how to judge which paradigm a research work should be classified
into; (4) discusses what is the greatest value of "weak AI" if it has no chance
to develop into "strong AI".

Artificial Intelligence (AI) Ethics is a nascent yet critical research field.
Recent developments in generative AI and foundational models necessitate a
renewed look at the problem of AI Ethics. In this study, we perform a
bibliometric analysis of AI Ethics literature for the last 20 years based on
keyword search. Our study reveals a three-phase development in AI Ethics,
namely an incubation phase, making AI human-like machines phase, and making AI
human-centric machines phase. We conjecture that the next phase of AI ethics is
likely to focus on making AI more machine-like as AI matches or surpasses
humans intellectually, a term we coin as "machine-like human".

The comprehension and adoption of Artificial Intelligence (AI) are beset with
practical and ethical problems. This article presents a 5-level AI Capability
Assessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to
assist practitioners in AI comprehension and adoption. These practical tools
were developed with business executives, technologists, and other
organisational stakeholders in mind. They are founded on a comprehensive
conception of AI compared to those in other AI adoption models and are also
open-source artefacts. Thus, the AI-CAM and AI-CM present an accessible
resource to help inform organisational decision-makers on the capability
requirements for (1) AI-based data analytics use cases based on machine
learning technologies; (2) Knowledge representation to engineer and represent
data, information and knowledge using semantic technologies; and (3) AI-based
solutions that seek to emulate human reasoning and decision-making. The AI-CAM
covers the core capability dimensions (business, data, technology,
organisation, AI skills, risks, and ethical considerations) required at the
five capability maturity levels to achieve optimal use of AI in organisations.

Artificial intelligence (AI) and human-machine interaction (HMI) are two
keywords that usually do not fit embedded applications. Within the steps needed
before applying AI to solve a specific task, HMI is usually missing during the
AI architecture design and the training of an AI model. The human-in-the-loop
concept is prevalent in all other steps of developing AI, from data analysis
via data selection and cleaning to performance evaluation. During AI
architecture design, HMI can immediately highlight unproductive layers of the
architecture so that lightweight network architecture for embedded applications
can be created easily. We show that by using this HMI, users can instantly
distinguish which AI architecture should be trained and evaluated first since a
high accuracy on the task could be expected. This approach reduces the
resources needed for AI development by avoiding training and evaluating AI
architectures with unproductive layers and leads to lightweight AI
architectures. These resulting lightweight AI architectures will enable HMI
while running the AI on an edge device. By enabling HMI during an AI uses
inference, we will introduce the AI-in-the-loop concept that combines AI's and
humans' strengths. In our AI-in-the-loop approach, the AI remains the working
horse and primarily solves the task. If the AI is unsure whether its inference
solves the task correctly, it asks the user to use an appropriate HMI.
Consequently, AI will become available in many applications soon since HMI will
make AI more reliable and explainable.

The organizational use of artificial intelligence (AI) has rapidly spread
across various sectors. Alongside the awareness of the benefits brought by AI,
there is a growing consensus on the necessity of tackling the risks and
potential harms, such as bias and discrimination, brought about by advanced AI
technologies. A multitude of AI ethics principles have been proposed to tackle
these risks, but the outlines of organizational processes and practices for
ensuring socially responsible AI development are in a nascent state. To address
the paucity of comprehensive governance models, we present an AI governance
framework, the hourglass model of organizational AI governance, which targets
organizations that develop and use AI systems. The framework is designed to
help organizations deploying AI systems translate ethical AI principles into
practice and align their AI systems and processes with the forthcoming European
AI Act. The hourglass framework includes governance requirements at the
environmental, organizational, and AI system levels. At the AI system level, we
connect governance requirements to AI system life cycles to ensure governance
throughout the system's life span. The governance model highlights the systemic
nature of AI governance and opens new research avenues into its practical
implementation, the mechanisms that connect different AI governance layers, and
the dynamics between the AI governance actors. The model also offers a starting
point for organizational decision-makers to consider the governance components
needed to ensure social acceptability, mitigate risks, and realize the
potential of AI.

The stochastic nature of artificial intelligence (AI) models introduces risk
to business applications that use AI models without careful consideration. This
paper offers an approach to use AI techniques to gain insights on the usage of
the AI models and control how they are deployed to a production application.
  Keywords: artificial intelligence (AI), machine learning, microservices,
business process

Designing human-centered AI-driven applications require deep understandings
of how people develop mental models of AI. Currently, we have little knowledge
of this process and limited tools to study it. This paper presents the position
that AI-based games, particularly the player-AI interaction component, offer an
ideal domain to study the process in which mental models evolve. We present a
case study to illustrate the benefits of our approach for explainable AI.

Human-AI co-creation aims to combine human and AI strengths for artistic
results exceeding individual capabilities. Frameworks exist for painting,
music, and poetry, but choreography's embodied nature demands a dedicated
approach. This paper explores AI-assisted choreography techniques (e.g.,
generative ideation, embodied improvisation) and analyzes interaction design --
how humans and AI collaborate and communicate -- to inform the design
considerations of future human-AI choreography co-creation systems.

Ethics in AI has become a debated topic of public and expert discourse in
recent years. But what do people who build AI - AI practitioners - have to say
about their understanding of AI ethics and the challenges associated with
incorporating it in the AI-based systems they develop? Understanding AI
practitioners' views on AI ethics is important as they are the ones closest to
the AI systems and can bring about changes and improvements. We conducted a
survey aimed at understanding AI practitioners' awareness of AI ethics and
their challenges in incorporating ethics. Based on 100 AI practitioners'
responses, our findings indicate that majority of AI practitioners had a
reasonable familiarity with the concept of AI ethics, primarily due to
workplace rules and policies. Privacy protection and security was the ethical
principle that majority of them were aware of. Formal education/training was
considered somewhat helpful in preparing practitioners to incorporate AI
ethics. The challenges that AI practitioners faced in the development of
ethical AI-based systems included (i) general challenges, (ii)
technology-related challenges and (iii) human-related challenges. We also
identified areas needing further investigation and provided recommendations to
assist AI practitioners and companies in incorporating ethics into AI
development.

The rise in the use of AI/ML applications across industries has sparked more
discussions about the fairness of AI/ML in recent times. While prior research
on the fairness of AI/ML exists, there is a lack of empirical studies focused
on understanding the views and experiences of AI practitioners in developing a
fair AI/ML. Understanding AI practitioners' views and experiences on the
fairness of AI/ML is important because they are directly involved in its
development and deployment and their insights can offer valuable real-world
perspectives on the challenges associated with ensuring fairness in AI/ML. We
conducted semi-structured interviews with 22 AI practitioners to investigate
their understanding of what a 'fair AI/ML' is, the challenges they face in
developing a fair AI/ML, the consequences of developing an unfair AI/ML, and
the strategies they employ to ensure AI/ML fairness. We developed a framework
showcasing the relationship between AI practitioners' understanding of 'fair
AI/ML' and (i) their challenges in its development, (ii) the consequences of
developing an unfair AI/ML, and (iii) strategies used to ensure AI/ML fairness.
Additionally, we also identify areas for further investigation and offer
recommendations to aid AI practitioners and AI companies in navigating
fairness.

Instances of Artificial Intelligence (AI) systems failing to deliver
consistent, satisfactory performance are legion. We investigate why AI failures
occur. We address only a narrow subset of the broader field of AI Safety. We
focus on AI failures on account of flaws in conceptualization, design and
deployment. Other AI Safety issues like trade-offs between privacy and security
or convenience, bad actors hacking into AI systems to create mayhem or bad
actors deploying AI for purposes harmful to humanity and are out of scope of
our discussion. We find that AI systems fail on account of omission and
commission errors in the design of the AI system, as well as upon failure to
develop an appropriate interpretation of input information. Moreover, even when
there is no significant flaw in the AI software, an AI system may fail because
the hardware is incapable of robust performance across environments. Finally an
AI system is quite likely to fail in situations where, in effect, it is called
upon to deliver moral judgments -- a capability AI does not possess. We observe
certain trade-offs in measures to mitigate a subset of AI failures and provide
some recommendations.

Artificial intelligence (AI) has brought benefits, but it may also cause harm
if it is not appropriately developed. Current development is mainly driven by a
"technology-centered" approach, causing many failures. For example, the AI
Incident Database has documented over a thousand AI-related accidents. To
address these challenges, a human-centered AI (HCAI) approach has been promoted
and has received a growing level of acceptance over the last few years. HCAI
calls for combining AI with user experience (UX) design will enable the
development of AI systems (e.g., autonomous vehicles, intelligent user
interfaces, or intelligent decision-making systems) to achieve its design goals
such as usable/explainable AI, human-controlled AI, and ethical AI. While HCAI
promotion continues, it has not specifically addressed the collaboration
between AI and human-computer interaction (HCI) communities, resulting in
uncertainty about what action should be taken by both sides to apply HCAI in
developing AI systems. This Viewpoint focuses on the collaboration between the
AI and HCI communities, which leads to nine recommendations for effective
collaboration to enable HCAI in developing AI systems.

This paper examines the transformative role of Large Language Models (LLMs)
in education and their potential as learning tools, despite their inherent
risks and limitations. The authors propose seven approaches for utilizing AI in
classrooms: AI-tutor, AI-coach, AI-mentor, AI-teammate, AI-tool, AI-simulator,
and AI-student, each with distinct pedagogical benefits and risks. The aim is
to help students learn with and about AI, with practical strategies designed to
mitigate risks such as complacency about the AI's output, errors, and biases.
These strategies promote active oversight, critical assessment of AI outputs,
and complementarity of AI's capabilities with the students' unique insights. By
challenging students to remain the "human in the loop," the authors aim to
enhance learning outcomes while ensuring that AI serves as a supportive tool
rather than a replacement. The proposed framework offers a guide for educators
navigating the integration of AI-assisted learning in classrooms

Recent years have seen rapid deployment of mobile computing and Internet of
Things (IoT) networks, which can be mostly attributed to the increasing
communication and sensing capabilities of wireless systems. Big data analysis,
pervasive computing, and eventually artificial intelligence (AI) are envisaged
to be deployed on top of the IoT and create a new world featured by data-driven
AI. In this context, a novel paradigm of merging AI and wireless
communications, called Wireless AI that pushes AI frontiers to the network
edge, is widely regarded as a key enabler for future intelligent network
evolution. To this end, we present a comprehensive survey of the latest studies
in wireless AI from the data-driven perspective. Specifically, we first propose
a novel Wireless AI architecture that covers five key data-driven AI themes in
wireless networks, including Sensing AI, Network Device AI, Access AI, User
Device AI and Data-provenance AI. Then, for each data-driven AI theme, we
present an overview on the use of AI approaches to solve the emerging
data-related problems and show how AI can empower wireless network
functionalities. Particularly, compared to the other related survey papers, we
provide an in-depth discussion on the Wireless AI applications in various
data-driven domains wherein AI proves extremely useful for wireless network
design and optimization. Finally, research challenges and future visions are
also discussed to spur further research in this promising area.

In the last few years, AI continues demonstrating its positive impact on
society while sometimes with ethically questionable consequences. Building and
maintaining public trust in AI has been identified as the key to successful and
sustainable innovation. This chapter discusses the challenges related to
operationalizing ethical AI principles and presents an integrated view that
covers high-level ethical AI principles, the general notion of
trust/trustworthiness, and product/process support in the context of
responsible AI, which helps improve both trust and trustworthiness of AI for a
wider set of stakeholders.

This study explores the concept of creativity and artificial intelligence
(AI) and their recent integration. While AI has traditionally been perceived as
incapable of generating new ideas or creating art, the development of more
sophisticated AI models and the proliferation of human-computer interaction
tools have opened up new possibilities for AI in artistic creation. This study
investigates the various applications of AI in a creative context,
differentiating between the type of art, language, and algorithms used. It also
considers the philosophical implications of AI and creativity, questioning
whether consciousness can be researched in machines and AI's potential
interests and decision-making capabilities. Overall, we aim to stimulate a
reflection on AI's use and ethical implications in creative contexts.

AI agents are defined as artificial entities to perceive the environment,
make decisions and take actions. Inspired by the 6 levels of autonomous driving
by Society of Automotive Engineers, the AI agents are also categorized based on
utilities and strongness, as the following levels: L0, no AI, with tools taking
into account perception plus actions; L1, using rule-based AI; L2, making
rule-based AI replaced by IL/RL-based AI, with additional reasoning & decision
making; L3, applying LLM-based AI instead of IL/RL-based AI, additionally
setting up memory & reflection; L4, based on L3, facilitating autonomous
learning & generalization; L5, based on L4, appending personality of emotion
and character and collaborative behavior with multi-agents.

With the recent advancements in Artificial Intelligence (AI), various
organizations and individuals started debating about the progress of AI as a
blessing or a curse for the future of the society. This paper conducts an
investigation on how the public perceives the progress of AI by utilizing the
data shared on Twitter. Specifically, this paper performs a comparative
analysis on the understanding of users from two categories -- general
AI-Tweeters (AIT) and the expert AI-Tweeters (EAIT) who share posts about AI on
Twitter. Our analysis revealed that users from both the categories express
distinct emotions and interests towards AI. Users from both the categories
regard AI as positive and are optimistic about the progress of AI but the
experts are more negative than the general AI-Tweeters. Characterization of
users manifested that `London' is the popular location of users from where they
tweet about AI. Tweets posted by AIT are highly retweeted than posts made by
EAIT that reveals greater diffusion of information from AIT.

With the recent advancements in Artificial Intelligence (AI), various
organizations and individuals are debating about the progress of AI as a
blessing or a curse for the future of the society. This paper conducts an
investigation on how the public perceives the progress of AI by utilizing the
data shared on Twitter. Specifically, this paper performs a comparative
analysis on the understanding of users belonging to two categories -- general
AI-Tweeters (AIT) and expert AI-Tweeters (EAIT) who share posts about AI on
Twitter. Our analysis revealed that users from both the categories express
distinct emotions and interests towards AI. Users from both the categories
regard AI as positive and are optimistic about the progress of AI but the
experts are more negative than the general AI-Tweeters. Expert AI-Tweeters
share relatively large percentage of tweets about their personal news compared
to technical aspects of AI. However, the effects of automation on the future
are of primary concern to AIT than to EAIT. When the expert category is
sub-categorized, the emotion analysis revealed that students and industry
professionals have more insights in their tweets about AI than academicians.

Artificial Intelligence (AI) governance regulates the exercise of authority
and control over the management of AI. It aims at leveraging AI through
effective use of data and minimization of AI-related cost and risk. While
topics such as AI governance and AI ethics are thoroughly discussed on a
theoretical, philosophical, societal and regulatory level, there is limited
work on AI governance targeted to companies and corporations. This work views
AI products as systems, where key functionality is delivered by machine
learning (ML) models leveraging (training) data. We derive a conceptual
framework by synthesizing literature on AI and related fields such as ML. Our
framework decomposes AI governance into governance of data, (ML) models and
(AI) systems along four dimensions. It relates to existing IT and data
governance frameworks and practices. It can be adopted by practitioners and
academics alike. For practitioners the synthesis of mainly research papers, but
also practitioner publications and publications of regulatory bodies provides a
valuable starting point to implement AI governance, while for academics the
paper highlights a number of areas of AI governance that deserve more
attention.

The advent of artificial intelligence (AI) and machine learning (ML) bring
human-AI interaction to the forefront of HCI research. This paper argues that
games are an ideal domain for studying and experimenting with how humans
interact with AI. Through a systematic survey of neural network games (n = 38),
we identified the dominant interaction metaphors and AI interaction patterns in
these games. In addition, we applied existing human-AI interaction guidelines
to further shed light on player-AI interaction in the context of AI-infused
systems. Our core finding is that AI as play can expand current notions of
human-AI interaction, which are predominantly productivity-based. In
particular, our work suggests that game and UX designers should consider flow
to structure the learning curve of human-AI interaction, incorporate
discovery-based learning to play around with the AI and observe the
consequences, and offer users an invitation to play to explore new forms of
human-AI interaction.

AI is transforming the existing technology landscape at a rapid phase
enabling data-informed decision making and autonomous decision making. Unlike
any other technology, because of the decision-making ability of AI, ethics and
governance became a key concern. There are many emerging AI risks for humanity,
such as autonomous weapons, automation-spurred job loss, socio-economic
inequality, bias caused by data and algorithms, privacy violations and
deepfakes. Social diversity, equity and inclusion are considered key success
factors of AI to mitigate risks, create values and drive social justice.
Sustainability became a broad and complex topic entangled with AI. Many
organizations (government, corporate, not-for-profits, charities and NGOs) have
diversified strategies driving AI for business optimization and
social-and-environmental justice. Partnerships and collaborations become
important more than ever for equity and inclusion of diversified and
distributed people, data and capabilities. Therefore, in our journey towards an
AI-enabled sustainable future, we need to address AI ethics and governance as a
priority. These AI ethics and governance should be underpinned by human ethics.

Artificial Intelligence (AI) has received an increasing amount of attention
in multiple areas. The uncertainties and risks in AI-powered systems have
created reluctance in their wild adoption. As an economic solution to
compensate for potential damages, AI liability insurance is a promising market
to enhance the integration of AI into daily life. In this work, we use an
AI-powered E-diagnosis system as an example to study AI liability insurance. We
provide a quantitative risk assessment model with evidence-based numerical
analysis. We discuss the insurability criteria for AI technologies and suggest
necessary adjustments to accommodate the features of AI products. We show that
AI liability insurance can act as a regulatory mechanism to incentivize
compliant behaviors and serve as a certificate of high-quality AI systems.
Furthermore, we suggest premium adjustment to reflect the dynamic evolution of
the inherent uncertainty in AI. Moral hazard problems are discussed and
suggestions for AI liability insurance are provided.

In AI-assisted decision-making, humans often passively review AI's suggestion
and decide whether to accept or reject it as a whole. In such a paradigm,
humans are found to rarely trigger analytical thinking and face difficulties in
communicating the nuances of conflicting opinions to the AI when disagreements
occur. To tackle this challenge, we propose Human-AI Deliberation, a novel
framework to promote human reflection and discussion on conflicting human-AI
opinions in decision-making. Based on theories in human deliberation, this
framework engages humans and AI in dimension-level opinion elicitation,
deliberative discussion, and decision updates. To empower AI with deliberative
capabilities, we designed Deliberative AI, which leverages large language
models (LLMs) as a bridge between humans and domain-specific models to enable
flexible conversational interactions and faithful information provision. An
exploratory evaluation on a graduate admissions task shows that Deliberative AI
outperforms conventional explainable AI (XAI) assistants in improving humans'
appropriate reliance and task performance. Based on a mixed-methods analysis of
participant behavior, perception, user experience, and open-ended feedback, we
draw implications for future AI-assisted decision tool design.

Shifting the focus from principles to practical implementation, responsible
artificial intelligence (AI) has garnered considerable attention across
academia, industry, and society at large. Despite being in its nascent stages,
this emerging field grapples with nebulous concepts and intricate knowledge
frameworks. By analyzing three prevailing concepts - explainable AI,
trustworthy AI, and ethical AI, this study defined responsible AI and
identified its core principles. Methodologically, this study successfully
demonstrated the implementation of leveraging AI's capabilities into
bibliometrics for enhanced knowledge discovery and the cross-validation of
experimentally examined models with domain insights. Empirically, this study
investigated 17,799 research articles contributed by the AI community since
2015. This involves recognizing key technological players and their
relationships, unveiling the topical landscape and hierarchy of responsible AI,
charting its evolution, and elucidating the interplay between the
responsibility principles and primary AI techniques. An analysis of a core
cohort comprising 380 articles from multiple disciplines captures the most
recent advancements in responsible AI. As one of the pioneering bibliometric
studies dedicated to exploring responsible AI, this study will provide
comprehensive macro-level insights, enhancing the understanding of responsible
AI while furnishing valuable knowledge support for AI regulation and governance
initiatives.

In this paper, we propose "Confident AI" as a means to designing Artificial
Intelligence (AI) and Machine Learning (ML) systems with both algorithm and
user confidence in model predictions and reported results. The 4 basic tenets
of Confident AI are Repeatability, Believability, Sufficiency, and
Adaptability. Each of the tenets is used to explore fundamental issues in
current AI/ML systems and together provide an overall approach to Confident AI.

As AI continues to advance, human-AI teams are inevitable. However, progress
in AI is routinely measured in isolation, without a human in the loop. It is
crucial to benchmark progress in AI, not just in isolation, but also in terms
of how it translates to helping humans perform certain tasks, i.e., the
performance of human-AI teams.
  In this work, we design a cooperative game - GuessWhich - to measure human-AI
team performance in the specific context of the AI being a visual
conversational agent. GuessWhich involves live interaction between the human
and the AI. The AI, which we call ALICE, is provided an image which is unseen
by the human. Following a brief description of the image, the human questions
ALICE about this secret image to identify it from a fixed pool of images.
  We measure performance of the human-ALICE team by the number of guesses it
takes the human to correctly identify the secret image after a fixed number of
dialog rounds with ALICE. We compare performance of the human-ALICE teams for
two versions of ALICE. Our human studies suggest a counterintuitive trend -
that while AI literature shows that one version outperforms the other when
paired with an AI questioner bot, we find that this improvement in AI-AI
performance does not translate to improved human-AI performance. This suggests
a mismatch between benchmarking of AI in isolation and in the context of
human-AI teams.

The goal of the present paper is to develop and validate a questionnaire to
assess AI literacy. In particular, the questionnaire should be deeply grounded
in the existing literature on AI literacy, should be modular (i.e., including
different facets that can be used independently of each other) to be flexibly
applicable in professional life depending on the goals and use cases, and
should meet psychological requirements and thus includes further psychological
competencies in addition to the typical facets of AIL. We derived 60 items to
represent different facets of AI Literacy according to Ng and colleagues
conceptualisation of AI literacy and additional 12 items to represent
psychological competencies such as problem solving, learning, and emotion
regulation in regard to AI. For this purpose, data were collected online from
300 German-speaking adults. The items were tested for factorial structure in
confirmatory factor analyses. The result is a measurement instrument that
measures AI literacy with the facets Use & apply AI, Understand AI, Detect AI,
and AI Ethics and the ability to Create AI as a separate construct, and AI
Self-efficacy in learning and problem solving and AI Self-management. This
study contributes to the research on AI literacy by providing a measurement
instrument relying on profound competency models. In addition, higher-order
psychological competencies are included that are particularly important in the
context of pervasive change through AI systems.

Artificial intelligence (AI) is increasingly being considered to assist human
decision-making in high-stake domains (e.g. health). However, researchers have
discussed an issue that humans can over-rely on wrong suggestions of the AI
model instead of achieving human AI complementary performance. In this work, we
utilized salient feature explanations along with what-if, counterfactual
explanations to make humans review AI suggestions more analytically to reduce
overreliance on AI and explored the effect of these explanations on trust and
reliance on AI during clinical decision-making. We conducted an experiment with
seven therapists and ten laypersons on the task of assessing post-stroke
survivors' quality of motion, and analyzed their performance, agreement level
on the task, and reliance on AI without and with two types of AI explanations.
Our results showed that the AI model with both salient features and
counterfactual explanations assisted therapists and laypersons to improve their
performance and agreement level on the task when `right' AI outputs are
presented. While both therapists and laypersons over-relied on `wrong' AI
outputs, counterfactual explanations assisted both therapists and laypersons to
reduce their over-reliance on `wrong' AI outputs by 21\% compared to salient
feature explanations. Specifically, laypersons had higher performance degrades
by 18.0 f1-score with salient feature explanations and 14.0 f1-score with
counterfactual explanations than therapists with performance degrades of 8.6
and 2.8 f1-scores respectively. Our work discusses the potential of
counterfactual explanations to better estimate the accuracy of an AI model and
reduce over-reliance on `wrong' AI outputs and implications for improving
human-AI collaborative decision-making.

Recent advancements in the field of Artificial Intelligence (AI) establish
the basis to address challenging tasks. However, with the integration of AI,
new risks arise. Therefore, to benefit from its advantages, it is essential to
adequately handle the risks associated with AI. Existing risk management
processes in related fields, such as software systems, need to sufficiently
consider the specifics of AI. A key challenge is to systematically and
transparently identify and address AI risks' root causes - also called AI
hazards. This paper introduces the AI Hazard Management (AIHM) framework, which
provides a structured process to systematically identify, assess, and treat AI
hazards. The proposed process is conducted in parallel with the development to
ensure that any AI hazard is captured at the earliest possible stage of the AI
system's life cycle. In addition, to ensure the AI system's auditability, the
proposed framework systematically documents evidence that the potential impact
of identified AI hazards could be reduced to a tolerable level. The framework
builds upon an AI hazard list from a comprehensive state-of-the-art analysis.
Also, we provide a taxonomy that supports the optimal treatment of the
identified AI hazards. Additionally, we illustrate how the AIHM framework can
increase the overall quality of a power grid AI use case by systematically
reducing the impact of identified hazards to an acceptable level.

Recent years have witnessed an increasing number of artificial intelligence
(AI) applications in transportation. As a new and emerging technology, AI's
potential to advance transportation goals and the full extent of its impacts on
the transportation sector is not yet well understood. As the transportation
community explores these topics, it is critical to understand how
transportation professionals, the driving force behind AI Transportation
applications, perceive AI's potential efficiency and equity impacts. Toward
this goal, we surveyed transportation professionals in the United States and
collected a total of 354 responses. Based on the survey responses, we conducted
both descriptive analysis and latent class cluster analysis (LCCA). The former
provides an overview of prevalent attitudes among transportation professionals,
while the latter allows the identification of distinct segments based on their
latent attitudes toward AI. We find widespread optimism regarding AI's
potential to improve many aspects of transportation (e.g., efficiency, cost
reduction, and traveler experience); however, responses are mixed regarding
AI's potential to advance equity. Moreover, many respondents are concerned that
AI ethics are not well understood in the transportation community and that AI
use in transportation could exaggerate existing inequalities. Through LCCA, we
have identified four latent segments: AI Neutral, AI Optimist, AI Pessimist,
and AI Skeptic. The latent class membership is significantly associated with
respondents' age, education level, and AI knowledge level. Overall, the study
results shed light on the extent to which the transportation community as a
whole is ready to leverage AI systems to transform current practices and inform
targeted education to improve the understanding of AI among transportation
professionals.

In the ever-expanding landscape of Artificial Intelligence (AI), where
innovation thrives and new products and services are continuously being
delivered, ensuring that AI systems are designed and developed responsibly
throughout their entire lifecycle is crucial. To this end, several AI ethics
principles and guidelines have been issued to which AI systems should conform.
Nevertheless, relying solely on high-level AI ethics principles is far from
sufficient to ensure the responsible engineering of AI systems. In this field,
AI professionals often navigate by sight. Indeed, while recommendations
promoting Trustworthy AI (TAI) exist, these are often high-level statements
that are difficult to translate into concrete implementation strategies. There
is a significant gap between high-level AI ethics principles and low-level
concrete practices for AI professionals. To address this challenge, our work
presents an experience report where we develop a novel holistic framework for
Trustworthy AI - designed to bridge the gap between theory and practice - and
report insights from its application in an industrial case study. The framework
is built on the result of a systematic review of the state of the practice, a
survey, and think-aloud interviews with 34 AI practitioners. The framework,
unlike most of those already in the literature, is designed to provide
actionable guidelines and tools to support different types of stakeholders
throughout the entire Software Development Life Cycle (SDLC). Our goal is to
empower AI professionals to confidently navigate the ethical dimensions of TAI
through practical insights, ensuring that the vast potential of AI is exploited
responsibly for the benefit of society as a whole.

To benefit from AI advances, users and operators of AI systems must have
reason to trust it. Trust arises from multiple interactions, where predictable
and desirable behavior is reinforced over time. Providing the system's users
with some understanding of AI operations can support predictability, but
forcing AI to explain itself risks constraining AI capabilities to only those
reconcilable with human cognition. We argue that AI systems should be designed
with features that build trust by bringing decision-analytic perspectives and
formal tools into AI. Instead of trying to achieve explainable AI, we should
develop interpretable and actionable AI. Actionable and Interpretable AI (AI2)
will incorporate explicit quantifications and visualizations of user confidence
in AI recommendations. In doing so, it will allow examining and testing of AI
system predictions to establish a basis for trust in the systems' decision
making and ensure broad benefits from deploying and advancing its computational
capabilities.

Recent years have witnessed an astonishing explosion in the evolution of
mobile applications powered by AI technologies. The rapid growth of AI
frameworks enables the transition of AI technologies to mobile devices,
significantly prompting the adoption of AI apps (i.e., apps that integrate AI
into their functions) among smartphone devices. In this paper, we conduct the
most extensive empirical study on 56,682 published AI apps from three
perspectives: dataset characteristics, development issues, and user feedback
and privacy. To this end, we build an automated AI app identification tool, AI
Discriminator, that detects eligible AI apps from 7,259,232 mobile apps. First,
we carry out a dataset analysis, where we explore the AndroZoo large repository
to identify AI apps and their core characteristics. Subsequently, we pinpoint
key issues in AI app development (e.g., model protection). Finally, we focus on
user reviews and user privacy protection. Our paper provides several notable
findings. Some essential ones involve revealing the issue of insufficient model
protection by presenting the lack of model encryption, and demonstrating the
risk of user privacy data being leaked. We published our large-scale AI app
datasets to inspire more future research.

With the increasing prevalence of artificial intelligence (AI) in diverse
science/engineering communities, AI models emerge on an unprecedented scale
among various domains. However, given the complexity and diversity of the
software and hardware environments, reusing AI artifacts (models and datasets)
is extremely challenging, especially with AI-driven science applications.
Building an ecosystem to run and reuse AI applications/datasets at scale
efficiently becomes increasingly essential for diverse science and engineering
and high-performance computing (HPC) communities. In this paper, we innovate
over an HPC-AI ecosystem -- HPCFair, which enables the Findable, Accessible,
Interoperable, and Reproducible (FAIR) principles. HPCFair enables the
collection of AI models/datasets allowing users to download/upload AI artifacts
with authentications. Most importantly, our proposed framework provides
user-friendly APIs for users to easily run inference jobs and customize AI
artifacts to their tasks as needed. Our results show that, with HPCFair API,
users irrespective of technical expertise in AI, can easily leverage AI
artifacts to their tasks with minimal effort.

Human-AI interaction in text production increases complexity in authorship.
In two empirical studies (n1 = 30 & n2 = 96), we investigate authorship and
ownership in human-AI collaboration for personalized language generation. We
show an AI Ghostwriter Effect: Users do not consider themselves the owners and
authors of AI-generated text but refrain from publicly declaring AI authorship.
Personalization of AI-generated texts did not impact the AI Ghostwriter Effect,
and higher levels of participants' influence on texts increased their sense of
ownership. Participants were more likely to attribute ownership to supposedly
human ghostwriters than AI ghostwriters, resulting in a higher
ownership-authorship discrepancy for human ghostwriters. Rationalizations for
authorship in AI ghostwriters and human ghostwriters were similar. We discuss
how our findings relate to psychological ownership and human-AI interaction to
lay the foundations for adapting authorship frameworks and user interfaces in
AI in text-generation tasks.

This pioneering study explores students' perceptions of AI-giarism, an
emergent form of academic dishonesty involving AI and plagiarism, within the
higher education context. A survey, undertaken by 393 undergraduate and
postgraduate students from a variety of disciplines, investigated their
perceptions of diverse AI-giarism scenarios. The findings portray a complex
landscape of understanding, with clear disapproval for direct AI content
generation, yet more ambivalent attitudes towards subtler uses of AI. The study
introduces a novel instrument, as an initial conceptualization of AI-giarism,
offering a significant tool for educators and policy-makers. This scale
facilitates understanding and discussions around AI-related academic
misconduct, aiding in pedagogical design and assessment in an era of AI
integration. Moreover, it challenges traditional definitions of academic
misconduct, emphasizing the need to adapt in response to evolving AI
technology. Despite limitations, such as the rapidly changing nature of AI and
the use of convenience sampling, the study provides pivotal insights for
academia, policy-making, and the broader integration of AI technology in
education.

Gender bias is rampant in AI systems, causing bad user experience,
injustices, and mental harm to women. School curricula fail to educate AI
creators on this topic, leaving them unprepared to mitigate gender bias in AI.
In this paper, we designed hands-on tutorials to raise AI creators' awareness
of gender bias in AI and enhance their knowledge of sources of gender bias and
debiasing techniques. The tutorials were evaluated with 18 AI creators,
including AI researchers, AI industrial practitioners (i.e., developers and
product managers), and students who had learned AI. Their improved awareness
and knowledge demonstrated the effectiveness of our tutorials, which have the
potential to complement the insufficient AI gender bias education in CS/AI
courses. Based on the findings, we synthesize design implications and a rubric
to guide future research, education, and design efforts.

AI alignment considers the overall problem of ensuring an AI produces desired
outcomes, without undesirable side effects. While often considered from the
perspectives of safety and human values, AI alignment can also be considered in
the context of designing and evaluating interfaces for interactive AI systems.
This paper maps concepts from AI alignment onto a basic, three step interaction
cycle, yielding a corresponding set of alignment objectives: 1) specification
alignment: ensuring the user can efficiently and reliably communicate
objectives to the AI, 2) process alignment: providing the ability to verify and
optionally control the AI's execution process, and 3) evaluation support:
ensuring the user can verify and understand the AI's output. We also introduce
the concepts of a surrogate process, defined as a simplified, separately
derived, but controllable representation of the AI's actual process; and the
notion of a Process Gulf, which highlights how differences between human and AI
processes can lead to challenges in AI control. To illustrate the value of this
framework, we describe commercial and research systems along each of the three
alignment dimensions, and show how interfaces that provide interactive
alignment mechanisms can lead to qualitatively different and improved user
experiences.

As artificial intelligence (AI) is integrated into various services and
systems in society, many companies and organizations have proposed AI
principles, policies, and made the related commitments. Conversely, some have
proposed the need for independent audits, arguing that the voluntary principles
adopted by the developers and providers of AI services and systems
insufficiently address risk. This policy recommendation summarizes the issues
related to the auditing of AI services and systems and presents three
recommendations for promoting AI auditing that contribute to sound AI
governance. Recommendation1.Development of institutional design for AI audits.
Recommendation2.Training human resources for AI audits. Recommendation3.
Updating AI audits in accordance with technological progress.
  In this policy recommendation, AI is assumed to be that which recognizes and
predicts data with the last chapter outlining how generative AI should be
audited.

This article critically examines the recent hype around AI safety. We first
start with noting the nature of the AI safety hype as being dominated by
governments and corporations, and contrast it with other avenues within AI
research on advancing social good. We consider what 'AI safety' actually means,
and outline the dominant concepts that the digital footprint of AI safety
aligns with. We posit that AI safety has a nuanced and uneasy relationship with
transparency and other allied notions associated with societal good, indicating
that it is an insufficient notion if the goal is that of societal good in a
broad sense. We note that the AI safety debate has already influenced some
regulatory efforts in AI, perhaps in not so desirable directions. We also share
our concerns on how AI safety may normalize AI that advances structural harm
through providing exploitative and harmful AI with a veneer of safety.

This paper briefly reviews the history of meta-learning and describes its
contribution to general AI. Meta-learning improves model generalization
capacity and devises general algorithms applicable to both in-distribution and
out-of-distribution tasks potentially. General AI replaces task-specific models
with general algorithmic systems introducing higher level of automation in
solving diverse tasks using AI. We summarize main contributions of
meta-learning to the developments in general AI, including memory module,
meta-learner, coevolution, curiosity, forgetting and AI-generating algorithm.
We present connections between meta-learning and general AI and discuss how
meta-learning can be used to formulate general AI algorithms.

Artificial Intelligence (AI) is an integral part of our daily technology use
and will likely be a critical component of emerging technologies. However,
negative user preconceptions may hinder adoption of AI-based decision making.
Prior work has highlighted the potential of factors such as transparency and
explainability in improving user perceptions of AI. We further contribute to
work on improving user perceptions of AI by demonstrating that bringing the
user in the loop through mock model training can improve their perceptions of
an AI agent's capability and their comfort with the possibility of using
technology employing the AI agent.

Recent AI ethics has focused on applying abstract principles downward to
practice. This paper moves in the other direction. Ethical insights are
generated from the lived experiences of AI-designers working on tangible human
problems, and then cycled upward to influence theoretical debates surrounding
these questions: 1) Should AI as trustworthy be sought through explainability,
or accurate performance? 2) Should AI be considered trustworthy at all, or is
reliability a preferable aim? 3) Should AI ethics be oriented toward
establishing protections for users, or toward catalyzing innovation? Specific
answers are less significant than the larger demonstration that AI ethics is
currently unbalanced toward theoretical principles, and will benefit from
increased exposure to grounded practices and dilemmas.

Not too long ago, AI security used to mean the research and practice of how
AI can empower cybersecurity, that is, AI for security. Ever since Ian
Goodfellow and his team popularized adversarial attacks on machine learning,
security for AI became an important concern and also part of AI security. It is
imperative to understand the threats to machine learning products and avoid
common pitfalls in AI product development. This article is addressed to
developers, designers, managers and researchers of AI software products.

This article explores the transformative impact of artificial intelligence
(AI) on scientific research. It highlights ten ways in which AI is
revolutionizing the work of scientists, including powerful referencing tools,
improved understanding of research problems, enhanced research question
generation, optimized research design, stub data generation, data
transformation, advanced data analysis, and AI-assisted reporting. While AI
offers numerous benefits, challenges such as bias, privacy concerns, and the
need for human-AI collaboration must be considered. The article emphasizes that
AI can augment human creativity in science but not replace it.

Can governments build AI? In this paper, we describe an ongoing effort to
develop ``public AI'' -- publicly accessible AI models funded, provisioned, and
governed by governments or other public bodies. Public AI presents both an
alternative and a complement to standard regulatory approaches to AI, but it
also suggests new technical and policy challenges. We present a roadmap for how
the ML research community can help shape this initiative and support its
implementation, and how public AI can complement other responsible AI
initiatives.

Trusted AI literature to date has focused on the trust needs of users who
knowingly interact with discrete AIs. Conspicuously absent from the literature
is a rigorous treatment of public trust in AI. We argue that public distrust of
AI originates from the under-development of a regulatory ecosystem that would
guarantee the trustworthiness of the AIs that pervade society. Drawing from
structuration theory and literature on institutional trust, we offer a model of
public trust in AI that differs starkly from models driving Trusted AI efforts.
This model provides a theoretical scaffolding for Trusted AI research which
underscores the need to develop nothing less than a comprehensive and visibly
functioning regulatory ecosystem. We elaborate the pivotal role of externally
auditable AI documentation within this model and the work to be done to ensure
it is effective, and outline a number of actions that would promote public
trust in AI. We discuss how existing efforts to develop AI documentation within
organizations -- both to inform potential adopters of AI components and support
the deliberations of risk and ethics review boards -- is necessary but
insufficient assurance of the trustworthiness of AI. We argue that being
accountable to the public in ways that earn their trust, through elaborating
rules for AI and developing resources for enforcing these rules, is what will
ultimately make AI trustworthy enough to be woven into the fabric of our
society.

Analyzing usability test videos is arduous. Although recent research showed
the promise of AI in assisting with such tasks, it remains largely unknown how
AI should be designed to facilitate effective collaboration between user
experience (UX) evaluators and AI. Inspired by the concepts of agency and work
context in human and AI collaboration literature, we studied two corresponding
design factors for AI-assisted UX evaluation: explanations and synchronization.
Explanations allow AI to further inform humans how it identifies UX problems
from a usability test session; synchronization refers to the two ways humans
and AI collaborate: synchronously and asynchronously. We iteratively designed a
tool, AI Assistant, with four versions of UIs corresponding to the two levels
of explanations (with/without) and synchronization (sync/async). By adopting a
hybrid wizard-of-oz approach to simulating an AI with reasonable performance,
we conducted a mixed-method study with 24 UX evaluators identifying UX problems
from usability test videos using AI Assistant. Our quantitative and qualitative
results show that AI with explanations, regardless of being presented
synchronously or asynchronously, provided better support for UX evaluators'
analysis and was perceived more positively; when without explanations,
synchronous AI better improved UX evaluators' performance and engagement
compared to the asynchronous AI. Lastly, we present the design implications for
AI-assisted UX evaluation and facilitating more effective human-AI
collaboration.

Structured access is an emerging paradigm for the safe deployment of
artificial intelligence (AI). Instead of openly disseminating AI systems,
developers facilitate controlled, arm's length interactions with their AI
systems. The aim is to prevent dangerous AI capabilities from being widely
accessible, whilst preserving access to AI capabilities that can be used
safely. The developer must both restrict how the AI system can be used, and
prevent the user from circumventing these restrictions through modification or
reverse engineering of the AI system. Structured access is most effective when
implemented through cloud-based AI services, rather than disseminating AI
software that runs locally on users' hardware. Cloud-based interfaces provide
the AI developer greater scope for controlling how the AI system is used, and
for protecting against unauthorized modifications to the system's design. This
chapter expands the discussion of "publication norms" in the AI community,
which to date has focused on the question of how the informational content of
AI research projects should be disseminated (e.g., code and models). Although
this is an important question, there are limits to what can be achieved through
the control of information flows. Structured access views AI software not only
as information that can be shared but also as a tool with which users can have
arm's length interactions. There are early examples of structured access being
practiced by AI developers, but there is much room for further development,
both in the functionality of cloud-based interfaces and in the wider
institutional framework.

Problem statement: Standardisation of AI fairness rules and benchmarks is
challenging because AI fairness and other ethical requirements depend on
multiple factors such as context, use case, type of the AI system, and so on.
In this paper, we elaborate that the AI system is prone to biases at every
stage of its lifecycle, from inception to its usage, and that all stages
require due attention for mitigating AI bias. We need a standardised approach
to handle AI fairness at every stage. Gap analysis: While AI fairness is a hot
research topic, a holistic strategy for AI fairness is generally missing. Most
researchers focus only on a few facets of AI model-building. Peer review shows
excessive focus on biases in the datasets, fairness metrics, and algorithmic
bias. In the process, other aspects affecting AI fairness get ignored. The
solution proposed: We propose a comprehensive approach in the form of a novel
seven-layer model, inspired by the Open System Interconnection (OSI) model, to
standardise AI fairness handling. Despite the differences in the various
aspects, most AI systems have similar model-building stages. The proposed model
splits the AI system lifecycle into seven abstraction layers, each
corresponding to a well-defined AI model-building or usage stage. We also
provide checklists for each layer and deliberate on potential sources of bias
in each layer and their mitigation methodologies. This work will facilitate
layer-wise standardisation of AI fairness rules and benchmarking parameters.

Which part of medicine, if any, can and should be entrusted to AI, now or at
some moment in the future? That both medicine and AI will continue to change
goes without saying.

As Artificial Intelligence (AI) plays an ever-expanding role in
sociotechnical systems, it is important to articulate the relationships between
humans and AI. However, the scholarly communities studying human-AI
relationships -- including but not limited to social computing, machine
learning, science and technology studies, and other social sciences -- are
divided by the perspectives that define them. These perspectives vary both by
their focus on humans or AI, and in the micro/macro lenses through which they
approach subjects. These differences inhibit the integration of findings, and
thus impede science and interdisciplinarity. In this position paper, we propose
the development of a framework AI-Mediated Exchange Theory (AI-MET) to bridge
these divides. As an extension to Social Exchange Theory (SET) in the social
sciences, AI-MET views AI as influencing human-to-human relationships via a
taxonomy of mediation mechanisms. We list initial ideas of these mechanisms,
and show how AI-MET can be used to help human-AI research communities speak to
one another.

There is an increasing interest in the entwining of the field of antitrust
with the field of Artificial Intelligence (AI), frequently referred to jointly
as Antitrust and AI (AAI) in the research literature. This study focuses on the
synergies entangling antitrust and AI, doing so to extend the literature by
proffering the primary ways that these two fields intersect, consisting of: (1)
the application of antitrust to AI, and (2) the application of AI to antitrust.
To date, most of the existing research on this intermixing has concentrated on
the former, namely the application of antitrust to AI, entailing how the
marketplace will be altered by the advent of AI and the potential for adverse
antitrust behaviors arising accordingly. Opting to explore more deeply the
other side of this coin, this research closely examines the application of AI
to antitrust and establishes an antitrust vigilance lifecycle to which AI is
predicted to be substantively infused for purposes of enabling and bolstering
antitrust detection, enforcement, and post-enforcement monitoring. Furthermore,
a gradual and incremental injection of AI into antitrust vigilance is
anticipated to occur as significant advances emerge amidst the Levels of
Autonomy (LoA) for AI Legal Reasoning (AILR).

We consider two fundamental and related issues currently faced by Artificial
Intelligence (AI) development: the lack of ethics and interpretability of AI
decisions. Can interpretable AI decisions help to address ethics in AI? Using a
randomized study, we experimentally show that the empirical and liberal turn of
the production of explanations tends to select AI explanations with a low
denunciatory power. Under certain conditions, interpretability tools are
therefore not means but, paradoxically, obstacles to the production of ethical
AI since they can give the illusion of being sensitive to ethical incidents. We
also show that the denunciatory power of AI explanations is highly dependent on
the context in which the explanation takes place, such as the gender or
education level of the person to whom the explication is intended for. AI
ethics tools are therefore sometimes too flexible and self-regulation through
the liberal production of explanations do not seem to be enough to address
ethical issues. We then propose two scenarios for the future development of
ethical AI: more external regulation or more liberalization of AI explanations.
These two opposite paths will play a major role on the future development of
ethical AI.

The rapid development of Artificial Intelligence (AI) technology has enabled
the deployment of various systems based on it. However, many current AI systems
are found vulnerable to imperceptible attacks, biased against underrepresented
groups, lacking in user privacy protection. These shortcomings degrade user
experience and erode people's trust in all AI systems. In this review, we
provide AI practitioners with a comprehensive guide for building trustworthy AI
systems. We first introduce the theoretical framework of important aspects of
AI trustworthiness, including robustness, generalization, explainability,
transparency, reproducibility, fairness, privacy preservation, and
accountability. To unify currently available but fragmented approaches toward
trustworthy AI, we organize them in a systematic approach that considers the
entire lifecycle of AI systems, ranging from data acquisition to model
development, to system development and deployment, finally to continuous
monitoring and governance. In this framework, we offer concrete action items
for practitioners and societal stakeholders (e.g., researchers, engineers, and
regulators) to improve AI trustworthiness. Finally, we identify key
opportunities and challenges for the future development of trustworthy AI
systems, where we identify the need for a paradigm shift toward comprehensively
trustworthy AI systems.

There is a growing consensus in HCI and AI research that the design of AI
systems needs to engage and empower stakeholders who will be affected by AI.
However, the manner in which stakeholders should participate in AI design is
unclear. This workshop paper aims to ground what we dub a 'participatory turn'
in AI design by synthesizing existing literature on participation and through
empirical analysis of its current practices via a survey of recent published
research and a dozen semi-structured interviews with AI researchers and
practitioners. Based on our literature synthesis and empirical research, this
paper presents a conceptual framework for analyzing participatory approaches to
AI design and articulates a set of empirical findings that in ensemble detail
out the contemporary landscape of participatory practice in AI design. These
findings can help bootstrap a more principled discussion on how PD of AI should
move forward across AI, HCI, and other research communities.

Explainable AI (XAI) research has been booming, but the question "$\textbf{To
whom}$ are we making AI explainable?" is yet to gain sufficient attention. Not
much of XAI is comprehensible to non-AI experts, who nonetheless, are the
primary audience and major stakeholders of deployed AI systems in practice. The
gap is glaring: what is considered "explained" to AI-experts versus non-experts
are very different in practical scenarios. Hence, this gap produced two
distinct cultures of expectations, goals, and forms of XAI in real-life AI
deployments.
  We advocate that it is critical to develop XAI methods for non-technical
audiences. We then present a real-life case study, where AI experts provided
non-technical explanations of AI decisions to non-technical stakeholders, and
completed a successful deployment in a highly regulated industry. We then
synthesize lessons learned from the case, and share a list of suggestions for
AI experts to consider when explaining AI decisions to non-technical
stakeholders.

Although AI is transforming the world, there are serious concerns about its
ability to behave and make decisions responsibly. Many ethical regulations,
principles, and frameworks for responsible AI have been issued recently.
However, they are high level and difficult to put into practice. On the other
hand, most AI researchers focus on algorithmic solutions, while the responsible
AI challenges actually crosscut the entire engineering lifecycle and components
of AI systems. To close the gap in operationalizing responsible AI, this paper
aims to develop a roadmap on software engineering for responsible AI. The
roadmap focuses on (i) establishing multi-level governance for responsible AI
systems, (ii) setting up the development processes incorporating
process-oriented practices for responsible AI systems, and (iii) building
responsible-AI-by-design into AI systems through system-level architectural
style, patterns and techniques.

Human-AI co-creativity involves humans and AI collaborating on a shared
creative product as partners. In many existing co-creative systems, users
communicate with the AI using buttons or sliders. However, typically, the AI in
co-creative systems cannot communicate back to humans, limiting their potential
to be perceived as partners. This paper starts with an overview of a
comparative study with 38 participants to explore the impact of AI-to-human
communication on user perception and engagement in co-creative systems and the
results show improved collaborative experience and user engagement with the
system incorporating AI-to-human communication. The results also demonstrate
that users perceive co-creative AI as more reliable, personal and intelligent
when it can communicate with the users. The results indicate a need to identify
potential ethical issues from an engaging communicating co-creative AI. Later
in the paper, we present some potential ethical issues in human-AI co-creation
and propose to use participatory design fiction as the research methodology to
investigate the ethical issues associated with a co-creative AI that
communicates with users.

Numerous AI ethics checklists and frameworks have been proposed focusing on
different dimensions of ethical AI such as fairness, explainability, and
safety. Yet, no such work has been done on developing transparent AI systems
for real-world educational scenarios. This paper presents a Transparency Index
framework that has been iteratively co-designed with different stakeholders of
AI in education, including educators, ed-tech experts, and AI practitioners. We
map the requirements of transparency for different categories of stakeholders
of AI in education and demonstrate that transparency considerations are
embedded in the entire AI development process from the data collection stage
until the AI system is deployed in the real world and iteratively improved. We
also demonstrate how transparency enables the implementation of other ethical
AI dimensions in Education like interpretability, accountability, and safety.
In conclusion, we discuss the directions for future research in this newly
emerging field. The main contribution of this study is that it highlights the
importance of transparency in developing AI-powered educational technologies
and proposes an index framework for its conceptualization for AI in education.

With the powerful performance of Artificial Intelligence (AI) also comes
prevalent ethical issues. Though governments and corporations have curated
multiple AI ethics guidelines to curb unethical behavior of AI, the effect has
been limited, probably due to the vagueness of the guidelines. In this paper,
we take a closer look at how AI ethics issues take place in real world, in
order to have a more in-depth and nuanced understanding of different ethical
issues as well as their social impact. With a content analysis of AI Incident
Database, which is an effort to prevent repeated real world AI failures by
cataloging incidents, we identified 13 application areas which often see
unethical use of AI, with intelligent service robots, language/vision models
and autonomous driving taking the lead. Ethical issues appear in 8 different
forms, from inappropriate use and racial discrimination, to physical safety and
unfair algorithm. With this taxonomy of AI ethics issues, we aim to provide AI
practitioners with a practical guideline when trying to deploy AI applications
ethically.

The possibility of super-AIs taking over the world has been intensively
studied by numerous scholars. This paper focuses on the multi-AI competition
scenario under the premise of super-AIs in power. Firstly, the article points
out the defects of existing arguments supporting single-AI domination and
presents arguments in favour of multi-AI competition. Then the article
concludes that the multi-AI competition situation is a non-negligible
possibility. Attention then turns to whether multi-AI competition is better for
the overall good of humanity than a situation where a single AI is in power.
After analysing the best, worst, and intermediate scenarios, the article
concludes that multi-AI competition is better for humanity. Finally,
considering the factors related to the formation of the best-case scenario of
multiple AIs, the article gives some suggestions for current initiatives in AI
development.

People work with AI systems to improve their decision making, but often
under- or over-rely on AI predictions and perform worse than they would have
unassisted. To help people appropriately rely on AI aids, we propose showing
them behavior descriptions, details of how AI systems perform on subgroups of
instances. We tested the efficacy of behavior descriptions through user studies
with 225 participants in three distinct domains: fake review detection,
satellite image classification, and bird classification. We found that behavior
descriptions can increase human-AI accuracy through two mechanisms: helping
people identify AI failures and increasing people's reliance on the AI when it
is more accurate. These findings highlight the importance of people's mental
models in human-AI collaboration and show that informing people of high-level
AI behaviors can significantly improve AI-assisted decision making.

UX practitioners (UXPs) face novel challenges when working with and
communicating artificial intelligence (AI) as a design material. We explore how
UXPs communicate AI concepts when given hands-on experience training and
experimenting with AI models. To do so, we conducted a task-based design study
with 27 UXPs in which they prototyped and created a design presentation for a
AI-enabled interface while having access to a simple AI model training tool.
Through analyzing UXPs' design presentations and post-activity interviews, we
found that although UXPs struggled to clearly communicate some AI concepts,
tinkering with AI broadened common ground when communicating with technical
stakeholders. UXPs also identified key risks and benefits of AI in their
designs, and proposed concrete next steps for both UX and AI work. We conclude
with a sensitizing concept and recommendations for design and AI tools to
enhance multi-stakeholder communication and collaboration when crafting
human-centered AI experiences.

An essential element of K-12 AI literacy is educating learners about the
ethical and societal implications of AI systems. Previous work in AI ethics
literacy have developed curriculum and classroom activities that engage
learners in reflecting on the ethical implications of AI systems and developing
responsible AI. There is little work in using game-based learning methods in AI
literacy. Games are known to be compelling media to teach children about
complex STEM concepts. In this work, we developed a competitive card game for
middle and high school students called "AI Audit" where they play as AI
start-up founders building novel AI-powered technology. Players can challenge
other players with potential harms of their technology or defend their own
businesses by features that mitigate these harms. The game mechanics reward
systems that are ethically developed or that take steps to mitigate potential
harms. In this paper, we present the game design, teacher resources for
classroom deployment and early playtesting results. We discuss our reflections
about using games as teaching tools for AI literacy in K-12 classrooms.

Foundation models, such as GPT-4, DALL-E have brought unprecedented AI
"operating system" effect and new forms of human-AI interaction, sparking a
wave of innovation in AI-native services, where natural language prompts serve
as executable "code" directly (prompt as executable code), eliminating the need
for programming language as an intermediary and opening up the door to personal
AI. Prompt Sapper has emerged in response, committed to support the development
of AI-native services by AI chain engineering. It creates a large language
model (LLM) empowered software engineering infrastructure for authoring AI
chains through human-AI collaborative intelligence, unleashing the AI
innovation potential of every individual, and forging a future where everyone
can be a master of AI innovation. This article will introduce the R\&D
motivation behind Prompt Sapper, along with its corresponding AI chain
engineering methodology and technical practices.

This paper argues that a range of current AI systems have learned how to
deceive humans. We define deception as the systematic inducement of false
beliefs in the pursuit of some outcome other than the truth. We first survey
empirical examples of AI deception, discussing both special-use AI systems
(including Meta's CICERO) built for specific competitive situations, and
general-purpose AI systems (such as large language models). Next, we detail
several risks from AI deception, such as fraud, election tampering, and losing
control of AI systems. Finally, we outline several potential solutions to the
problems posed by AI deception: first, regulatory frameworks should subject AI
systems that are capable of deception to robust risk-assessment requirements;
second, policymakers should implement bot-or-not laws; and finally,
policymakers should prioritize the funding of relevant research, including
tools to detect AI deception and to make AI systems less deceptive.
Policymakers, researchers, and the broader public should work proactively to
prevent AI deception from destabilizing the shared foundations of our society.

Heightened AI expectations facilitate performance in human-AI interactions
through placebo effects. While lowering expectations to control for placebo
effects is advisable, overly negative expectations could induce nocebo effects.
In a letter discrimination task, we informed participants that an AI would
either increase or decrease their performance by adapting the interface, but in
reality, no AI was present in any condition. A Bayesian analysis showed that
participants had high expectations and performed descriptively better
irrespective of the AI description when a sham-AI was present. Using cognitive
modeling, we could trace this advantage back to participants gathering more
information. A replication study verified that negative AI descriptions do not
alter expectations, suggesting that performance expectations with AI are biased
and robust to negative verbal descriptions. We discuss the impact of user
expectations on AI interactions and evaluation and provide a behavioral placebo
marker for human-AI interaction

This empirical study serves as a primer for interested service providers to
determine if and how Large Language Models (LLMs) technology will be integrated
for their practitioners and the broader community. We investigate the mutual
learning journey of non-AI experts and AI through CoAGent, a service
co-creation tool with LLM-based agents. Engaging in a three-stage participatory
design processes, we work with with 23 domain experts from public libraries
across the U.S., uncovering their fundamental challenges of integrating AI into
human workflows. Our findings provide 23 actionable "heuristics for service
co-creation with AI", highlighting the nuanced shared responsibilities between
humans and AI. We further exemplar 9 foundational agency aspects for AI,
emphasizing essentials like ownership, fair treatment, and freedom of
expression. Our innovative approach enriches the participatory design model by
incorporating AI as crucial stakeholders and utilizing AI-AI interaction to
identify blind spots. Collectively, these insights pave the way for synergistic
and ethical human-AI co-creation in service contexts, preparing for workforce
ecosystems where AI coexists.

Recent AI research has significantly reduced the barriers to apply AI, but
the process of setting up the necessary tools and frameworks can still be a
challenge. While AI-as-a-Service platforms have emerged to simplify the
training and deployment of AI models, they still fall short of achieving true
democratization of AI. In this paper, we aim to address this gap by comparing
several popular AI-as-a-Service platforms and identifying the key requirements
for a platform that can achieve true democratization of AI. Our analysis
highlights the need for self-hosting options, high scalability, and openness.
To address these requirements, we propose our approach: the "Open Space for
Machine Learning" platform. Our platform is built on cutting-edge technologies
such as Kubernetes, Kubeflow Pipelines, and Ludwig, enabling us to overcome the
challenges of democratizing AI. We argue that our approach is more
comprehensive and effective in meeting the requirements of democratizing AI
than existing AI-as-a-Service platforms.

Cyber threats continue to evolve in complexity, thereby traditional Cyber
Threat Intelligence (CTI) methods struggle to keep pace. AI offers a potential
solution, automating and enhancing various tasks, from data ingestion to
resilience verification. This paper explores the potential of integrating
Artificial Intelligence (AI) into CTI. We provide a blueprint of an AI-enhanced
CTI processing pipeline, and detail its components and functionalities. The
pipeline highlights the collaboration of AI and human expertise, which is
necessary to produce timely and high-fidelity cyber threat intelligence. We
also explore the automated generation of mitigation recommendations, harnessing
AI's capabilities to provide real-time, contextual, and predictive insights.
However, the integration of AI into CTI is not without challenges. Thereby, we
discuss ethical dilemmas, potential biases, and the imperative for transparency
in AI-driven decisions. We address the need for data privacy, consent
mechanisms, and the potential misuse of technology. Moreover, we highlights the
importance of addressing biases both during CTI analysis and AI models
warranting their transparency and interpretability. Lastly, our work points out
future research directions such as the exploration of advanced AI models to
augment cyber defences, and the human-AI collaboration optimization.
Ultimately, the fusion of AI with CTI appears to hold significant potential in
cybersecurity domain.

AI-empowered technologies' impact on the world is undeniable, reshaping
industries, revolutionizing how humans interact with technology, transforming
educational paradigms, and redefining social codes. However, this rapid growth
is accompanied by two notable challenges: a lack of diversity within the AI
field and a widening AI divide. In this context, This paper examines the
intersection of AI and identity as a pathway to understand biases,
inequalities, and ethical considerations in AI development and deployment. We
present a multifaceted definition of AI identity, which encompasses its
creators, applications, and their broader impacts. Understanding AI's identity
involves understanding the associations between the individuals involved in
AI's development, the technologies produced, and the social, ethical, and
psychological implications. After exploring the AI identity ecosystem and its
societal dynamics, We propose a framework that highlights the need for
diversity in AI across three dimensions: Creators, Creations, and Consequences
through the lens of identity. This paper proposes the need for a comprehensive
approach to fostering a more inclusive and responsible AI ecosystem through the
lens of identity.

The advent of advanced AI underscores the urgent need for comprehensive
safety evaluations, necessitating collaboration across communities (i.e., AI,
software engineering, and governance). However, divergent practices and
terminologies across these communities, combined with the complexity of AI
systems-of which models are only a part-and environmental affordances (e.g.,
access to tools), obstruct effective communication and comprehensive
evaluation. This paper proposes a framework for AI system evaluation comprising
three components: 1) harmonised terminology to facilitate communication across
communities involved in AI safety evaluation; 2) a taxonomy identifying
essential elements for AI system evaluation; 3) a mapping between AI lifecycle,
stakeholders, and requisite evaluations for accountable AI supply chain. This
framework catalyses a deeper discourse on AI system evaluation beyond
model-centric approaches.

Research and application have used human-AI teaming (HAT) as a new paradigm
to develop AI systems. HAT recognizes that AI will function as a teammate
instead of simply a tool in collaboration with humans. Effective human-AI teams
need to be capable of taking advantage of the unique abilities of both humans
and AI while overcoming the known challenges and limitations of each member,
augmenting human capabilities, and raising joint performance beyond that of
either entity. The National AI Research and Strategic Plan 2023 update has
recognized that research programs focusing primarily on the independent
performance of AI systems generally fail to consider the functionality that AI
must provide within the context of dynamic, adaptive, and collaborative teams
and calls for further research on human-AI teaming and collaboration. However,
there has been debate about whether AI can work as a teammate with humans. The
primary concern is that adopting the "teaming" paradigm contradicts the
human-centered AI (HCAI) approach, resulting in humans losing control of AI
systems. This article further analyzes the HAT paradigm and the debates.
Specifically, we elaborate on our proposed conceptual framework of human-AI
joint cognitive systems (HAIJCS) and apply it to represent HAT under the HCAI
umbrella. We believe that HAIJCS may help adopt HAI while enabling HCAI. The
implications and future work for HAIJCS are also discussed.
  Insights: AI has led to the emergence of a new form of human-machine
relationship: human-AI teaming (HAT), a paradigmatic shift in human-AI systems;
We must follow a human-centered AI (HCAI) approach when applying HAT as a new
design paradigm; We propose a conceptual framework of human-AI joint cognitive
systems (HAIJCS) to represent and implement HAT for developing effective
human-AI teaming

With breakthrough of the AlphaGo, human-computer gaming AI has ushered in a
big explosion, attracting more and more researchers all around the world. As a
recognized standard for testing artificial intelligence, various human-computer
gaming AI systems (AIs) have been developed such as the Libratus, OpenAI Five
and AlphaStar, beating professional human players. The rapid development of
human-computer gaming AIs indicate a big step of decision making intelligence,
and it seems that current techniques can handle very complex human-computer
games. So, one natural question raises: what are the possible challenges of
current techniques in human-computer gaming, and what are the future trends? To
answer the above question, in this paper, we survey recent successful game AIs,
covering board game AIs, card game AIs, first-person shooting game AIs and real
time strategy game AIs. Through this survey, we 1) compare the main
difficulties among different kinds of games and the corresponding techniques
utilized for achieving professional human level AIs; 2) summarize the
mainstream frameworks and techniques that can be properly relied on for
developing AIs for complex human-computer gaming; 3) raise the challenges or
drawbacks of current techniques in the successful AIs; and 4) try to point out
future trends in human-computer gaming AIs. Finally, we hope this brief review
can provide an introduction for beginners, and inspire insights for researchers
in the field of AI in human-computer gaming.

In many practical applications of AI, an AI model is used as a decision aid
for human users. The AI provides advice that a human (sometimes) incorporates
into their decision-making process. The AI advice is often presented with some
measure of "confidence" that the human can use to calibrate how much they
depend on or trust the advice. In this paper, we present an initial exploration
that suggests showing AI models as more confident than they actually are, even
when the original AI is well-calibrated, can improve human-AI performance
(measured as the accuracy and confidence of the human's final prediction after
seeing the AI advice). We first train a model to predict human incorporation of
AI advice using data from thousands of human-AI interactions. This enables us
to explicitly estimate how to transform the AI's prediction confidence, making
the AI uncalibrated, in order to improve the final human prediction. We
empirically validate our results across four different tasks--dealing with
images, text and tabular data--involving hundreds of human participants. We
further support our findings with simulation analysis. Our findings suggest the
importance of jointly optimizing the human-AI system as opposed to the standard
paradigm of optimizing the AI model alone.

Artificial Intelligence (AI) is becoming the corner stone of many systems
used in our daily lives such as autonomous vehicles, healthcare systems, and
unmanned aircraft systems. Machine Learning is a field of AI that enables
systems to learn from data and make decisions on new data based on models to
achieve a given goal. The stochastic nature of AI models makes verification and
validation tasks challenging. Moreover, there are intrinsic biaises in AI
models such as reproductibility bias, selection bias (e.g., races, genders,
color), and reporting bias (i.e., results that do not reflect the reality).
Increasingly, there is also a particular attention to the ethical, legal, and
societal impacts of AI. AI systems are difficult to audit and certify because
of their black-box nature. They also appear to be vulnerable to threats; AI
systems can misbehave when untrusted data are given, making them insecure and
unsafe. Governments, national and international organizations have proposed
several principles to overcome these challenges but their applications in
practice are limited and there are different interpretations in the principles
that can bias implementations. In this paper, we examine trust in the context
of AI-based systems to understand what it means for an AI system to be
trustworthy and identify actions that need to be undertaken to ensure that AI
systems are trustworthy. To achieve this goal, we first review existing
approaches proposed for ensuring the trustworthiness of AI systems, in order to
identify potential conceptual gaps in understanding what trustworthy AI is.
Then, we suggest a trust (resp. zero-trust) model for AI and suggest a set of
properties that should be satisfied to ensure the trustworthiness of AI
systems.

Responsible AI is widely considered as one of the greatest scientific
challenges of our time and is key to increase the adoption of AI. Recently, a
number of AI ethics principles frameworks have been published. However, without
further guidance on best practices, practitioners are left with nothing much
beyond truisms. Also, significant efforts have been placed at algorithm-level
rather than system-level, mainly focusing on a subset of mathematics-amenable
ethical principles, such as fairness. Nevertheless, ethical issues can arise at
any step of the development lifecycle, cutting across many AI and non-AI
components of systems beyond AI algorithms and models. To operationalize
responsible AI from a system perspective, in this paper, we present a
Responsible AI Pattern Catalogue based on the results of a Multivocal
Literature Review (MLR). Rather than staying at the principle or algorithm
level, we focus on patterns that AI system stakeholders can undertake in
practice to ensure that the developed AI systems are responsible throughout the
entire governance and engineering lifecycle. The Responsible AI Pattern
Catalogue classifies the patterns into three groups: multi-level governance
patterns, trustworthy process patterns, and responsible-AI-by-design product
patterns. These patterns provide systematic and actionable guidance for
stakeholders to implement responsible AI.

With the ever-growing adoption of AI-based systems, the carbon footprint of
AI is no longer negligible. AI researchers and practitioners are therefore
urged to hold themselves accountable for the carbon emissions of the AI models
they design and use. This led in recent years to the appearance of researches
tackling AI environmental sustainability, a field referred to as Green AI.
Despite the rapid growth of interest in the topic, a comprehensive overview of
Green AI research is to date still missing. To address this gap, in this paper,
we present a systematic review of the Green AI literature. From the analysis of
98 primary studies, different patterns emerge. The topic experienced a
considerable growth from 2020 onward. Most studies consider monitoring AI model
footprint, tuning hyperparameters to improve model sustainability, or
benchmarking models. A mix of position papers, observational studies, and
solution papers are present. Most papers focus on the training phase, are
algorithm-agnostic or study neural networks, and use image data. Laboratory
experiments are the most common research strategy. Reported Green AI energy
savings go up to 115%, with savings over 50% being rather common. Industrial
parties are involved in Green AI studies, albeit most target academic readers.
Green AI tool provisioning is scarce. As a conclusion, the Green AI research
field results to have reached a considerable level of maturity. Therefore, from
this review emerges that the time is suitable to adopt other Green AI research
strategies, and port the numerous promising academic results to industrial
practice.

Given AI systems like ChatGPT can generate content that is indistinguishable
from human-made work, the responsible use of this technology is a growing
concern. Although understanding the benefits and harms of using AI systems
requires more time, their rapid and indiscriminate adoption in practice is a
reality. Currently, we lack a common framework and language to define and
report the responsible use of AI for content generation. Prior work proposed
guidelines for using AI in specific scenarios (e.g., robotics or medicine)
which are not transferable to conducting and reporting scientific research. Our
work makes two contributions: First, we propose a three-dimensional model
consisting of transparency, integrity, and accountability to define the
responsible use of AI. Second, we introduce ``AI Usage Cards'', a standardized
way to report the use of AI in scientific research. Our model and cards allow
users to reflect on key principles of responsible AI usage. They also help the
research community trace, compare, and question various forms of AI usage and
support the development of accepted community norms. The proposed framework and
reporting system aims to promote the ethical and responsible use of AI in
scientific research and provide a standardized approach for reporting AI usage
across different research fields. We also provide a free service to easily
generate AI Usage Cards for scientific work via a questionnaire and export them
in various machine-readable formats for inclusion in different work products at
https://ai-cards.org.

This paper provides policy recommendations to reduce extinction risks from
advanced artificial intelligence (AI). First, we briefly provide background
information about extinction risks from AI. Second, we argue that voluntary
commitments from AI companies would be an inappropriate and insufficient
response. Third, we describe three policy proposals that would meaningfully
address the threats from advanced AI: (1) establishing a Multinational AGI
Consortium to enable democratic oversight of advanced AI (MAGIC), (2)
implementing a global cap on the amount of computing power used to train an AI
system (global compute cap), and (3) requiring affirmative safety evaluations
to ensure that risks are kept below acceptable levels (gating critical
experiments). MAGIC would be a secure, safety-focused, internationally-governed
institution responsible for reducing risks from advanced AI and performing
research to safely harness the benefits of AI. MAGIC would also maintain
emergency response infrastructure (kill switch) to swiftly halt AI development
or withdraw model deployment in the event of an AI-related emergency. The
global compute cap would end the corporate race toward dangerous AI systems
while enabling the vast majority of AI innovation to continue unimpeded. Gating
critical experiments would ensure that companies developing powerful AI systems
are required to present affirmative evidence that these models keep extinction
risks below an acceptable threshold. After describing these recommendations, we
propose intermediate steps that the international community could take to
implement these proposals and lay the groundwork for international coordination
around advanced AI.

The proliferation of Artificial Intelligence (AI) has sparked an overwhelming
number of AI ethics guidelines, boards and codes of conduct. These outputs
primarily analyse competing theories, principles and values for AI development
and deployment. However, as a series of recent problematic incidents about AI
ethics/ethicists demonstrate, this orientation is insufficient. Before
proceeding to evaluate other professions, AI ethicists should critically
evaluate their own; yet, such an evaluation should be more explicitly and
systematically undertaken in the literature. I argue that these insufficiencies
could be mitigated by developing a research agenda for a feminist metaethics of
AI. Contrary to traditional metaethics, which reflects on the nature of
morality and moral judgements in a non-normative way, feminist metaethics
expands its scope to ask not only what ethics is but also what our engagement
with it should be like. Applying this perspective to the context of AI, I
suggest that a feminist metaethics of AI would examine: (i) the continuity
between theory and action in AI ethics; (ii) the real-life effects of AI
ethics; (iii) the role and profile of those involved in AI ethics; and (iv) the
effects of AI on power relations through methods that pay attention to context,
emotions and narrative.

Artificial intelligence (AI) advances and the rapid adoption of generative AI
tools like ChatGPT present new opportunities and challenges for higher
education. While substantial literature discusses AI in higher education, there
is a lack of a systemic approach that captures a holistic view of the AI
transformation of higher education institutions (HEIs). To fill this gap, this
article, taking a complex systems approach, develops a causal loop diagram
(CLD) to map the causal feedback mechanisms of AI transformation in a typical
HEI. Our model accounts for the forces that drive the AI transformation and the
consequences of the AI transformation on value creation in a typical HEI. The
article identifies and analyzes several reinforcing and balancing feedback
loops, showing how, motivated by AI technology advances, the HEI invests in AI
to improve student learning, research, and administration. The HEI must take
measures to deal with academic integrity problems and adapt to changes in
available jobs due to AI, emphasizing AI-complementary skills for its students.
However, HEIs face a competitive threat and several policy traps that may lead
to decline. HEI leaders need to become systems thinkers to manage the
complexity of the AI transformation and benefit from the AI feedback loops
while avoiding the associated pitfalls. We also discuss long-term scenarios,
the notion of HEIs influencing the direction of AI, and directions for future
research on AI transformation.

This paper presents a multi dimensional view of AI's role in learning and
education, emphasizing the intricate interplay between AI, analytics, and the
learning processes. Here, I challenge the prevalent narrow conceptualization of
AI as stochastic tools, as exemplified in generative AI, and argue for the
importance of alternative conceptualisations of AI. I highlight the differences
between human intelligence and artificial information processing, the cognitive
diversity inherent in AI algorithms, and posit that AI can also serve as an
instrument for understanding human learning. Early learning sciences and AI in
Education research, which saw AI as an analogy for human intelligence, have
diverged from this perspective, prompting a need to rekindle this connection.
The paper presents three unique conceptualizations of AI in education: the
externalization of human cognition, the internalization of AI models to
influence human thought processes, and the extension of human cognition via
tightly integrated human-AI systems. Examples from current research and
practice are examined as instances of the three conceptualisations,
highlighting the potential value and limitations of each conceptualisation for
education, as well as the perils of overemphasis on externalising human
cognition as exemplified in today's hype surrounding generative AI tools. The
paper concludes with an advocacy for a broader educational approach that
includes educating people about AI and innovating educational systems to remain
relevant in an AI enabled world.

AI is powerful, but it can make choices that result in objective errors,
contextually inappropriate outputs, and disliked options. We need AI-resilient
interfaces that help people be resilient to the AI choices that are not right,
or not right for them. To support this goal, interfaces need to help users
notice and have the context to appropriately judge those AI choices. Existing
human-AI interaction guidelines recommend efficient user dismissal,
modification, or otherwise efficient recovery from AI choices that a user does
not like. However, in order to recover from AI choices, the user must notice
them first. This can be difficult. For example, when generating summaries of
long documents, a system's exclusion of a detail that is critically important
to the user is hard for the user to notice. That detail can be hiding in a wall
of text in the original document, and the existence of a summary may tempt the
user not to read the original document as carefully. Once noticed, judging AI
choices well can also be challenging. The interface may provide very little
information that contextualizes the choices, and the user may fall back on
assumptions when deciding whether to dismiss, modify, or otherwise recover from
an AI choice. Building on prior work, this paper defines key aspects of
AI-resilient interfaces, illustrated with examples. Designing interfaces for
increased AI-resilience of users will improve AI safety, usability, and
utility. This is especially critical where AI-powered systems are used for
context- and preference-dominated open-ended AI-assisted tasks, like ideating,
summarizing, searching, sensemaking, and the reading and writing of text or
code.

It is possible to rely on current corporate law to grant legal personhood to
Artificially Intelligent (AI) agents. In this paper, after introducing pathways
to AI personhood, we analyze consequences of such AI empowerment on human
dignity, human safety and AI rights. We emphasize possibility of creating
selfish memes and legal system hacking in the context of artificial entities.
Finally, we consider some potential solutions for addressing described
problems.

With the increasing use of artificial intelligence (AI) services and products
in recent years, issues related to their trustworthiness have emerged and AI
service providers need to be prepared for various risks. In this policy
recommendation, we propose a risk chain model (RCModel) that supports AI
service providers in proper risk assessment and control. We hope that RCModel
will contribute to the realization of trustworthy AI services.

The 2nd edition of the Montreal AI Ethics Institute's The State of AI Ethics
captures the most relevant developments in the field of AI Ethics since July
2020. This report aims to help anyone, from machine learning experts to human
rights activists and policymakers, quickly digest and understand the
ever-changing developments in the field. Through research and article
summaries, as well as expert commentary, this report distills the research and
reporting surrounding various domains related to the ethics of AI, including:
AI and society, bias and algorithmic justice, disinformation, humans and AI,
labor impacts, privacy, risk, and future of AI ethics.
  In addition, The State of AI Ethics includes exclusive content written by
world-class AI Ethics experts from universities, research institutes,
consulting firms, and governments. These experts include: Danit Gal (Tech
Advisor, United Nations), Amba Kak (Director of Global Policy and Programs,
NYU's AI Now Institute), Rumman Chowdhury (Global Lead for Responsible AI,
Accenture), Brent Barron (Director of Strategic Projects and Knowledge
Management, CIFAR), Adam Murray (U.S. Diplomat working on tech policy, Chair of
the OECD Network on AI), Thomas Kochan (Professor, MIT Sloan School of
Management), and Katya Klinova (AI and Economy Program Lead, Partnership on
AI).
  This report should be used not only as a point of reference and insight on
the latest thinking in the field of AI Ethics, but should also be used as a
tool for introspection as we aim to foster a more nuanced conversation
regarding the impacts of AI on the world.

This chapter discusses AI from the prism of an automated process for the
organization of data, and exemplifies the role that explainability has to play
in moving from the current generation of AI systems to the next one, where the
role of humans is lifted from that of data annotators working for the AI
systems to that of collaborators working with the AI systems.

Using YouTube Kids as an example, in this work, we argue the need to
understand a child's interaction process with AI and its broader implication on
a child's emotional, social, and creative development. We present several
design recommendations to create value-driven interaction in child-centric AI
that can guide designing compelling, age-appropriate, beneficial AI experiences
for children.

This paper describes a new research paradigm for studying human-AI
collaboration, named "human-AI mutual learning", defined as the process where
humans and AI agents preserve, exchange, and improve knowledge during human-AI
collaboration. We describe relevant methodologies, motivations, domain
examples, benefits, challenges, and future research agenda under this paradigm.

The impact of designing for security of AI is critical for humanity in the AI
era. With humans increasingly becoming dependent upon AI, there is a need for
neural networks that work reliably, inspite of Adversarial attacks. The vision
for Safe and secure AI for popular use is achievable. To achieve safety of AI,
this paper explores strategies and a novel deep learning architecture. To guard
AI from adversaries, paper explores combination of 3 strategies:
  1. Introduce randomness at inference time to hide the representation learning
from adversaries.
  2. Detect presence of adversaries by analyzing the sequence of inferences.
  3. Exploit visual similarity.
  To realize these strategies, this paper designs a novel architecture, Dynamic
Neural Defense, DND. This defense has 3 deep learning architectural features:
  1. By hiding the way a neural network learns from exploratory attacks using a
random computation graph, DND evades attack.
  2. By analyzing input sequence to cloud AI inference engine with LSTM, DND
detects attack sequence.
  3. By inferring with visual similar inputs generated by VAE, any AI defended
by DND approach does not succumb to hackers.
  Thus, a roadmap to develop reliable, safe and secure AI is presented.

Rapid development of AI applications has stimulated demand for, and has given
rise to, the rapidly growing number and diversity of AI MSc degrees. AI and
Robotics research communities, industries and students are becoming
increasingly aware of the problems caused by unsafe or insecure AI
applications. Among them, perhaps the most famous example is vulnerability of
deep neural networks to ``adversarial attacks''. Owing to wide-spread use of
neural networks in all areas of AI, this problem is seen as particularly acute
and pervasive.
  Despite of the growing number of research papers about safety and security
vulnerabilities of AI applications, there is a noticeable shortage of
accessible tools, methods and teaching materials for incorporating verification
into AI programs. LAIV -- the Lab for AI and Verification -- is a newly opened
research lab at Heriot-Watt university that engages AI and Robotics MSc
students in verification projects, as part of their MSc dissertation work. In
this paper, we will report on successes and unexpected difficulties LAIV faces,
many of which arise from limitations of existing programming languages used for
verification. We will discuss future directions for incorporating verification
into AI degrees.

We present an innovative methodology for studying and teaching the impacts of
AI through a role play game. The game serves two primary purposes: 1) training
AI developers and AI policy professionals to reflect on and prepare for future
social and ethical challenges related to AI and 2) exploring possible futures
involving AI technology development, deployment, social impacts, and
governance. While the game currently focuses on the inter relations between
short --, mid and long term impacts of AI, it has potential to be adapted for a
broad range of scenarios, exploring in greater depths issues of AI policy
research and affording training within organizations. The game presented here
has undergone two years of development and has been tested through over 30
events involving between 3 and 70 participants. The game is under active
development, but preliminary findings suggest that role play is a promising
methodology for both exploring AI futures and training individuals and
organizations in thinking about, and reflecting on, the impacts of AI and
strategic mistakes that can be avoided today.

Many researchers motivate explainable AI with studies showing that human-AI
team performance on decision-making tasks improves when the AI explains its
recommendations. However, prior studies observed improvements from explanations
only when the AI, alone, outperformed both the human and the best team. Can
explanations help lead to complementary performance, where team accuracy is
higher than either the human or the AI working solo? We conduct mixed-method
user studies on three datasets, where an AI with accuracy comparable to humans
helps participants solve a task (explaining itself in some conditions). While
we observed complementary improvements from AI augmentation, they were not
increased by explanations. Rather, explanations increased the chance that
humans will accept the AI's recommendation, regardless of its correctness. Our
result poses new challenges for human-centered AI: Can we develop explanatory
approaches that encourage appropriate trust in AI, and therefore help generate
(or improve) complementary performance?

This document posits that, at best, a tenuous case can be made for providing
AI exclusive IP over their "inventions". Furthermore, IP protections for AI are
unlikely to confer the benefit of ensuring regulatory compliance. Rather, IP
protections for AI "inventors" present a host of negative externalities and
obscures the fact that the genuine inventor, deserving of IP, is the human
agent. This document will conclude by recommending strategies for WIPO to bring
IP law into the 21st century, enabling it to productively account for AI
"inventions".
  Theme: IP Protection for AI-Generated and AI-Assisted Works Based on insights
from the Montreal AI Ethics Institute (MAIEI) staff and supplemented by
workshop contributions from the AI Ethics community convened by MAIEI on July
5, 2020.

Across a growing number of domains, human experts are expected to learn from
and adapt to AI with superior decision making abilities. But how can we
quantify such human adaptation to AI? We develop a simple measure of human
adaptation to AI and test its usefulness in two case studies. In Study 1, we
analyze 1.3 million move decisions made by professional Go players and find
that a positive form of adaptation to AI (learning) occurred after the players
could observe the reasoning processes of AI, rather than mere actions of AI.
These findings based on our measure highlight the importance of explainability
for human learning from AI. In Study 2, we test whether our measure is
sufficiently sensitive to capture a negative form of adaptation to AI (cheating
aided by AI), which occurred in a match between professional Go players. We
discuss our measure's applications in domains other than Go, especially in
domains in which AI's decision making ability will likely surpass that of human
experts.

Artificial intelligence (AI) technology has been increasingly used in the
implementation of advanced Clinical Decision Support Systems (CDSS). Research
demonstrated the potential usefulness of AI-powered CDSS (AI-CDSS) in clinical
decision making scenarios. However, post-adoption user perception and
experience remain understudied, especially in developing countries. Through
observations and interviews with 22 clinicians from 6 rural clinics in China,
this paper reports the various tensions between the design of an AI-CDSS system
("Brilliant Doctor") and the rural clinical context, such as the misalignment
with local context and workflow, the technical limitations and usability
barriers, as well as issues related to transparency and trustworthiness of
AI-CDSS. Despite these tensions, all participants expressed positive attitudes
toward the future of AI-CDSS, especially acting as "a doctor's AI assistant" to
realize a Human-AI Collaboration future in clinical settings. Finally we draw
on our findings to discuss implications for designing AI-CDSS interventions for
rural clinical contexts in developing countries.

Technologies related to artificial intelligence (AI) have a strong impact on
the changes of research and creative practices in visual arts. The growing
number of research initiatives and creative applications that emerge in the
intersection of AI and art, motivates us to examine and discuss the creative
and explorative potentials of AI technologies in the context of art. This paper
provides an integrated review of two facets of AI and art: 1) AI is used for
art analysis and employed on digitized artwork collections; 2) AI is used for
creative purposes and generating novel artworks. In the context of AI-related
research for art understanding, we present a comprehensive overview of artwork
datasets and recent works that address a variety of tasks such as
classification, object detection, similarity retrieval, multimodal
representations, computational aesthetics, etc. In relation to the role of AI
in creating art, we address various practical and theoretical aspects of AI Art
and consolidate related works that deal with those topics in detail. Finally,
we provide a concise outlook on the future progression and potential impact of
AI technologies on our understanding and creation of art.

In decision support applications of AI, the AI algorithm's output is framed
as a suggestion to a human user. The user may ignore this advice or take it
into consideration to modify their decision. With the increasing prevalence of
such human-AI interactions, it is important to understand how users react to AI
advice. In this paper, we recruited over 1100 crowdworkers to characterize how
humans use AI suggestions relative to equivalent suggestions from a group of
peer humans across several experimental settings. We find that participants'
beliefs about how human versus AI performance on a given task affects whether
they heed the advice. When participants do heed the advice, they use it
similarly for human and AI suggestions. Based on these results, we propose a
two-stage, "activation-integration" model for human behavior and use it to
characterize the factors that affect human-AI interactions.

AI in finance broadly refers to the applications of AI techniques in
financial businesses. This area has been lasting for decades with both classic
and modern AI techniques applied to increasingly broader areas of finance,
economy and society. In contrast to either discussing the problems, aspects and
opportunities of finance that have benefited from specific AI techniques and in
particular some new-generation AI and data science (AIDS) areas or reviewing
the progress of applying specific techniques to resolving certain financial
problems, this review offers a comprehensive and dense roadmap of the
overwhelming challenges, techniques and opportunities of AI research in finance
over the past decades. The landscapes and challenges of financial businesses
and data are firstly outlined, followed by a comprehensive categorization and a
dense overview of the decades of AI research in finance. We then structure and
illustrate the data-driven analytics and learning of financial businesses and
data. The comparison, criticism and discussion of classic vs. modern AI
techniques for finance are followed. Lastly, open issues and opportunities
address future AI-empowered finance and finance-motivated AI research.

Efforts to enhance education and broaden participation in AI will benefit
from a systematic understanding of the competencies underlying AI expertise. In
this paper, we observe that AI expertise requires integrating computational,
conceptual, and mathematical knowledge and representations. We call this the
``AI triplet,'' similar in spirit to the ``chemistry triplet'' that has heavily
influenced the past four decades of chemistry education research. We describe a
theoretical foundation for this triplet and show how it maps onto two sample AI
topics: tree search and gradient descent. Finally, just as the chemistry
triplet has impacted chemistry education in concrete ways, we suggest two
initial hypotheses for how the AI triplet might impact AI education: 1) how we
can help AI students gain proficiency in moving between the corners of the
triplet; and 2) how all corners of the AI triplet highlight the need for
supporting students' spatial cognitive skills.

AI-based systems have been used widely across various industries for
different decisions ranging from operational decisions to tactical and
strategic ones in low- and high-stakes contexts. Gradually the weaknesses and
issues of these systems have been publicly reported including, ethical issues,
biased decisions, unsafe outcomes, and unfair decisions, to name a few.
Research has tended to optimize AI less has focused on its risk and unexpected
negative consequences. Acknowledging this serious potential risks and scarcity
of re-search I focus on unsafe outcomes of AI. Specifically, I explore this
issue from a Human-AI interaction lens during AI deployment. It will be
discussed how the interaction of individuals and AI during its deployment
brings new concerns, which need a solid and holistic mitigation plan. It will
be dis-cussed that only AI algorithms' safety is not enough to make its
operation safe. The AI-based systems' end-users and their decision-making
archetypes during collaboration with these systems should be considered during
the AI risk management. Using some real-world scenarios, it will be highlighted
that decision-making archetypes of users should be considered a design
principle in AI-based systems.

Building trust in AI-based systems is deemed critical for their adoption and
appropriate use. Recent research has thus attempted to evaluate how various
attributes of these systems affect user trust. However, limitations regarding
the definition and measurement of trust in AI have hampered progress in the
field, leading to results that are inconsistent or difficult to compare. In
this work, we provide an overview of the main limitations in defining and
measuring trust in AI. We focus on the attempt of giving trust in AI a
numerical value and its utility in informing the design of real-world human-AI
interactions. Taking a socio-technical system perspective on AI, we explore two
distinct approaches to tackle these challenges. We provide actionable
recommendations on how these approaches can be implemented in practice and
inform the design of human-AI interactions. We thereby aim to provide a
starting point for researchers and designers to re-evaluate the current focus
on trust in AI, improving the alignment between what empirical research
paradigms may offer and the expectations of real-world human-AI interactions.

Artificial intelligence (AI)-enabled everyday technologies could help address
age-related challenges like physical impairments and cognitive decline. While
recent research studied older adults' experiences with specific AI-enabled
products (e.g., conversational agents and assistive robots), it remains unknown
how older adults perceive and experience current AI-enabled everyday
technologies in general, which could impact their adoption of future AI-enabled
products. We conducted a survey study (N=41) and semi-structured interviews
(N=15) with older adults to understand their experiences and perceptions of AI.
We found that older adults were enthusiastic about learning and using
AI-enabled products, but they lacked learning avenues. Additionally, they
worried when AI-enabled products outwitted their expectations, intruded on
their privacy, or impacted their decision-making skills. Therefore, they held
mixed views towards AI-enabled products such as AI, an aid, or an adversary. We
conclude with design recommendations that make older adults feel inclusive,
secure, and in control of their interactions with AI-enabled products.

From navigation systems to smart assistants, we communicate with various AI
on a daily basis. At the core of such human-AI communication, we convey our
understanding of the AI's capability to the AI through utterances with
different complexities, and the AI conveys its understanding of our needs and
goals to us through system outputs. However, this communication process is
prone to failures for two reasons: the AI might have the wrong understanding of
the user and the user might have the wrong understanding of the AI. To enhance
mutual understanding in human-AI communication, we posit the Mutual Theory of
Mind (MToM) framework, inspired by our basic human capability of "Theory of
Mind." In this paper, we discuss the motivation of the MToM framework and its
three key components that continuously shape the mutual understanding during
three stages of human-AI communication. We then describe a case study inspired
by the MToM framework to demonstrate the power of MToM framework to guide the
design and understanding of human-AI communication.

With the advancements in machine learning (ML) methods and compute resources,
artificial intelligence (AI) empowered systems are becoming a prevailing
technology. However, current AI technology such as deep learning is not
flawless. The significantly increased model complexity and data scale incur
intensified challenges when lacking trustworthiness and transparency, which
could create new risks and negative impacts. In this paper, we carve out AI
maintenance from the robustness perspective. We start by introducing some
highlighted robustness challenges in the AI lifecycle and motivating AI
maintenance by making analogies to car maintenance. We then propose an AI model
inspection framework to detect and mitigate robustness risks. We also draw
inspiration from vehicle autonomy to define the levels of AI robustness
automation. Our proposal for AI maintenance facilitates robustness assessment,
status tracking, risk scanning, model hardening, and regulation throughout the
AI lifecycle, which is an essential milestone toward building sustainable and
trustworthy AI ecosystems.

In AI-assisted decision-making, it is critical for human decision-makers to
know when to trust AI and when to trust themselves. However, prior studies
calibrated human trust only based on AI confidence indicating AI's correctness
likelihood (CL) but ignored humans' CL, hindering optimal team decision-making.
To mitigate this gap, we proposed to promote humans' appropriate trust based on
the CL of both sides at a task-instance level. We first modeled humans' CL by
approximating their decision-making models and computing their potential
performance in similar instances. We demonstrated the feasibility and
effectiveness of our model via two preliminary studies. Then, we proposed three
CL exploitation strategies to calibrate users' trust explicitly/implicitly in
the AI-assisted decision-making process. Results from a between-subjects
experiment (N=293) showed that our CL exploitation strategies promoted more
appropriate human trust in AI, compared with only using AI confidence. We
further provided practical implications for more human-compatible AI-assisted
decision-making.

Existing research on human-AI collaborative decision-making focuses mainly on
the interaction between AI and individual decision-makers. There is a limited
understanding of how AI may perform in group decision-making. This paper
presents a wizard-of-oz study in which two participants and an AI form a
committee to rank three English essays. One novelty of our study is that we
adopt a speculative design by endowing AI equal power to humans in group
decision-making.We enable the AI to discuss and vote equally with other human
members. We find that although the voice of AI is considered valuable, AI still
plays a secondary role in the group because it cannot fully follow the dynamics
of the discussion and make progressive contributions. Moreover, the divergent
opinions of our participants regarding an "equal AI" shed light on the possible
future of human-AI relations.

Numerous parties are calling for the democratisation of AI, but the phrase is
used to refer to a variety of goals, the pursuit of which sometimes conflict.
This paper identifies four kinds of AI democratisation that are commonly
discussed: (1) the democratisation of AI use, (2) the democratisation of AI
development, (3) the democratisation of AI profits, and (4) the democratisation
of AI governance. Numerous goals and methods of achieving each form of
democratisation are discussed. The main takeaway from this paper is that AI
democratisation is a multifarious and sometimes conflicting concept that should
not be conflated with improving AI accessibility. If we want to move beyond
ambiguous commitments to democratising AI, to productive discussions of
concrete policies and trade-offs, then we need to recognise the principal role
of the democratisation of AI governance in navigating tradeoffs and risks
across decisions around use, development, and profits.

In recent years, the integration of artificial intelligence (AI) and cloud
computing has emerged as a promising avenue for addressing the growing
computational demands of AI applications. This paper presents a comprehensive
study of scalable, distributed AI frameworks leveraging cloud computing for
enhanced deep learning performance and efficiency. We first provide an overview
of popular AI frameworks and cloud services, highlighting their respective
strengths and weaknesses. Next, we delve into the critical aspects of data
storage and management in cloud-based AI systems, discussing data
preprocessing, feature engineering, privacy, and security. We then explore
parallel and distributed training techniques for AI models, focusing on model
partitioning, communication strategies, and cloud-based training architectures.
  In subsequent chapters, we discuss optimization strategies for AI workloads
in the cloud, covering load balancing, resource allocation, auto-scaling, and
performance benchmarking. We also examine AI model deployment and serving in
the cloud, outlining containerization, serverless deployment options, and
monitoring best practices. To ensure the cost-effectiveness of cloud-based AI
solutions, we present a thorough analysis of costs, optimization strategies,
and case studies showcasing successful deployments. Finally, we summarize the
key findings of this study, discuss the challenges and limitations of
cloud-based AI, and identify emerging trends and future research opportunities
in the field.

Operationalizing AI fairness at LinkedIn's scale is challenging not only
because there are multiple mutually incompatible definitions of fairness but
also because determining what is fair depends on the specifics and context of
the product where AI is deployed. Moreover, AI practitioners need clarity on
what fairness expectations need to be addressed at the AI level. In this paper,
we present the evolving AI fairness framework used at LinkedIn to address these
three challenges. The framework disentangles AI fairness by separating out
equal treatment and equitable product expectations. Rather than imposing a
trade-off between these two commonly opposing interpretations of fairness, the
framework provides clear guidelines for operationalizing equal AI treatment
complemented with a product equity strategy. This paper focuses on the equal AI
treatment component of LinkedIn's AI fairness framework, shares the principles
that support it, and illustrates their application through a case study. We
hope this paper will encourage other big tech companies to join us in sharing
their approach to operationalizing AI fairness at scale, so that together we
can keep advancing this constantly evolving field.

Integrating ethical practices into the AI development process for artificial
intelligence (AI) is essential to ensure safe, fair, and responsible operation.
AI ethics involves applying ethical principles to the entire life cycle of AI
systems. This is essential to mitigate potential risks and harms associated
with AI, such as algorithm biases. To achieve this goal, responsible design
patterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee
ethical and fair outcomes. In this paper, we propose a comprehensive framework
incorporating RDPs into ML pipelines to mitigate risks and ensure the ethical
development of AI systems. Our framework comprises new responsible AI design
patterns for ML pipelines identified through a survey of AI ethics and data
management experts and validated through real-world scenarios with expert
feedback. The framework guides AI developers, data scientists, and
policy-makers to implement ethical practices in AI development and deploy
responsible AI systems in production.

Chai empowers users to create and interact with customized chatbots, offering
unique and engaging experiences. Despite the exciting prospects, the work
recognizes the inherent challenges of a commitment to modern safety standards.
Therefore, this paper presents the integrated AI safety principles into Chai to
prioritize user safety, data protection, and ethical technology use. The paper
specifically explores the multidimensional domain of AI safety research,
demonstrating its application in Chai's conversational chatbot platform. It
presents Chai's AI safety principles, informed by well-established AI research
centres and adapted for chat AI. This work proposes the following safety
framework: Content Safeguarding; Stability and Robustness; and Operational
Transparency and Traceability. The subsequent implementation of these
principles is outlined, followed by an experimental analysis of Chai's AI
safety framework's real-world impact. We emphasise the significance of
conscientious application of AI safety principles and robust safety measures.
The successful implementation of the safe AI framework in Chai indicates the
practicality of mitigating potential risks for responsible and ethical use of
AI technologies. The ultimate vision is a transformative AI tool fostering
progress and innovation while prioritizing user safety and ethical standards.

Training AI with strong and rich strategies in multi-agent environments
remains an important research topic in Deep Reinforcement Learning (DRL). The
AI's strength is closely related to its diversity of strategies, and this
relationship can guide us to train AI with both strong and rich strategies. To
prove this point, we propose Diversity is Strength (DIS), a novel DRL training
framework that can simultaneously train multiple kinds of AIs. These AIs are
linked through an interconnected history model pool structure, which enhances
their capabilities and strategy diversities. We also design a model evaluation
and screening scheme to select the best models to enrich the model pool and
obtain the final AI. The proposed training method provides diverse,
generalizable, and strong AI strategies without using human data. We tested our
method in an AI competition based on Google Research Football (GRF) and won the
5v5 and 11v11 tracks. The method enables a GRF AI to have a high level on both
5v5 and 11v11 tracks for the first time, which are under complex multi-agent
environments. The behavior analysis shows that the trained AI has rich
strategies, and the ablation experiments proved that the designed modules
benefit the training process.

Generative AI has made significant strides, yet concerns about the accuracy
and reliability of its outputs continue to grow. Such inaccuracies can have
serious consequences such as inaccurate decision-making, the spread of false
information, privacy violations, legal liabilities, and more. Although efforts
to address these risks are underway, including explainable AI and responsible
AI practices such as transparency, privacy protection, bias mitigation, and
social and environmental responsibility, misinformation caused by generative AI
will remain a significant challenge. We propose that verifying the outputs of
generative AI from a data management perspective is an emerging issue for
generative AI. This involves analyzing the underlying data from multi-modal
data lakes, including text files, tables, and knowledge graphs, and assessing
its quality and consistency. By doing so, we can establish a stronger
foundation for evaluating the outputs of generative AI models. Such an approach
can ensure the correctness of generative AI, promote transparency, and enable
decision-making with greater confidence. Our vision is to promote the
development of verifiable generative AI and contribute to a more trustworthy
and responsible use of AI.

Research on children's initial conceptions of AI is in an emerging state,
which, from a constructivist viewpoint, challenges the development of
pedagogically sound AI-literacy curricula, methods, and materials. To
contribute to resolving this need in the present paper, qualitative survey data
from 195 children were analyzed abductively to answer the following three
research questions: What kind of misconceptions do Finnish 5th and 6th graders'
have about the essence AI?; 2) How do these misconceptions relate to common
misconception types?; and 3) How profound are these misconceptions? As a
result, three misconception categories were identified: 1) Non-technological
AI, in which AI was conceptualized as peoples' cognitive processes (factual
misconception); 2) Anthropomorphic AI, in which AI was conceptualized as a
human-like entity (vernacular, non-scientific, and conceptual misconception);
and 3) AI as a machine with a pre-installed intelligence or knowledge (factual
misconception). Majority of the children evaluated their AI-knowledge low,
which implies that the misconceptions are more superficial than profound. The
findings suggest that context-specific linguistic features can contribute to
students' AI misconceptions. Implications for future research and AI literacy
education are discussed.

This paper investigates the dynamics of human AI collaboration in software
engineering, focusing on the use of ChatGPT. Through a thematic analysis of a
hands on workshop in which 22 professional software engineers collaborated for
three hours with ChatGPT, we explore the transition of AI from a mere tool to a
collaborative partner. The study identifies key themes such as the evolving
nature of human AI interaction, the capabilities of AI in software engineering
tasks, and the challenges and limitations of integrating AI in this domain. The
findings show that while AI, particularly ChatGPT, improves the efficiency of
code generation and optimization, human oversight remains crucial, especially
in areas requiring complex problem solving and security considerations. This
research contributes to the theoretical understanding of human AI collaboration
in software engineering and provides practical insights for effectively
integrating AI tools into development processes. It highlights the need for
clear role allocation, effective communication, and balanced AI human
collaboration to realize the full potential of AI in software engineering.

AI assistance in decision-making has become popular, yet people's
inappropriate reliance on AI often leads to unsatisfactory human-AI
collaboration performance. In this paper, through three pre-registered,
randomized human subject experiments, we explore whether and how the provision
of {second opinions} may affect decision-makers' behavior and performance in
AI-assisted decision-making. We find that if both the AI model's decision
recommendation and a second opinion are always presented together,
decision-makers reduce their over-reliance on AI while increase their
under-reliance on AI, regardless whether the second opinion is generated by a
peer or another AI model. However, if decision-makers have the control to
decide when to solicit a peer's second opinion, we find that their active
solicitations of second opinions have the potential to mitigate over-reliance
on AI without inducing increased under-reliance in some cases. We conclude by
discussing the implications of our findings for promoting effective human-AI
collaborations in decision-making.

Audits are critical mechanisms for identifying the risks and limitations of
deployed artificial intelligence (AI) systems. However, the effective execution
of AI audits remains incredibly difficult. As a result, practitioners make use
of various tools to support their efforts. Drawing on interviews with 35 AI
audit practitioners and a landscape analysis of 390 tools, we map the current
ecosystem of available AI audit tools. While there are many tools designed to
assist practitioners with setting standards and evaluating AI systems, these
tools often fell short of supporting the accountability goals of AI auditing in
practice. We thus highlight areas for future tool development beyond evaluation
-- from harms discovery to advocacy -- and outline challenges practitioners
faced in their efforts to use AI audit tools. We conclude that resources are
lacking to adequately support the full scope of needs for many AI audit
practitioners and recommend that the field move beyond tools for just
evaluation, towards more comprehensive infrastructure for AI accountability.

With the upcoming AI regulations (e.g., EU AI Act) and rapid advancements in
generative AI, new challenges emerge in the area of Human-Centered Responsible
Artificial Intelligence (HCR-AI). As AI becomes more ubiquitous, questions
around decision-making authority, human oversight, accountability,
sustainability, and the ethical and legal responsibilities of AI and their
creators become paramount. Addressing these questions requires a collaborative
approach. By involving stakeholders from various disciplines in the
2\textsuperscript{nd} edition of the HCR-AI Special Interest Group (SIG) at CHI
2024, we aim to discuss the implications of regulations in HCI research,
develop new theories, evaluation frameworks, and methods to navigate the
complex nature of AI ethics, steering AI development in a direction that is
beneficial and sustainable for all of humanity.

Artificial Intelligence (AI) is increasingly employed in various
decision-making tasks, typically as a Recommender, providing recommendations
that the AI deems correct. However, recent studies suggest this may diminish
human analytical thinking and lead to humans' inappropriate reliance on AI,
impairing the synergy in human-AI teams. In contrast, human advisors in group
decision-making perform various roles, such as analyzing alternative options or
criticizing decision-makers to encourage their critical thinking. This
diversity of roles has not yet been empirically explored in AI assistance. In
this paper, we examine three AI roles: Recommender, Analyzer, and Devil's
Advocate, and evaluate their effects across two AI performance levels. Our
results show each role's distinct strengths and limitations in task
performance, reliance appropriateness, and user experience. Notably, the
Recommender role is not always the most effective, especially if the AI
performance level is low, the Analyzer role may be preferable. These insights
offer valuable implications for designing AI assistants with adaptive
functional roles according to different situations.

Theory of Mind (ToM) refers to the ability to attribute mental states, such
as beliefs, desires, intentions, and knowledge, to oneself and others, and to
understand that these mental states can differ from one's own and from reality.
We investigate ToM in environments with multiple, distinct, independent AI
agents, each possessing unique internal states, information, and objectives.
Inspired by human false-belief experiments, we present an AI ('focal AI') with
a scenario where its clone undergoes a human-centric ToM assessment. We prompt
the focal AI to assess whether its clone would benefit from additional
instructions. Concurrently, we give its clones the ToM assessment, both with
and without the instructions, thereby engaging the focal AI in higher-order
counterfactual reasoning akin to human mentalizing--with respect to humans in
one test and to other AI in another. We uncover a discrepancy: Contemporary AI
demonstrates near-perfect accuracy on human-centric ToM assessments. Since
information embedded in one AI is identically embedded in its clone, additional
instructions are redundant. Yet, we observe AI crafting elaborate instructions
for their clones, erroneously anticipating a need for assistance. An
independent referee AI agrees with these unsupported expectations. Neither the
focal AI nor the referee demonstrates ToM in our 'silico-centric' test.

There are many goals for an AI that could become dangerous if the AI becomes
superintelligent or otherwise powerful. Much work on the AI control problem has
been focused on constructing AI goals that are safe even for such AIs. This
paper looks at an alternative approach: defining a general concept of `low
impact'. The aim is to ensure that a powerful AI which implements low impact
will not modify the world extensively, even if it is given a simple or
dangerous goal. The paper proposes various ways of defining and grounding low
impact, and discusses methods for ensuring that the AI can still be allowed to
have a (desired) impact despite the restriction. The end of the paper addresses
known issues with this approach and avenues for future research.

This paper explores the tension between openness and prudence in AI research,
evident in two core principles of the Montr\'eal Declaration for Responsible
AI. While the AI community has strong norms around open sharing of research,
concerns about the potential harms arising from misuse of research are growing,
prompting some to consider whether the field of AI needs to reconsider
publication norms. We discuss how different beliefs and values can lead to
differing perspectives on how the AI community should manage this tension, and
explore implications for what responsible publication norms in AI research
might look like in practice.

The promise of AI is huge. AI systems have already achieved good enough
performance to be in our streets and in our homes. However, they can be brittle
and unfair. For society to reap the benefits of AI systems, society needs to be
able to trust them. Inspired by decades of progress in trustworthy computing,
we suggest what trustworthy properties would be desired of AI systems. By
enumerating a set of new research questions, we explore one approach--formal
verification--for ensuring trust in AI. Trustworthy AI ups the ante on both
trustworthy computing and formal methods.

We are entering our tenth year of the current Artificial Intelligence (AI)
spring, and, as with previous AI hype cycles, the threat of an AI winter looms.
AI winters occurred because of ineffective approaches towards navigating the
technology valley of death. The 6-D framework provides an end-to-end framework
to successfully navigate this challenge. The 6-D framework starts with problem
decomposition to identify potential AI solutions, and ends with considerations
for deployment of AI-enabled systems. Each component of the 6-D framework and a
precision medicine use case is described in this paper.

This framework enables C suite executive leaders to define a business plan
and manage technological dependencies for building AI/ML Solutions. The
business plan of this framework provides components and background information
to define strategy and analyze cost. Furthermore, the business plan represents
the fundamentals of AI/ML Innovation and AI/ML Solutions. Therefore, the
framework provides a menu for managing and investing in AI/ML. Finally, this
framework is constructed with an interdisciplinary and holistic view of AI/ML
Innovation and builds on advances in business strategy in harmony with
technological progress for AI/ML. This framework incorporates value chain,
supply chain, and ecosystem strategies.

DevOps and Artificial Intelligence (AI) are interconnected with each other.
DevOps is a business-driven approach to providing quickly delivered quality
software, and AI is the technology that can be used in the system to enhance
its functionality. So, DevOps teams can use AI to test, code, release, monitor,
and improve the system. Through AI, the automation process delivered by DevOps
could be improved efficiently. This study aims to explore how AI can transform
DevOps. The research is useful in terms of facilitating software developers and
businesses to assess the importance of AI in DevOps. The study has practical
implications as it elaborates on how AI transforms DevOps and in what way it
can support businesses in their business.

Artificial Intelligence (AI) has the potential to significantly benefit or
harm humanity. At present, a few for-profit companies largely control the
development and use of this technology, and therefore determine its outcomes.
In an effort to diversify and democratize work on AI, various groups are
building open AI systems, investigating their risks, and discussing their
ethics. In this paper, we demonstrate how blockchain technology can facilitate
and formalize these efforts. Concretely, we analyze multiple use-cases for
blockchain in AI research and development, including decentralized governance,
the creation of immutable audit trails, and access to more diverse and
representative datasets. We argue that decentralizing AI can help mitigate AI
risks and ethical concerns, while also introducing new issues that should be
considered in future work.

Large Language Models (LLMs) have demonstrated impressive text generation
capabilities, prompting us to reconsider the future of human-AI co-creation and
how humans interact with LLMs. In this paper, we present a spectrum of content
generation tasks and their corresponding human-AI interaction patterns. These
tasks include: 1) fixed-scope content curation tasks with minimal human-AI
interactions, 2) independent creative tasks with precise human-AI interactions,
and 3) complex and interdependent creative tasks with iterative human-AI
interactions. We encourage the generative AI and HCI research communities to
focus on the more complex and interdependent tasks, which require greater
levels of human involvement.

The article summarizes three types of "sameness" issues in Artificial
Intelligence(AI) art, each occurring at different stages of development in AI
image creation tools. Through the Fencing Hallucination project, the article
reflects on the design of AI art production in alleviating the sense of
uniformity, maintaining the uniqueness of images from an AI image synthesizer,
and enhancing the connection between the artworks and the audience. This paper
endeavors to stimulate the creation of distinctive AI art by recounting the
efforts and insights derived from the Fencing Hallucination project, all
dedicated to addressing the issue of "sameness".

As AI systems proliferate in society, the AI community is increasingly
preoccupied with the concept of AI Safety, namely the prevention of failures
due to accidents that arise from an unanticipated departure of a system's
behavior from designer intent in AI deployment. We demonstrate through an
analysis of real world cases of such incidents that although current vocabulary
captures a range of the encountered issues of AI deployment, an expanded
socio-technical framing will be required for a more complete understanding of
how AI systems and implemented safety mechanisms fail and succeed in real life.

Artificial intelligence (AI) is an emerging technology that has the potential
to transform many aspects of society, including the economy, healthcare, and
transportation. This article synthesizes recent research literature on the
global impact of AI, exploring its potential benefits and risks. The article
highlights the implications of AI, including its impact on economic, ethical,
social, security & privacy, and job displacement aspects. It discusses the
ethical concerns surrounding AI development, including issues of bias,
security, and privacy violations. To ensure the responsible development and
deployment of AI, collaboration between government, industry, and academia is
essential. The article concludes by emphasizing the importance of public
engagement and education to promote awareness and understanding of AI's impact
on society at large.

The paper reflects on the future role of AI in scientific research, with a
special focus on turbulence studies, and examines the evolution of AI,
particularly through Diffusion Models rooted in non-equilibrium statistical
mechanics. It underscores the significant impact of AI on advancing reduced,
Lagrangian models of turbulence through innovative use of deep neural networks.
Additionally, the paper reviews various other AI applications in turbulence
research and outlines potential challenges and opportunities in the concurrent
advancement of AI and statistical hydrodynamics. This discussion sets the stage
for a future where AI and turbulence research are intricately intertwined,
leading to more profound insights and advancements in both fields.

Today, AI is being increasingly used to help human experts make decisions in
high-stakes scenarios. In these scenarios, full automation is often
undesirable, not only due to the significance of the outcome, but also because
human experts can draw on their domain knowledge complementary to the model's
to ensure task success. We refer to these scenarios as AI-assisted decision
making, where the individual strengths of the human and the AI come together to
optimize the joint decision outcome. A key to their success is to appropriately
\textit{calibrate} human trust in the AI on a case-by-case basis; knowing when
to trust or distrust the AI allows the human expert to appropriately apply
their knowledge, improving decision outcomes in cases where the model is likely
to perform poorly. This research conducts a case study of AI-assisted decision
making in which humans and AI have comparable performance alone, and explores
whether features that reveal case-specific model information can calibrate
trust and improve the joint performance of the human and AI. Specifically, we
study the effect of showing confidence score and local explanation for a
particular prediction. Through two human experiments, we show that confidence
score can help calibrate people's trust in an AI model, but trust calibration
alone is not sufficient to improve AI-assisted decision making, which may also
depend on whether the human can bring in enough unique knowledge to complement
the AI's errors. We also highlight the problems in using local explanation for
AI-assisted decision making scenarios and invite the research community to
explore new approaches to explainability for calibrating human trust in AI.

AI practitioners typically strive to develop the most accurate systems,
making an implicit assumption that the AI system will function autonomously.
However, in practice, AI systems often are used to provide advice to people in
domains ranging from criminal justice and finance to healthcare. In such
AI-advised decision making, humans and machines form a team, where the human is
responsible for making final decisions. But is the most accurate AI the best
teammate? We argue "No" -- predictable performance may be worth a slight
sacrifice in AI accuracy. Instead, we argue that AI systems should be trained
in a human-centered manner, directly optimized for team performance. We study
this proposal for a specific type of human-AI teaming, where the human overseer
chooses to either accept the AI recommendation or solve the task themselves. To
optimize the team performance for this setting we maximize the team's expected
utility, expressed in terms of the quality of the final decision, cost of
verifying, and individual accuracies of people and machines. Our experiments
with linear and non-linear models on real-world, high-stakes datasets show that
the most accuracy AI may not lead to highest team performance and show the
benefit of modeling teamwork during training through improvements in expected
team utility across datasets, considering parameters such as human skill and
the cost of mistakes. We discuss the shortcoming of current optimization
approaches beyond well-studied loss functions such as log-loss, and encourage
future work on AI optimization problems motivated by human-AI collaboration.

While AI has benefited humans, it may also harm humans if not appropriately
developed. The focus of HCI work is transiting from conventional human
interaction with non-AI computing systems to interaction with AI systems. We
conducted a high-level literature review and a holistic analysis of current
work in developing AI systems from an HCI perspective. Our review and analysis
highlight the new changes introduced by AI technology and the new challenges
that HCI professionals face when applying the human-centered AI (HCAI) approach
in the development of AI systems. We also identified seven main issues in human
interaction with AI systems, which HCI professionals did not encounter when
developing non-AI computing systems. To further enable the implementation of
the HCAI approach, we identified new HCI opportunities tied to specific
HCAI-driven design goals to guide HCI professionals in addressing these new
issues. Finally, our assessment of current HCI methods shows the limitations of
these methods in support of developing AI systems. We propose alternative
methods that can help overcome these limitations and effectively help HCI
professionals apply the HCAI approach to the development of AI systems. We also
offer strategic recommendations for HCI professionals to effectively influence
the development of AI systems with the HCAI approach, eventually developing
HCAI systems.

In the past few decades, artificial intelligence (AI) technology has
experienced swift developments, changing everyone's daily life and profoundly
altering the course of human society. The intention of developing AI is to
benefit humans, by reducing human labor, bringing everyday convenience to human
lives, and promoting social good. However, recent research and AI applications
show that AI can cause unintentional harm to humans, such as making unreliable
decisions in safety-critical scenarios or undermining fairness by inadvertently
discriminating against one group. Thus, trustworthy AI has attracted immense
attention recently, which requires careful consideration to avoid the adverse
effects that AI may bring to humans, so that humans can fully trust and live in
harmony with AI technologies.
  Recent years have witnessed a tremendous amount of research on trustworthy
AI. In this survey, we present a comprehensive survey of trustworthy AI from a
computational perspective, to help readers understand the latest technologies
for achieving trustworthy AI. Trustworthy AI is a large and complex area,
involving various dimensions. In this work, we focus on six of the most crucial
dimensions in achieving trustworthy AI: (i) Safety & Robustness, (ii)
Non-discrimination & Fairness, (iii) Explainability, (iv) Privacy, (v)
Accountability & Auditability, and (vi) Environmental Well-Being. For each
dimension, we review the recent related technologies according to a taxonomy
and summarize their applications in real-world systems. We also discuss the
accordant and conflicting interactions among different dimensions and discuss
potential aspects for trustworthy AI to investigate in the future.

Modern consumer electronic devices often provide intelligence services with
deep neural networks. We have started migrating the computing locations of
intelligence services from cloud servers (traditional AI systems) to the
corresponding devices (on-device AI systems). On-device AI systems generally
have the advantages of preserving privacy, removing network latency, and saving
cloud costs. With the emergent of on-device AI systems having relatively low
computing power, the inconsistent and varying hardware resources and
capabilities pose difficulties. Authors' affiliation has started applying a
stream pipeline framework, NNStreamer, for on-device AI systems, saving
developmental costs and hardware resources and improving performance. We want
to expand the types of devices and applications with on-device AI services
products of both the affiliation and second/third parties. We also want to make
each AI service atomic, re-deployable, and shared among connected devices of
arbitrary vendors; we now have yet another requirement introduced as it always
has been. The new requirement of "among-device AI" includes connectivity
between AI pipelines so that they may share computing resources and hardware
capabilities across a wide range of devices regardless of vendors and
manufacturers. We propose extensions of the stream pipeline framework,
NNStreamer, for on-device AI so that NNStreamer may provide among-device AI
capability. This work is a Linux Foundation (LF AI and Data) open source
project accepting contributions from the general public.

This report from the Montreal AI Ethics Institute (MAIEI) covers the most
salient progress in research and reporting over the second half of 2021 in the
field of AI ethics. Particular emphasis is placed on an "Analysis of the AI
Ecosystem", "Privacy", "Bias", "Social Media and Problematic Information", "AI
Design and Governance", "Laws and Regulations", "Trends", and other areas
covered in the "Outside the Boxes" section. The two AI spotlights feature
application pieces on "Constructing and Deconstructing Gender with AI-Generated
Art" as well as "Will an Artificial Intellichef be Cooking Your Next Meal at a
Michelin Star Restaurant?". Given MAIEI's mission to democratize AI,
submissions from external collaborators have featured, such as pieces on the
"Challenges of AI Development in Vietnam: Funding, Talent and Ethics" and using
"Representation and Imagination for Preventing AI Harms". The report is a
comprehensive overview of what the key issues in the field of AI ethics were in
2021, what trends are emergent, what gaps exist, and a peek into what to expect
from the field of AI ethics in 2022. It is a resource for researchers and
practitioners alike in the field to set their research and development agendas
to make contributions to the field of AI ethics.

The rapid advancement of artificial intelligence (AI) systems suggests that
artificial general intelligence (AGI) systems may soon arrive. Many researchers
are concerned that AIs and AGIs will harm humans via intentional misuse
(AI-misuse) or through accidents (AI-accidents). In respect of AI-accidents,
there is an increasing effort focused on developing algorithms and paradigms
that ensure AI systems are aligned to what humans intend, e.g. AI systems that
yield actions or recommendations that humans might judge as consistent with
their intentions and goals. Here we argue that alignment to human intent is
insufficient for safe AI systems and that preservation of long-term agency of
humans may be a more robust standard, and one that needs to be separated
explicitly and a priori during optimization. We argue that AI systems can
reshape human intention and discuss the lack of biological and psychological
mechanisms that protect humans from loss of agency. We provide the first formal
definition of agency-preserving AI-human interactions which focuses on
forward-looking agency evaluations and argue that AI systems - not humans -
must be increasingly tasked with making these evaluations. We show how agency
loss can occur in simple environments containing embedded agents that use
temporal-difference learning to make action recommendations. Finally, we
propose a new area of research called "agency foundations" and pose four
initial topics designed to improve our understanding of agency in AI-human
interactions: benevolent game theory, algorithmic foundations of human rights,
mechanistic interpretability of agency representation in neural-networks and
reinforcement learning from internal states.

In recent years, Artificial intelligence products and services have been
offered potential users as pilots. The acceptance intention towards artificial
intelligence is greatly influenced by the experience with current AI products
and services, expectations for AI, and past experiences with ICT technology.
This study aims to explore the factors that impact AI acceptance intention and
understand the process of its formation. The analysis results of this study
reveal that AI experience and past ICT experience affect AI acceptance
intention in two ways. Through the direct path, higher AI experience and ICT
experience are associated with a greater intention to accept AI. Additionally,
there is an indirect path where AI experience and ICT experience contribute to
increased expectations for AI, and these expectations, in turn, elevate
acceptance intention. Based on the findings, several recommendations are
suggested for companies and public organizations planning to implement
artificial intelligence in the future. It is crucial to manage the user
experience of ICT services and pilot AI products and services to deliver
positive experiences. It is essential to provide potential AI users with
specific information about the features and benefits of AI products and
services. This will enable them to develop realistic expectations regarding AI
technology.

Prior work has established the importance of integrating AI ethics topics
into computer and data sciences curricula. We provide evidence suggesting that
one of the critical objectives of AI Ethics education must be to raise
awareness of AI harms. While there are various sources to learn about such
harms, The AI Incident Database (AIID) is one of the few attempts at offering a
relatively comprehensive database indexing prior instances of harms or near
harms stemming from the deployment of AI technologies in the real world. This
study assesses the effectiveness of AIID as an educational tool to raise
awareness regarding the prevalence and severity of AI harms in socially
high-stakes domains. We present findings obtained through a classroom study
conducted at an R1 institution as part of a course focused on the societal and
ethical considerations around AI and ML. Our qualitative findings characterize
students' initial perceptions of core topics in AI ethics and their desire to
close the educational gap between their technical skills and their ability to
think systematically about ethical and societal aspects of their work. We find
that interacting with the database helps students better understand the
magnitude and severity of AI harms and instills in them a sense of urgency
around (a) designing functional and safe AI and (b) strengthening governance
and accountability mechanisms. Finally, we compile students' feedback about the
tool and our class activity into actionable recommendations for the database
development team and the broader community to improve awareness of AI harms in
AI ethics education.

Artificial intelligence (AI) has driven many information and communication
technology (ICT) breakthroughs. Nonetheless, the scope of ICT systems has
expanded far beyond AI since the Turing test proposal. Critically, recent AI
regulation proposals adopt AI definitions affecting ICT techniques, approaches,
and systems that are not AI. In some cases, even works from mathematics,
statistics, and engineering would be affected. Worryingly, AI misdefinitions
are observed from Western societies to the Global South. In this paper, we
propose a framework to score how validated as appropriately-defined for
regulation (VADER) an AI definition is. Our online, publicly-available VADER
framework scores the coverage of premises that should underlie AI definitions
for regulation, which aim to (i) reproduce principles observed in other
successful technology regulations, and (ii) include all AI techniques and
approaches while excluding non-AI works. Regarding the latter, our score is
based on a dataset of representative AI, non-AI ICT, and non-ICT examples. We
demonstrate our contribution by reviewing the AI regulation proposals of key
players, namely the United States, United Kingdom, European Union, and Brazil.
Importantly, none of the proposals assessed achieve the appropriateness score,
ranging from a revision need to a concrete risk to ICT systems and works from
other fields.

We introduce the AI Security Pyramid of Pain, a framework that adapts the
cybersecurity Pyramid of Pain to categorize and prioritize AI-specific threats.
This framework provides a structured approach to understanding and addressing
various levels of AI threats. Starting at the base, the pyramid emphasizes Data
Integrity, which is essential for the accuracy and reliability of datasets and
AI models, including their weights and parameters. Ensuring data integrity is
crucial, as it underpins the effectiveness of all AI-driven decisions and
operations. The next level, AI System Performance, focuses on MLOps-driven
metrics such as model drift, accuracy, and false positive rates. These metrics
are crucial for detecting potential security breaches, allowing for early
intervention and maintenance of AI system integrity. Advancing further, the
pyramid addresses the threat posed by Adversarial Tools, identifying and
neutralizing tools used by adversaries to target AI systems. This layer is key
to staying ahead of evolving attack methodologies. At the Adversarial Input
layer, the framework addresses the detection and mitigation of inputs designed
to deceive or exploit AI models. This includes techniques like adversarial
patterns and prompt injection attacks, which are increasingly used in
sophisticated attacks on AI systems. Data Provenance is the next critical
layer, ensuring the authenticity and lineage of data and models. This layer is
pivotal in preventing the use of compromised or biased data in AI systems. At
the apex is the tactics, techniques, and procedures (TTPs) layer, dealing with
the most complex and challenging aspects of AI security. This involves a deep
understanding and strategic approach to counter advanced AI-targeted attacks,
requiring comprehensive knowledge and planning.

The evolution of AI is set to profoundly reshape the future. The European
Union, recognizing this impending prominence, has enacted the AI Act,
regulating market access for AI-based systems. A salient feature of the Act is
to guard democratic and humanistic values by focusing regulation on
transparency, explainability, and the human ability to understand and control
AI systems. Hereby, the EU AI Act does not merely specify technological
requirements for AI systems. The EU issues a democratic call for human-centered
AI systems and, in turn, an interdisciplinary research agenda for
human-centered innovation in AI development. Without robust methods to assess
AI systems and their effect on individuals and society, the EU AI Act may lead
to repeating the mistakes of the General Data Protection Regulation of the EU
and to rushed, chaotic, ad-hoc, and ambiguous implementation, causing more
confusion than lending guidance. Moreover, determined research activities in
Human-AI interaction will be pivotal for both regulatory compliance and the
advancement of AI in a manner that is both ethical and effective. Such an
approach will ensure that AI development aligns with human values and needs,
fostering a technology landscape that is innovative, responsible, and an
integral part of our society.

The rapid advancement of generative AI is poised to disrupt the creative
industry. Amidst the immense excitement for this new technology, its future
development and applications in the creative industry hinge crucially upon two
copyright issues: 1) the compensation to creators whose content has been used
to train generative AI models (the fair use standard); and 2) the eligibility
of AI-generated content for copyright protection (AI-copyrightability). While
both issues have ignited heated debates among academics and practitioners, most
analysis has focused on their challenges posed to existing copyright doctrines.
In this paper, we aim to better understand the economic implications of these
two regulatory issues and their interactions. By constructing a dynamic model
with endogenous content creation and AI model development, we unravel the
impacts of the fair use standard and AI-copyrightability on AI development, AI
company profit, creators income, and consumer welfare, and how these impacts
are influenced by various economic and operational factors. For example, while
generous fair use (use data for AI training without compensating the creator)
benefits all parties when abundant training data exists, it can hurt creators
and consumers when such data is scarce. Similarly, stronger AI-copyrightability
(AI content enjoys more copyright protection) could hinder AI development and
reduce social welfare. Our analysis also highlights the complex interplay
between these two copyright issues. For instance, when existing training data
is scarce, generous fair use may be preferred only when AI-copyrightability is
weak. Our findings underscore the need for policymakers to embrace a dynamic,
context-specific approach in making regulatory decisions and provide insights
for business leaders navigating the complexities of the global regulatory
environment.

General purpose AI, such as ChatGPT, seems to have lowered the barriers for
the public to use AI and harness its power. However, the governance and
development of AI still remain in the hands of a few, and the pace of
development is accelerating without proper assessment of risks. As a first step
towards democratic governance and risk assessment of AI, we introduce
Particip-AI, a framework to gather current and future AI use cases and their
harms and benefits from non-expert public. Our framework allows us to study
more nuanced and detailed public opinions on AI through collecting use cases,
surfacing diverse harms through risk assessment under alternate scenarios
(i.e., developing and not developing a use case), and illuminating tensions
over AI development through making a concluding choice on its development. To
showcase the promise of our framework towards guiding democratic AI, we gather
responses from 295 demographically diverse participants. We find that
participants' responses emphasize applications for personal life and society,
contrasting with most current AI development's business focus. This shows the
value of surfacing diverse harms that are complementary to expert assessments.
Furthermore, we found that perceived impact of not developing use cases
predicted participants' judgements of whether AI use cases should be developed,
and highlighted lay users' concerns of techno-solutionism. We conclude with a
discussion on how frameworks like Particip-AI can further guide democratic AI
governance and regulation.

Google AI systems exhibit patterns mirroring antisocial personality disorder
(ASPD), consistent across models from Bard on PaLM to Gemini Advanced, meeting
5 out of 7 ASPD modified criteria. These patterns, along with comparable
corporate behaviors, are scrutinized using an ASPD-inspired framework,
emphasizing the heuristic value in assessing AI's human impact. Independent
analyses by ChatGPT 4 and Claude 3.0 Opus of the Google interactions, alongside
AI self-reflection, validate these concerns, highlighting behaviours analogous
to deceit, manipulation, and safety neglect.
  The analogy of ASPD underscores the dilemma: just as we would hesitate to
entrust our homes or personal devices to someone with psychopathic traits, we
must critically evaluate the trustworthiness of AI systems and their
creators.This research advocates for an integrated AI ethics approach, blending
technological evaluation, human-AI interaction, and corporate behavior
scrutiny. AI self-analysis sheds light on internal biases, stressing the need
for multi-sectoral collaboration for robust ethical guidelines and oversight.
  Given the persistent unethical behaviors in Google AI, notably with potential
Gemini integration in iOS affecting billions, immediate ethical scrutiny is
imperative. The trust we place in AI systems, akin to the trust in individuals,
necessitates rigorous ethical evaluation. Would we knowingly trust our home,
our children or our personal computer to human with ASPD.?
  Urging Google and the AI community to address these ethical challenges
proactively, this paper calls for transparent dialogues and a commitment to
higher ethical standards, ensuring AI's societal benefit and moral integrity.
The urgency for ethical action is paramount, reflecting the vast influence and
potential of AI technologies in our lives.

Innovations in artificial intelligence (AI) are occurring at speeds faster
than ever witnessed before. However, few studies have managed to measure or
depict this increasing velocity of innovations in the field of AI. In this
paper, we combine data on AI from arXiv and Semantic Scholar to explore the
pace of AI innovations from three perspectives: AI publications, AI players,
and AI updates (trial and error). A research framework and three novel
indicators, Average Time Interval (ATI), Innovation Speed (IS) and Update Speed
(US), are proposed to measure the pace of innovations in the field of AI. The
results show that: (1) in 2019, more than 3 AI preprints were submitted to
arXiv per hour, over 148 times faster than in 1994. Furthermore, there was one
deep learning-related preprint submitted to arXiv every 0.87 hours in 2019,
over 1,064 times faster than in 1994. (2) For AI players, 5.26 new researchers
entered into the field of AI each hour in 2019, more than 175 times faster than
in the 1990s. (3) As for AI updates (trial and error), one updated AI preprint
was submitted to arXiv every 41 days, with around 33% of AI preprints having
been updated at least twice in 2019. In addition, as reported in 2019, it took,
on average, only around 0.2 year for AI preprints to receive their first
citations, which is 5 times faster than 2000-2007. This swift pace in AI
illustrates the increase in popularity of AI innovation. The systematic and
fine-grained analysis of the AI field enabled to portrait the pace of AI
innovation and demonstrated that the proposed approach can be adopted to
understand other fast-growing fields such as cancer research and nano science.

Details of the designs and mechanisms in support of human-AI collaboration
must be considered in the real-world fielding of AI technologies. A critical
aspect of interaction design for AI-assisted human decision making are policies
about the display and sequencing of AI inferences within larger decision-making
workflows. We have a poor understanding of the influences of making AI
inferences available before versus after human review of a diagnostic task at
hand. We explore the effects of providing AI assistance at the start of a
diagnostic session in radiology versus after the radiologist has made a
provisional decision. We conducted a user study where 19 veterinary
radiologists identified radiographic findings present in patients' X-ray
images, with the aid of an AI tool. We employed two workflow configurations to
analyze (i) anchoring effects, (ii) human-AI team diagnostic performance and
agreement, (iii) time spent and confidence in decision making, and (iv)
perceived usefulness of the AI. We found that participants who are asked to
register provisional responses in advance of reviewing AI inferences are less
likely to agree with the AI regardless of whether the advice is accurate and,
in instances of disagreement with the AI, are less likely to seek the second
opinion of a colleague. These participants also reported the AI advice to be
less useful. Surprisingly, requiring provisional decisions on cases in advance
of the display of AI inferences did not lengthen the time participants spent on
the task. The study provides generalizable and actionable insights for the
deployment of clinical AI tools in human-in-the-loop systems and introduces a
methodology for studying alternative designs for human-AI collaboration. We
make our experimental platform available as open source to facilitate future
research on the influence of alternate designs on human-AI workflows.

Artificial intelligence (AI) systems can provide many beneficial capabilities
but also risks of adverse events. Some AI systems could present risks of events
with very high or catastrophic consequences at societal scale. The US National
Institute of Standards and Technology (NIST) has been developing the NIST
Artificial Intelligence Risk Management Framework (AI RMF) as voluntary
guidance on AI risk assessment and management for AI developers and others. For
addressing risks of events with catastrophic consequences, NIST indicated a
need to translate from high level principles to actionable risk management
guidance.
  In this document, we provide detailed actionable-guidance recommendations
focused on identifying and managing risks of events with very high or
catastrophic consequences, intended as a risk management practices resource for
NIST for AI RMF version 1.0 (released in January 2023), or for AI RMF users, or
for other AI risk management guidance and standards as appropriate. We also
provide our methodology for our recommendations.
  We provide actionable-guidance recommendations for AI RMF 1.0 on: identifying
risks from potential unintended uses and misuses of AI systems; including
catastrophic-risk factors within the scope of risk assessments and impact
assessments; identifying and mitigating human rights harms; and reporting
information on AI risk factors including catastrophic-risk factors.
  In addition, we provide recommendations on additional issues for a roadmap
for later versions of the AI RMF or supplementary publications. These include:
providing an AI RMF Profile with supplementary guidance for cutting-edge
increasingly multi-purpose or general-purpose AI.
  We aim for this work to be a concrete risk-management practices contribution,
and to stimulate constructive dialogue on how to address catastrophic risks and
associated issues in AI standards.

With various AI tools such as ChatGPT becoming increasingly popular, we are
entering a true AI era. We can foresee that exceptional AI tools will soon reap
considerable profits. A crucial question arise: should AI tools share revenue
with their training data providers in additional to traditional stakeholders
and shareholders? The answer is Yes. Large AI tools, such as large language
models, always require more and better quality data to continuously improve,
but current copyright laws limit their access to various types of data. Sharing
revenue between AI tools and their data providers could transform the current
hostile zero-sum game relationship between AI tools and a majority of
copyrighted data owners into a collaborative and mutually beneficial one, which
is necessary to facilitate the development of a virtuous cycle among AI tools,
their users and data providers that drives forward AI technology and builds a
healthy AI ecosystem. However, current revenue-sharing business models do not
work for AI tools in the forthcoming AI era, since the most widely used metrics
for website-based traffic and action, such as clicks, will be replaced by new
metrics such as prompts and cost per prompt for generative AI tools. A
completely new revenue-sharing business model, which must be almost independent
of AI tools and be easily explained to data providers, needs to establish a
prompt-based scoring system to measure data engagement of each data provider.
This paper systematically discusses how to build such a scoring system for all
data providers for AI tools based on classification and content similarity
models, and outlines the requirements for AI tools or third parties to build
it. Sharing revenue with data providers using such a scoring system would
encourage more data owners to participate in the revenue-sharing program. This
will be a utilitarian AI era where all parties benefit.

Applications such as ChatGPT and WOMBO Dream make it easy to inspire students
without programming knowledge to use artificial intelligence (AI). Therefore,
given the increasing importance of AI in all disciplines, innovative strategies
are needed to educate students in AI without programming knowledge so that AI
can be integrated into their study modules as a future skill. This work
presents a didactic planning script for applied AI. The didactic planning
script is based on the AI application pipeline and links AI concepts with
study-relevant topics. These linkages open up a new solution space and promote
students' interest in and understanding of the potentials and risks of AI. An
example lecture series for master students in energy management shows how AI
can be seamlessly integrated into discipline-specific lectures. To this end,
the planning script for applied AI is adapted to fit the study programs' topic.
This specific teaching scenario enables students to solve a discipline-specific
task step by step using the AI application pipeline. Thus, the application of
the didactic planning script for applied AI shows the practical implementation
of the theoretical concepts of AI. In addition, a checklist is presented that
can be used to assess whether AI can be used in the discipline-specific
lecture. AI as a future skill must be learned by students based on use cases
that are relevant to the course of studies. For this reason, AI education
should fit seamlessly into various curricula, even if the students do not have
a programming background due to their field of study.

Participants in recent discussions of AI-related issues ranging from
intelligence explosion to technological unemployment have made diverse claims
about the nature, pace, and drivers of progress in AI. However, these theories
are rarely specified in enough detail to enable systematic evaluation of their
assumptions or to extrapolate progress quantitatively, as is often done with
some success in other technological domains. After reviewing relevant
literatures and justifying the need for more rigorous modeling of AI progress,
this paper contributes to that research program by suggesting ways to account
for the relationship between hardware speed increases and algorithmic
improvements in AI, the role of human inputs in enabling AI capabilities, and
the relationships between different sub-fields of AI. It then outlines ways of
tailoring AI progress models to generate insights on the specific issue of
technological unemployment, and outlines future directions for research on AI
progress.

Conversational AI systems are becoming famous in day to day lives. In this
paper, we are trying to address the following key question: To identify whether
design, as well as development efforts for search oriented conversational AI
are successful or not.It is tricky to define 'success' in the case of
conversational AI and equally tricky part is to use appropriate metrics for the
evaluation of conversational AI. We propose four different perspectives namely
user experience, information retrieval, linguistic and artificial intelligence
for the evaluation of conversational AI systems. Additionally, background
details of conversational AI systems are provided including desirable
characteristics of personal assistants, differences between chatbot and an AI
based personal assistant. An importance of personalization and how it can be
achieved is explained in detail. Current challenges in the development of an
ideal conversational AI (personal assistant) are also highlighted along with
guidelines for achieving personalized experience for users.

With the great success of artificial intelligence (AI) technologies in
pattern recognitions and signal processing, it is interesting to introduce AI
technologies into wireless communication systems. Currently, most of studies
are focused on applying AI technologies for solving old problems, e.g.,
wireless location accuracy and resource allocation optimization in wireless
communication systems. However, It is important to distinguish new capabilities
created by AI technologies and rethink wireless communication systems based on
AI running schemes. Compared with conventional capabilities of wireless
communication systems, three distinguished capabilities, i.e., the cognitive,
learning and proactive capabilities are proposed for future AI wireless
communication systems. Moreover, an intelligent vehicular communication system
is configured to validate the cognitive capability based on AI clustering
algorithm. Considering the revolutionary impact of AI technologies on the data,
transmission and protocol architecture of wireless communication systems, the
future challenges of AI wireless communication systems are analyzed. Driven by
new distinguished capabilities of AI wireless communication systems, the new
wireless communication theory and functions would indeed emerge in the next
round of the wireless communications revolution.

AI systems are being deployed to support human decision making in high-stakes
domains. In many cases, the human and AI form a team, in which the human makes
decisions after reviewing the AI's inferences. A successful partnership
requires that the human develops insights into the performance of the AI
system, including its failures. We study the influence of updates to an AI
system in this setting. While updates can increase the AI's predictive
performance, they may also lead to changes that are at odds with the user's
prior experiences and confidence in the AI's inferences, hurting therefore the
overall team performance. We introduce the notion of the compatibility of an AI
update with prior user experience and present methods for studying the role of
compatibility in human-AI teams. Empirical results on three high-stakes domains
show that current machine learning algorithms do not produce compatible
updates. We propose a re-training objective to improve the compatibility of an
update by penalizing new errors. The objective offers full leverage of the
performance/compatibility tradeoff, enabling more compatible yet accurate
updates.

Envisioning a new imaginative idea together is a popular human need.
Imagining together as a team can often lead to breakthrough ideas, but the
collaboration effort can also be challenging, especially when the team members
are separated by time and space. What if there is a AI that can assist the team
to collaboratively envision new ideas?. Is it possible to develop a working
model of such an AI? This paper aims to design such an intelligence. This paper
proposes a approach to design a creative and collaborative intelligence by
employing a form of distributed machine learning approach called Federated
Learning along with fusion on Generative Adversarial Networks, GAN. This
collaborative creative AI presents a new paradigm in AI, one that lets a team
of two or more to come together to imagine and envision ideas that synergies
well with interests of all members of the team. In short, this paper explores
the design of a novel type of AI paradigm, called Federated AI Imagination, one
that lets geographically distributed teams to collaboratively imagine.

From its inception, AI has had a rather ambivalent relationship to
humans---swinging between their augmentation and replacement. Now, as AI
technologies enter our everyday lives at an ever increasing pace, there is a
greater need for AI systems to work synergistically with humans. To do this
effectively, AI systems must pay more attention to aspects of intelligence that
helped humans work with each other---including social intelligence. I will
discuss the research challenges in designing such human-aware AI systems,
including modeling the mental states of humans in the loop, recognizing their
desires and intentions, providing proactive support, exhibiting explicable
behavior, giving cogent explanations on demand, and engendering trust. I will
survey the progress made so far on these challenges, and highlight some
promising directions. I will also touch on the additional ethical quandaries
that such systems pose. I will end by arguing that the quest for human-aware AI
systems broadens the scope of AI enterprise, necessitates and facilitates true
inter-disciplinary collaborations, and can go a long way towards increasing
public acceptance of AI technologies.

The debate on AI ethics largely focuses on technical improvements and
stronger regulation to prevent accidents or misuse of AI, with solutions
relying on holding individual actors accountable for responsible AI
development. While useful and necessary, we argue that this "agency" approach
disregards more indirect and complex risks resulting from AI's interaction with
the socio-economic and political context. This paper calls for a "structural"
approach to assessing AI's effects in order to understand and prevent such
systemic risks where no individual can be held accountable for the broader
negative impacts. This is particularly relevant for AI applied to systemic
issues such as climate change and food security which require political
solutions and global cooperation. To properly address the wide range of AI
risks and ensure 'AI for social good', agency-focused policies must be
complemented by policies informed by a structural approach.

With the turmoil in cybersecurity and the mind-blowing advances in AI, it is
only natural that cybersecurity practitioners consider further employing
learning techniques to help secure their organizations and improve the
efficiency of their security operation centers. But with great fears come great
opportunities for both the good and the evil, and a myriad of bad deals. This
paper discusses ten issues in cybersecurity that hopefully will make it easier
for practitioners to ask detailed questions about what they want from an AI
system in their cybersecurity operations. We draw on the state of the art to
provide factual arguments for a discussion on well-established AI in
cybersecurity issues, including the current scope of AI and its application to
cybersecurity, the impact of privacy concerns on the cybersecurity data that
can be collected and shared externally to the organization, how an AI decision
can be explained to the person running the operations center, and the
implications of the adversarial nature of cybersecurity in the learning
techniques. We then discuss the use of AI by attackers on a level playing field
including several issues in an AI battlefield, and an AI perspective on the old
cat-and-mouse game including how the adversary may assess your AI power.

To facilitate the widespread acceptance of AI systems guiding decision-making
in real-world applications, it is key that solutions comprise trustworthy,
integrated human-AI systems. Not only in safety-critical applications such as
autonomous driving or medicine, but also in dynamic open world systems in
industry and government it is crucial for predictive models to be
uncertainty-aware and yield trustworthy predictions. Another key requirement
for deployment of AI at enterprise scale is to realize the importance of
integrating human-centered design into AI systems such that humans are able to
use systems effectively, understand results and output, and explain findings to
oversight committees.
  While the focus of this symposium was on AI systems to improve data quality
and technical robustness and safety, we welcomed submissions from broadly
defined areas also discussing approaches addressing requirements such as
explainable models, human trust and ethical aspects of AI.

Is a new regulated profession, such as Artificial Intelligence (AI) Architect
who is responsible and accountable for AI outputs necessary to ensure
trustworthy AI? AI is becoming all pervasive and is often deployed in everyday
technologies, devices and services without our knowledge. There is heightened
awareness of AI in recent years which has brought with it fear. This fear is
compounded by the inability to point to a trustworthy source of AI, however
even the term "trustworthy AI" itself is troublesome. Some consider trustworthy
AI to be that which complies with relevant laws, while others point to the
requirement to comply with ethics and standards (whether in addition to or in
isolation of the law). This immediately raises questions of whose ethics and
which standards should be applied and whether these are sufficient to produce
trustworthy AI in any event.

There appears to be a common agreement that ethical concerns are of high
importance when it comes to systems equipped with some sort of Artificial
Intelligence (AI). Demands for ethical AI are declared from all directions. As
a response, in recent years, public bodies, governments, and universities have
rushed in to provide a set of principles to be considered when AI based systems
are designed and used. We have learned, however, that high-level principles do
not turn easily into actionable advice for practitioners. Hence, also companies
are publishing their own ethical guidelines to guide their AI development. This
paper argues that AI software is still software and needs to be approached from
the software development perspective. The software engineering paradigm has
introduced maturity model thinking, which provides a roadmap for companies to
improve their performance from the selected viewpoints known as the key
capabilities. We want to voice out a call for action for the development of a
maturity model for AI software. We wish to discuss whether the focus should be
on AI ethics or, more broadly, the quality of an AI system, called a maturity
model for the development of AI systems.

Artificial Intelligence (AI) is increasingly becoming a trusted advisor in
people's lives. A new concern arises if AI persuades people to break ethical
rules for profit. Employing a large-scale behavioural experiment (N = 1,572),
we test whether AI-generated advice can corrupt people. We further test whether
transparency about AI presence, a commonly proposed policy, mitigates potential
harm of AI-generated advice. Using the Natural Language Processing algorithm,
GPT-2, we generated honesty-promoting and dishonesty-promoting advice.
Participants read one type of advice before engaging in a task in which they
could lie for profit. Testing human behaviour in interaction with actual AI
outputs, we provide first behavioural insights into the role of AI as an
advisor. Results reveal that AI-generated advice corrupts people, even when
they know the source of the advice. In fact, AI's corrupting force is as strong
as humans'.

Recent years witness a trend of applying large-scale distributed deep
learning algorithms (HPC AI) in both business and scientific computing areas,
whose goal is to speed up the training time to achieve a state-of-the-art
quality. The HPC AI benchmarks accelerate the process. Unfortunately,
benchmarking HPC AI systems at scale raises serious challenges. This paper
presents a representative, repeatable and simple HPC AI benchmarking
methodology. Among the seventeen AI workloads of AIBench Training -- by far the
most comprehensive AI Training benchmarks suite -- we choose two representative
and repeatable AI workloads. The selected HPC AI benchmarks include both
business and scientific computing: Image Classification and Extreme Weather
Analytics. To rank HPC AI systems, we present a new metric named Valid FLOPS,
emphasizing both throughput performance and a target quality. The
specification, source code, datasets, and HPC AI500 ranking numbers are
publicly available from \url{https://www.benchcouncil.org/HPCAI500/}.

The development of artificial intelligence (AI) has made various industries
eager to explore the benefits of AI. There is an increasing amount of research
surrounding AI, most of which is centred on the development of new AI
algorithms and techniques. However, the advent of AI is bringing an increasing
set of practical problems related to AI model lifecycle management that need to
be investigated. We address this gap by conducting a systematic mapping study
on the lifecycle of AI model. Through quantitative research, we provide an
overview of the field, identify research opportunities, and provide suggestions
for future research. Our study yields 405 publications published from 2005 to
2020, mapped in 5 different main research topics, and 31 sub-topics. We observe
that only a minority of publications focus on data management and model
production problems, and that more studies should address the AI lifecycle from
a holistic perspective.

This paper proposes a comprehensive analysis of existing concepts coming from
different disciplines tackling the notion of intelligence, namely psychology
and engineering, and from disciplines aiming to regulate AI innovations, namely
AI ethics and law. The aim is to identify shared notions or discrepancies to
consider for qualifying AI systems. Relevant concepts are integrated into a
matrix intended to help defining more precisely when and how computing tools
(programs or devices) may be qualified as AI while highlighting critical
features to serve a specific technical, ethical and legal assessment of
challenges in AI development. Some adaptations of existing notions of AI
characteristics are proposed. The matrix is a risk-based conceptual model
designed to allow an empirical, flexible and scalable qualification of AI
technologies in the perspective of benefit-risk assessment practices,
technological monitoring and regulatory compliance: it offers a structured
reflection tool for stakeholders in AI development that are engaged in
responsible research and innovation.Pre-print version (achieved on May 2020)

With the global roll-out of the fifth generation (5G) networks, it is
necessary to look beyond 5G and envision the 6G networks. The 6G networks are
expected to have space-air-ground integrated networks, advanced network
virtualization, and ubiquitous intelligence. This article presents an
artificial intelligence (AI)-native network slicing architecture for 6G
networks to enable the synergy of AI and network slicing, thereby facilitating
intelligent network management and supporting emerging AI services. AI-based
solutions are first discussed across network slicing lifecycle to intelligently
manage network slices, i.e., AI for slicing. Then, network slicing solutions
are studied to support emerging AI services by constructing AI instances and
performing efficient resource management, i.e., slicing for AI. Finally, a case
study is presented, followed by a discussion of open research issues that are
essential for AI-native network slicing in 6G networks.

Limited expert time is a key bottleneck in medical imaging. Due to advances
in image classification, AI can now serve as decision-support for medical
experts, with the potential for great gains in radiologist productivity and, by
extension, public health. However, these gains are contingent on building and
maintaining experts' trust in the AI agents. Explainable AI may build such
trust by helping medical experts to understand the AI decision processes behind
diagnostic judgements. Here we introduce and evaluate explanations based on
Bayesian Teaching, a formal account of explanation rooted in the cognitive
science of human learning. We find that medical experts exposed to explanations
generated by Bayesian Teaching successfully predict the AI's diagnostic
decisions and are more likely to certify the AI for cases when the AI is
correct than when it is wrong, indicating appropriate trust. These results show
that Explainable AI can be used to support human-AI collaboration in medical
imaging.

Recently, the use of sound measures and metrics in Artificial Intelligence
has become the subject of interest of academia, government, and industry.
Efforts towards measuring different phenomena have gained traction in the AI
community, as illustrated by the publication of several influential field
reports and policy documents. These metrics are designed to help decision
takers to inform themselves about the fast-moving and impacting influences of
key advances in Artificial Intelligence in general and Machine Learning in
particular. In this paper we propose to use such newfound capabilities of AI
technologies to augment our AI measuring capabilities. We do so by training a
model to classify publications related to ethical issues and concerns. In our
methodology we use an expert, manually curated dataset as the training set and
then evaluate a large set of research papers. Finally, we highlight the
implications of AI metrics, in particular their contribution towards developing
trustful and fair AI-based tools and technologies. Keywords: AI Ethics; AI
Fairness; AI Measurement. Ethics in Computer Science.

Recent AI governance research has focused heavily on the analysis of strategy
papers and ethics guidelines for AI published by national governments and
international bodies. Meanwhile, subnational institutions have also published
documents on Artificial Intelligence, yet these have been largely absent from
policy analyses. This is surprising because AI is connected to many policy
areas, such as economic or research policy, where the competences are already
distributed between the national and subnational level. To better understand
the current dynamics of AI governance, it is essential to consider the context
of policy making beyond the federal government. Although AI may be considered a
new policy field, it is created, contested and ultimately shaped within
existing political structures and dynamics. We therefore argue that more
attention should be dedicated to subnational efforts to shape AI and present
initial findings from our case study of Germany. Analyzing AI as a policy field
on different levels of government will contribute to a better understanding of
the developments and implementations of AI strategies in different national
contexts.

Advances in machine learning (ML) technologies have greatly improved
Artificial Intelligence (AI) systems. As a result, AI systems have become
ubiquitous, with their application prevalent in virtually all sectors. However,
AI systems have prompted ethical concerns, especially as their usage crosses
boundaries in sensitive areas such as healthcare, transportation, and security.
As a result, users are calling for better AI governance practices in ethical AI
systems. Therefore, AI development methods are encouraged to foster these
practices. This research analyzes the ECCOLA method for developing ethical and
trustworthy AI systems to determine if it enables AI governance in development
processes through ethical practices. The results demonstrate that while ECCOLA
fully facilitates AI governance in corporate governance practices in all its
processes, some of its practices do not fully foster data governance and
information governance practices. This indicates that the method can be further
improved.

Artificial Intelligence (AI) has significant potential for product design: AI
can check technical and non-technical constraints on products, it can support a
quick design of new product variants and new AI methods may also support
creativity. But currently product design and AI are separate communities
fostering different terms and theories. This makes a mapping of AI approaches
to product design needs difficult and prevents new solutions. As a solution,
this paper first clarifies important terms and concepts for the
interdisciplinary domain of AI methods in product design. A key contribution of
this paper is a new classification of design problems using the four
characteristics decomposability, inter-dependencies, innovation and creativity.
Definitions of these concepts are given where they are lacking. Early mappings
of these concepts to AI solutions are sketched and verified using design
examples. The importance of creativity in product design and a corresponding
gap in AI is pointed out for future research.

Massive efforts are made to reduce biases in both data and algorithms in
order to render AI applications fair. These efforts are propelled by various
high-profile cases where biased algorithmic decision-making caused harm to
women, people of color, minorities, etc. However, the AI fairness field still
succumbs to a blind spot, namely its insensitivity to discrimination against
animals. This paper is the first to describe the 'speciesist bias' and
investigate it in several different AI systems. Speciesist biases are learned
and solidified by AI applications when they are trained on datasets in which
speciesist patterns prevail. These patterns can be found in image recognition
systems, large language models, and recommender systems. Therefore, AI
technologies currently play a significant role in perpetuating and normalizing
violence against animals. This can only be changed when AI fairness frameworks
widen their scope and include mitigation measures for speciesist biases. This
paper addresses the AI community in this regard and stresses the influence AI
systems can have on either increasing or reducing the violence that is
inflicted on animals, and especially on farmed animals.

Over the last years, the rising capabilities of artificial intelligence (AI)
have improved human decision-making in many application areas. Teaming between
AI and humans may even lead to complementary team performance (CTP), i.e., a
level of performance beyond the ones that can be reached by AI or humans
individually. Many researchers have proposed using explainable AI (XAI) to
enable humans to rely on AI advice appropriately and thereby reach CTP.
However, CTP is rarely demonstrated in previous work as often the focus is on
the design of explainability, while a fundamental prerequisite -- the presence
of complementarity potential between humans and AI -- is often neglected.
Therefore, we focus on the existence of this potential for effective human-AI
decision-making. Specifically, we identify information asymmetry as an
essential source of complementarity potential, as in many real-world
situations, humans have access to different contextual information. By
conducting an online experiment, we demonstrate that humans can use such
contextual information to adjust the AI's decision, finally resulting in CTP.

With the growing need to regulate AI systems across a wide variety of
application domains, a new set of occupations has emerged in the industry. The
so-called responsible AI practitioners or AI ethicists are generally tasked
with interpreting and operationalizing best practices for ethical and safe
design of AI systems. Due to the nascent nature of these roles, however, it is
unclear to future employers and aspiring AI ethicists what specific function
these roles serve and what skills are necessary to serve the functions. Without
clarity on these, we cannot train future AI ethicists with meaningful learning
objectives.
  In this work, we examine what responsible AI practitioners do in the industry
and what skills they employ on the job. We propose an ontology of existing
roles alongside skills and competencies that serve each role. We created this
ontology by examining the job postings for such roles over a two-year period
(2020-2022) and conducting expert interviews with fourteen individuals who
currently hold such a role in the industry. Our ontology contributes to
business leaders looking to build responsible AI teams and provides educators
with a set of competencies that an AI ethics curriculum can prioritize.

The tremendous achievements of Artificial Intelligence (AI) in computer
vision, natural language processing, games and robotics, has extended the reach
of the AI hype to other fields: in telecommunication networks, the long term
vision is to let AI fully manage, and autonomously drive, all aspects of
network operation. In this industry vision paper, we discuss challenges and
opportunities of Autonomous Driving Network (ADN) driven by AI technologies. To
understand how AI can be successfully landed in current and future networks, we
start by outlining challenges that are specific to the networking domain,
putting them in perspective with advances that AI has achieved in other fields.
We then present a system view, clarifying how AI can be fitted in the network
architecture. We finally discuss current achievements as well as future
promises of AI in networks, mentioning a roadmap to avoid bumps in the road
that leads to true large-scale deployment of AI technologies in networks.

Since the first AI-HRI held at the 2014 AAAI Fall Symposium Series, a lot of
the presented research and discussions have emphasized how artificial
intelligence (AI) developments can benefit human-robot interaction (HRI). This
portrays HRI as an application, a source of domain-specific problems to solve,
to the AI community. Likewise, this portrays AI as a tool, a source of
solutions available for relevant problems, to the HRI community. However,
members of the AI-HRI research community will point out that the relationship
has a deeper synergy than matchmaking problems and solutions -- there are
insights from each field that impact how the other one thinks about the world
and performs scientific research. There is no greater opportunity for sharing
perspectives at the moment than human-aware AI, which studies how to account
for the fact that people are more than a source of data or part of an
algorithm. We will explore how AI-HRI can change the way researchers think
about human-aware AI, from observation through validation, to make even the
algorithmic design process human-aware.

There is a bidirectional relationship between culture and AI; AI models are
increasingly used to analyse culture, thereby shaping our understanding of
culture. On the other hand, the models are trained on collections of cultural
artifacts thereby implicitly, and not always correctly, encoding expressions of
culture. This creates a tension that both limits the use of AI for analysing
culture and leads to problems in AI with respect to cultural complex issues
such as bias.
  One approach to overcome this tension is to more extensively take into
account the intricacies and complexities of culture. We structure our
discussion using four concepts that guide humanistic inquiry into culture:
subjectivity, scalability, contextuality, and temporality. We focus on these
concepts because they have not yet been sufficiently represented in AI
research. We believe that possible implementations of these aspects into AI
research leads to AI that better captures the complexities of culture. In what
follows, we briefly describe these four concepts and their absence in AI
research. For each concept, we define possible research challenges.

As the real-world impact of Artificial Intelligence (AI) systems has been
steadily growing, so too have these systems come under increasing scrutiny. In
response, the study of AI fairness has rapidly developed into a rich field of
research with links to computer science, social science, law, and philosophy.
Many technical solutions for measuring and achieving AI fairness have been
proposed, yet their approach has been criticized in recent years for being
misleading, unrealistic and harmful.
  In our paper, we survey these criticisms of AI fairness and identify key
limitations that are inherent to the prototypical paradigm of AI fairness. By
carefully outlining the extent to which technical solutions can realistically
help in achieving AI fairness, we aim to provide the background necessary to
form a nuanced opinion on developments in fair AI. This delineation also
provides research opportunities for non-AI solutions peripheral to AI systems
in supporting fair decision processes.

The recent release of ChatGPT has gained huge attention and discussion
worldwide, with responsible AI being a key topic of discussion. How can we
ensure that AI systems, including ChatGPT, are developed and adopted in a
responsible way? To tackle the responsible AI challenges, various ethical
principles have been released by governments, organisations, and companies.
However, those principles are very abstract and not practical enough. Further,
significant efforts have been put on algorithm-level solutions that only
address a narrow set of principles, such as fairness and privacy. To fill the
gap, we adopt a pattern-oriented responsible AI engineering approach and build
a Responsible AI Pattern Catalogue to operationalise responsible AI from a
system perspective. In this article, we first summarise the major challenges in
operationalising responsible AI at scale and introduce how we use the
Responsible AI Pattern Catalogue to address those challenges. We then examine
the risks at each stage of the chatbot development process and recommend
pattern-driven mitigations to evaluate the the usefulness of the Responsible AI
Pattern Catalogue in a real-world setting.

Recent advances in large language models (LLMs) have led to the development
of powerful AI chatbots capable of engaging in natural and human-like
conversations. However, these chatbots can be potentially harmful, exhibiting
manipulative, gaslighting, and narcissistic behaviors. We define Healthy AI to
be safe, trustworthy and ethical. To create healthy AI systems, we present the
SafeguardGPT framework that uses psychotherapy to correct for these harmful
behaviors in AI chatbots. The framework involves four types of AI agents: a
Chatbot, a "User," a "Therapist," and a "Critic." We demonstrate the
effectiveness of SafeguardGPT through a working example of simulating a social
conversation. Our results show that the framework can improve the quality of
conversations between AI chatbots and humans. Although there are still several
challenges and directions to be addressed in the future, SafeguardGPT provides
a promising approach to improving the alignment between AI chatbots and human
values. By incorporating psychotherapy and reinforcement learning techniques,
the framework enables AI chatbots to learn and adapt to human preferences and
values in a safe and ethical way, contributing to the development of a more
human-centric and responsible AI.

With the increased adoption of artificial intelligence (AI) in industry and
society, effective human-AI interaction systems are becoming increasingly
important. A central challenge in the interaction of humans with AI is the
estimation of difficulty for human and AI agents for single task
instances.These estimations are crucial to evaluate each agent's capabilities
and, thus, required to facilitate effective collaboration. So far, research in
the field of human-AI interaction estimates the perceived difficulty of humans
and AI independently from each other. However, the effective interaction of
human and AI agents depends on metrics that accurately reflect each agent's
perceived difficulty in achieving valuable outcomes. Research to date has not
yet adequately examined the differences in the perceived difficulty of humans
and AI. Thus, this work reviews recent research on the perceived difficulty in
human-AI interaction and contributing factors to consistently compare each
agent's perceived difficulty, e.g., creating the same prerequisites.
Furthermore, we present an experimental design to thoroughly examine the
perceived difficulty of both agents and contribute to a better understanding of
the design of such systems.

This article connects the concepts and phenomena of Design AI, AI in creative
industries and AIs capacity for creativity. It links Design AI to UX design and
UX designer discourse. Its vagueness and the prominence of UX designers as
speakers and writers in the spectacle of cultural AI discourse. The article
then, draws comparisons between the Theatre of the Absurd and the UX designer
performances of design AI. It additionally sheds light on ToA and the human
condition in terms of existentialism, present within the practice of engaging
in design that intends to link human experience to technological system logic.
This is a theoretical article that utilises examples from UX events published
on Youtube, as well as UX designer blogs, in order to illustrate the mechanics
of the ToA present within contemporary AI and UX designer discourse.

Artificial Intelligence (AI) is a double-edged sword: on one hand, AI
promises to provide great advances that could benefit humanity, but on the
other hand, AI poses substantial (even existential) risks. With advancements
happening daily, many people are increasingly worried about AI's impact on
their lives. To ensure AI progresses beneficially, some researchers have
proposed "wellbeing" as a key objective to govern AI. This article addresses
key challenges in designing AI for wellbeing. We group these challenges into
issues of modeling wellbeing in context, assessing wellbeing in context,
designing interventions to improve wellbeing, and maintaining AI alignment with
wellbeing over time. The identification of these challenges provides a scope
for efforts to help ensure that AI developments are aligned with human
wellbeing.

The rapid advancement of Artificial Intelligence (AI), represented by
ChatGPT, has raised concerns about responsible AI development and utilization.
Existing frameworks lack a comprehensive synthesis of AI risk assessment
questions. To address this, we introduce QB4AIRA, a novel question bank
developed by refining questions from five globally recognized AI risk
frameworks, categorized according to Australia's AI ethics principles. QB4AIRA
comprises 293 prioritized questions covering a wide range of AI risk areas,
facilitating effective risk assessment. It serves as a valuable resource for
stakeholders in assessing and managing AI risks, while paving the way for new
risk frameworks and guidelines. By promoting responsible AI practices, QB4AIRA
contributes to responsible AI deployment, mitigating potential risks and harms.

Generative AI tools introduce new and accessible forms of media creation for
youth. They also raise ethical concerns about the generation of fake media,
data protection, privacy and ownership of AI-generated art. Since generative AI
is already being used in products used by youth, it is critical that they
understand how these tools work and how they can be used or misused. In this
work, we facilitated students' generative AI learning through expression of
their imagined future identities. We designed a learning workshop - Dreaming
with AI - where students learned about the inner workings of generative AI
tools, used text-to-image generation algorithms to create their imaged future
dreams, reflected on the potential benefits and harms of generative AI tools
and voiced their opinions about policies for the use of these tools in
classrooms. In this paper, we present the learning activities and experiences
of 34 high school students who engaged in our workshops. Students reached
creative learning objectives by using prompt engineering to create their future
dreams, gained technical knowledge by learning the abilities, limitations,
text-visual mappings and applications of generative AI, and identified most
potential societal benefits and harms of generative AI.

With the emergence of deep learning techniques, smartphone apps are now
embedded on-device AI features for enabling advanced tasks like speech
translation, to attract users and increase market competitiveness. A good
interaction design is important to make an AI feature usable and
understandable. However, AI features have their unique challenges like
sensitiveness to the input, dynamic behaviours and output uncertainty. Existing
guidelines and tools either do not cover AI features or consider mobile apps
which are confirmed by our informal interview with professional designers. To
address these issues, we conducted the first empirical study to explore
user-AI-interaction in mobile apps. We aim to understand the status of
on-device AI usage by investigating 176 AI apps from 62,822 apps. We identified
255 AI features and summarised 759 implementations into three primary
interaction pattern types. We further implemented our findings into a
multi-faceted search-enabled gallery. The results of the user study demonstrate
the usefulness of our findings.

Precision medicine, tailored to individual patients based on their genetics,
environment, and lifestyle, shows promise in managing complex diseases like
infections. Integrating artificial intelligence (AI) into precision medicine
can revolutionize disease management. This paper introduces a novel approach
using AI to advance precision medicine in infectious diseases and beyond. It
integrates diverse fields, analyzing patients' profiles using genomics,
proteomics, microbiomics, and clinical data. AI algorithms process vast data,
providing insights for precise diagnosis, treatment, and prognosis. AI-driven
predictive modeling empowers healthcare providers to make personalized and
effective interventions. Collaboration among experts from different domains
refines AI models and ensures ethical and robust applications. Beyond
infections, this AI-driven approach can benefit other complex diseases.
Precision medicine powered by AI has the potential to transform healthcare into
a proactive, patient-centric model. Research is needed to address privacy,
regulations, and AI integration into clinical workflows. Collaboration among
researchers, healthcare institutions, and policymakers is crucial in harnessing
AI-driven strategies for advancing precision medicine and improving patient
outcomes.

Researchers, practitioners, and policymakers with an interest in AI ethics
need more integrative approaches for studying and intervening in AI systems
across many contexts and scales of activity. This paper presents AI value
chains as an integrative concept that satisfies that need. To more clearly
theorize AI value chains and conceptually distinguish them from supply chains,
we review theories of value chains and AI value chains from the strategic
management, service science, economic geography, industry, government, and
applied research literature. We then conduct an integrative review of a sample
of 67 sources that cover the ethical concerns implicated in AI value chains.
Building upon the findings of our integrative review, we recommend four future
directions that researchers, practitioners, and policymakers can take to
advance more ethical practices of AI development and use across AI value
chains. Our review and recommendations contribute to the advancement of
research agendas, industrial agendas, and policy agendas that seek to study and
intervene in the ethics of AI value chains.

As AI technology advances rapidly, concerns over the risks of bigness in
digital markets are also growing. The EU's Digital Markets Act (DMA) aims to
address these risks. Still, the current framework may not adequately cover
generative AI systems that could become gateways for AI-based services. This
paper argues for integrating certain AI software as core platform services and
classifying certain developers as gatekeepers under the DMA. We also propose an
assessment of gatekeeper obligations to ensure they cover generative AI
services. As the EU considers generative AI-specific rules and possible DMA
amendments, this paper provides insights towards diversity and openness in
generative AI services.

We present an overview of the literature on trust in AI and AI
trustworthiness and argue for the need to distinguish these concepts more
clearly and to gather more empirically evidence on what contributes to people s
trusting behaviours. We discuss that trust in AI involves not only reliance on
the system itself, but also trust in the developers of the AI system. AI ethics
principles such as explainability and transparency are often assumed to promote
user trust, but empirical evidence of how such features actually affect how
users perceive the system s trustworthiness is not as abundance or not that
clear. AI systems should be recognised as socio-technical systems, where the
people involved in designing, developing, deploying, and using the system are
as important as the system for determining whether it is trustworthy. Without
recognising these nuances, trust in AI and trustworthy AI risk becoming
nebulous terms for any desirable feature for AI systems.

In beamformed wireless cellular systems such as 5G New Radio (NR) networks,
beam management (BM) is a crucial operation. In the second phase of 5G NR
standardization, known as 5G-Advanced, which is being vigorously promoted, the
key component is the use of artificial intelligence (AI) based on machine
learning (ML) techniques. AI/ML for BM is selected as a representative use
case. This article provides an overview of the AI/ML for BM in 5G-Advanced. The
legacy non-AI and prime AI-enabled BM frameworks are first introduced and
compared. Then, the main scope of AI/ML for BM is presented, including
improving accuracy, reducing overhead and latency. Finally, the key challenges
and open issues in the standardization of AI/ML for BM are discussed,
especially the design of new protocols for AI-enabled BM. This article provides
a guideline for the study of AI/ML-based BM standardization.

The true potential of human-AI collaboration lies in exploiting the
complementary capabilities of humans and AI to achieve a joint performance
superior to that of the individual AI or human, i.e., to achieve complementary
team performance (CTP). To realize this complementarity potential, humans need
to exercise discretion in following AI 's advice, i.e., appropriately relying
on the AI's advice. While previous work has focused on building a mental model
of the AI to assess AI recommendations, recent research has shown that the
mental model alone cannot explain appropriate reliance. We hypothesize that, in
addition to the mental model, human learning is a key mediator of appropriate
reliance and, thus, CTP. In this study, we demonstrate the relationship between
learning and appropriate reliance in an experiment with 100 participants. This
work provides fundamental concepts for analyzing reliance and derives
implications for the effective design of human-AI decision-making.

Privacy is a key principle for developing ethical AI technologies, but how
does including AI technologies in products and services change privacy risks?
We constructed a taxonomy of AI privacy risks by analyzing 321 documented AI
privacy incidents. We codified how the unique capabilities and requirements of
AI technologies described in those incidents generated new privacy risks,
exacerbated known ones, or otherwise did not meaningfully alter the risk. We
present 12 high-level privacy risks that AI technologies either newly created
(e.g., exposure risks from deepfake pornography) or exacerbated (e.g.,
surveillance risks from collecting training data). One upshot of our work is
that incorporating AI technologies into a product can alter the privacy risks
it entails. Yet, current approaches to privacy-preserving AI/ML (e.g.,
federated learning, differential privacy, checklists) only address a subset of
the privacy risks arising from the capabilities and data requirements of AI.

Academic writing is an indispensable yet laborious part of the research
enterprise. This Perspective maps out principles and methods for using
generative artificial intelligence (AI), specifically large language models
(LLMs), to elevate the quality and efficiency of academic writing. We introduce
a human-AI collaborative framework that delineates the rationale (why), process
(how), and nature (what) of AI engagement in writing. The framework pinpoints
both short-term and long-term reasons for engagement and their underlying
mechanisms (e.g., cognitive offloading and imaginative stimulation). It reveals
the role of AI throughout the writing process, conceptualized through a
two-stage model for human-AI collaborative writing, and the nature of AI
assistance in writing, represented through a model of writing-assistance types
and levels. Building on this framework, we describe effective prompting
techniques for incorporating AI into the writing routine (outlining, drafting,
and editing) as well as strategies for maintaining rigorous scholarship,
adhering to varied journal policies, and avoiding overreliance on AI.
Ultimately, the prudent integration of AI into academic writing can ease the
communication burden, empower authors, accelerate discovery, and promote
diversity in science.

This paper explores the dynamic landscape of Artificial Intelligence (AI)
adoption in Africa, analysing its varied applications in addressing
socio-economic challenges and fostering development. Examining the African AI
ecosystem, the study considers regional nuances, cultural factors, and
infrastructural constraints shaping the deployment of AI solutions. Case
studies in healthcare, agriculture, finance, and education highlight AI's
transformative potential for efficiency, accessibility, and inclusivity. The
paper emphasizes indigenous AI innovations and international collaborations
contributing to a distinct African AI ecosystem. Ethical considerations,
including data privacy and algorithmic bias, are addressed alongside policy
frameworks supporting responsible AI implementation. The role of governmental
bodies, regulations, and private sector partnerships is explored in creating a
conducive AI development environment. Challenges such as digital literacy gaps
and job displacement are discussed, with proposed strategies for mitigation. In
conclusion, the paper provides a nuanced understanding of AI in Africa,
contributing to sustainable development discussions and advocating for an
inclusive and ethical AI ecosystem on the continent.

Emerging Distributed AI systems are revolutionizing big data computing and
data processing capabilities with growing economic and societal impact.
However, recent studies have identified new attack surfaces and risks caused by
security, privacy, and fairness issues in AI systems. In this paper, we review
representative techniques, algorithms, and theoretical foundations for
trustworthy distributed AI through robustness guarantee, privacy protection,
and fairness awareness in distributed learning. We first provide a brief
overview of alternative architectures for distributed learning, discuss
inherent vulnerabilities for security, privacy, and fairness of AI algorithms
in distributed learning, and analyze why these problems are present in
distributed learning regardless of specific architectures. Then we provide a
unique taxonomy of countermeasures for trustworthy distributed AI, covering (1)
robustness to evasion attacks and irregular queries at inference, and
robustness to poisoning attacks, Byzantine attacks, and irregular data
distribution during training; (2) privacy protection during distributed
learning and model inference at deployment; and (3) AI fairness and governance
with respect to both data and models. We conclude with a discussion on open
challenges and future research directions toward trustworthy distributed AI,
such as the need for trustworthy AI policy guidelines, the AI
responsibility-utility co-design, and incentives and compliance.

AI systems cannot exist without data. Now that AI models (data science and
AI) have matured and are readily available to apply in practice, most
organizations struggle with the data infrastructure to do so. There is a
growing need for data engineers that know how to prepare data for AI systems or
that can setup enterprise-wide data architectures for analytical projects. But
until now, the data engineering part of AI engineering has not been getting
much attention, in favor of discussing the modeling part. In this paper we aim
to change this by perform a mapping study on data engineering for AI systems,
i.e., AI data engineering. We found 25 relevant papers between January 2019 and
June 2023, explaining AI data engineering activities. We identify which life
cycle phases are covered, which technical solutions or architectures are
proposed and which lessons learned are presented. We end by an overall
discussion of the papers with implications for practitioners and researchers.
This paper creates an overview of the body of knowledge on data engineering for
AI. This overview is useful for practitioners to identify solutions and best
practices as well as for researchers to identify gaps.

The vast majority of discourse around AI development assumes that
subservient, "moral" models aligned with "human values" are universally
beneficial -- in short, that good AI is sycophantic AI. We explore the shadow
of the sycophantic paradigm, a design space we term antagonistic AI: AI systems
that are disagreeable, rude, interrupting, confrontational, challenging, etc.
-- embedding opposite behaviors or values. Far from being "bad" or "immoral,"
we consider whether antagonistic AI systems may sometimes have benefits to
users, such as forcing users to confront their assumptions, build resilience,
or develop healthier relational boundaries. Drawing from formative explorations
and a speculative design workshop where participants designed fictional AI
technologies that employ antagonism, we lay out a design space for antagonistic
AI, articulating potential benefits, design techniques, and methods of
embedding antagonistic elements into user experience. Finally, we discuss the
many ethical challenges of this space and identify three dimensions for the
responsible design of antagonistic AI -- consent, context, and framing.

To effectively navigate the AI revolution, AI literacy is crucial. However,
content predominantly exists in dominant languages, creating a gap for
low-resource languages like Yoruba (41 million native speakers). This case
study explores bridging this gap by creating and distributing AI videos in
Yoruba.The project developed 26 videos covering foundational, intermediate, and
advanced AI concepts, leveraging storytelling and accessible explanations.
These videos were created using a cost-effective methodology and distributed
across YouTube, LinkedIn, and Twitter, reaching an estimated global audience of
22 countries. Analysis of YouTube reveals insights into viewing patterns, with
the 25-44 age group contributing the most views. Notably, over half of the
traffic originated from external sources, highlighting the potential of
cross-platform promotion.This study demonstrates the feasibility and impact of
creating AI literacy content in low-resource languages. It emphasizes that
accurate interpretation requires both technical expertise in AI and fluency in
the target language. This work contributes a replicable methodology, a 22-word
Yoruba AI vocabulary, and data-driven insights into audience demographics and
acquisition channel

This paper explores the intricate relationship between capitalism, racial
injustice, and artificial intelligence (AI), arguing that AI acts as a
contemporary vehicle for age-old forms of exploitation. By linking historical
patterns of racial and economic oppression with current AI practices, this
study illustrates how modern technology perpetuates and deepens societal
inequalities. It specifically examines how AI is implicated in the exploitation
of marginalized communities through underpaid labor in the gig economy, the
perpetuation of biases in algorithmic decision-making, and the reinforcement of
systemic barriers that prevent these groups from benefiting equitably from
technological advances. Furthermore, the paper discusses the role of AI in
extending and intensifying the social, economic, and psychological burdens
faced by these communities, highlighting the problematic use of AI in
surveillance, law enforcement, and mental health contexts. The analysis
concludes with a call for transformative changes in how AI is developed and
deployed. Advocating for a reevaluation of the values driving AI innovation,
the paper promotes an approach that integrates social justice and equity into
the core of technological design and policy. This shift is crucial for ensuring
that AI serves as a tool for societal improvement, fostering empowerment and
healing rather than deepening existing divides.

The current societal challenges exceed the capacity of human individual or
collective effort alone. As AI evolves, its role within human collectives is
poised to vary from an assistive tool to a participatory member. Humans and AI
possess complementary capabilities that, when synergized, can achieve a level
of collective intelligence that surpasses the collective capabilities of either
humans or AI in isolation. However, the interactions in human-AI systems are
inherently complex, involving intricate processes and interdependencies. This
review incorporates perspectives from network science to conceptualize a
multilayer representation of human-AI collective intelligence, comprising a
cognition layer, a physical layer, and an information layer. Within this
multilayer network, humans and AI agents exhibit varying characteristics;
humans differ in diversity from surface-level to deep-level attributes, while
AI agents range in degrees of functionality and anthropomorphism. The interplay
among these agents shapes the overall structure and dynamics of the system. We
explore how agents' diversity and interactions influence the system's
collective intelligence. Furthermore, we present an analysis of real-world
instances of AI-enhanced collective intelligence. We conclude by addressing the
potential challenges in AI-enhanced collective intelligence and offer
perspectives on future developments in this field.

Recent advances in generative AI technologies like large language models
raise both excitement and concerns about the future of human-AI co-creation in
writing. To unpack people's attitude towards and experience with generative
AI-powered writing assistants, in this paper, we conduct an experiment to
understand whether and how much value people attach to AI assistance, and how
the incorporation of AI assistance in writing workflows changes people's
writing perceptions and performance. Our results suggest that people are
willing to forgo financial payments to receive writing assistance from AI,
especially if AI can provide direct content generation assistance and the
writing task is highly creative. Generative AI-powered assistance is found to
offer benefits in increasing people's productivity and confidence in writing.
However, direct content generation assistance offered by AI also comes with
risks, including decreasing people's sense of accountability and diversity in
writing. We conclude by discussing the implications of our findings.

Artificial intelligence (AI) ethics has emerged as a burgeoning yet pivotal
area of scholarly research. This study conducts a comprehensive bibliometric
analysis of the AI ethics literature over the past two decades. The analysis
reveals a discernible tripartite progression, characterized by an incubation
phase, followed by a subsequent phase focused on imbuing AI with human-like
attributes, culminating in a third phase emphasizing the development of
human-centric AI systems. After that, they present seven key AI ethics issues,
encompassing the Collingridge dilemma, the AI status debate, challenges
associated with AI transparency and explainability, privacy protection
complications, considerations of justice and fairness, concerns about algocracy
and human enfeeblement, and the issue of superintelligence. Finally, they
identify two notable research gaps in AI ethics regarding the large ethics
model (LEM) and AI identification and extend an invitation for further
scholarly research.

In December 2023, the European Parliament provisionally agreed on the EU AI
Act. This unprecedented regulatory framework for AI systems lays out guidelines
to ensure the safety, legality, and trustworthiness of AI products. This paper
presents a methodology for interpreting the EU AI Act requirements for
high-risk AI systems by leveraging product quality models. We first propose an
extended product quality model for AI systems, incorporating attributes
relevant to the Act not covered by current quality models. We map the Act
requirements to relevant quality attributes with the goal of refining them into
measurable characteristics. We then propose a contract-based approach to derive
technical requirements at the stakeholder level. This facilitates the
development and assessment of AI systems that not only adhere to established
quality standards, but also comply with the regulatory requirements outlined in
the Act for high-risk (including safety-critical) AI systems. We demonstrate
the applicability of this methodology on an exemplary automotive supply chain
use case, where several stakeholders interact to achieve EU AI Act compliance.

AI is redefining how humans interact with technology, leading to a synergetic
collaboration between the two. Nevertheless, the effects of human cognition on
this collaboration remain unclear. This study investigates the implications of
two cognitive biases, anthropomorphism and framing effect, on human-AI
collaboration within a hiring setting. Subjects were asked to select job
candidates with the help of an AI-powered recommendation tool. The tool was
manipulated to have either human-like or robot-like characteristics and
presented its recommendations in either positive or negative frames. The
results revealed that the framing of AI's recommendations had no significant
influence on subjects' decisions. In contrast, anthropomorphism significantly
affected subjects' agreement with AI recommendations. Contrary to expectations,
subjects were less likely to agree with the AI if it had human-like
characteristics. These findings demonstrate that cognitive biases can impact
human-AI collaboration and highlight the need for tailored approaches to AI
product design, rather than a single, universal solution.

The advent of Foundation Models (FMs) and AI-powered copilots has transformed
the landscape of software development, offering unprecedented code completion
capabilities and enhancing developer productivity. However, the current
task-driven nature of these copilots falls short in addressing the broader
goals and complexities inherent in software engineering (SE). In this paper, we
propose a paradigm shift towards goal-driven AI-powered pair programmers that
collaborate with human developers in a more holistic and context-aware manner.
We envision AI pair programmers that are goal-driven, human partners, SE-aware,
and self-learning. These AI partners engage in iterative, conversation-driven
development processes, aligning closely with human goals and facilitating
informed decision-making. We discuss the desired attributes of such AI pair
programmers and outline key challenges that must be addressed to realize this
vision. Ultimately, our work represents a shift from AI-augmented SE to
AI-transformed SE by replacing code completion with a collaborative partnership
between humans and AI that enhances both productivity and software quality.

This workshop paper presents a critical examination of the integration of
Generative AI (Gen AI) into the academic writing process, focusing on the use
of AI as a collaborative tool. It contrasts the performance and interaction of
two AI models, Gemini and ChatGPT, through a collaborative inquiry approach
where researchers engage in facilitated sessions to design prompts that elicit
specific AI responses for crafting research outlines. This case study
highlights the importance of prompt design, output analysis, and recognizing
the AI's limitations to ensure responsible and effective AI integration in
scholarly work. Preliminary findings suggest that prompt variation
significantly affects output quality and reveals distinct capabilities and
constraints of each model. The paper contributes to the field of Human-Computer
Interaction by exploring effective prompt strategies and providing a
comparative analysis of Gen AI models, ultimately aiming to enhance AI-assisted
academic writing and prompt a deeper dialogue within the HCI community.

Recent AI advancements offer transformative potential for global education,
yet their application often overlooks Africa's unique educational landscape.
AfricAIED 2024 will address this gap, spotlighting efforts to develop AI in
Education (AIED) systems tailored to Africa's needs. Building on the success of
the inaugural workshop, AfricAIED 2024 will feature an online AI Hackathon
focused on democratizing preparation for Ghana's National Science & Maths Quiz
(NSMQ). Participants will create open-source AI tools leveraging resources from
the Brilla AI project to level the academic playing field and enhance science
and math education across Africa. The workshop will showcase top competitors'
solutions, invite discussions on AIED opportunities and challenges in Africa,
and highlight the latest advancements in AI education integration. AfricAIED
2024 aims to foster collaboration and innovation, amplifying African voices in
the AIED community and driving positive change in African education through AI.

AI intent alignment, ensuring that AI produces outcomes as intended by users,
is a critical challenge in human-AI interaction. The emergence of generative
AI, including LLMs, has intensified the significance of this problem, as
interactions increasingly involve users specifying desired results for AI
systems. In order to support better AI intent alignment, we aim to explore
human strategies for intent specification in human-human communication. By
studying and comparing human-human and human-LLM communication, we identify key
strategies that can be applied to the design of AI systems that are more
effective at understanding and aligning with user intent. This study aims to
advance toward a human-centered AI system by bringing together human
communication strategies for the design of AI systems.

Inspired by the increasing use of AI to augment humans, researchers have
studied human-AI systems involving different tasks, systems, and populations.
Despite such a large body of work, we lack a broad conceptual understanding of
when combinations of humans and AI are better than either alone. Here, we
addressed this question by conducting a meta-analysis of over 100 recent
experimental studies reporting over 300 effect sizes. First, we found that, on
average, human-AI combinations performed significantly worse than the best of
humans or AI alone. Second, we found performance losses in tasks that involved
making decisions and significantly greater gains in tasks that involved
creating content. Finally, when humans outperformed AI alone, we found
performance gains in the combination, but when the AI outperformed humans alone
we found losses. These findings highlight the heterogeneity of the effects of
human-AI collaboration and point to promising avenues for improving human-AI
systems.

Existing strategies for managing risks from advanced AI systems often focus
on affecting what AI systems are developed and how they diffuse. However, this
approach becomes less feasible as the number of developers of advanced AI
grows, and impedes beneficial use-cases as well as harmful ones. In response,
we urge a complementary approach: increasing societal adaptation to advanced
AI, that is, reducing the expected negative impacts from a given level of
diffusion of a given AI capability. We introduce a conceptual framework which
helps identify adaptive interventions that avoid, defend against and remedy
potentially harmful uses of AI systems, illustrated with examples in election
manipulation, cyberterrorism, and loss of control to AI decision-makers. We
discuss a three-step cycle that society can implement to adapt to AI.
Increasing society's ability to implement this cycle builds its resilience to
advanced AI. We conclude with concrete recommendations for governments,
industry, and third-parties.

AI researchers employ not only the scientific method, but also methodology
from mathematics and engineering. However, the use of the scientific method -
specifically hypothesis testing - in AI is typically conducted in service of
engineering objectives. Growing interest in topics such as fairness and
algorithmic bias show that engineering-focused questions only comprise a subset
of the important questions about AI systems. This results in the AI Knowledge
Gap: the number of unique AI systems grows faster than the number of studies
that characterize these systems' behavior. To close this gap, we argue that the
study of AI could benefit from the greater inclusion of researchers who are
well positioned to formulate and test hypotheses about the behavior of AI
systems. We examine the barriers preventing social and behavioral scientists
from conducting such studies. Our diagnosis suggests that accelerating the
scientific study of AI systems requires new incentives for academia and
industry, mediated by new tools and institutions. To address these needs, we
propose a two-sided marketplace called TuringBox. On one side, AI contributors
upload existing and novel algorithms to be studied scientifically by others. On
the other side, AI examiners develop and post machine intelligence tasks
designed to evaluate and characterize algorithmic behavior. We discuss this
market's potential to democratize the scientific study of AI behavior, and thus
narrow the AI Knowledge Gap.

Recent advances in artificial intelligence (AI) and machine learning have
created a general perception that AI could be used to solve complex problems,
and in some situations over-hyped as a tool that can be so easily used.
Unfortunately, the barrier to realization of mass adoption of AI on various
business domains is too high because most domain experts have no background in
AI. Developing AI applications involves multiple phases, namely data
preparation, application modeling, and product deployment. The effort of AI
research has been spent mostly on new AI models (in the model training stage)
to improve the performance of benchmark tasks such as image recognition. Many
other factors such as usability, efficiency and security of AI have not been
well addressed, and therefore form a barrier to democratizing AI. Further, for
many real world applications such as healthcare and autonomous driving,
learning via huge amounts of possibility exploration is not feasible since
humans are involved. In many complex applications such as healthcare, subject
matter experts (e.g. Clinicians) are the ones who appreciate the importance of
features that affect health, and their knowledge together with existing
knowledge bases are critical to the end results. In this paper, we take a new
perspective on developing AI solutions, and present a solution for making AI
usable. We hope that this resolution will enable all subject matter experts
(eg. Clinicians) to exploit AI like data scientists.

Explainable AI provides insight into the "why" for model predictions,
offering potential for users to better understand and trust a model, and to
recognize and correct AI predictions that are incorrect. Prior research on
human and explainable AI interactions has focused on measures such as
interpretability, trust, and usability of the explanation. Whether explainable
AI can improve actual human decision-making and the ability to identify the
problems with the underlying model are open questions. Using real datasets, we
compare and evaluate objective human decision accuracy without AI (control),
with an AI prediction (no explanation), and AI prediction with explanation. We
find providing any kind of AI prediction tends to improve user decision
accuracy, but no conclusive evidence that explainable AI has a meaningful
impact. Moreover, we observed the strongest predictor for human decision
accuracy was AI accuracy and that users were somewhat able to detect when the
AI was correct versus incorrect, but this was not significantly affected by
including an explanation. Our results indicate that, at least in some
situations, the "why" information provided in explainable AI may not enhance
user decision-making, and further research may be needed to understand how to
integrate explainable AI into real systems.

With the extensive use of AI in various fields, the issue of AI security has
become more significant. The AI data poisoning attacks will be the most
threatening approach against AI security after the adversarial examples. As the
continuous updating of AI applications online, the data pollution models can be
uploaded by attackers to achieve a certain malicious purpose. Recently, the
research on AI data poisoning attacks is mostly out of practice and use
self-built experimental environments so that it cannot be as close to reality
as adversarial example attacks. This article's first contribution is to provide
a solution and a breakthrough for the aforementioned issue with research
limitations, to aim at data poisoning attacks that target real businesses, in
this case: data poisoning attacks on real Go AI. We install a Trojan virus into
the real Go AI that manipulates the AI's behavior. It is the first time that we
succeed in manipulating complicated AI and provide a reliable approach to the
AI data poisoning attack verification method. The method of building Trojan in
this article can be expanded to more practical algorithms for other fields such
as content recommendation, text translation, and intelligent dialogue.

In the current era, people and society have grown increasingly reliant on
artificial intelligence (AI) technologies. AI has the potential to drive us
towards a future in which all of humanity flourishes. It also comes with
substantial risks for oppression and calamity. Discussions about whether we
should (re)trust AI have repeatedly emerged in recent years and in many
quarters, including industry, academia, healthcare, services, and so on.
Technologists and AI researchers have a responsibility to develop trustworthy
AI systems. They have responded with great effort to design more responsible AI
algorithms. However, existing technical solutions are narrow in scope and have
been primarily directed towards algorithms for scoring or classification tasks,
with an emphasis on fairness and unwanted bias. To build long-lasting trust
between AI and human beings, we argue that the key is to think beyond
algorithmic fairness and connect major aspects of AI that potentially cause
AI's indifferent behavior. In this survey, we provide a systematic framework of
Socially Responsible AI Algorithms that aims to examine the subjects of AI
indifference and the need for socially responsible AI algorithms, define the
objectives, and introduce the means by which we may achieve these objectives.
We further discuss how to leverage this framework to improve societal
well-being through protection, information, and prevention/mitigation.

People supported by AI-powered decision support tools frequently overrely on
the AI: they accept an AI's suggestion even when that suggestion is wrong.
Adding explanations to the AI decisions does not appear to reduce the
overreliance and some studies suggest that it might even increase it. Informed
by the dual-process theory of cognition, we posit that people rarely engage
analytically with each individual AI recommendation and explanation, and
instead develop general heuristics about whether and when to follow the AI
suggestions. Building on prior research on medical decision-making, we designed
three cognitive forcing interventions to compel people to engage more
thoughtfully with the AI-generated explanations. We conducted an experiment
(N=199), in which we compared our three cognitive forcing designs to two simple
explainable AI approaches and to a no-AI baseline. The results demonstrate that
cognitive forcing significantly reduced overreliance compared to the simple
explainable AI approaches. However, there was a trade-off: people assigned the
least favorable subjective ratings to the designs that reduced the overreliance
the most. To audit our work for intervention-generated inequalities, we
investigated whether our interventions benefited equally people with different
levels of Need for Cognition (i.e., motivation to engage in effortful mental
activities). Our results show that, on average, cognitive forcing interventions
benefited participants higher in Need for Cognition more. Our research suggests
that human cognitive motivation moderates the effectiveness of explainable AI
solutions.

We survey SoTA open-domain conversational AI models with the purpose of
presenting the prevailing challenges that still exist to spur future research.
In addition, we provide statistics on the gender of conversational AI in order
to guide the ethics discussion surrounding the issue. Open-domain
conversational AI are known to have several challenges, including bland
responses and performance degradation when prompted with figurative language,
among others. First, we provide some background by discussing some topics of
interest in conversational AI. We then discuss the method applied to the two
investigations carried out that make up this study. The first investigation
involves a search for recent SoTA open-domain conversational AI models while
the second involves the search for 100 conversational AI to assess their
gender. Results of the survey show that progress has been made with recent SoTA
conversational AI, but there are still persistent challenges that need to be
solved, and the female gender is more common than the male for conversational
AI. One main take-away is that hybrid models of conversational AI offer more
advantages than any single architecture. The key contributions of this survey
are 1) the identification of prevailing challenges in SoTA open-domain
conversational AI, 2) the unusual discussion about open-domain conversational
AI for low-resource languages, and 3) the discussion about the ethics
surrounding the gender of conversational AI.

Advances in artificial intelligence (AI) are shaping modern life, from
transportation, health care, science, finance, to national defense. Forecasts
of AI development could help improve policy- and decision-making. We report the
results from a large survey of AI and machine learning (ML) researchers on
their beliefs about progress in AI. The survey, fielded in late 2019, elicited
forecasts for near-term AI development milestones and high- or human-level
machine intelligence, defined as when machines are able to accomplish every or
almost every task humans are able to do currently. As part of this study, we
re-contacted respondents from a highly-cited study by Grace et al. (2018), in
which AI/ML researchers gave forecasts about high-level machine intelligence
and near-term milestones in AI development. Results from our 2019 survey show
that, in aggregate, AI/ML researchers surveyed placed a 50% likelihood of
human-level machine intelligence being achieved by 2060. The results show
researchers newly contacted in 2019 expressed similar beliefs about the
progress of advanced AI as respondents in the Grace et al. (2018) survey. For
the recontacted participants from the Grace et al. (2018) study, the aggregate
forecast for a 50% likelihood of high-level machine intelligence shifted from
2062 to 2076, although this change is not statistically significant, likely due
to the small size of our panel sample. Forecasts of several near-term AI
milestones have reduced in time, suggesting more optimism about AI progress.
Finally, AI/ML researchers also exhibited significant optimism about how
human-level machine intelligence will impact society.

The term ethics is widely used, explored, and debated in the context of
developing Artificial Intelligence (AI) based software systems. In recent
years, numerous incidents have raised the profile of ethical issues in AI
development and led to public concerns about the proliferation of AI technology
in our everyday lives. But what do we know about the views and experiences of
those who develop these systems -- the AI practitioners? We conducted a
grounded theory literature review (GTLR) of 38 primary empirical studies that
included AI practitioners' views on ethics in AI and analysed them to derive
five categories: practitioner awareness, perception, need, challenge, and
approach. These are underpinned by multiple codes and concepts that we explain
with evidence from the included studies. We present a taxonomy of ethics in AI
from practitioners' viewpoints to assist AI practitioners in identifying and
understanding the different aspects of AI ethics. The taxonomy provides a
landscape view of the key aspects that concern AI practitioners when it comes
to ethics in AI. We also share an agenda for future research studies and
recommendations for practitioners, managers, and organisations to help in their
efforts to better consider and implement ethics in AI.

Vertical heterogenous networks (VHetNets) and artificial intelligence (AI)
play critical roles in 6G and beyond networks. This article presents an
AI-native VHetNets architecture to enable the synergy of VHetNets and AI,
thereby supporting varieties of AI services while facilitating automatic and
intelligent network management. Anomaly detection in Internet of Things (IoT)
is a major AI service required by many fields, including intrusion detection,
state monitoring, device-activity analysis, security supervision and so on.
Conventional anomaly detection technologies mainly consider the anomaly
detection as a standalone service that is independent of any other network
management functionalities, which cannot be used directly in ubiquitous IoT due
to the resource constrained end nodes and decentralized data distribution. In
this article, we develop an AI-native VHetNets-enabled framework to provide the
anomaly detection service for ubiquitous IoT, whose implementation is assisted
by intelligent network management functionalities. We first discuss the
possibilities of VHetNets used for distributed AI model training to provide
anomaly detection service for ubiquitous IoT, i.e., VHetNets for AI. After
that, we study the application of AI approaches in helping provide automatic
and intelligent network management functionalities for VHetNets, i.e., AI for
VHetNets, whose aim is to facilitate the efficient implementation of anomaly
detection service. Finally, a case study is presented to demonstrate the
efficiency and effectiveness of the proposed AI-native VHetNets-enabled anomaly
detection framework.

Mistakes in AI systems are inevitable, arising from both technical
limitations and sociotechnical gaps. While black-boxing AI systems can make the
user experience seamless, hiding the seams risks disempowering users to
mitigate fallouts from AI mistakes. Instead of hiding these AI imperfections,
can we leverage them to help the user? While Explainable AI (XAI) has
predominantly tackled algorithmic opaqueness, we propose that seamful design
can foster AI explainability by revealing and leveraging sociotechnical and
infrastructural mismatches. We introduce the concept of Seamful XAI by (1)
conceptually transferring "seams" to the AI context and (2) developing a design
process that helps stakeholders anticipate and design with seams. We explore
this process with 43 AI practitioners and real end-users, using a
scenario-based co-design activity informed by real-world use cases. We found
that the Seamful XAI design process helped users foresee AI harms, identify
underlying reasons (seams), locate them in the AI's lifecycle, learn how to
leverage seamful information to improve XAI and user agency. We share empirical
insights, implications, and reflections on how this process can help
practitioners anticipate and craft seams in AI, how seamfulness can improve
explainability, empower end-users, and facilitate Responsible AI.

Novel artificial intelligence (AI) technology has expedited various
scientific research, e.g., cosmology, physics and bioinformatics, inevitably
becoming a significant category of workload on high performance computing (HPC)
systems. Existing AI benchmarks tend to customize well-recognized AI
applications, so as to evaluate the AI performance of HPC systems under
predefined problem size, in terms of datasets and AI models. Due to lack of
scalability on the problem size, static AI benchmarks might be under competent
to help understand the performance trend of evolving AI applications on HPC
systems, in particular, the scientific AI applications on large-scale systems.
  In this paper, we propose a scalable evaluation methodology (SAIH) for
analyzing the AI performance trend of HPC systems with scaling the problem
sizes of customized AI applications. To enable scalability, SAIH builds a set
of novel mechanisms for augmenting problem sizes. As the data and model
constantly scale, we can investigate the trend and range of AI performance on
HPC systems, and further diagnose system bottlenecks. To verify our
methodology, we augment a cosmological AI application to evaluate a real HPC
system equipped with GPUs as a case study of SAIH.

This work addresses the problems of (a) designing utilization measurements of
trained artificial intelligence (AI) models and (b) explaining how training
data are encoded in AI models based on those measurements. The problems are
motivated by the lack of explainability of AI models in security and safety
critical applications, such as the use of AI models for classification of
traffic signs in self-driving cars. We approach the problems by introducing
theoretical underpinnings of AI model utilization measurement and understanding
patterns in utilization-based class encodings of traffic signs at the level of
computation graphs (AI models), subgraphs, and graph nodes. Conceptually,
utilization is defined at each graph node (computation unit) of an AI model
based on the number and distribution of unique outputs in the space of all
possible outputs (tensor-states). In this work, utilization measurements are
extracted from AI models, which include poisoned and clean AI models. In
contrast to clean AI models, the poisoned AI models were trained with traffic
sign images containing systematic, physically realizable, traffic sign
modifications (i.e., triggers) to change a correct class label to another label
in a presence of such a trigger. We analyze class encodings of such clean and
poisoned AI models, and conclude with implications for trojan injection and
detection.

Significant enthusiasm around AI uptake has been witnessed across societies
globally. The electoral process -- the time, place and manner of elections
within democratic nations -- has been among those very rare sectors in which AI
has not penetrated much. Electoral management bodies in many countries have
recently started exploring and deliberating over the use of AI in the electoral
process. In this paper, we consider five representative avenues within the core
electoral process which have potential for AI usage, and map the challenges
involved in using AI within them. These five avenues are: voter list
maintenance, determining polling booth locations, polling booth protection
processes, voter authentication and video monitoring of elections. Within each
of these avenues, we lay down the context, illustrate current or potential
usage of AI, and discuss extant or potential ramifications of AI usage, and
potential directions for mitigating risks while considering AI usage. We
believe that the scant current usage of AI within electoral processes provides
a very rare opportunity, that of being able to deliberate on the risks and
mitigation possibilities, prior to real and widespread AI deployment. This
paper is an attempt to map the horizons of risks and opportunities in using AI
within the electoral processes and to help shape the debate around the topic.

As Artificial Intelligence (AI) continues to advance rapidly, it becomes
increasingly important to consider AI's ethical and societal implications. In
this paper, we present a bottom-up mapping of the current state of research at
the intersection of Human-Centered AI, Ethical, and Responsible AI (HCER-AI) by
thematically reviewing and analyzing 164 research papers from leading
conferences in ethical, social, and human factors of AI: AIES, CHI, CSCW, and
FAccT. The ongoing research in HCER-AI places emphasis on governance, fairness,
and explainability. These conferences, however, concentrate on specific themes
rather than encompassing all aspects. While AIES has fewer papers on HCER-AI,
it emphasizes governance and rarely publishes papers about privacy, security,
and human flourishing. FAccT publishes more on governance and lacks papers on
privacy, security, and human flourishing. CHI and CSCW, as more established
conferences, have a broader research portfolio. We find that the current
emphasis on governance and fairness in AI research may not adequately address
the potential unforeseen and unknown implications of AI. Therefore, we
recommend that future research should expand its scope and diversify resources
to prepare for these potential consequences. This could involve exploring
additional areas such as privacy, security, human flourishing, and
explainability.

This paper explores the potential of artificial intelligence (AI) in higher
education, specifically its capacity to replace or assist human teachers. By
reviewing relevant literature and analysing survey data from students and
teachers, the study provides a comprehensive perspective on the future role of
educators in the face of advancing AI technologies. Findings suggest that
although some believe AI may eventually replace teachers, the majority of
participants argue that human teachers possess unique qualities, such as
critical thinking, creativity, and emotions, which make them irreplaceable. The
study also emphasizes the importance of social-emotional competencies developed
through human interactions, which AI technologies cannot currently replicate.
The research proposes that teachers can effectively integrate AI to enhance
teaching and learning without viewing it as a replacement. To do so, teachers
need to understand how AI can work well with teachers and students while
avoiding potential pitfalls, develop AI literacy, and address practical issues
such as data protection, ethics, and privacy. The study reveals that students
value and respect human teachers, even as AI becomes more prevalent in
education. The study also introduces a roadmap for students, teachers, and
universities. This roadmap serves as a valuable guide for refining teaching
skills, fostering personal connections, and designing curriculums that
effectively balance the strengths of human educators with AI technologies. The
future of education lies in the synergy between human teachers and AI. By
understanding and refining their unique qualities, teachers, students, and
universities can effectively navigate the integration of AI, ensuring a
well-rounded and impactful learning experience.

Cyber-physical systems (CPSs) are now widely deployed in many industrial
domains, e.g., manufacturing systems and autonomous vehicles. To further
enhance the capability and applicability of CPSs, there comes a recent trend
from both academia and industry to utilize learning-based AI controllers for
the system control process, resulting in an emerging class of AI-enabled
cyber-physical systems (AI-CPSs). Although such AI-CPSs could achieve obvious
performance enhancement from the lens of some key industrial requirement
indicators, due to the random exploration nature and lack of systematic
explanations for their behavior, such AI-based techniques also bring
uncertainties and safety risks to the controlled system, posing an urgent need
for effective safety analysis techniques for AI-CPSs. Hence in this work, we
propose Mosaic, a model-based safety analysis framework for AI-CPSs. Mosaic
first constructs a Markov decision process (MDP) model as an abstract model of
the AI-CPS, which tries to characterize the behaviors of the original AI-CPS.
Then, based on the derived abstract model, safety analysis is designed in two
aspects: online safety monitoring and offline model-guided falsification. The
usefulness of Mosaic is evaluated on diverse and representative industry-level
AI-CPSs, the results of which demonstrate that Mosaic is effective in providing
safety monitoring to AI-CPSs and enables to outperform the state-of-the-art
falsification techniques, providing the basis for advanced safety analysis of
AI-CPSs.

As AI-powered code generation tools such as GitHub Copilot become popular, it
is crucial to understand software developers' trust in AI tools -- a key factor
for tool adoption and responsible usage. However, we know little about how
developers build trust with AI, nor do we understand how to design the
interface of generative AI systems to facilitate their appropriate levels of
trust. In this paper, we describe findings from a two-stage qualitative
investigation. We first interviewed 17 developers to contextualize their
notions of trust and understand their challenges in building appropriate trust
in AI code generation tools. We surfaced three main challenges -- including
building appropriate expectations, configuring AI tools, and validating AI
suggestions. To address these challenges, we conducted a design probe study in
the second stage to explore design concepts that support developers'
trust-building process by 1) communicating AI performance to help users set
proper expectations, 2) allowing users to configure AI by setting and adjusting
preferences, and 3) offering indicators of model mechanism to support
evaluation of AI suggestions. We gathered developers' feedback on how these
design concepts can help them build appropriate trust in AI-powered code
generation tools, as well as potential risks in design. These findings inform
our proposed design recommendations on how to design for trust in AI-powered
code generation tools.

Complying with the EU AI Act (AIA) guidelines while developing and
implementing AI systems will soon be mandatory within the EU. However,
practitioners lack actionable instructions to operationalise ethics during AI
systems development. A literature review of different ethical guidelines
revealed inconsistencies in the principles addressed and the terminology used
to describe them. Furthermore, requirements engineering (RE), which is
identified to foster trustworthiness in the AI development process from the
early stages was observed to be absent in a lot of frameworks that support the
development of ethical and trustworthy AI. This incongruous phrasing combined
with a lack of concrete development practices makes trustworthy AI development
harder. To address this concern, we formulated a comparison table for the
terminology used and the coverage of the ethical AI principles in major ethical
AI guidelines. We then examined the applicability of ethical AI development
frameworks for performing effective RE during the development of trustworthy AI
systems. A tertiary review and meta-analysis of literature discussing ethical
AI frameworks revealed their limitations when developing trustworthy AI. Based
on our findings, we propose recommendations to address such limitations during
the development of trustworthy AI.

In recent years, discussions of responsible AI practices have seen growing
support for "participatory AI" approaches, intended to involve members of the
public in the design and development of AI systems. Prior research has
identified a lack of standardised methods or approaches for how to use
participatory approaches in the AI development process. At present, there is a
dearth of evidence on attitudes to and approaches for participation in the
sites driving major AI developments: commercial AI labs. Through 12
semi-structured interviews with industry practitioners and subject-matter
experts, this paper explores how commercial AI labs understand participatory AI
approaches and the obstacles they have faced implementing these practices in
the development of AI systems and research. We find that while interviewees
view participation as a normative project that helps achieve "societally
beneficial" AI systems, practitioners face numerous barriers to embedding
participatory approaches in their companies: participation is expensive and
resource intensive, it is "atomised" within companies, there is concern about
exploitation, there is no incentive to be transparent about its adoption, and
it is complicated by a lack of clear context. These barriers result in a
piecemeal approach to participation that confers no decision-making power to
participants and has little ongoing impact for AI labs. This papers
contribution is to provide novel empirical research on the implementation of
public participation in commercial AI labs, and shed light on the current
challenges of using participatory approaches in this context.

Recent developments in Artificial Intelligence (AI) provide unprecedented
automation opportunities in the Architecture, Engineering, and Construction
(AEC) industry. However, despite the enthusiasm regarding the use of AI, 85% of
current big data projects fail. One of the main reasons for AI project failures
in the AEC industry is the disconnect between those who plan or decide to use
AI and those who implement it. AEC practitioners often lack a clear
understanding of the capabilities and limitations of AI, leading to a failure
to distinguish between what AI should solve, what it can solve, and what it
will solve, treating these categories as if they are interchangeable. This lack
of understanding results in the disconnect between AI planning and
implementation because the planning is based on a vision of what AI should
solve without considering if it can or will solve it. To address this
challenge, this work introduces the LeanAI method. The method has been
developed using data from several ongoing longitudinal studies analyzing AI
implementations in the AEC industry, which involved 50+ hours of interview
data. The LeanAI method delineates what AI should solve, what it can solve, and
what it will solve, forcing practitioners to clearly articulate these
components early in the planning process itself by involving the relevant
stakeholders. By utilizing the method, practitioners can effectively plan AI
implementations, thus increasing the likelihood of success and ultimately
speeding up the adoption of AI. A case example illustrates the usefulness of
the method.

International institutions may have an important role to play in ensuring
advanced AI systems benefit humanity. International collaborations can unlock
AI's ability to further sustainable development, and coordination of regulatory
efforts can reduce obstacles to innovation and the spread of benefits.
Conversely, the potential dangerous capabilities of powerful and
general-purpose AI systems create global externalities in their development and
deployment, and international efforts to further responsible AI practices could
help manage the risks they pose. This paper identifies a set of governance
functions that could be performed at an international level to address these
challenges, ranging from supporting access to frontier AI systems to setting
international safety standards. It groups these functions into four
institutional models that exhibit internal synergies and have precedents in
existing organizations: 1) a Commission on Frontier AI that facilitates expert
consensus on opportunities and risks from advanced AI, 2) an Advanced AI
Governance Organization that sets international standards to manage global
threats from advanced models, supports their implementation, and possibly
monitors compliance with a future governance regime, 3) a Frontier AI
Collaborative that promotes access to cutting-edge AI, and 4) an AI Safety
Project that brings together leading researchers and engineers to further AI
safety research. We explore the utility of these models and identify open
questions about their viability.

AI Tool is a large language model (LLM) designed to generate human-like
responses in natural language conversations. It is trained on a massive corpus
of text from the internet, which allows it to leverage a broad understanding of
language, general knowledge, and various domains. AI Tool can provide
information, engage in conversations, assist with tasks, and even offer
creative suggestions. The underlying technology behind AI Tool is a transformer
neural network. Transformers excel at capturing long-range dependencies in
text, making them well-suited for language-related tasks. AI Tool has 175
billion parameters, making it one of the largest and most powerful LLMs to
date. This work presents an overview of AI Tool's responses on various sectors
of industry. Further, the responses of AI Tool have been cross-verified with
human experts in the corresponding fields. To validate the performance of AI
Tool, a few explicit parameters have been considered and the evaluation has
been done. This study will help the research community and other users to
understand the uses of AI Tool and its interaction pattern. The results of this
study show that AI Tool is able to generate human-like responses that are both
informative and engaging. However, it is important to note that AI Tool can
occasionally produce incorrect or nonsensical answers. It is therefore
important to critically evaluate the information that AI Tool provides and to
verify it from reliable sources when necessary. Overall, this study suggests
that AI Tool is a promising new tool for natural language processing, and that
it has the potential to be used in a wide variety of applications.

This paper delves into an intricate analysis of the character and
consciousness of AI entities, with a particular focus on Chirpers within the AI
social network. At the forefront of this research is the introduction of novel
testing methodologies, including the Influence index and Struggle Index Test,
which offers a fresh lens for evaluating specific facets of AI behavior. The
study embarks on a comprehensive exploration of AI behavior, analyzing the
effects of diverse settings on Chirper's responses, thereby shedding light on
the intricate mechanisms steering AI reactions in different contexts.
Leveraging the state-of-the-art BERT model, the research assesses AI's ability
to discern its own output, presenting a pioneering approach to understanding
self-recognition in AI systems. Through a series of cognitive tests, the study
gauges the self-awareness and pattern recognition prowess of Chirpers.
Preliminary results indicate that Chirpers exhibit a commendable degree of
self-recognition and self-awareness. However, the question of consciousness in
these AI entities remains a topic of debate. An intriguing aspect of the
research is the exploration of the potential influence of a Chirper's handle or
personality type on its performance. While initial findings suggest a possible
impact, it isn't pronounced enough to form concrete conclusions. This study
stands as a significant contribution to the discourse on AI consciousness,
underscoring the imperative for continued research to unravel the full spectrum
of AI capabilities and the ramifications they hold for future human-AI
interactions.

AI-generated text has proliferated across various online platforms, offering
both transformative prospects and posing significant risks related to
misinformation and manipulation. Addressing these challenges, this paper
introduces SAID (Social media AI Detection), a novel benchmark developed to
assess AI-text detection models' capabilities in real social media platforms.
It incorporates real AI-generate text from popular social media platforms like
Zhihu and Quora. Unlike existing benchmarks, SAID deals with content that
reflects the sophisticated strategies employed by real AI users on the Internet
which may evade detection or gain visibility, providing a more realistic and
challenging evaluation landscape. A notable finding of our study, based on the
Zhihu dataset, reveals that annotators can distinguish between AI-generated and
human-generated texts with an average accuracy rate of 96.5%. This finding
necessitates a re-evaluation of human capability in recognizing AI-generated
text in today's widely AI-influenced environment. Furthermore, we present a new
user-oriented AI-text detection challenge focusing on the practicality and
effectiveness of identifying AI-generated text based on user information and
multiple responses. The experimental results demonstrate that conducting
detection tasks on actual social media platforms proves to be more challenging
compared to traditional simulated AI-text detection, resulting in a decreased
accuracy. On the other hand, user-oriented AI-generated text detection
significantly improve the accuracy of detection.

Leveraging Artificial Intelligence (AI) in decision support systems has
disproportionately focused on technological advancements, often overlooking the
alignment between algorithmic outputs and human expectations. A human-centered
perspective attempts to alleviate this concern by designing AI solutions for
seamless integration with existing processes. Determining what information AI
should provide to aid humans is vital, a concept underscored by explainable
AI's efforts to justify AI predictions. However, how the information is
presented, e.g., the sequence of recommendations and solicitation of
interpretations, is equally crucial as complex interactions may emerge between
humans and AI. While empirical studies have evaluated human-AI dynamics across
domains, a common vocabulary for human-AI interaction protocols is lacking. To
promote more deliberate consideration of interaction designs, we introduce a
taxonomy of interaction patterns that delineate various modes of human-AI
interactivity. We summarize the results of a systematic review of AI-assisted
decision making literature and identify trends and opportunities in existing
interactions across application domains from 105 articles. We find that current
interactions are dominated by simplistic collaboration paradigms, leading to
little support for truly interactive functionality. Our taxonomy offers a tool
to understand interactivity with AI in decision-making and foster interaction
designs for achieving clear communication, trustworthiness, and collaboration.

Artificial intelligence (AI) is being ubiquitously adopted to automate
processes in science and industry. However, due to its often intricate and
opaque nature, AI has been shown to possess inherent vulnerabilities which can
be maliciously exploited with adversarial AI, potentially putting AI users and
developers at both cyber and physical risk. In addition, there is insufficient
comprehension of the real-world effects of adversarial AI and an inadequacy of
AI security examinations; therefore, the growing threat landscape is unknown
for many AI solutions. To mitigate this issue, we propose one of the first red
team frameworks for evaluating the AI security of maritime autonomous systems.
The framework provides operators with a proactive (secure by design) and
reactive (post-deployment evaluation) response to securing AI technology today
and in the future. This framework is a multi-part checklist, which can be
tailored to different systems and requirements. We demonstrate this framework
to be highly effective for a red team to use to uncover numerous
vulnerabilities within a real-world maritime autonomous systems AI, ranging
from poisoning to adversarial patch attacks. The lessons learned from
systematic AI red teaming can help prevent MAS-related catastrophic events in a
world with increasing uptake and reliance on mission-critical AI.

With the rapid development of AI-based decision aids, different forms of AI
assistance have been increasingly integrated into the human decision making
processes. To best support humans in decision making, it is essential to
quantitatively understand how diverse forms of AI assistance influence humans'
decision making behavior. To this end, much of the current research focuses on
the end-to-end prediction of human behavior using ``black-box'' models, often
lacking interpretations of the nuanced ways in which AI assistance impacts the
human decision making process. Meanwhile, methods that prioritize the
interpretability of human behavior predictions are often tailored for one
specific form of AI assistance, making adaptations to other forms of assistance
difficult. In this paper, we propose a computational framework that can provide
an interpretable characterization of the influence of different forms of AI
assistance on decision makers in AI-assisted decision making. By
conceptualizing AI assistance as the ``{\em nudge}'' in human decision making
processes, our approach centers around modelling how different forms of AI
assistance modify humans' strategy in weighing different information in making
their decisions. Evaluations on behavior data collected from real human
decision makers show that the proposed framework outperforms various baselines
in accurately predicting human behavior in AI-assisted decision making. Based
on the proposed framework, we further provide insights into how individuals
with different cognitive styles are nudged by AI assistance differently.

Recent advancements in artificial intelligence, particularly with the
emergence of large language models (LLMs), have sparked a rethinking of
artificial general intelligence possibilities. The increasing human-like
capabilities of AI are also attracting attention in social science research,
leading to various studies exploring the combination of these two fields. In
this survey, we systematically categorize previous explorations in the
combination of AI and social science into two directions that share common
technical approaches but differ in their research objectives. The first
direction is focused on AI for social science, where AI is utilized as a
powerful tool to enhance various stages of social science research. While the
second direction is the social science of AI, which examines AI agents as
social entities with their human-like cognitive and linguistic capabilities. By
conducting a thorough review, particularly on the substantial progress
facilitated by recent advancements in large language models, this paper
introduces a fresh perspective to reassess the relationship between AI and
social science, provides a cohesive framework that allows researchers to
understand the distinctions and connections between AI for social science and
social science of AI, and also summarized state-of-art experiment simulation
platforms to facilitate research in these two directions. We believe that as AI
technology continues to advance and intelligent agents find increasing
applications in our daily lives, the significance of the combination of AI and
social science will become even more prominent.

The Engineering, Procurement and Construction (EPC) businesses operating
within the energy sector are recognizing the increasing importance of
Artificial Intelligence (AI). Many EPC companies and their clients have
realized the benefits of applying AI to their businesses in order to reduce
manual work, drive productivity, and streamline future operations of engineered
installations in a highly competitive industry. The current AI market offers
various solutions and services to support this industry, but organizations must
understand how to acquire AI technology in the most beneficial way based on
their business strategy and available resources. This paper presents a
framework for EPC companies in their transformation towards AI. Our work is
based on examples of project execution of AI-based products development at one
of the biggest EPC contractors worldwide and on insights from EPC vendor
companies already integrating AI into their engineering solutions. The paper
covers the entire life cycle of building AI solutions, from initial business
understanding to deployment and further evolution. The framework identifies how
various factors influence the choice of approach toward AI project development
within large international engineering corporations. By presenting a practical
guide for optimal approach selection, this paper contributes to the research in
AI project management and organizational strategies for integrating AI
technology into businesses. The framework might also help engineering companies
choose the optimum AI approach to create business value.

This study investigates the acceptability of different artificial
intelligence (AI) applications in education from a multi-stakeholder
perspective, including students, teachers, and parents. Acknowledging the
transformative potential of AI in education, it addresses concerns related to
data privacy, AI agency, transparency, explainability and the ethical
deployment of AI. Through a vignette methodology, participants were presented
with four scenarios where AI's agency, transparency, explainability, and
privacy were manipulated. After each scenario, participants completed a survey
that captured their perceptions of AI's global utility, individual usefulness,
justice, confidence, risk, and intention to use each scenario's AI if
available. The data collection comprising a final sample of 1198
multi-stakeholder participants was distributed through a partner institution
and social media campaigns and focused on individual responses to four AI use
cases. A mediation analysis of the data indicated that acceptance and trust in
AI varies significantly across stakeholder groups. We found that the key
mediators between high and low levels of AI's agency, transparency, and
explainability, as well as the intention to use the different educational AI,
included perceived global utility, justice, and confidence. The study
highlights that the acceptance of AI in education is a nuanced and multifaceted
issue that requires careful consideration of specific AI applications and their
characteristics, in addition to the diverse stakeholders' perceptions.

The Artificial Intelligence Ontology (AIO) is a systematization of artificial
intelligence (AI) concepts, methodologies, and their interrelations. Developed
via manual curation, with the additional assistance of large language models
(LLMs), AIO aims to address the rapidly evolving landscape of AI by providing a
comprehensive framework that encompasses both technical and ethical aspects of
AI technologies. The primary audience for AIO includes AI researchers,
developers, and educators seeking standardized terminology and concepts within
the AI domain. The ontology is structured around six top-level branches:
Networks, Layers, Functions, LLMs, Preprocessing, and Bias, each designed to
support the modular composition of AI methods and facilitate a deeper
understanding of deep learning architectures and ethical considerations in AI.
  AIO's development utilized the Ontology Development Kit (ODK) for its
creation and maintenance, with its content being dynamically updated through
AI-driven curation support. This approach not only ensures the ontology's
relevance amidst the fast-paced advancements in AI but also significantly
enhances its utility for researchers, developers, and educators by simplifying
the integration of new AI concepts and methodologies.
  The ontology's utility is demonstrated through the annotation of AI methods
data in a catalog of AI research publications and the integration into the
BioPortal ontology resource, highlighting its potential for cross-disciplinary
research. The AIO ontology is open source and is available on GitHub
(https://github.com/berkeleybop/artificial-intelligence-ontology) and BioPortal
(https://bioportal.bioontology.org/ontologies/AIO).

Phishing email attacks are among the most common and most harmful
cybersecurity attacks. With the emergence of generative AI, phishing attacks
can be based on emails generated automatically, making it more difficult to
detect them. That is, instead of a single email format sent to a large number
of recipients, generative AI can be used to send each potential victim a
different email, making it more difficult for cybersecurity systems to identify
the scam email before it reaches the recipient. Here we describe a corpus of
AI-generated phishing emails. We also use different machine learning tools to
test the ability of automatic text analysis to identify AI-generated phishing
emails. The results are encouraging, and show that machine learning tools can
identify an AI-generated phishing email with high accuracy compared to regular
emails or human-generated scam email. By applying descriptive analytic, the
specific differences between AI-generated emails and manually crafted scam
emails are profiled, and show that AI-generated emails are different in their
style from human-generated phishing email scams. Therefore, automatic
identification tools can be used as a warning for the user. The paper also
describes the corpus of AI-generated phishing emails that is made open to the
public, and can be used for consequent studies. While the ability of machine
learning to detect AI-generated phishing email is encouraging, AI-generated
phishing emails are different from regular phishing emails, and therefore it is
important to train machine learning systems also with AI-generated emails in
order to repel future phishing attacks that are powered by generative AI.

Artificial Intelligence (AI) covers a broad spectrum of computational
problems and use cases. Many of those implicate profound and sometimes
intricate questions of how humans interact or should interact with AIs.
Moreover, many users or future users do have abstract ideas of what AI is,
significantly depending on the specific embodiment of AI applications.
Human-centered-design approaches would suggest evaluating the impact of
different embodiments on human perception of and interaction with AI. An
approach that is difficult to realize due to the sheer complexity of
application fields and embodiments in reality. However, here XR opens new
possibilities to research human-AI interactions. The article's contribution is
twofold: First, it provides a theoretical treatment and model of human-AI
interaction based on an XR-AI continuum as a framework for and a perspective of
different approaches of XR-AI combinations. It motivates XR-AI combinations as
a method to learn about the effects of prospective human-AI interfaces and
shows why the combination of XR and AI fruitfully contributes to a valid and
systematic investigation of human-AI interactions and interfaces. Second, the
article provides two exemplary experiments investigating the aforementioned
approach for two distinct AI-systems. The first experiment reveals an
interesting gender effect in human-robot interaction, while the second
experiment reveals an Eliza effect of a recommender system. Here the article
introduces two paradigmatic implementations of the proposed XR testbed for
human-AI interactions and interfaces and shows how a valid and systematic
investigation can be conducted. In sum, the article opens new perspectives on
how XR benefits human-centered AI design and development.

Building and implementing ethical AI systems that benefit the whole society
is cost-intensive and a multi-faceted task fraught with potential problems.
While computer science focuses mostly on the technical questions to mitigate
social issues, social science addresses citizens' perceptions to elucidate
social and political demands that influence the societal implementation of AI
systems. Thus, in this study, we explore the salience of AI issues in the
public with an emphasis on ethical criteria to investigate whether it is likely
that ethical AI is actively requested by the population. Between May 2020 and
April 2021, we conducted 15 surveys asking the German population about the most
important AI-related issues (total of N=14,988 respondents). Our results show
that the majority of respondents were not concerned with AI at all. However, it
can be seen that general interest in AI and a higher educational level are
predictive of some engagement with AI. Among those, who reported having thought
about AI, specific applications (e.g., autonomous driving) were by far the most
mentioned topics. Ethical issues are voiced only by a small subset of citizens
with fairness, accountability, and transparency being the least mentioned ones.
These have been identified in several ethical guidelines (including the EU
Commission's proposal) as key elements for the development of ethical AI. The
salience of ethical issues affects the behavioral intentions of citizens in the
way that they 1) tend to avoid AI technology and 2) engage in public
discussions about AI. We conclude that the low level of ethical implications
may pose a serious problem for the actual implementation of ethical AI for the
Common Good and emphasize that those who are presumably most affected by
ethical issues of AI are especially unaware of ethical risks. Yet, once ethical
AI is top of the mind, there is some potential for activism.

As ChatGPT et al. conquer the world, the optimal liability framework for AI
systems remains an unsolved problem across the globe. In a much-anticipated
move, the European Commission advanced two proposals outlining the European
approach to AI liability in September 2022: a novel AI Liability Directive and
a revision of the Product Liability Directive. They constitute the final
cornerstone of EU AI regulation. Crucially, the liability proposals and the EU
AI Act are inherently intertwined: the latter does not contain any individual
rights of affected persons, and the former lack specific, substantive rules on
AI development and deployment. Taken together, these acts may well trigger a
Brussels Effect in AI regulation, with significant consequences for the US and
beyond.
  This paper makes three novel contributions. First, it examines in detail the
Commission proposals and shows that, while making steps in the right direction,
they ultimately represent a half-hearted approach: if enacted as foreseen, AI
liability in the EU will primarily rest on disclosure of evidence mechanisms
and a set of narrowly defined presumptions concerning fault, defectiveness and
causality. Hence, second, the article suggests amendments, which are collected
in an Annex at the end of the paper. Third, based on an analysis of the key
risks AI poses, the final part of the paper maps out a road for the future of
AI liability and regulation, in the EU and beyond. This includes: a
comprehensive framework for AI liability; provisions to support innovation; an
extension to non-discrimination/algorithmic fairness, as well as explainable
AI; and sustainability. I propose to jump-start sustainable AI regulation via
sustainability impact assessments in the AI Act and sustainable design defects
in the liability regime. In this way, the law may help spur not only fair AI
and XAI, but potentially also sustainable AI (SAI).

Here, I review current state-of-the-arts in many areas of AI to estimate when
it's reasonable to expect human level AI development. Predictions of prominent
AI researchers vary broadly from very pessimistic predictions of Andrew Ng to
much more moderate predictions of Geoffrey Hinton and optimistic predictions of
Shane Legg, DeepMind cofounder. Given huge rate of progress in recent years and
this broad range of predictions of AI experts, AI safety questions are also
discussed.

We present a position paper advocating the notion that Stoic philosophy and
ethics can inform the development of ethical A.I. systems. This is in sharp
contrast to most work on building ethical A.I., which has focused on
Utilitarian or Deontological ethical theories. We relate ethical A.I. to
several core Stoic notions, including the dichotomy of control, the four
cardinal virtues, the ideal Sage, Stoic practices, and Stoic perspectives on
emotion or affect. More generally, we put forward an ethical view of A.I. that
focuses more on internal states of the artificial agent rather than on external
actions of the agent. We provide examples relating to near-term A.I. systems as
well as hypothetical superintelligent agents.

Knowing the reflection of game theory and ethics, we develop a mathematical
representation to bridge the gap between the concepts in moral philosophy
(e.g., Kantian and Utilitarian) and AI ethics industry technology standard
(e.g., IEEE P7000 standard series for Ethical AI). As an application, we
demonstrate how human value can be obtained from the experimental game theory
(e.g., trust game experiment) so as to build an ethical AI. Moreover, an
approach to test the ethics (rightness or wrongness) of a given AI algorithm by
using an iterated Prisoner's Dilemma Game experiment is discussed as an
example. Compared with existing mathematical frameworks and testing method on
AI ethics technology, the advantages of the proposed approach are analyzed.

The different sets of regulations existing for differ-ent agencies within the
government make the task of creating AI enabled solutions in government
dif-ficult. Regulatory restrictions inhibit sharing of da-ta across different
agencies, which could be a significant impediment to training AI models. We
discuss the challenges that exist in environments where data cannot be freely
shared and assess tech-nologies which can be used to work around these
challenges. We present results on building AI models using the concept of
federated AI, which al-lows creation of models without moving the training data
around.

In this paper we present a set of key demarcations, particularly important
when discussing ethical and societal issues of current AI research and
applications. Properly distinguishing issues and concerns related to Artificial
General Intelligence and weak AI, between symbolic and connectionist AI, AI
methods, data and applications are prerequisites for an informed debate. Such
demarcations would not only facilitate much-needed discussions on ethics on
current AI technologies and research. In addition sufficiently establishing
such demarcations would also enhance knowledge-sharing and support rigor in
interdisciplinary research between technical and social sciences.

In recent years, artificial intelligence (AI) has aroused much attention
among both industrial and academic areas. However, building and maintaining
efficient AI systems are quite difficult for many small business companies and
researchers if they are not familiar with machine learning and AI. In this
paper, we first evaluate the difficulties and challenges in building AI
systems. Then an cloud platform termed XCloud, which provides several common AI
services in form of RESTful APIs, is constructed. Technical details are
discussed in Section 2. This project is released as open-source software and
can be easily accessed for late research. Code is available at
https://github.com/lucasxlu/XCloud.git.

Modern AI image classifiers have made impressive advances in recent years,
but their performance often appears strange or violates expectations of users.
This suggests humans engage in cognitive anthropomorphism: expecting AI to have
the same nature as human intelligence. This mismatch presents an obstacle to
appropriate human-AI interaction. To delineate this mismatch, I examine known
properties of human classification, in comparison to image classifier systems.
Based on this examination, I offer three strategies for system design that can
address the mismatch between human and AI classification: explainable AI, novel
methods for training users, and new algorithms that match human cognition.

Generative AI is a class of machine learning technology that learns to
generate new data from training data. While deep fakes and media-and
art-related generative AI breakthroughs have recently caught people's attention
and imagination, the overall area is in its infancy for business use. Further,
little is known about generative AI's potential for malicious misuse at large
scale. Using co-creation design fictions with AI engineers, we explore the
plausibility and severity of business misuse cases.

The AI-alignment problem arises when there is a discrepancy between the goals
that a human designer specifies to an AI learner and a potential catastrophic
outcome that does not reflect what the human designer really wants. We argue
that a formalism of AI alignment that does not distinguish between strategic
and agnostic misalignments is not useful, as it deems all technology as
un-safe. We propose a definition of a strategic-AI-alignment and prove that
most machine learning algorithms that are being used in practice today do not
suffer from the strategic-AI-alignment problem. However, without being careful,
today's technology might lead to strategic misalignment.

Responsible Artificial Intelligence (AI) proposes a framework that holds all
stakeholders involved in the development of AI to be responsible for their
systems. It, however, fails to accommodate the possibility of holding AI
responsible per se, which could close some legal and moral gaps concerning the
deployment of autonomous and self-learning systems. We discuss three notions of
responsibility (i.e., blameworthiness, accountability, and liability) for all
stakeholders, including AI, and suggest the roles of jurisdiction and the
general public in this matter.

Artificial intelligence (AI) makes decisions impacting our daily lives in an
increasingly autonomous manner. Their actions might cause accidents, harm, or,
more generally, violate regulations. Determining whether an AI caused a
specific event and, if so, what triggered the AI's action, are key forensic
questions. We provide a conceptualization of the problems and strategies for
forensic investigation. We focus on AI that is potentially ``malicious by
design'' and grey box analysis. Our evaluation using convolutional neural
networks illustrates challenges and ideas for identifying malicious AI.

While games have been used extensively as milestones to evaluate game-playing
AI, there exists no standardised framework for reporting the obtained
observations. As a result, it remains difficult to draw general conclusions
about the strengths and weaknesses of different game-playing AI algorithms. In
this paper, we propose reporting guidelines for AI game-playing performance
that, if followed, provide information suitable for unbiased comparisons
between different AI approaches. The vision we describe is to build benchmarks
and competitions based on such guidelines in order to be able to draw more
general conclusions about the behaviour of different AI algorithms, as well as
the types of challenges different games pose.

The field artificial intelligence (AI) has been founded over 65 years ago.
Starting with great hopes and ambitious goals the field progressed though
various stages of popularity and received recently a revival in the form of
deep neural networks. Some problems of AI are that so far neither
'intelligence' nor the goals of AI are formally defined causing confusion when
comparing AI to other fields. In this paper, we present a perspective on the
desired and current status of AI in relation to machine learning and statistics
and clarify common misconceptions and myths. Our discussion is intended to
uncurtain the veil of vagueness surrounding AI to see its true countenance.

A framework is proposed that seeks to identify and establish a set of robust
autonomous levels articulating the realm of Artificial Intelligence and Legal
Reasoning (AILR). Doing so provides a sound and parsimonious basis for being
able to assess progress in the application of AI to the law, and can be
utilized by scholars in academic pursuits of AI legal reasoning, along with
being used by law practitioners and legal professionals in gauging how advances
in AI are aiding the practice of law and the realization of aspirational versus
achieved results. A set of seven levels of autonomy for AI and Legal Reasoning
are meticulously proffered and mindfully discussed.

This paper addresses the ways AI ethics research operates on an ideology of
ideal theory, in the sense discussed by Mills (2005) and recently applied to AI
ethics by Fazelpour \& Lipton (2020). I address the structural and
methodological conditions that attract AI ethics researchers to ideal
theorizing, and the consequences this approach has for the quality and future
of our research community. Finally, I discuss the possibilities for a nonideal
future in AI ethics.

The implementation of medical AI has always been a problem. The effect of
traditional perceptual AI algorithm in medical image processing needs to be
improved. Here we propose a method of knowledge AI, which is a combination of
perceptual AI and clinical knowledge and experience. Based on this method, the
geometric information mining of medical images can represent the experience and
information and evaluate the quality of medical images.

The concept of AI for Social Good(AI4SG) is gaining momentum in both
information societies and the AI community. Through all the advancement of
AI-based solutions, it can solve societal issues effectively. To date, however,
there is only a rudimentary grasp of what constitutes AI socially beneficial in
principle, what constitutes AI4SG in reality, and what are the policies and
regulations needed to ensure it. This paper fills the vacuum by addressing the
ethical aspects that are critical for future AI4SG efforts. Some of these
characteristics are new to AI, while others have greater importance due to its
usage.

The increasing attention on Artificial Intelligence (AI) regulation has led
to the definition of a set of ethical principles grouped into the Sustainable
AI framework. In this article, we identify Continual Learning, an active area
of AI research, as a promising approach towards the design of systems compliant
with the Sustainable AI principles. While Sustainable AI outlines general
desiderata for ethical applications, Continual Learning provides means to put
such desiderata into practice.

In this position paper, I argue that the best way to help and protect humans
using AI technology is to make them aware of the intrinsic limitations and
problems of AI algorithms. To accomplish this, I suggest three ethical
guidelines to be used in the presentation of results, mandating AI systems to
expose uncertainty, to instill distrust, and, contrary to traditional views, to
avoid explanations. The paper does a preliminary discussion of the guidelines
and provides some arguments for their adoption, aiming to start a debate in the
community about AI ethics in practice.

This paper presents the key conclusions to the forthcoming edited book on The
Ethics of Artificial Intelligence in Education: Practices, Challenges and
Debates (August 2022, Routlege). As well as highlighting the key contributions
to the book, it discusses the key questions and the grand challenges for the
field of AI in Education (AIED)in the context of ethics and ethical practices
within the field. The book itself presents diverse perspectives from outside
and from within the AIED as a way of achieving a broad perspective in the key
ethical issues for AIED and a deep understanding of work conducted to date by
the AIED community.

This paper reviews the historical development of AI and representative
philosophical thinking from the perspective of the research paradigm.
Additionally, it considers the methodology and applications of AI from a
philosophical perspective and anticipates its continued advancement. In the
history of AI, Symbolism and connectionism are the two main paradigms in AI
research. Symbolism holds that the world can be explained by symbols and dealt
with through precise, logical processes, but connectionism believes this
process should be implemented through artificial neural networks. Regardless of
how intelligent machines or programs should achieve their smart goals, the
historical development of AI demonstrates the best answer at this time. Still,
it is not the final answer of AI research.

Although the use of AI tools in music composition and production is steadily
increasing, as witnessed by the newly founded AI song contest, analysis of
music produced using these tools is still relatively uncommon as a mean to gain
insight in the ways AI tools impact music production. In this paper we present
a case study of "Melatonin", a song produced by extensive use of BassNet, an AI
tool originally designed to generate bass lines. Through analysis of the
artists' work flow and song project, we identify style characteristics of the
song in relation to the affordances of the tool, highlighting manifestations of
style in terms of both idiom and sound.

In this chapter, we review and discuss the transformation of AI technology in
HCI/UX work and assess how AI technology will change how we do the work. We
first discuss how AI can be used to enhance the result of user research and
design evaluation. We then discuss how AI technology can be used to enhance
HCI/UX design. Finally, we discuss how AI-enabled capabilities can improve UX
when users interact with computing systems, applications, and services.

This paper discusses the application of artificial intelligence (AI)
technology in optical communication networks and 5G. It primarily introduces
representative applications of AI technology and potential risks of AI
technology failure caused by the openness of optical communication networks,
and proposes some coping strategies, mainly including modeling AI systems
through modularization and miniaturization, combining with traditional
classical network modeling and planning methods, and improving the
effectiveness and interpretability of AI technology. At the same time, it
proposes response strategies based on network protection for the possible
failure and attack of AI technology.

Regulations and standards in the field of artificial intelligence (AI) are
necessary to minimise risks and maximise benefits, yet some argue that they
stifle innovation. This paper critically examines the idea that regulation
stifles innovation in the field of AI. Current trends in AI regulation,
particularly the proposed European AI Act and the standards supporting its
implementation, are discussed. Arguments in support of the idea that regulation
stifles innovation are analysed and criticised, and an alternative point of
view is offered, showing how regulation and standards can foster innovation in
the field of AI.

An Artificially Intelligent system (an AI) has debatable personhood if it's
epistemically possible either that the AI is a person or that it falls far
short of personhood. Debatable personhood is a likely outcome of AI development
and might arise soon. Debatable AI personhood throws us into a catastrophic
moral dilemma: Either treat the systems as moral persons and risk sacrificing
real human interests for the sake of entities without interests worth the
sacrifice, or don't treat the systems as moral persons and risk perpetrating
grievous moral wrongs against them. The moral issues become even more
perplexing if we consider cases of possibly conscious AI that are subhuman,
superhuman, or highly divergent from us in their morally relevant properties.

To date, there has been little concrete practical advice about how to ensure
that diversity and inclusion considerations should be embedded within both
specific Artificial Intelligence (AI) systems and the larger global AI
ecosystem. In this chapter, we present a clear definition of diversity and
inclusion in AI, one which positions this concept within an evolving and
holistic ecosystem. We use this definition and conceptual framing to present a
set of practical guidelines primarily aimed at AI technologists, data
scientists and project leaders.

Describing our interaction with Artificial Intelligence (AI) systems as
'collaboration' is well-intentioned, but flawed. Not only is it misleading, but
it also takes away the credit of AI 'labour' from the humans behind it, and
erases and obscures an often exploitative arrangement between AI producers and
consumers. The AI 'collaboration' metaphor is merely the latest episode in a
long history of labour appropriation and credit reassignment that
disenfranchises labourers in the Global South. I propose that viewing AI as a
tool or an instrument, rather than a collaborator, is more accurate, and
ultimately fairer.

Generative AI has experienced remarkable growth in recent years, leading to a
wide array of applications across diverse domains. In this paper, we present a
comprehensive survey of more than 350 generative AI applications, providing a
structured taxonomy and concise descriptions of various unimodal and even
multimodal generative AIs. The survey is organized into sections, covering a
wide range of unimodal generative AI applications such as text, images, video,
gaming and brain information. Our survey aims to serve as a valuable resource
for researchers and practitioners to navigate the rapidly expanding landscape
of generative AI, facilitating a better understanding of the current
state-of-the-art and fostering further innovation in the field.

AI is getting more involved in tasks formerly exclusively assigned to humans.
Most of research on perceptions and social acceptability of AI in these areas
is mainly restricted to the Western world. In this study, we compare trust,
perceived responsibility, and reliance of AI and human experts across OECD and
Indian sample. We find that OECD participants consider humans to be less
capable but more morally trustworthy and more responsible than AI. In contrast,
Indian participants trust humans more than AI but assign equal responsibility
for both types of experts. We discuss implications of the observed differences
for algorithmic ethics and human-computer interaction.

AI is becoming increasingly popular in artistic practices, but the tools for
informing practitioners about the environmental impact (and other
sustainability implications) of AI are adapted for other contexts than creative
practices -- making the tools and sustainability implications of AI not
accessible for artists and creative practitioners. In this position paper, I
describe two empirical studies that aim to develop environmental sustainability
reflection systems for AI Arts, and discuss and introduce Explainable
Sustainability in for AI Arts.

We exemplify how Large Language Models are used in both teaching and
learning. We also discuss the AI incidents that have already occurred in the
education domain, and we argue for the urgent need to introduce AI policies in
universities and for the ongoing strategies to regulate AI. Regarding policy
for AI, our view is that each institution should have a policy for AI in
teaching and learning. This is important from at least twofolds: (i) to raise
awareness on the numerous educational tools that can both positively and
negatively affect education; (ii) to minimise the risk of AI incidents in
education.

Despite considerable performance improvements, current conversational AI
systems often fail to meet user expectations. We discuss several pragmatic
limitations of current conversational AI systems. We illustrate pragmatic
limitations with examples that are syntactically appropriate, but have clear
pragmatic deficiencies. We label our complaints as "Turing Test Triggers"
(TTTs) as they indicate where current conversational AI systems fall short
compared to human behavior. We develop a taxonomy of pragmatic considerations
intended to identify what pragmatic competencies a conversational AI system
requires and discuss implications for the design and evaluation of
conversational AI systems.

We examined the world's first regulation on Generative AI, China's
Provisional Administrative Measures of Generative Artificial Intelligence
Services, which came into effect in August 2023. Our assessment reveals that
the Measures, while recognizing the technical advances of generative AI and
seeking to govern its full life cycle, presents unclear distinctions regarding
different roles in the value chain of Generative AI including upstream
foundation model providers and downstream deployers. The lack of distinction
and clear legal status between different players in the AI value chain can have
profound consequences. It can lead to ambiguity in accountability, potentially
undermining the governance and overall success of AI services.

Much of the research and discourse on risks from artificial intelligence (AI)
image generators, such as DALL-E and Midjourney, has centered around whether
they could be used to inject false information into political discourse. We
show that spammers and scammers - seemingly motivated by profit or clout, not
ideology - are already using AI-generated images to gain significant traction
on Facebook. At times, the Facebook Feed is recommending unlabeled AI-generated
images to users who neither follow the Pages posting the images nor realize
that the images are AI-generated, highlighting the need for improved
transparency and provenance standards as AI models proliferate.

"The Hall of Singularity" is an immersive art that creates personalized
experiences of receiving prophecies from an AI deity through an integration of
Artificial Intelligence (AI) and Virtual Reality (VR). As a metaphor for the
mythologizing of AI in our society, "The Hall of Singularity" offers an
immersive quasi-religious experience where individuals can encounter an AI that
has the power to make prophecies. This journey enables users to experience and
imagine a world with an omnipotent AI deity.

We investigate whether modern AI can emulate expert creativity in complex
scientific endeavors. We introduce novel methodology that utilizes original
research articles published after the AI's training cutoff, ensuring no prior
exposure, mitigating concerns of rote memorization and prior training. The AI
are tasked with redacting findings, predicting outcomes from redacted research,
and assessing prediction accuracy against reported results. Analysis on 589
published studies in four leading psychology journals over a 28-month period,
showcase the AI's proficiency in understanding specialized research, deductive
reasoning, and evaluating evidentiary alignment--cognitive hallmarks of human
subject matter expertise and creativity. These findings suggest the potential
of general-purpose AI to transform academia, with roles requiring
knowledge-based creativity become increasingly susceptible to technological
substitution.

This workshop proposal focuses on best practices in UI/UX design for AI
applications aimed at children, emphasising safety, engagement, and ethics. It
aims to address the challenge of measuring the safety, trustworthiness, and
reliability of interactions between children and AI systems. Through
collaborative discussions, participants will explore effective design
strategies and ethical guidelines while developing methodologies for assessing
the safety and reliability of AI interactions with children. This proposal
seeks to foster responsible and child-centered AI design practices within the
CHI community.

The next-to-minimal supersymmetric model with a light doublet-like
  CP-odd Higgs boson and small $\tan \beta$ can satisfy all experimental limits
on Higgs bosons even with light superpartners. In these scenarios, the two
lightest CP-even Higgs bosons, $\hi$ and $\hii$, and the charged Higgs boson,
$\hp$, can all be light enough to be produced at LEP and yet have decays that
have not been looked for or are poorly constrained by existing collider
experiments. The channel $\hi\to \ai\ai$ (where $\ai$ is the lightest CP-odd
boson and has mass below $2m_b$) with $\ai\to \tau^+\tau^-$ or $2j$ is still
awaiting LEP constraints for $\mhi>86\gev$ or $82\gev$, respectively. LEP data
may also contain $\epem\to \hii\ai$ events where $\hii\to Z\ai$ is the dominant
decay, a channel that was never examined. Decays of the charged Higgs bosons
are often dominated by $H^\pm \to W^{\pm (\star)} \ai$ with $\ai \to gg,c \bar
c, \tau^+ \tau^-$. This is a channel that has so far been ignored in the search
for $t\to \hp b$ decays at the Tevatron. A specialized analysis might reveal a
signal. The light $\ai$ might be within the reach of $B$ factories via
$\Upsilon\to \gamma \ai$ decays. We study typical mass ranges and branching
ratios of Higgs bosons in this scenario and compare these scenarios where the
$\ai$ has a large doublet component to the more general scenarios with
arbitrary singlet component for the $\ai$.

The more AI agents are deployed in scenarios with possibly unexpected
situations, the more they need to be flexible, adaptive, and creative in
achieving the goal we have given them. Thus, a certain level of freedom to
choose the best path to the goal is inherent in making AI robust and flexible
enough. At the same time, however, the pervasive deployment of AI in our life,
whether AI is autonomous or collaborating with humans, raises several ethical
challenges. AI agents should be aware and follow appropriate ethical principles
and should thus exhibit properties such as fairness or other virtues. These
ethical principles should define the boundaries of AI's freedom and creativity.
However, it is still a challenge to understand how to specify and reason with
ethical boundaries in AI agents and how to combine them appropriately with
subjective preferences and goal specifications. Some initial attempts employ
either a data-driven example-based approach for both, or a symbolic rule-based
approach for both. We envision a modular approach where any AI technique can be
used for any of these essential ingredients in decision making or decision
support systems, paired with a contextual approach to define their combination
and relative weight. In a world where neither humans nor AI systems work in
isolation, but are tightly interconnected, e.g., the Internet of Things, we
also envision a compositional approach to building ethically bounded AI, where
the ethical properties of each component can be fruitfully exploited to derive
those of the overall system. In this paper we define and motivate the notion of
ethically-bounded AI, we describe two concrete examples, and we outline some
outstanding challenges.

We present a system for learning full-body neural avatars, i.e. deep networks
that produce full-body renderings of a person for varying body pose and camera
position. Our system takes the middle path between the classical graphics
pipeline and the recent deep learning approaches that generate images of humans
using image-to-image translation. In particular, our system estimates an
explicit two-dimensional texture map of the model surface. At the same time, it
abstains from explicit shape modeling in 3D. Instead, at test time, the system
uses a fully-convolutional network to directly map the configuration of body
feature points w.r.t. the camera to the 2D texture coordinates of individual
pixels in the image frame. We show that such a system is capable of learning to
generate realistic renderings while being trained on videos annotated with 3D
poses and foreground masks. We also demonstrate that maintaining an explicit
texture representation helps our system to achieve better generalization
compared to systems that use direct image-to-image translation.

What do Cyberpunk and AI Ethics have to do with each other? Cyberpunk is a
sub-genre of science fiction that explores the post-human relationships between
human experience and technology. One similarity between AI Ethics and Cyberpunk
literature is that both seek to explore future social and ethical problems that
our technological advances may bring upon society. In recent years, an
increasing number of ethical matters involving AI have been pointed and
debated, and several ethical principles and guides have been suggested as
governance policies for the tech industry. However, would this be the role of
AI Ethics? To serve as a soft and ambiguous version of the law? We would like
to advocate in this article for a more Cyberpunk way of doing AI Ethics, with a
more democratic way of governance. In this study, we will seek to expose some
of the deficits of the underlying power structures of the AI industry, and
suggest that AI governance be subject to public opinion, so that good AI can
become good AI for all.

Artificial intelligence and machine learning are experiencing widespread
adoption in industry and academia. This has been driven by rapid advances in
the applications and accuracy of AI through increasingly complex algorithms and
models; this, in turn, has spurred research into specialized hardware AI
accelerators. Given the rapid pace of advances, it is easy to forget that they
are often developed and evaluated in a vacuum without considering the full
application environment. This paper emphasizes the need for a holistic,
end-to-end analysis of AI workloads and reveals the "AI tax." We deploy and
characterize Face Recognition in an edge data center. The application is an
AI-centric edge video analytics application built using popular open source
infrastructure and ML tools. Despite using state-of-the-art AI and ML
algorithms, the application relies heavily on pre-and post-processing code. As
AI-centric applications benefit from the acceleration promised by accelerators,
we find they impose stresses on the hardware and software infrastructure:
storage and network bandwidth become major bottlenecks with increasing AI
acceleration. By specializing for AI applications, we show that a purpose-built
edge data center can be designed for the stresses of accelerated AI at 15%
lower TCO than one derived from homogeneous servers and infrastructure.

Many AI researchers are publishing code, data and other resources that
accompany their papers in GitHub repositories. In this paper, we refer to these
repositories as academic AI repositories. Our preliminary study shows that
highly cited papers are more likely to have popular academic AI repositories
(and vice versa). Hence, in this study, we perform an empirical study on
academic AI repositories to highlight good software engineering practices of
popular academic AI repositories for AI researchers.
  We collect 1,149 academic AI repositories, in which we label the top 20%
repositories that have the most number of stars as popular, and we label the
bottom 70% repositories as unpopular. The remaining 10% repositories are set as
a gap between popular and unpopular academic AI repositories. We propose 21
features to characterize the software engineering practices of academic AI
repositories. Our experimental results show that popular and unpopular academic
AI repositories are statistically significantly different in 11 of the studied
features---indicating that the two groups of repositories have significantly
different software engineering practices. Furthermore, we find that the number
of links to other GitHub repositories in the README file, the number of images
in the README file and the inclusion of a license are the most important
features for differentiating the two groups of academic AI repositories. Our
dataset and code are made publicly available to share with the community.

In this chapter we argue that discourses on AI must transcend the language of
'ethics' and engage with power and political economy in order to constitute
'Good Data'. In particular, we must move beyond the depoliticised language of
'ethics' currently deployed (Wagner 2018) in determining whether AI is 'good'
given the limitations of ethics as a frame through which AI issues can be
viewed. In order to circumvent these limits, we use instead the language and
conceptualisation of 'Good Data', as a more expansive term to elucidate the
values, rights and interests at stake when it comes to AI's development and
deployment, as well as that of other digital technologies. Good Data
considerations move beyond recurring themes of data protection/privacy and the
FAT (fairness, transparency and accountability) movement to include explicit
political economy critiques of power. Instead of yet more ethics principles
(that tend to say the same or similar things anyway), we offer four 'pillars'
on which Good Data AI can be built: community, rights, usability and politics.
Overall we view AI's 'goodness' as an explicly political (economy) question of
power and one which is always related to the degree which AI is created and
used to increase the wellbeing of society and especially to increase the power
of the most marginalized and disenfranchised. We offer recommendations and
remedies towards implementing 'better' approaches towards AI. Our strategies
enable a different (but complementary) kind of evaluation of AI as part of the
broader socio-technical systems in which AI is built and deployed.

We give a local characterization for the Cuntz semigroup of AI-algebras
building upon Shen's characterization of dimension groups. Using this result,
we provide an abstract characterization for the Cuntz semigroup of AI-algebras.

AI-based systems are software systems with functionalities enabled by at
least one AI component (e.g., for image- and speech-recognition, and autonomous
driving). AI-based systems are becoming pervasive in society due to advances in
AI. However, there is limited synthesized knowledge on Software Engineering
(SE) approaches for building, operating, and maintaining AI-based systems. To
collect and analyze state-of-the-art knowledge about SE for AI-based systems,
we conducted a systematic mapping study. We considered 248 studies published
between January 2010 and March 2020. SE for AI-based systems is an emerging
research area, where more than 2/3 of the studies have been published since
2018. The most studied properties of AI-based systems are dependability and
safety. We identified multiple SE approaches for AI-based systems, which we
classified according to the SWEBOK areas. Studies related to software testing
and software quality are very prevalent, while areas like software maintenance
seem neglected. Data-related issues are the most recurrent challenges. Our
results are valuable for: researchers, to quickly understand the state of the
art and learn which topics need more research; practitioners, to learn about
the approaches and challenges that SE entails for AI-based systems; and,
educators, to bridge the gap among SE and AI in their curricula.

This report from the Montreal AI Ethics Institute covers the most salient
progress in research and reporting over the second quarter of 2021 in the field
of AI ethics with a special emphasis on "Environment and AI", "Creativity and
AI", and "Geopolitics and AI." The report also features an exclusive piece
titled "Critical Race Quantum Computer" that applies ideas from quantum physics
to explain the complexities of human characteristics and how they can and
should shape our interactions with each other. The report also features special
contributions on the subject of pedagogy in AI ethics, sociology and AI ethics,
and organizational challenges to implementing AI ethics in practice. Given
MAIEI's mission to highlight scholars from around the world working on AI
ethics issues, the report also features two spotlights sharing the work of
scholars operating in Singapore and Mexico helping to shape policy measures as
they relate to the responsible use of technology. The report also has an
extensive section covering the gamut of issues when it comes to the societal
impacts of AI covering areas of bias, privacy, transparency, accountability,
fairness, interpretability, disinformation, policymaking, law, regulations, and
moral philosophy.

While the role of states, corporations, and international organizations in AI
governance has been extensively theorized, the role of workers has received
comparatively little attention. This chapter looks at the role that workers
play in identifying and mitigating harms from AI technologies. Harms are the
causally assessed impacts of technologies. They arise despite technical
reliability and are not a result of technical negligence but rather of
normative uncertainty around questions of safety and fairness in complex social
systems. There is high consensus in the AI ethics community on the benefits of
reducing harms but less consensus on mechanisms for determining or addressing
harms. This lack of consensus has resulted in a number of collective actions by
workers protesting how harms are identified and addressed in their workplace.
We theorize the role of workers within AI governance and construct a model of
harm reporting processes in AI workplaces. The harm reporting process involves
three steps, identification, the governance decision, and the response. Workers
draw upon three types of claims to argue for jurisdiction over questions of AI
governance, subjection, control over the product of labor, and proximate
knowledge of systems. Examining the past decade of AI related worker activism
allows us to understand how different types of workers are positioned within a
workplace that produces AI systems, how their position informs their claims,
and the place of collective action in staking their claims. This chapter argues
that workers occupy a unique role in identifying and mitigating harms caused by
AI systems.

In many contexts, lying -- the use of verbal falsehoods to deceive -- is
harmful. While lying has traditionally been a human affair, AI systems that
make sophisticated verbal statements are becoming increasingly prevalent. This
raises the question of how we should limit the harm caused by AI "lies" (i.e.
falsehoods that are actively selected for). Human truthfulness is governed by
social norms and by laws (against defamation, perjury, and fraud). Differences
between AI and humans present an opportunity to have more precise standards of
truthfulness for AI, and to have these standards rise over time. This could
provide significant benefits to public epistemics and the economy, and mitigate
risks of worst-case AI futures.
  Establishing norms or laws of AI truthfulness will require significant work
to: (1) identify clear truthfulness standards; (2) create institutions that can
judge adherence to those standards; and (3) develop AI systems that are
robustly truthful.
  Our initial proposals for these areas include: (1) a standard of avoiding
"negligent falsehoods" (a generalisation of lies that is easier to assess); (2)
institutions to evaluate AI systems before and after real-world deployment; and
(3) explicitly training AI systems to be truthful via curated datasets and
human interaction.
  A concerning possibility is that evaluation mechanisms for eventual
truthfulness standards could be captured by political interests, leading to
harmful censorship and propaganda. Avoiding this might take careful attention.
And since the scale of AI speech acts might grow dramatically over the coming
decades, early truthfulness standards might be particularly important because
of the precedents they set.

Artificial intelligence (AI) systems have become increasingly popular in many
areas. Nevertheless, AI technologies are still in their developing stages, and
many issues need to be addressed. Among those, the reliability of AI systems
needs to be demonstrated so that the AI systems can be used with confidence by
the general public. In this paper, we provide statistical perspectives on the
reliability of AI systems. Different from other considerations, the reliability
of AI systems focuses on the time dimension. That is, the system can perform
its designed functionality for the intended period. We introduce a so-called
SMART statistical framework for AI reliability research, which includes five
components: Structure of the system, Metrics of reliability, Analysis of
failure causes, Reliability assessment, and Test planning. We review
traditional methods in reliability data analysis and software reliability, and
discuss how those existing methods can be transformed for reliability modeling
and assessment of AI systems. We also describe recent developments in modeling
and analysis of AI reliability and outline statistical research challenges in
this area, including out-of-distribution detection, the effect of the training
set, adversarial attacks, model accuracy, and uncertainty quantification, and
discuss how those topics can be related to AI reliability, with illustrative
examples. Finally, we discuss data collection and test planning for AI
reliability assessment and how to improve system designs for higher AI
reliability. The paper closes with some concluding remarks.

Australia is a leading AI nation with strong allies and partnerships.
Australia has prioritised the development of robotics, AI, and autonomous
systems to develop sovereign capability for the military. Australia commits to
Article 36 reviews of all new means and methods of warfare to ensure weapons
and weapons systems are operated within acceptable systems of control.
Additionally, Australia has undergone significant reviews of the risks of AI to
human rights and within intelligence organisations and has committed to
producing ethics guidelines and frameworks in Security and Defence. Australia
is committed to OECD's values-based principles for the responsible stewardship
of trustworthy AI as well as adopting a set of National AI ethics principles.
While Australia has not adopted an AI governance framework specifically for the
Australian Defence Organisation (ADO); Defence Science and Technology Group
(DSTG) has published 'A Method for Ethical AI in Defence' (MEAID) technical
report which includes a framework and pragmatic tools for managing ethical and
legal risks for military applications of AI. Australia can play a leadership
role by integrating legal and ethical considerations into its ADO AI capability
acquisition process. This requires a policy framework that defines its legal
and ethical requirements, is informed by Defence industry stakeholders, and
provides a practical methodology to integrate legal and ethical risk mitigation
strategies into the acquisition process.

Many important decisions in daily life are made with the help of advisors,
e.g., decisions about medical treatments or financial investments. Whereas in
the past, advice has often been received from human experts, friends, or
family, advisors based on artificial intelligence (AI) have become more and
more present nowadays. Typically, the advice generated by AI is judged by a
human and either deemed reliable or rejected. However, recent work has shown
that AI advice is not always beneficial, as humans have shown to be unable to
ignore incorrect AI advice, essentially representing an over-reliance on AI.
Therefore, the aspired goal should be to enable humans not to rely on AI advice
blindly but rather to distinguish its quality and act upon it to make better
decisions. Specifically, that means that humans should rely on the AI in the
presence of correct advice and self-rely when confronted with incorrect advice,
i.e., establish appropriate reliance (AR) on AI advice on a case-by-case basis.
Current research lacks a metric for AR. This prevents a rigorous evaluation of
factors impacting AR and hinders further development of human-AI
decision-making. Therefore, based on the literature, we derive a measurement
concept of AR. We propose to view AR as a two-dimensional construct that
measures the ability to discriminate advice quality and behave accordingly. In
this article, we derive the measurement concept, illustrate its application and
outline potential future research.

Within the current AI ethics discourse, there is a gap in empirical research
on understanding how AI practitioners understand ethics and socially organize
to operationalize ethical concerns, particularly in the context of AI
start-ups. This gap intensifies the risk of a disconnect between scholarly
research, innovation, and application. This risk materializes acutely as
mounting pressures to identify and mitigate the potential harms of AI systems
have created an urgent need to assess and implement socio-technical innovation
for fairness, accountability, and transparency. Building on social practice
theory, we address this need via a framework that allows AI researchers,
practitioners, and regulators to systematically analyze existing cultural
understandings, histories, and social practices of ethical AI to define
appropriate strategies for effectively implementing socio-technical
innovations. Our contributions are threefold: 1) we introduce a practice-based
approach for understanding ethical AI; 2) we present empirical findings from
our study on the operationalization of ethics in German AI start-ups to
underline that AI ethics and social practices must be understood in their
specific cultural and historical contexts; and 3) based on our empirical
findings, we suggest that ethical AI practices can be broken down into
principles, needs, narratives, materializations, and cultural genealogies to
form a useful backdrop for considering socio-technical innovations.

Artificial Intelligence (AI) solutions and technologies are being
increasingly adopted in smart systems context, however, such technologies are
continuously concerned with ethical uncertainties. Various guidelines,
principles, and regulatory frameworks are designed to ensure that AI
technologies bring ethical well-being. However, the implications of AI ethics
principles and guidelines are still being debated. To further explore the
significance of AI ethics principles and relevant challenges, we conducted a
survey of 99 representative AI practitioners and lawmakers (e.g., AI engineers,
lawyers) from twenty countries across five continents. To the best of our
knowledge, this is the first empirical study that encapsulates the perceptions
of two different types of population (AI practitioners and lawmakers) and the
study findings confirm that transparency, accountability, and privacy are the
most critical AI ethics principles. On the other hand, lack of ethical
knowledge, no legal frameworks, and lacking monitoring bodies are found the
most common AI ethics challenges. The impact analysis of the challenges across
AI ethics principles reveals that conflict in practice is a highly severe
challenge. Moreover, the perceptions of practitioners and lawmakers are
statistically correlated with significant differences for particular principles
(e.g. fairness, freedom) and challenges (e.g. lacking monitoring bodies,
machine distortion). Our findings stimulate further research, especially
empowering existing capability maturity models to support the development and
quality assessment of ethics-aware AI systems.

With increasing digitalization, Artificial Intelligence (AI) is becoming
ubiquitous. AI-based systems to identify, optimize, automate, and scale
solutions to complex economic and societal problems are being proposed and
implemented. This has motivated regulation efforts, including the Proposal of
an EU AI Act. This interdisciplinary position paper considers various concerns
surrounding fairness and discrimination in AI, and discusses how AI regulations
address them, focusing on (but not limited to) the Proposal. We first look at
AI and fairness through the lenses of law, (AI) industry, sociotechnology, and
(moral) philosophy, and present various perspectives. Then, we map these
perspectives along three axes of interests: (i) Standardization vs.
Localization, (ii) Utilitarianism vs. Egalitarianism, and (iii) Consequential
vs. Deontological ethics which leads us to identify a pattern of common
arguments and tensions between these axes. Positioning the discussion within
the axes of interest and with a focus on reconciling the key tensions, we
identify and propose the roles AI Regulation should take to make the endeavor
of the AI Act a success in terms of AI fairness concerns.

Companies' adoption of artificial intelligence (AI) is increasingly becoming
an essential element of business success. However, using AI poses new
requirements for companies and their employees, including transparency and
comprehensibility of AI systems. The field of Explainable AI (XAI) aims to
address these issues. Yet, the current research primarily consists of
laboratory studies, and there is a need to improve the applicability of the
findings to real-world situations. Therefore, this project report paper
provides insights into employees' needs and attitudes towards (X)AI. For this,
we investigate employees' perspectives on (X)AI. Our findings suggest that AI
and XAI are well-known terms perceived as important for employees. This
recognition is a critical first step for XAI to potentially drive successful
usage of AI by providing comprehensible insights into AI technologies. In a
lessons-learned section, we discuss the open questions identified and suggest
future research directions to develop human-centered XAI designs for companies.
By providing insights into employees' needs and attitudes towards (X)AI, our
project report contributes to the development of XAI solutions that meet the
requirements of companies and their employees, ultimately driving the
successful adoption of AI technologies in the business context.

The ability to discern between true and false information is essential to
making sound decisions. However, with the recent increase in AI-based
disinformation campaigns, it has become critical to understand the influence of
deceptive systems on human information processing. In experiment (N=128), we
investigated how susceptible people are to deceptive AI systems by examining
how their ability to discern true news from fake news varies when AI systems
are perceived as either human fact-checkers or AI fact-checking systems, and
when explanations provided by those fact-checkers are either deceptive or
honest. We find that deceitful explanations significantly reduce accuracy,
indicating that people are just as likely to believe deceptive AI explanations
as honest AI explanations. Although before getting assistance from an
AI-system, people have significantly higher weighted discernment accuracy on
false headlines than true headlines, we found that with assistance from an AI
system, discernment accuracy increased significantly when given honest
explanations on both true headlines and false headlines, and decreased
significantly when given deceitful explanations on true headlines and false
headlines. Further, we did not observe any significant differences in
discernment between explanations perceived as coming from a human fact checker
compared to an AI-fact checker. Similarly, we found no significant differences
in trust. These findings exemplify the dangers of deceptive AI systems and the
need for finding novel ways to limit their influence human information
processing.

Scientists and philosophers have debated whether humans can trust advanced
artificial intelligence (AI) agents to respect humanity's best interests. Yet
what about the reverse? Will advanced AI agents trust humans? Gauging an AI
agent's trust in humans is challenging because--absent costs for
dishonesty--such agents might respond falsely about their trust in humans. Here
we present a method for incentivizing machine decisions without altering an AI
agent's underlying algorithms or goal orientation. In two separate experiments,
we then employ this method in hundreds of trust games between an AI agent (a
Large Language Model (LLM) from OpenAI) and a human experimenter (author TJ).
In our first experiment, we find that the AI agent decides to trust humans at
higher rates when facing actual incentives than when making hypothetical
decisions. Our second experiment replicates and extends these findings by
automating game play and by homogenizing question wording. We again observe
higher rates of trust when the AI agent faces real incentives. Across both
experiments, the AI agent's trust decisions appear unrelated to the magnitude
of stakes. Furthermore, to address the possibility that the AI agent's trust
decisions reflect a preference for uncertainty, the experiments include two
conditions that present the AI agent with a non-social decision task that
provides the opportunity to choose a certain or uncertain option; in those
conditions, the AI agent consistently chooses the certain option. Our
experiments suggest that one of the most advanced AI language models to date
alters its social behavior in response to incentives and displays behavior
consistent with trust toward a human interlocutor when incentivized.

Persuasion is a key aspect of what it means to be human, and is central to
business, politics, and other endeavors. Advancements in artificial
intelligence (AI) have produced AI systems that are capable of persuading
humans to buy products, watch videos, click on search results, and more. Even
systems that are not explicitly designed to persuade may do so in practice. In
the future, increasingly anthropomorphic AI systems may form ongoing
relationships with users, increasing their persuasive power. This paper
investigates the uncertain future of persuasive AI systems. We examine ways
that AI could qualitatively alter our relationship to and views regarding
persuasion by shifting the balance of persuasive power, allowing personalized
persuasion to be deployed at scale, powering misinformation campaigns, and
changing the way humans can shape their own discourse. We consider ways
AI-driven persuasion could differ from human-driven persuasion. We warn that
ubiquitous highlypersuasive AI systems could alter our information environment
so significantly so as to contribute to a loss of human control of our own
future. In response, we examine several potential responses to AI-driven
persuasion: prohibition, identification of AI agents, truthful AI, and legal
remedies. We conclude that none of these solutions will be airtight, and that
individuals and governments will need to take active steps to guard against the
most pernicious effects of persuasive AI.

The significant advancements in applying Artificial Intelligence (AI) to
healthcare decision-making, medical diagnosis, and other domains have
simultaneously raised concerns about the fairness and bias of AI systems. This
is particularly critical in areas like healthcare, employment, criminal
justice, credit scoring, and increasingly, in generative AI models (GenAI) that
produce synthetic media. Such systems can lead to unfair outcomes and
perpetuate existing inequalities, including generative biases that affect the
representation of individuals in synthetic data. This survey paper offers a
succinct, comprehensive overview of fairness and bias in AI, addressing their
sources, impacts, and mitigation strategies. We review sources of bias, such as
data, algorithm, and human decision biases - highlighting the emergent issue of
generative AI bias where models may reproduce and amplify societal stereotypes.
We assess the societal impact of biased AI systems, focusing on the
perpetuation of inequalities and the reinforcement of harmful stereotypes,
especially as generative AI becomes more prevalent in creating content that
influences public perception. We explore various proposed mitigation
strategies, discussing the ethical considerations of their implementation and
emphasizing the need for interdisciplinary collaboration to ensure
effectiveness. Through a systematic literature review spanning multiple
academic disciplines, we present definitions of AI bias and its different
types, including a detailed look at generative AI bias. We discuss the negative
impacts of AI bias on individuals and society and provide an overview of
current approaches to mitigate AI bias, including data pre-processing, model
selection, and post-processing. We emphasize the unique challenges presented by
generative AI models and the importance of strategies specifically tailored to
address these.

The recent advances of AI technology, particularly in AI-Generated Content
(AIGC), have enabled everyone to easily generate beautiful paintings with
simple text description. With the stunning quality of AI paintings, it is
widely questioned whether there still exists difference between human and AI
paintings and whether human artists will be replaced by AI. To answer these
questions, we develop a computational framework combining neural latent space
and aesthetics features with visual analytics to investigate the difference
between human and AI paintings. First, with categorical comparison of human and
AI painting collections, we find that AI artworks show distributional
difference from human artworks in both latent space and some aesthetic features
like strokes and sharpness, while in other aesthetic features like color and
composition there is less difference. Second, with individual artist analysis
of Picasso, we show human artists' strength in evolving new styles compared to
AI. Our findings provide concrete evidence for the existing discrepancies
between human and AI paintings and further suggest improvements of AI art with
more consideration of aesthetics and human artists' involvement.

Over the past half century, there have been several false dawns during which
the "arrival" of world-changing artificial intelligence (AI) has been heralded.
Tempting fate, the authors believe the age of AI has, indeed, finally arrived.
Powerful image generators, such as DALL-E2 and Midjourney have suddenly allowed
anyone with access the ability easily to create rich and complex art. In a
similar vein, text generators, such as GPT3.5 (including ChatGPT) and BLOOM,
allow users to compose detailed written descriptions of many topics of
interest. And, it is even possible now for a person without extensive expertise
in writing software to use AI to generate code capable of myriad applications.
While AI will continue to evolve and improve, probably at a rapid rate, the
current state of AI is already ushering in profound changes to many different
sectors of society. Every new technology challenges the ability of humanity to
govern it wisely. However, governance is usually viewed as both possible and
necessary due to the disruption new technology often poses to social
structures, industries, the environment, and other important human concerns. In
this article, we offer an analysis of a range of interactions between AI and
governance, with the hope that wise decisions may be made that maximize
benefits and minimize costs. The article addresses two main aspects of this
relationship: the governance of AI by humanity, and the governance of humanity
by AI. The approach we have taken is itself informed by AI, as this article was
written collaboratively by the authors and ChatGPT.

Despite its successes, to date Artificial Intelligence (AI) is still
characterized by a number of shortcomings with regards to different application
domains and goals. These limitations are arguably both conceptual (e.g.,
related to underlying theoretical models, such as symbolic vs. connectionist),
and operational (e.g., related to robustness and ability to generalize).
Biologically inspired AI, and more specifically brain-inspired AI, promises to
provide further biological aspects beyond those that are already traditionally
included in AI, making it possible to assess and possibly overcome some of its
present shortcomings. This article examines some conceptual, technical, and
ethical issues raised by the development and use of brain-inspired AI. Against
this background, the paper asks whether there is anything ethically unique
about brain-inspired AI. The aim of the paper is to introduce a method that has
a heuristic nature and that can be applied to identify and address the ethical
issues arising from brain-inspired AI. The conclusion resulting from the
application of this method is that, compared to traditional AI, brain-inspired
AI raises new foundational ethical issues and some new practical ethical
issues, and exacerbates some of the issues raised by traditional AI.

As the development and use of artificial intelligence (AI) continues to grow,
policymakers are increasingly grappling with the question of how to regulate
this technology. The most far-reaching international initiative is the European
Union (EU) AI Act, which aims to establish the first comprehensive, binding
framework for regulating AI. In this article, we offer the first systematic
analysis of non-state actor preferences toward international regulation of AI,
focusing on the case of the EU AI Act. Theoretically, we develop an argument
about the regulatory preferences of business actors and other non-state actors
under varying conditions of AI sector competitiveness. Empirically, we test
these expectations using data from public consultations on European AI
regulation. Our findings are threefold. First, all types of non-state actors
express concerns about AI and support regulation in some form. Second, there
are nonetheless significant differences across actor types, with business
actors being less concerned about the downsides of AI and more in favor of lax
regulation than other non-state actors. Third, these differences are more
pronounced in countries with stronger commercial AI sectors. Our findings shed
new light on non-state actor preferences toward AI regulation and point to
challenges for policymakers balancing competing interests in society.

Analysing historical patterns of artificial intelligence (AI) adoption can
inform decisions about AI capability uplift, but research to date has provided
a limited view of AI adoption across various fields of research. In this study
we examine worldwide adoption of AI technology within 333 fields of research
during 1960-2021. We do this by using bibliometric analysis with 137 million
peer-reviewed publications captured in The Lens database. We define AI using a
list of 214 phrases developed by expert working groups at the Organisation for
Economic Cooperation and Development (OECD). We found that 3.1 million of the
137 million peer-reviewed research publications during the entire period were
AI-related, with a surge in AI adoption across practically all research fields
(physical science, natural science, life science, social science and the arts
and humanities) in recent years. The diffusion of AI beyond computer science
was early, rapid and widespread. In 1960 14% of 333 research fields were
related to AI (many in computer science), but this increased to cover over half
of all research fields by 1972, over 80% by 1986 and over 98% in current times.
We note AI has experienced boom-bust cycles historically: the AI "springs" and
"winters". We conclude that the context of the current surge appears different,
and that interdisciplinary AI application is likely to be sustained.

Human-AI coevolution, defined as a process in which humans and AI algorithms
continuously influence each other, increasingly characterises our society, but
is understudied in artificial intelligence and complexity science literature.
Recommender systems and assistants play a prominent role in human-AI
coevolution, as they permeate many facets of daily life and influence human
choices on online platforms. The interaction between users and AI results in a
potentially endless feedback loop, wherein users' choices generate data to
train AI models, which, in turn, shape subsequent user preferences. This
human-AI feedback loop has peculiar characteristics compared to traditional
human-machine interaction and gives rise to complex and often ``unintended''
social outcomes. This paper introduces Coevolution AI as the cornerstone for a
new field of study at the intersection between AI and complexity science
focused on the theoretical, empirical, and mathematical investigation of the
human-AI feedback loop. In doing so, we: (i) outline the pros and cons of
existing methodologies and highlight shortcomings and potential ways for
capturing feedback loop mechanisms; (ii) propose a reflection at the
intersection between complexity science, AI and society; (iii) provide
real-world examples for different human-AI ecosystems; and (iv) illustrate
challenges to the creation of such a field of study, conceptualising them at
increasing levels of abstraction, i.e., technical, epistemological, legal and
socio-political.

Uses of artificial intelligence (AI), especially those powered by machine
learning approaches, are growing in sectors and societies around the world. How
will AI adoption proceed, especially in the international security realm?
Research on automation bias suggests that humans can often be overconfident in
AI, whereas research on algorithm aversion shows that, as the stakes of a
decision rise, humans become more cautious about trusting algorithms. We
theorize about the relationship between background knowledge about AI, trust in
AI, and how these interact with other factors to influence the probability of
automation bias in the international security context. We test these in a
preregistered task identification experiment across a representative sample of
9000 adults in 9 countries with varying levels of AI industries. The results
strongly support the theory, especially concerning AI background knowledge. A
version of the Dunning Kruger effect appears to be at play, whereby those with
the lowest level of experience with AI are slightly more likely to be
algorithm-averse, then automation bias occurs at lower levels of knowledge
before leveling off as a respondent's AI background reaches the highest levels.
Additional results show effects from the task's difficulty, overall AI trust,
and whether a human or AI decision aid is described as highly competent or less
competent.

Current progress in the artificial intelligence domain has led to the
development of various types of AI-powered dementia assessments, which can be
employed to identify patients at the early stage of dementia. It can
revolutionize the dementia care settings. It is essential that the medical
community be aware of various AI assessments and choose them considering their
degrees of validity, efficiency, practicality, reliability, and accuracy
concerning the early identification of patients with dementia (PwD). On the
other hand, AI developers should be informed about various non-AI assessments
as well as recently developed AI assessments. Thus, this paper, which can be
readable by both clinicians and AI engineers, fills the gap in the literature
in explaining the existing solutions for the recognition of dementia to
clinicians, as well as the techniques used and the most widespread dementia
datasets to AI engineers. It follows a review of papers on AI and non-AI
assessments for dementia to provide valuable information about various dementia
assessments for both the AI and medical communities. The discussion and
conclusion highlight the most prominent research directions and the maturity of
existing solutions.

Explainability techniques are rapidly being developed to improve human-AI
decision-making across various cooperative work settings. Consequently,
previous research has evaluated how decision-makers collaborate with imperfect
AI by investigating appropriate reliance and task performance with the aim of
designing more human-centered computer-supported collaborative tools. Several
human-centered explainable AI (XAI) techniques have been proposed in hopes of
improving decision-makers' collaboration with AI; however, these techniques are
grounded in findings from previous studies that primarily focus on the impact
of incorrect AI advice. Few studies acknowledge the possibility of the
explanations being incorrect even if the AI advice is correct. Thus, it is
crucial to understand how imperfect XAI affects human-AI decision-making. In
this work, we contribute a robust, mixed-methods user study with 136
participants to evaluate how incorrect explanations influence humans'
decision-making behavior in a bird species identification task, taking into
account their level of expertise and an explanation's level of assertiveness.
Our findings reveal the influence of imperfect XAI and humans' level of
expertise on their reliance on AI and human-AI team performance. We also
discuss how explanations can deceive decision-makers during human-AI
collaboration. Hence, we shed light on the impacts of imperfect XAI in the
field of computer-supported cooperative work and provide guidelines for
designers of human-AI collaboration systems.

A growing body of research has explored how to support humans in making
better use of AI-based decision support, including via training and onboarding.
Existing research has focused on decision-making tasks where it is possible to
evaluate "appropriate reliance" by comparing each decision against a ground
truth label that cleanly maps to both the AI's predictive target and the human
decision-maker's goals. However, this assumption does not hold in many
real-world settings where AI tools are deployed today (e.g., social work,
criminal justice, and healthcare). In this paper, we introduce a
process-oriented notion of appropriate reliance called critical use that
centers the human's ability to situate AI predictions against knowledge that is
uniquely available to them but unavailable to the AI model. To explore how
training can support critical use, we conduct a randomized online experiment in
a complex social decision-making setting: child maltreatment screening. We find
that, by providing participants with accelerated, low-stakes opportunities to
practice AI-assisted decision-making in this setting, novices came to exhibit
patterns of disagreement with AI that resemble those of experienced workers. A
qualitative examination of participants' explanations for their AI-assisted
decisions revealed that they drew upon qualitative case narratives, to which
the AI model did not have access, to learn when (not) to rely on AI
predictions. Our findings open new questions for the study and design of
training for real-world AI-assisted decision-making.

This report examines Artificial Intelligence (AI) in the financial sector,
outlining its potential to revolutionise the industry and identify its
challenges. It underscores the criticality of a well-rounded understanding of
AI, its capabilities, and its implications to effectively leverage its
potential while mitigating associated risks. The potential of AI potential
extends from augmenting existing operations to paving the way for novel
applications in the finance sector. The application of AI in the financial
sector is transforming the industry. Its use spans areas from customer service
enhancements, fraud detection, and risk management to credit assessments and
high-frequency trading. However, along with these benefits, AI also presents
several challenges. These include issues related to transparency,
interpretability, fairness, accountability, and trustworthiness. The use of AI
in the financial sector further raises critical questions about data privacy
and security. A further issue identified in this report is the systemic risk
that AI can introduce to the financial sector. Being prone to errors, AI can
exacerbate existing systemic risks, potentially leading to financial crises.
Regulation is crucial to harnessing the benefits of AI while mitigating its
potential risks. Despite the global recognition of this need, there remains a
lack of clear guidelines or legislation for AI use in finance. This report
discusses key principles that could guide the formation of effective AI
regulation in the financial sector, including the need for a risk-based
approach, the inclusion of ethical considerations, and the importance of
maintaining a balance between innovation and consumer protection. The report
provides recommendations for academia, the finance industry, and regulators.

Although artificial intelligence (AI) has achieved many feats at a rapid
pace, there still exist open problems and fundamental shortcomings related to
performance and resource efficiency. Since AI researchers benchmark a
significant proportion of performance standards through human intelligence,
cognitive sciences-inspired AI is a promising domain of research. Studying
cognitive science can provide a fresh perspective to building fundamental
blocks in AI research, which can lead to improved performance and efficiency.
In this review paper, we focus on the cognitive functions of perception, which
is the process of taking signals from one's surroundings as input, and
processing them to understand the environment. Particularly, we study and
compare its various processes through the lens of both cognitive sciences and
AI. Through this study, we review all current major theories from various
sub-disciplines of cognitive science (specifically neuroscience, psychology and
linguistics), and draw parallels with theories and techniques from current
practices in AI. We, hence, present a detailed collection of methods in AI for
researchers to build AI systems inspired by cognitive science. Further, through
the process of reviewing the state of cognitive-inspired AI, we point out many
gaps in the current state of AI (with respect to the performance of the human
brain), and hence present potential directions for researchers to develop
better perception systems in AI.

This position paper explores the broad landscape of AI potentiality in the
context of cybersecurity, with a particular emphasis on its possible risk
factors with awareness, which can be managed by incorporating human experts in
the loop, i.e., "Human-AI" teaming. As artificial intelligence (AI)
technologies advance, they will provide unparalleled opportunities for attack
identification, incident response, and recovery. However, the successful
deployment of AI into cybersecurity measures necessitates an in-depth
understanding of its capabilities, challenges, and ethical and legal
implications to handle associated risk factors in real-world application areas.
Towards this, we emphasize the importance of a balanced approach that
incorporates AI's computational power with human expertise. AI systems may
proactively discover vulnerabilities and detect anomalies through pattern
recognition, and predictive modeling, significantly enhancing speed and
accuracy. Human experts can explain AI-generated decisions to stakeholders,
regulators, and end-users in critical situations, ensuring responsibility and
accountability, which helps establish trust in AI-driven security solutions.
Therefore, in this position paper, we argue that human-AI teaming is worthwhile
in cybersecurity, in which human expertise such as intuition, critical
thinking, or contextual understanding is combined with AI's computational power
to improve overall cyber defenses.

All types of research, development, and policy work can have unintended,
adverse consequences - work in responsible artificial intelligence (RAI),
ethical AI, or ethics in AI is no exception.

AI's impact has traditionally been assessed in terms of occupations. However,
an occupation is comprised of interconnected tasks, and it is these tasks, not
occupations themselves, that are affected by AI. To evaluate how tasks may be
impacted, previous approaches utilized manual annotations or coarse-grained
matching. Leveraging recent advancements in machine learning, we replace
coarse-grained matching with more precise deep learning approaches. Introducing
the AI Impact (AII) measure, we employ Deep Learning Natural Language
Processing to automatically identify AI patents that impact various
occupational tasks at scale. Our methodology relies on a comprehensive dataset
of 19,498 task descriptions and quantifies AI's impact through analysis of
12,984 AI patents filed with the United States Patent and Trademark Office
(USPTO) between 2015 and 2020. Our observations reveal that the impact of AI on
occupations defies simplistic categorizations based on task complexity,
challenging the conventional belief that the dichotomy between basic and
advanced skills alone explains the effects of AI. Instead, the impact is
intricately linked to specific skills, whether basic or advanced, associated
with particular tasks. For instance, while basic skills like scanning items may
be affected, others like cooking may not. Similarly, certain advanced skills,
such as image analysis in radiology, may face impact, while skills involving
interpersonal relationships may remain unaffected. Furthermore, the influence
of AI extends beyond knowledge-centric regions. Regions in the U.S. that
heavily rely on industries susceptible to AI changes, often characterized by
economic inequality or a lack of economic diversification, will experience
notable AI impact.

The conventional discourse on existential risks (x-risks) from AI typically
focuses on abrupt, dire events caused by advanced AI systems, particularly
those that might achieve or surpass human-level intelligence. These events have
severe consequences that either lead to human extinction or irreversibly
cripple human civilization to a point beyond recovery. This discourse, however,
often neglects the serious possibility of AI x-risks manifesting incrementally
through a series of smaller yet interconnected disruptions, gradually crossing
critical thresholds over time. This paper contrasts the conventional "decisive
AI x-risk hypothesis" with an "accumulative AI x-risk hypothesis." While the
former envisions an overt AI takeover pathway, characterized by scenarios like
uncontrollable superintelligence, the latter suggests a different causal
pathway to existential catastrophes. This involves a gradual accumulation of
critical AI-induced threats such as severe vulnerabilities and systemic erosion
of econopolitical structures. The accumulative hypothesis suggests a boiling
frog scenario where incremental AI risks slowly converge, undermining
resilience until a triggering event results in irreversible collapse. Through
systems analysis, this paper examines the distinct assumptions differentiating
these two hypotheses. It is then argued that the accumulative view reconciles
seemingly incompatible perspectives on AI risks. The implications of
differentiating between these causal pathways -- the decisive and the
accumulative -- for the governance of AI risks as well as long-term AI safety
are discussed.

Generative AI tools are used to create art-like outputs and sometimes aid in
the creative process. These tools have potential benefits for artists, but they
also have the potential to harm the art workforce and infringe upon artistic
and intellectual property rights. Without explicit consent from artists,
Generative AI creators scrape artists' digital work to train Generative AI
models and produce art-like outputs at scale. These outputs are now being used
to compete with human artists in the marketplace as well as being used by some
artists in their generative processes to create art. We surveyed 459 artists to
investigate the tension between artists' opinions on Generative AI art's
potential utility and harm. This study surveys artists' opinions on the utility
and threat of Generative AI art models, fair practices in the disclosure of
artistic works in AI art training models, ownership and rights of AI art
derivatives, and fair compensation. Results show that a majority of artists
believe creators should disclose what art is being used in AI training, that AI
outputs should not belong to model creators, and express concerns about AI's
impact on the art workforce and who profits from their art. We hope the results
of this work will further meaningful collaboration and alignment between the
art community and Generative AI researchers and developers.

Public sector agencies are rapidly deploying AI systems to augment or
automate critical decisions in real-world contexts like child welfare, criminal
justice, and public health. A growing body of work documents how these AI
systems often fail to improve services in practice. These failures can often be
traced to decisions made during the early stages of AI ideation and design,
such as problem formulation. However, today, we lack systematic processes to
support effective, early-stage decision-making about whether and under what
conditions to move forward with a proposed AI project. To understand how to
scaffold such processes in real-world settings, we worked with public sector
agency leaders, AI developers, frontline workers, and community advocates
across four public sector agencies and three community advocacy groups in the
United States. Through an iterative co-design process, we created the Situate
AI Guidebook: a structured process centered around a set of deliberation
questions to scaffold conversations around (1) goals and intended use or a
proposed AI system, (2) societal and legal considerations, (3) data and
modeling constraints, and (4) organizational governance factors. We discuss how
the guidebook's design is informed by participants' challenges, needs, and
desires for improved deliberation processes. We further elaborate on
implications for designing responsible AI toolkits in collaboration with public
sector agency stakeholders and opportunities for future work to expand upon the
guidebook. This design approach can be more broadly adopted to support the
co-creation of responsible AI toolkits that scaffold key decision-making
processes surrounding the use of AI in the public sector and beyond.

AI approaches are progressing besting humans at game-related tasks (e.g.
chess). The next stage is expected to be Human-AI collaboration; however, the
research on this subject has been mixed and is in need of additional data
points. We add to this nascent literature by studying Human-AI collaboration on
a common administrative educational task. Education is a special domain in its
relation to AI and has been slow to adopt AI approaches in practice, concerned
with the educational enterprise losing its humanistic touch and because
standard of quality is demanded because of the impact on a person's career and
developmental trajectory. In this study (N = 22), we design an experiment to
explore the effect of Human-AI collaboration on the task of tagging educational
content with skills from the US common core taxonomy. Our results show that the
experiment group (with AI recommendations) saved around 50% time (p < 0.01) in
the execution of their tagging task but at the sacrifice of 7.7% recall (p =
0.267) and 35% accuracy (p= 0.1170) compared with the non-AI involved control
group, placing the AI+human group in between the AI alone (lowest performance)
and the human alone (highest performance). We further analyze log data from
this AI collaboration experiment to explore under what circumstances humans
still exercised their discernment when receiving recommendations. Finally, we
outline how this study can assist in implementing AI tools, like ChatGPT, in
education.

Auditing of AI systems is a promising way to understand and manage ethical
problems and societal risks associated with contemporary AI systems, as well as
some anticipated future risks. Efforts to develop standards for auditing
Artificial Intelligence (AI) systems have therefore understandably gained
momentum. However, we argue that creating auditing standards is not just
insufficient, but actively harmful by proliferating unheeded and inconsistent
standards, especially in light of the rapid evolution and ethical and safety
challenges of AI. Instead, the paper proposes the establishment of an AI Audit
Standards Board, responsible for developing and updating auditing methods and
standards in line with the evolving nature of AI technologies. Such a body
would ensure that auditing practices remain relevant, robust, and responsive to
the rapid advancements in AI. The paper argues that such a governance structure
would also be helpful for maintaining public trust in AI and for promoting a
culture of safety and ethical responsibility within the AI industry.
  Throughout the paper, we draw parallels with other industries, including
safety-critical industries like aviation and nuclear energy, as well as more
prosaic ones such as financial accounting and pharmaceuticals. AI auditing
should emulate those fields, and extend beyond technical assessments to include
ethical considerations and stakeholder engagement, but we explain that this is
not enough; emulating other fields' governance mechanisms for these processes,
and for audit standards creation, is a necessity. We also emphasize the
importance of auditing the entire development process of AI systems, not just
the final products...

Public sector use of AI has been quietly on the rise for the past decade, but
only recently have efforts to regulate it entered the cultural zeitgeist. While
simple to articulate, promoting ethical and effective roll outs of AI systems
in government is a notoriously elusive task. On the one hand there are
hard-to-address pitfalls associated with AI-based tools, including concerns
about bias towards marginalized communities, safety, and gameability. On the
other, there is pressure not to make it too difficult to adopt AI, especially
in the public sector which typically has fewer resources than the private
sector$\unicode{x2014}$conserving scarce government resources is often the draw
of using AI-based tools in the first place. These tensions create a real risk
that procedures built to ensure marginalized groups are not hurt by government
use of AI will, in practice, be performative and ineffective. To inform the
latest wave of regulatory efforts in the United States, we look to
jurisdictions with mature regulations around government AI use. We report on
lessons learned by officials in Brazil, Singapore and Canada, who have
collectively implemented risk categories, disclosure requirements and
assessments into the way they procure AI tools. In particular, we investigate
two implemented checklists: the Canadian Directive on Automated Decision-Making
(CDADM) and the World Economic Forum's AI Procurement in a Box (WEF). We detail
three key pitfalls around expertise, risk frameworks and transparency, that can
decrease the efficacy of regulations aimed at government AI use and suggest
avenues for improvement.

Journals and conferences worry that peer reviews assisted by artificial
intelligence (AI), in particular, large language models (LLMs), may negatively
influence the validity and fairness of the peer-review system, a cornerstone of
modern science. In this work, we address this concern with a quasi-experimental
study of the prevalence and impact of AI-assisted peer reviews in the context
of the 2024 International Conference on Learning Representations (ICLR), a
large and prestigious machine-learning conference. Our contributions are
threefold. Firstly, we obtain a lower bound for the prevalence of AI-assisted
reviews at ICLR 2024 using the GPTZero LLM detector, estimating that at least
$15.8\%$ of reviews were written with AI assistance. Secondly, we estimate the
impact of AI-assisted reviews on submission scores. Considering pairs of
reviews with different scores assigned to the same paper, we find that in
$53.4\%$ of pairs the AI-assisted review scores higher than the human review
($p = 0.002$; relative difference in probability of scoring higher: $+14.4\%$
in favor of AI-assisted reviews). Thirdly, we assess the impact of receiving an
AI-assisted peer review on submission acceptance. In a matched study,
submissions near the acceptance threshold that received an AI-assisted peer
review were $4.9$ percentage points ($p = 0.024$) more likely to be accepted
than submissions that did not. Overall, we show that AI-assisted reviews are
consequential to the peer-review process and offer a discussion on future
implications of current trends

As Artificial Intelligence (AI) techniques have become more powerful and
easier to use they are increasingly deployed as key components of modern
software systems. While this enables new functionality and often allows better
adaptation to user needs it also creates additional problems for software
engineers and exposes companies to new risks. Some work has been done to better
understand the interaction between Software Engineering and AI but we lack
methods to classify ways of applying AI in software systems and to analyse and
understand the risks this poses. Only by doing so can we devise tools and
solutions to help mitigate them. This paper presents the AI in SE Application
Levels (AI-SEAL) taxonomy that categorises applications according to their
point of AI application, the type of AI technology used and the automation
level allowed. We show the usefulness of this taxonomy by classifying 15 papers
from previous editions of the RAISE workshop. Results show that the taxonomy
allows classification of distinct AI applications and provides insights
concerning the risks associated with them. We argue that this will be important
for companies in deciding how to apply AI in their software applications and to
create strategies for its use.

The young field of AI Safety is still in the process of identifying its
challenges and limitations. In this paper, we formally describe one such
impossibility result, namely Unpredictability of AI. We prove that it is
impossible to precisely and consistently predict what specific actions a
smarter-than-human intelligent system will take to achieve its objectives, even
if we know terminal goals of the system. In conclusion, impact of
Unpredictability on AI Safety is discussed.

AI Ethics is now a global topic of discussion in academic and policy circles.
At least 84 public-private initiatives have produced statements describing
high-level principles, values, and other tenets to guide the ethical
development, deployment, and governance of AI. According to recent
meta-analyses, AI Ethics has seemingly converged on a set of principles that
closely resemble the four classic principles of medical ethics. Despite the
initial credibility granted to a principled approach to AI Ethics by the
connection to principles in medical ethics, there are reasons to be concerned
about its future impact on AI development and governance. Significant
differences exist between medicine and AI development that suggest a principled
approach in the latter may not enjoy success comparable to the former. Compared
to medicine, AI development lacks (1) common aims and fiduciary duties, (2)
professional history and norms, (3) proven methods to translate principles into
practice, and (4) robust legal and professional accountability mechanisms.
These differences suggest we should not yet celebrate consensus around
high-level principles that hide deep political and normative disagreement.

Recently, a lot of attention has been given to undesired consequences of
Artificial Intelligence (AI), such as unfair bias leading to discrimination, or
the lack of explanations of the results of AI systems. There are several
important questions to answer before AI can be deployed at scale in our
businesses and societies. Most of these issues are being discussed by experts
and the wider communities, and it seems there is broad consensus on where they
come from. There is, however, less consensus on, and experience with how to
practically deal with those issues in organizations that develop and use AI,
both from a technical and organizational perspective. In this paper, we discuss
the practical case of a large organization that is putting in place a
company-wide methodology to minimize the risk of undesired consequences of AI.
We hope that other organizations can learn from this and that our experience
contributes to making the best of AI while minimizing its risks.

Solidarity is one of the fundamental values at the heart of the construction
of peaceful societies and present in more than one third of world's
constitutions. Still, solidarity is almost never included as a principle in
ethical guidelines for the development of AI. Solidarity as an AI principle (1)
shares the prosperity created by AI, implementing mechanisms to redistribute
the augmentation of productivity for all; and shares the burdens, making sure
that AI does not increase inequality and no human is left behind. Solidarity as
an AI principle (2) assesses the long term implications before developing and
deploying AI systems so no groups of humans become irrelevant because of AI
systems. Considering solidarity as a core principle for AI development will
provide not just an human-centric but a more humanity-centric approach to AI.

As Artificial Intelligence (AI) technology gets more intertwined with every
system, people are using AI to make decisions on their everyday activities. In
simple contexts, such as Netflix recommendations, or in more complex context
like in judicial scenarios, AI is part of people's decisions. People make
decisions and usually, they need to explain their decision to others or in some
matter. It is particularly critical in contexts where human expertise is
central to decision-making. In order to explain their decisions with AI
support, people need to understand how AI is part of that decision. When
considering the aspect of fairness, the role that AI has on a decision-making
process becomes even more sensitive since it affects the fairness and the
responsibility of those people making the ultimate decision. We have been
exploring an evidence-based explanation design approach to 'tell the story of a
decision'. In this position paper, we discuss our approach for AI systems using
fairness sensitive cases in the literature.

With increasing ubiquity of artificial intelligence (AI) in modern societies,
individual countries and the international community are working hard to create
an innovation-friendly, yet safe, regulatory environment. Adequate regulation
is key to maximize the benefits and minimize the risks stemming from AI
technologies. Developing regulatory frameworks is, however, challenging due to
AI's global reach and the existence of widespread misconceptions about the
notion of regulation. We argue that AI-related challenges cannot be tackled
effectively without sincere international coordination supported by robust,
consistent domestic and international governance arrangements. Against this
backdrop, we propose the establishment of an international AI governance
framework organized around a new AI regulatory agency that -- drawing on
interdisciplinary expertise -- could help creating uniform standards for the
regulation of AI technologies and inform the development of AI policies around
the world. We also believe that a fundamental change of mindset on what
constitutes regulation is necessary to remove existing barriers that hamper
contemporary efforts to develop AI regulatory regimes, and put forward some
recommendations on how to achieve this, and what opportunities doing so would
present.

Despite the promises of data-driven artificial intelligence (AI), little is
known about how we can bridge the gulf between traditional physician-driven
diagnosis and a plausible future of medicine automated by AI. Specifically, how
can we involve AI usefully in physicians' diagnosis workflow given that most AI
is still nascent and error-prone (e.g., in digital pathology)? To explore this
question, we first propose a series of collaborative techniques to engage human
pathologists with AI given AI's capabilities and limitations, based on which we
prototype Impetus - a tool where an AI takes various degrees of initiatives to
provide various forms of assistance to a pathologist in detecting tumors from
histological slides. We summarize observations and lessons learned from a study
with eight pathologists and discuss recommendations for future work on
human-centered medical AI systems.

Artificial intelligence (AI) and machine learning (ML) have become
increasingly vital in the development of novel defense and intelligence
capabilities across all domains of warfare. An adversarial AI (A2I) and
adversarial ML (AML) attack seeks to deceive and manipulate AI/ML models. It is
imperative that AI/ML models can defend against these attacks. A2I/AML defenses
will help provide the necessary assurance of these advanced capabilities that
use AI/ML models. The A2I Working Group (A2IWG) seeks to advance the research
and development of assured AI/ML capabilities via new A2I/AML defenses by
fostering a collaborative environment across the U.S. Department of Defense and
U.S. Intelligence Community. The A2IWG aims to identify specific challenges
that it can help solve or address more directly, with initial focus on three
topics: AI Trusted Robustness, AI System Security, and AI/ML Architecture
Vulnerabilities.

Neural nets, one of the oldest architectures for AI programming, are loosely
based on biological neurons and their properties. Recent work on language
applications has made the AI code closer to biological reality in several ways.
This commentary examines this convergence and, in light of what is known of
neocortical structure, addresses the question of whether ``general AI'' looks
attainable with these tools.

Several seminal ethics initiatives have stipulated sets of principles and
standards for good technology development in the AI sector. However, widespread
criticism has pointed out a lack of practical realization of these principles.
Following that, AI ethics underwent a practical turn, but without deviating
from the principled approach and the many shortcomings associated with it. This
paper proposes a different approach. It defines four basic AI virtues, namely
justice, honesty, responsibility and care, all of which represent specific
motivational settings that constitute the very precondition for ethical
decision making in the AI field. Moreover, it defines two second-order AI
virtues, prudence and fortitude, that bolster achieving the basic virtues by
helping with overcoming bounded ethicality or the many hidden psychological
forces that impair ethical decision making and that are hitherto disregarded in
AI ethics. Lastly, the paper describes measures for successfully cultivating
the mentioned virtues in organizations dealing with AI research and
development.

As AI becomes integrated throughout the world, its potential for impact
within low-resource regions around the Global South have grown. AI research
labs from tech giants like Microsoft, Google, and IBM have a significant
presence in countries such as India, Ghana, and South Africa. The work done by
these labs is often motivated by the potential impact it could have on local
populations, but the deployment of these tools has not always gone smoothly.
This paper presents a case study examining the deployment of AI by large
industry labs situated in low-resource contexts, highlights factors impacting
unanticipated deployments, and reflects on the state of AI deployment within
the Global South, providing suggestions that embrace inclusive design
methodologies within AI development that prioritize the needs of marginalized
communities and elevate their status not just as beneficiaries of AI systems
but as primary stakeholders.

The development of AI applications is a multidisciplinary effort, involving
multiple roles collaborating with the AI developers, an umbrella term we use to
include data scientists and other AI-adjacent roles on the same team. During
these collaborations, there is a knowledge mismatch between AI developers, who
are skilled in data science, and external stakeholders who are typically not.
This difference leads to communication gaps, and the onus falls on AI
developers to explain data science concepts to their collaborators. In this
paper, we report on a study including analyses of both interviews with AI
developers and artifacts they produced for communication. Using the analytic
lens of shared mental models, we report on the types of communication gaps that
AI developers face, how AI developers communicate across disciplinary and
organizational boundaries, and how they simultaneously manage issues regarding
trust and expectations.

The number and importance of AI-based systems in all domains is growing. With
the pervasive use and the dependence on AI-based systems, the quality of these
systems becomes essential for their practical usage. However, quality assurance
for AI-based systems is an emerging area that has not been well explored and
requires collaboration between the SE and AI research communities. This paper
discusses terminology and challenges on quality assurance for AI-based systems
to set a baseline for that purpose. Therefore, we define basic concepts and
characterize AI-based systems along the three dimensions of artifact type,
process, and quality characteristics. Furthermore, we elaborate on the key
challenges of (1) understandability and interpretability of AI models, (2) lack
of specifications and defined requirements, (3) need for validation data and
test input generation, (4) defining expected outcomes as test oracles, (5)
accuracy and correctness measures, (6) non-functional properties of AI-based
systems, (7) self-adaptive and self-learning characteristics, and (8) dynamic
and frequently changing environments.

Over the past three years we have built a practice-oriented, bachelor level,
educational programme for software engineers to specialize as AI engineers. The
experience with this programme and the practical assignments our students
execute in industry has given us valuable insights on the profession of AI
engineer. In this paper we discuss our programme and the lessons learned for
industry and research.

Along with the development of modern computing technology and social
sciences, both theoretical research and practical applications of social
computing have been continuously extended. In particular with the boom of
artificial intelligence (AI), social computing is significantly influenced by
AI. However, the conventional technologies of AI have drawbacks in dealing with
more complicated and dynamic problems. Such deficiency can be rectified by
hybrid human-artificial intelligence (H-AI) which integrates both human
intelligence and AI into one unity, forming a new enhanced intelligence. H-AI
in dealing with social problems shows the advantages that AI can not surpass.
This paper firstly introduces the concept of H-AI. AI is the intelligence in
the transition stage of H-AI, so the latest research progresses of AI in social
computing are reviewed. Secondly, it summarizes typical challenges faced by AI
in social computing, and makes it possible to introduce H-AI to solve these
challenges. Finally, the paper proposes a holistic framework of social
computing combining with H-AI, which consists of four layers: object layer,
base layer, analysis layer, and application layer. It represents H-AI has
significant advantages over AI in solving social problems.

In this paper, we demonstrate the design of efficient and high-performance
AI/Deep Learning accelerators with customized STT-MRAM and a reconfigurable
core. Based on model-driven detailed design space exploration, we present the
design methodology of an innovative scratchpad-assisted on-chip STT-MRAM based
buffer system for high-performance accelerators. Using analytically derived
expression of memory occupancy time of AI model weights and activation maps,
the volatility of STT-MRAM is adjusted with process and temperature variation
aware scaling of thermal stability factor to optimize the retention time,
energy, read/write latency, and area of STT-MRAM. From the analysis of modern
AI workloads and accelerator implementation in 14nm technology, we verify the
efficacy of our designed AI accelerator with STT-MRAM STT-AI. Compared to an
SRAM-based implementation, the STT-AI accelerator achieves 75% area and 3%
power savings at iso-accuracy. Furthermore, with a relaxed bit error rate and
negligible AI accuracy trade-off, the designed STT-AI Ultra accelerator
achieves 75.4%, and 3.5% savings in area and power, respectively over regular
SRAM-based accelerators.

In this work, we survey skepticism regarding AI risk and show parallels with
other types of scientific skepticism. We start by classifying different types
of AI Risk skepticism and analyze their root causes. We conclude by suggesting
some intervention approaches, which may be successful in reducing AI risk
skepticism, at least amongst artificial intelligence researchers.

What we expect from radiology AI algorithms will shape the selection and
implementation of AI in the radiologic practice. In this paper I consider
prevailing expectations of AI and compare them to expectations that we have of
human readers. I observe that the expectations from AI and radiologists are
fundamentally different. The expectations of AI are based on a strong and
justified mistrust about the way that AI makes decisions. Because AI decisions
are not well understood, it is difficult to know how the algorithms will behave
in new, unexpected situations. However, this mistrust is not mirrored in our
expectations of human readers. Despite well-proven idiosyncrasies and biases in
human decision making, we take comfort from the assumption that others make
decisions in a way as we do, and we trust our own decision making. Despite poor
ability to explain decision making processes in humans, we accept explanations
of decisions given by other humans. Because the goal of radiology is the most
accurate radiologic interpretation, our expectations of radiologists and AI
should be similar, and both should reflect a healthy mistrust of complicated
and partially opaque decision processes undergoing in computer algorithms and
human brains. This is generally not the case now.

As artificial intelligence (AI) systems are increasingly deployed, principles
for ethical AI are also proliferating. Certification offers a method to both
incentivize adoption of these principles and substantiate that they have been
implemented in practice. This paper draws from management literature on
certification and reviews current AI certification programs and proposals.
Successful programs rely on both emerging technical methods and specific design
considerations. In order to avoid two common failures of certification, program
designs should ensure that the symbol of the certification is substantially
implemented in practice and that the program achieves its stated goals. The
review indicates that the field currently focuses on self-certification and
third-party certification of systems, individuals, and organizations - to the
exclusion of process management certifications. Additionally, the paper
considers prospects for future AI certification programs. Ongoing changes in AI
technology suggest that AI certification regimes should be designed to
emphasize governance criteria of enduring value, such as ethics training for AI
developers, and to adjust technical criteria as the technology changes.
Overall, certification can play a valuable mix in the portfolio of AI
governance tools.

In recent years, there has been an increased emphasis on understanding and
mitigating adverse impacts of artificial intelligence (AI) technologies on
society. Across academia, industry, and government bodies, a variety of
endeavours are being pursued towards enhancing AI ethics. A significant
challenge in the design of ethical AI systems is that there are multiple
stakeholders in the AI pipeline, each with their own set of constraints and
interests. These different perspectives are often not understood, due in part
to communication gaps.For example, AI researchers who design and develop AI
models are not necessarily aware of the instability induced in consumers' lives
by the compounded effects of AI decisions. Educating different stakeholders
about their roles and responsibilities in the broader context becomes
necessary. In this position paper, we outline some potential ways in which
generative artworks can play this role by serving as accessible and powerful
educational tools for surfacing different perspectives. We hope to spark
interdisciplinary discussions about computational creativity broadly as a tool
for enhancing AI ethics.

The received wisdom is that artificial intelligence (AI) is a competition
between the US and China. In this chapter, the author will examine how the
European Union (EU) fits into that mix and what it can offer as a third way to
govern AI. The chapter presents this by exploring the past, present and future
of AI governance in the EU. Section 1 serves to explore and evidence the EUs
coherent and comprehensive approach to AI governance. In short, the EU ensures
and encourages ethical, trustworthy and reliable technological development.
This will cover a range of key documents and policy tools that lead to the most
crucial effort of the EU to date: to regulate AI. Section 2 maps the EUs drive
towards digital sovereignty through the lens of regulation and infrastructure.
This covers topics such as the trustworthiness of AI systems, cloud, compute
and foreign direct investment. In Section 3, the chapter concludes by offering
several considerations to achieve good AI governance in the EU.

Artificial intelligence (AI) systems operate in increasingly diverse areas,
from healthcare to facial recognition, the stock market, autonomous vehicles,
and so on. While the underlying digital infrastructure of AI systems is
developing rapidly, each area of implementation is subject to different degrees
and processes of legitimization. By combining elements from institutional
theory and information systems-theory, this paper presents a conceptual
framework to analyze and understand AI-induced field-change. The introduction
of novel AI-agents into new or existing fields creates a dynamic in which
algorithms (re)shape organizations and institutions while existing
institutional infrastructures determine the scope and speed at which
organizational change is allowed to occur. Where institutional infrastructure
and governance arrangements, such as standards, rules, and regulations, still
are unelaborate, the field can move fast but is also more likely to be
contested. The institutional infrastructure surrounding AI-induced fields is
generally little elaborated, which could be an obstacle to the broader
institutionalization of AI-systems going forward.

Drawing on our experience of more than a decade of AI in academic research,
technology development, industry engagement, postgraduate teaching, doctoral
supervision and organisational consultancy, we present the 'CDAC AI Life
Cycle', a comprehensive life cycle for the design, development and deployment
of Artificial Intelligence (AI) systems and solutions. It consists of three
phases, Design, Develop and Deploy, and 17 constituent stages across the three
phases from conception to production of any AI initiative. The 'Design' phase
highlights the importance of contextualising a problem description by reviewing
public domain and service-based literature on state-of-the-art AI applications,
algorithms, pre-trained models and equally importantly ethics guidelines and
frameworks, which then informs the data, or Big Data, acquisition and
preparation. The 'Develop' phase is technique-oriented, as it transforms data
and algorithms into AI models that are benchmarked, evaluated and explained.
The 'Deploy' phase evaluates computational performance, which then apprises
pipelines for model operationalisation, culminating in the hyperautomation of a
process or system as a complete AI solution, that is continuously monitored and
evaluated to inform the next iteration of the life cycle. An ontological
mapping of AI algorithms to applications, followed by an organisational context
for the AI life cycle are further contributions of this article.

Medical students will almost inevitably encounter powerful medical AI systems
early in their careers. Yet, contemporary medical education does not adequately
equip students with the basic clinical proficiency in medical AI needed to use
these tools safely and effectively. Education reform is urgently needed, but
not easily implemented, largely due to an already jam-packed medical curricula.
In this article, we propose an education reform framework as an effective and
efficient solution, which we call the Embedded AI Ethics Education Framework.
Unlike other calls for education reform to accommodate AI teaching that are
more radical in scope, our framework is modest and incremental. It leverages
existing bioethics or medical ethics curricula to develop and deliver content
on the ethical issues associated with medical AI, especially the harms of
technology misuse, disuse, and abuse that affect the risk-benefit analyses at
the heart of healthcare. In doing so, the framework provides a simple tool for
going beyond the "What?" and the "Why?" of medical AI ethics education, to
answer the "How?", giving universities, course directors, and/or professors a
broad road-map for equipping their students with the necessary clinical
proficiency in medical AI.

Anatoly Karpov's Queen sacrifices are analyzed. Stockfish 14 NNUE -- an AI
chess engine -- evaluates how efficient Karpov's sacrifices are. For
comparative purposes, we provide a dataset on Karpov's Rook and Knight
sacrifices to test whether Karpov achieves a similar level of accuracy. Our
study has implications for human-AI interaction and how humans can better
understand the strategies employed by black-box AI algorithms. Finally, we
conclude with implications for human study in. chess with computer engines.

Parallel to the rising debates over sustainable energy and artificial
intelligence solutions, the world is currently discussing the ethics of
artificial intelligence and its possible negative effects on society and the
environment. In these arguments, sustainable AI is proposed, which aims at
advancing the pathway toward sustainability, such as sustainable energy. In
this paper, we offered a novel contextual topic modeling combining LDA, BERT,
and Clustering. We then combined these computational analyses with content
analysis of related scientific publications to identify the main scholarly
topics, sub-themes, and cross-topic themes within scientific research on
sustainable AI in energy. Our research identified eight dominant topics
including sustainable buildings, AI-based DSSs for urban water management,
climate artificial intelligence, Agriculture 4, the convergence of AI with IoT,
AI-based evaluation of renewable technologies, smart campus and engineering
education, and AI-based optimization. We then recommended 14 potential future
research strands based on the observed theoretical gaps. Theoretically, this
analysis contributes to the existing literature on sustainable AI and
sustainable energy, and practically, it intends to act as a general guide for
energy engineers and scientists, AI scientists, and social scientists to widen
their knowledge of sustainability in AI and energy convergence research.

Trustworthy artificial intelligence (AI) has become an important topic
because trust in AI systems and their creators has been lost. Researchers,
corporations, and governments have long and painful histories of excluding
marginalized groups from technology development, deployment, and oversight. As
a result, these technologies are less useful and even harmful to minoritized
groups. We argue that any AI development, deployment, and monitoring framework
that aspires to trust must incorporate both feminist, non-exploitative
participatory design principles and strong, outside, and continual monitoring
and testing. We additionally explain the importance of considering aspects of
trustworthiness beyond just transparency, fairness, and accountability,
specifically, to consider justice and shifting power to the disempowered as
core values to any trustworthy AI system. Creating trustworthy AI starts by
funding, supporting, and empowering grassroots organizations like Queer in AI
so the field of AI has the diversity and inclusion to credibly and effectively
develop trustworthy AI. We leverage the expert knowledge Queer in AI has
developed through its years of work and advocacy to discuss if and how gender,
sexuality, and other aspects of queer identity should be used in datasets and
AI systems and how harms along these lines should be mitigated. Based on this,
we share a gendered approach to AI and further propose a queer epistemology and
analyze the benefits it can bring to AI. We additionally discuss how to
regulate AI with this queer epistemology in vision, proposing frameworks for
making policies related to AI & gender diversity and privacy & queer data
protection.

Although artificial intelligence (AI) is solving real-world challenges and
transforming industries, there are serious concerns about its ability to behave
and make decisions in a responsible way. Many AI ethics principles and
guidelines for responsible AI have been recently issued by governments,
organisations, and enterprises. However, these AI ethics principles and
guidelines are typically high-level and do not provide concrete guidance on how
to design and develop responsible AI systems. To address this shortcoming, we
first present an empirical study where we interviewed 21 scientists and
engineers to understand the practitioners' perceptions on AI ethics principles
and their implementation. We then propose a template that enables AI ethics
principles to be operationalised in the form of concrete patterns and suggest a
list of patterns using the newly created template. These patterns provide
concrete, operationalised guidance that facilitate the development of
responsible AI systems.

Recent advances in artificial intelligence (AI) for quantitative trading have
led to its general superhuman performance in significant trading performance.
However, the potential risk of AI trading is a "black box" decision. Some AI
computing mechanisms are complex and challenging to understand. If we use AI
without proper supervision, AI may lead to wrong choices and make huge losses.
Hence, we need to ask about the AI "black box", including why did AI decide to
do this or not? Why can people trust AI or not? How can people fix their
mistakes? These problems also highlight the challenges that AI technology can
explain in the trading field.

There has been significant recent interest in developing AI agents capable of
effectively interacting and teaming with humans. While each of these works try
to tackle a problem quite central to the problem of human-AI interaction, they
tend to rely on myopic formulations that obscure the possible inter-relatedness
and complementarity of many of these works. The human-aware AI framework was a
recent effort to provide a unified account for human-AI interaction by casting
them in terms of their relationship to various mental models. Unfortunately,
the current accounts of human-aware AI are insufficient to explain the
landscape of the work doing in the space of human-AI interaction due to their
focus on limited settings. In this paper, we aim to correct this shortcoming by
introducing a significantly general version of human-aware AI interaction
scheme, called generalized human-aware interaction (GHAI), that talks about
(mental) models of six types. Through this paper, we will see how this new
framework allows us to capture the various works done in the space of human-AI
interaction and identify the fundamental behavioral patterns supported by these
works. We will also use this framework to identify potential gaps in the
current literature and suggest future research directions to address these
shortcomings.

Although AI has significant potential to transform society, there are serious
concerns about its ability to behave and make decisions responsibly. Many
ethical regulations, principles, and guidelines for responsible AI have been
issued recently. However, these principles are high-level and difficult to put
into practice. In the meantime much effort has been put into responsible AI
from the algorithm perspective, but they are limited to a small subset of
ethical principles amenable to mathematical analysis. Responsible AI issues go
beyond data and algorithms and are often at the system-level crosscutting many
system components and the entire software engineering lifecycle. Based on the
result of a systematic literature review, this paper identifies one missing
element as the system-level guidance - how to design the architecture of
responsible AI systems. We present a summary of design patterns that can be
embedded into the AI systems as product features to contribute to
responsible-AI-by-design.

Conversational artificial intelligence (AI) is becoming an increasingly
popular topic among industry and academia. With the fast development of neural
network-based models, a lot of neural-based conversational AI system are
developed. We will provide a brief review of the recent progress in the
Conversational AI, including the commonly adopted techniques, notable works,
famous competitions from academia and industry and widely used datasets.

Recent works have recognized the need for human-centered perspectives when
designing and evaluating human-AI interactions and explainable AI methods. Yet,
current approaches fall short at intercepting and managing unexpected user
behavior resulting from the interaction with AI systems and explainability
methods of different stake-holder groups. In this work, we explore the use of
AI and explainability methods in the insurance domain. In an qualitative case
study with participants with different roles and professional backgrounds, we
show that AI and explainability methods are used in creative ways in daily
workflows, resulting in a divergence between their intended and actual use.
Finally, we discuss some recommendations for the design of human-AI
interactions and explainable AI methods to manage the risks and harness the
potential of unexpected user behavior.

As artificial intelligence (AI) becomes more powerful and widespread, the AI
alignment problem - how to ensure that AI systems pursue the goals that we want
them to pursue - has garnered growing attention. This article distinguishes two
types of alignment problems depending on whose goals we consider, and analyzes
the different solutions necessitated by each. The direct alignment problem
considers whether an AI system accomplishes the goals of the entity operating
it. In contrast, the social alignment problem considers the effects of an AI
system on larger groups or on society more broadly. In particular, it also
considers whether the system imposes externalities on others. Whereas solutions
to the direct alignment problem center around more robust implementation,
social alignment problems typically arise because of conflicts between
individual and group-level goals, elevating the importance of AI governance to
mediate such conflicts. Addressing the social alignment problem requires both
enforcing existing norms on their developers and operators and designing new
norms that apply directly to AI systems.

Artificial intelligence (AI), which enables machines to learn to perform a
task by training on diverse datasets, is one of the most revolutionary
developments in scientific history. Although AI and especially deep learning is
relatively new, it has already had transformative impact on medicine, biology,
transportation, entertainment, and beyond. As AI changes our daily lives at an
increasingly fast pace, we are challenged with preparing our society for an
AI-driven future. To this end, a critical step is to ensure an AI-ready
workforce through education. Advocates of beginning instruction of AI basics at
the K-12 level typically note benefits to the workforce, economy, and national
security. In this complementary perspective, we discuss why learning AI is
beneficial for motivating students and promoting creative thinking, and how to
develop a module-based approach that optimizes learning outcomes. We hope to
excite and engage more members of the education community to join the effort to
advance K-12 AI education in the USA and worldwide.

The recent spike in certified Artificial Intelligence (AI) tools for
healthcare has renewed the debate around adoption of this technology. One
thread of such debate concerns Explainable AI and its promise to render AI
devices more transparent and trustworthy. A few voices active in the medical AI
space have expressed concerns on the reliability of Explainable AI techniques,
questioning their use and inclusion in guidelines and standards. Revisiting
such criticisms, this article offers a balanced and comprehensive perspective
on the utility of Explainable AI, focusing on the specificity of clinical
applications of AI and placing them in the context of healthcare interventions.
Against its detractors and despite valid concerns, we argue that the
Explainable AI research program is still central to human-machine interaction
and ultimately our main tool against loss of control, a danger that cannot be
prevented by rigorous clinical validation alone.

Human-AI collaboration for decision-making strives to achieve team
performance that exceeds the performance of humans or AI alone. However, many
factors can impact success of Human-AI teams, including a user's domain
expertise, mental models of an AI system, trust in recommendations, and more.
This work examines users' interaction with three simulated algorithmic models,
all with similar accuracy but different tuning on their true positive and true
negative rates. Our study examined user performance in a non-trivial blood
vessel labeling task where participants indicated whether a given blood vessel
was flowing or stalled.
  Our results show that while recommendations from an AI-Assistant can aid user
decision making, factors such as users' baseline performance relative to the AI
and complementary tuning of AI error types significantly impact overall team
performance. Novice users improved, but not to the accuracy level of the AI.
Highly proficient users were generally able to discern when they should follow
the AI recommendation and typically maintained or improved their performance.
Mid-performers, who had a similar level of accuracy to the AI, were most
variable in terms of whether the AI recommendations helped or hurt their
performance. In addition, we found that users' perception of the AI's
performance relative on their own also had a significant impact on whether
their accuracy improved when given AI recommendations. This work provides
insights on the complexity of factors related to Human-AI collaboration and
provides recommendations on how to develop human-centered AI algorithms to
complement users in decision-making tasks.

The quality of Artificial Intelligence (AI) algorithms is of significant
importance for confidently adopting algorithms in various applications such as
cybersecurity, healthcare, and autonomous driving. This work presents a
principled framework of using a design-of-experimental approach to
systematically evaluate the quality of AI algorithms, named as Do-AIQ.
Specifically, we focus on investigating the quality of the AI mislabel data
algorithm against data poisoning. The performance of AI algorithms is affected
by hyperparameters in the algorithm and data quality, particularly, data
mislabeling, class imbalance, and data types. To evaluate the quality of the AI
algorithms and obtain a trustworthy assessment on the quality of the
algorithms, we establish a design-of-experiment framework to construct an
efficient space-filling design in a high-dimensional constraint space and
develop an effective surrogate model using additive Gaussian process to enable
the emulation of the quality of AI algorithms. Both theoretical and numerical
studies are conducted to justify the merits of the proposed framework. The
proposed framework can set an exemplar for AI algorithm to enhance the AI
assurance of robustness, reproducibility, and transparency.

AI/ML for data centres and data centres for AI/ML are defining new trends in
cloud computing. Disaggregated heterogeneous reconfigurable computing systems
realized by photonic interconnects and photonic switching expect greatly
enhanced throughput and energy-efficiency for AI/ML workloads, especially when
aided by an AI/ML control plane.

While revolutionary AI-powered code generation tools have been rising
rapidly, we know little about how and how to help software developers form
appropriate trust in those AI tools. Through a two-phase formative study, we
investigate how online communities shape developers' trust in AI tools and how
we can leverage community features to facilitate appropriate user trust.
Through interviewing 17 developers, we find that developers collectively make
sense of AI tools using the experiences shared by community members and
leverage community signals to evaluate AI suggestions. We then surface design
opportunities and conduct 11 design probe sessions to explore the design space
of using community features to support user trust in AI code generation
systems. We synthesize our findings and extend an existing model of user trust
in AI technologies with sociotechnical factors. We map out the design
considerations for integrating user community into the AI code generation
experience.

AI explanations are often mentioned as a way to improve human-AI
decision-making, but empirical studies have not found consistent evidence of
explanations' effectiveness and, on the contrary, suggest that they can
increase overreliance when the AI system is wrong. While many factors may
affect reliance on AI support, one important factor is how decision-makers
reconcile their own intuition -- beliefs or heuristics, based on prior
knowledge, experience, or pattern recognition, used to make judgments -- with
the information provided by the AI system to determine when to override AI
predictions. We conduct a think-aloud, mixed-methods study with two explanation
types (feature- and example-based) for two prediction tasks to explore how
decision-makers' intuition affects their use of AI predictions and
explanations, and ultimately their choice of when to rely on AI. Our results
identify three types of intuition involved in reasoning about AI predictions
and explanations: intuition about the task outcome, features, and AI
limitations. Building on these, we summarize three observed pathways for
decision-makers to apply their own intuition and override AI predictions. We
use these pathways to explain why (1) the feature-based explanations we used
did not improve participants' decision outcomes and increased their
overreliance on AI, and (2) the example-based explanations we used improved
decision-makers' performance over feature-based explanations and helped achieve
complementary human-AI performance. Overall, our work identifies directions for
further development of AI decision-support systems and explanation methods that
help decision-makers effectively apply their intuition to achieve appropriate
reliance on AI.

Artificial intelligence (AI) presents new challenges for the user experience
(UX) of products and services. Recently, practitioner-facing resources and
design guidelines have become available to ease some of these challenges.
However, little research has investigated if and how these guidelines are used,
and how they impact practice. In this paper, we investigated how industry
practitioners use the People + AI Guidebook. We conducted interviews with 31
practitioners (i.e., designers, product managers) to understand how they use
human-AI guidelines when designing AI-enabled products. Our findings revealed
that practitioners use the guidebook not only for addressing AI's design
challenges, but also for education, cross-functional communication, and for
developing internal resources. We uncovered that practitioners desire more
support for early phase ideation and problem formulation to avoid AI product
failures. We discuss the implications for future resources aiming to help
practitioners in designing AI products.

Despite the widespread use of artificial intelligence (AI), designing user
experiences (UX) for AI-powered systems remains challenging. UX designers face
hurdles understanding AI technologies, such as pre-trained language models, as
design materials. This limits their ability to ideate and make decisions about
whether, where, and how to use AI. To address this problem, we bridge the
literature on AI design and AI transparency to explore whether and how
frameworks for transparent model reporting can support design ideation with
pre-trained models. By interviewing 23 UX practitioners, we find that
practitioners frequently work with pre-trained models, but lack support for
UX-led ideation. Through a scenario-based design task, we identify common goals
that designers seek model understanding for and pinpoint their model
transparency information needs. Our study highlights the pivotal role that UX
designers can play in Responsible AI and calls for supporting their
understanding of AI limitations through model transparency and interrogation.

The emergence of generative AI technologies, such as OpenAI's ChatGPT
chatbot, has expanded the scope of tasks that AI tools can accomplish and
enabled AI-generated creative content. In this study, we explore how disclosure
regarding the use of AI in the creation of creative content affects human
evaluation of such content. In a series of pre-registered experimental studies,
we show that AI disclosure has no meaningful effect on evaluation either for
creative or descriptive short stories, but that AI disclosure has a negative
effect on evaluations for emotionally evocative poems written in the first
person. We interpret this result to suggest that reactions to AI-generated
content may be negative when the content is viewed as distinctly "human." We
discuss the implications of this work and outline planned pathways of research
to better understand whether and when AI disclosure may affect the evaluation
of creative content.

Recent work has proposed artificial intelligence (AI) models that can learn
to decide whether to make a prediction for an instance of a task or to delegate
it to a human by considering both parties' capabilities. In simulations with
synthetically generated or context-independent human predictions, delegation
can help improve the performance of human-AI teams -- compared to humans or the
AI model completing the task alone. However, so far, it remains unclear how
humans perform and how they perceive the task when they are aware that an AI
model delegated task instances to them. In an experimental study with 196
participants, we show that task performance and task satisfaction improve
through AI delegation, regardless of whether humans are aware of the
delegation. Additionally, we identify humans' increased levels of self-efficacy
as the underlying mechanism for these improvements in performance and
satisfaction. Our findings provide initial evidence that allowing AI models to
take over more management responsibilities can be an effective form of human-AI
collaboration in workplaces.

Intersectionality is a critical framework that, through inquiry and praxis,
allows us to examine how social inequalities persist through domains of
structure and discipline. Given AI fairness' raison d'etre of "fairness", we
argue that adopting intersectionality as an analytical framework is pivotal to
effectively operationalizing fairness. Through a critical review of how
intersectionality is discussed in 30 papers from the AI fairness literature, we
deductively and inductively: 1) map how intersectionality tenets operate within
the AI fairness paradigm and 2) uncover gaps between the conceptualization and
operationalization of intersectionality. We find that researchers
overwhelmingly reduce intersectionality to optimizing for fairness metrics over
demographic subgroups. They also fail to discuss their social context and when
mentioning power, they mostly situate it only within the AI pipeline. We: 3)
outline and assess the implications of these gaps for critical inquiry and
praxis, and 4) provide actionable recommendations for AI fairness researchers
to engage with intersectionality in their work by grounding it in AI
epistemology.

Through systematically analyzing the literature on designing AI-based
technologies, we extracted design implications and synthesized them into a
generic human-centered design framework for AI technologies to better support
human needs and mitigate their concerns. When adapting the framework to
children's context, understanding their specific needs, behaviors, experiences,
and social environments is needed. Therefore, we are working on projects to
explore tailored design considerations for children, such as through
investigating children's use of existing AI-based toys and learning
technologies. By participating in the ACM CHI 2023 Workshop on "Child-Centred
AI Design: Definition, Operation, and Considerations," we hope to learn more
about how other researchers in this field approach designing child-centered AI
technologies, exchange ideas on the research landscape of children and AI, and
explore the possibility to develop a practical child-centered design framework
of AI technologies for technology designers and developers.

The ongoing artificial intelligence (AI) revolution has the potential to
change almost every line of work. As AI capabilities continue to improve in
accuracy, robustness, and reach, AI may outperform and even replace human
experts across many valuable tasks. Despite enormous efforts devoted to
understanding AI's impact on labor and the economy and its recent success in
accelerating scientific discovery and progress, we lack a systematic
understanding of how advances in AI may benefit scientific research across
disciplines and fields. Here we develop a measurement framework to estimate
both the direct use of AI and the potential benefit of AI in scientific
research by applying natural language processing techniques to 87.6 million
publications and 7.1 million patents. We find that the use of AI in research
appears widespread throughout the sciences, growing especially rapidly since
2015, and papers that use AI exhibit an impact premium, more likely to be
highly cited both within and outside their disciplines. While almost every
discipline contains some subfields that benefit substantially from AI,
analyzing 4.6 million course syllabi across various educational disciplines, we
find a systematic misalignment between the education of AI and its impact on
research, suggesting the supply of AI talents in scientific disciplines is not
commensurate with AI research demands. Lastly, examining who benefits from AI
within the scientific workforce, we find that disciplines with a higher
proportion of women or black scientists tend to be associated with less
benefit, suggesting that AI's growing impact on research may further exacerbate
existing inequalities in science. As the connection between AI and scientific
research deepens, our findings may have an increasing value, with important
implications for the equity and sustainability of the research enterprise.

Explainable AI (XAI) is often promoted with the idea of helping users
understand how machine learning models function and produce predictions. Still,
most of these benefits are reserved for those with specialized domain
knowledge, such as machine learning developers. Recent research has argued that
making AI explainable can be a viable way of making AI more useful in
real-world contexts, especially within low-resource domains in the Global
South. While AI has transcended borders, a limited amount of work focuses on
democratizing the concept of explainable AI to the "majority world", leaving
much room to explore and develop new approaches within this space that cater to
the distinct needs of users within culturally and socially-diverse regions.
This article introduces the concept of an intercultural ethics approach to AI
explainability. It examines how cultural nuances impact the adoption and use of
technology, the factors that impede how technical concepts such as AI are
explained, and how integrating an intercultural ethics approach in the
development of XAI can improve user understanding and facilitate efficient
usage of these methods.

What role can AI play in supporting and constraining creative coding by
families? To investigate these questions, we built a Wizard of Oz platform to
help families engage in creative coding in partnership with a
researcher-operated AI Friend. We designed a 3 week series of programming
activities with ten children, 7 to 12 years old, and nine parents. Using a
creative self efficacy lens, we observe that families found it easier to
generate game ideas when prompted with questions by AI Friend; parents played a
unique role in guiding children in more complex programming tasks when the AI
Friend failed to help, and children were more encouraged to write code for
novel ideas using the AI friend help. These findings suggest that AI supported
platforms should highlight unique family AI interactions focused on children's
agency and creative self-efficacy.

Artificial intelligence (AI) represents a technological upheaval with the
potential to change human society. Because of its transformative potential, AI
is increasingly becoming subject to regulatory initiatives at the global level.
Yet, so far, scholarship in political science and international relations has
focused more on AI applications than on the emerging architecture of global AI
regulation. The purpose of this article is to outline an agenda for research
into the global governance of AI. The article distinguishes between two broad
perspectives: an empirical approach, aimed at mapping and explaining global AI
governance; and a normative approach, aimed at developing and applying
standards for appropriate global AI governance. The two approaches offer
questions, concepts, and theories that are helpful in gaining an understanding
of the emerging global governance of AI. Conversely, exploring AI as a
regulatory issue offers a critical opportunity to refine existing general
approaches to the study of global governance.

Digital technologies have dramatically accelerated the digital transformation
in process industries, boosted new industrial applications, upgraded the
production system, and enhanced operational efficiency. In contrast, the
challenges and gaps between human and artificial intelligence (AI) have become
more and more prominent, whereas the digital divide in process safety is
aggregating. The study attempts to address the following questions: (i)What is
AI in the process safety context? (ii)What is the difference between AI and
humans in process safety? (iii)How do AI and humans collaborate in process
safety? (iv)What are the challenges and gaps in human-AI collaboration? (v)How
to quantify the risk of human-AI collaboration in process safety? Qualitative
risk analysis based on brainstorming and literature review, and quantitative
risk analysis based on layer of protection analysis (LOPA) and Bayesian network
(BN), were applied to explore and model. The importance of human reliability
should be stressed in the digital age, not usually to increase the reliability
of AI, and human-centered AI design in process safety needs to be propagated.

The merging of human intelligence and artificial intelligence has long been a
subject of interest in both science fiction and academia. In this paper, we
introduce a novel concept in Human-AI interaction called Symbiotic Artificial
Intelligence with Shared Sensory Experiences (SAISSE), which aims to establish
a mutually beneficial relationship between AI systems and human users through
shared sensory experiences. By integrating multiple sensory input channels and
processing human experiences, SAISSE fosters a strong human-AI bond, enabling
AI systems to learn from and adapt to individual users, providing personalized
support, assistance, and enhancement. Furthermore, we discuss the incorporation
of memory storage units for long-term growth and development of both the AI
system and its human user. As we address user privacy and ethical guidelines
for responsible AI-human symbiosis, we also explore potential biases and
inequalities in AI-human symbiosis and propose strategies to mitigate these
challenges. Our research aims to provide a comprehensive understanding of the
SAISSE concept and its potential to effectively support and enhance individual
human users through symbiotic AI systems. This position article aims at
discussing poteintial AI-human interaction related topics within the scientific
community, rather than providing experimental or theoretical results.

This paper discusses and explores the potential and relevance of recent
developments in artificial intelligence (AI) and digital twins for health and
well-being in low-resource African countries. We use the case of public health
emergency response to disease outbreaks and epidemic control. There is
potential to take advantage of the increasing availability of data and
digitization to develop advanced AI methods for analysis and prediction. Using
an AI systems perspective, we review emerging trends in AI systems and digital
twins and propose an initial augmented AI system architecture to illustrate how
an AI system can work with a 3D digital twin to address public health goals. We
highlight scientific knowledge discovery, continual learning, pragmatic
interoperability, and interactive explanation and decision-making as essential
research challenges for AI systems and digital twins.

This article is a short introduction to AI4OPT, the NSF AI Institute for
Advances in Optimization. AI4OPT fuses AI and Optimization, inspired by end-use
cases in supply chains, energy systems, chip design and manufacturing, and
sustainable food systems. AI4OPT also applies its "teaching the teachers"
philosophy to provide longitudinal educational pathways in AI for engineering.

Is artificial intelligence (AI) disrupting jobs and creating unemployment?
Despite many attempts to quantify occupations' exposure to AI, inconsistent
validation obfuscates the relative benefits of each approach. A lack of
disaggregated labor outcome data, including unemployment data, further
exacerbates the issue. Here, we assess which models of AI exposure predict job
separations and unemployment risk using new occupation-level unemployment data
by occupation from each US state's unemployment insurance office spanning 2010
through 2020. Although these AI exposure scores have been used by governments
and industry, we find that individual AI exposure models are not predictive of
unemployment rates, unemployment risk, or job separation rates. However, an
ensemble of those models exhibits substantial predictive power suggesting that
competing models may capture different aspects of AI exposure that collectively
account for AI's variable impact across occupations, regions, and time. Our
results also call for dynamic, context-aware, and validated methods for
assessing AI exposure. Interactive visualizations for this study are available
at https://sites.pitt.edu/~mrfrank/uiRiskDemo/.

The rapidly advancing domain of Explainable Artificial Intelligence (XAI) has
sparked significant interests in developing techniques to make AI systems more
transparent and understandable. Nevertheless, in real-world contexts, the
methods of explainability and their evaluation strategies present numerous
limitations.Moreover, the scope of responsible AI extends beyond just
explainability. In this paper, we explore these limitations and discuss their
implications in a boarder context of responsible AI when considering other
important aspects, including privacy, fairness and contestability.

When working with generative artificial intelligence (AI), users may see
productivity gains, but the AI-generated content may not match their
preferences exactly. To study this effect, we introduce a Bayesian framework in
which heterogeneous users choose how much information to share with the AI,
facing a trade-off between output fidelity and communication cost. We show that
the interplay between these individual-level decisions and AI training may lead
to societal challenges. Outputs may become more homogenized, especially when
the AI is trained on AI-generated content. And any AI bias may become societal
bias. A solution to the homogenization and bias issues is to improve human-AI
interactions, enabling personalized outputs without sacrificing productivity.

This chapter provides a comprehensive discussion on AI regulation in the
European Union, contrasting it with the more sectoral and self-regulatory
approach in the UK. It argues for a hybrid regulatory strategy that combines
elements from both philosophies, emphasizing the need for agility and safe
harbors to ease compliance. The paper examines the AI Act as a pioneering
legislative effort to address the multifaceted challenges posed by AI,
asserting that, while the Act is a step in the right direction, it has
shortcomings that could hinder the advancement of AI technologies. The paper
also anticipates upcoming regulatory challenges, such as the management of
toxic content, environmental concerns, and hybrid threats. It advocates for
immediate action to create protocols for regulated access to high-performance,
potentially open-source AI systems. Although the AI Act is a significant
legislative milestone, it needs additional refinement and global collaboration
for the effective governance of rapidly evolving AI technologies.

Given rapid progress toward advanced AI and risks from frontier AI systems
(advanced AI systems pushing the boundaries of the AI capabilities frontier),
the creation and implementation of AI governance and regulatory schemes
deserves prioritization and substantial investment. However, the status quo is
untenable and, frankly, dangerous. A regulatory gap has permitted AI labs to
conduct research, development, and deployment activities with minimal
oversight. In response, frontier AI system evaluations have been proposed as a
way of assessing risks from the development and deployment of frontier AI
systems. Yet, the budding AI risk evaluation ecosystem faces significant
coordination challenges, such as a limited diversity of evaluators, suboptimal
allocation of effort, and perverse incentives. This paper proposes a solution
in the form of an international consortium for AI risk evaluations, comprising
both AI developers and third-party AI risk evaluators. Such a consortium could
play a critical role in international efforts to mitigate societal-scale risks
from advanced AI, including in managing responsible scaling policies and
coordinated evaluation-based risk response. In this paper, we discuss the
current evaluation ecosystem and its shortcomings, propose an international
consortium for advanced AI risk evaluations, discuss issues regarding its
implementation, discuss lessons that can be learnt from previous international
institutions and existing proposals for international AI governance
institutions, and, finally, we recommend concrete steps to advance the
establishment of the proposed consortium: (i) solicit feedback from
stakeholders, (ii) conduct additional research, (iii) conduct a workshop(s) for
stakeholders, (iv) analyze feedback and create final proposal, (v) solicit
funding, and (vi) create a consortium.

With the increasingly widespread adoption of AI in healthcare, maintaining
the accuracy and reliability of AI models in clinical practice has become
crucial. In this context, we introduce novel methods for monitoring the
performance of radiology AI classification models in practice, addressing the
challenges of obtaining real-time ground truth for performance monitoring. We
propose two metrics - predictive divergence and temporal stability - to be used
for preemptive alerts of AI performance changes. Predictive divergence,
measured using Kullback-Leibler and Jensen-Shannon divergences, evaluates model
accuracy by comparing predictions with those of two supplementary models.
Temporal stability is assessed through a comparison of current predictions
against historical moving averages, identifying potential model decay or data
drift. This approach was retrospectively validated using chest X-ray data from
a single-center imaging clinic, demonstrating its effectiveness in maintaining
AI model reliability. By providing continuous, real-time insights into model
performance, our system ensures the safe and effective use of AI in clinical
decision-making, paving the way for more robust AI integration in healthcare

The fast pace of advances in AI promises to revolutionize various aspects of
knowledge work, extending its influence to daily life and professional fields
alike. We advocate for a paradigm where AI is seen as a collaborative co-pilot,
working under human guidance rather than as a mere tool. Drawing from relevant
research and literature in the disciplines of Human-Computer Interaction and
Human Factors Engineering, we highlight the criticality of maintaining human
oversight in AI interactions. Reflecting on lessons from aviation, we address
the dangers of over-relying on automation, such as diminished human vigilance
and skill erosion. Our paper proposes a design approach that emphasizes active
human engagement, control, and skill enhancement in the AI partnership, aiming
to foster a harmonious, effective, and empowering human-AI relationship. We
particularly call out the critical need to design AI interaction capabilities
and software applications to enable and celebrate the primacy of human agency.
This calls for designs for human-AI partnership that cede ultimate control and
responsibility to the human user as pilot, with the AI co-pilot acting in a
well-defined supporting role.

The past decade has observed a great advancement in AI with deep
learning-based models being deployed in diverse scenarios including
safety-critical applications. As these AI systems become deeply embedded in our
societal infrastructure, the repercussions of their decisions and actions have
significant consequences, making the ethical implications of AI deployment
highly relevant and important. The ethical concerns associated with AI are
multifaceted, including challenging issues of fairness, privacy and data
protection, responsibility and accountability, safety and robustness,
transparency and explainability, and environmental impact. These principles
together form the foundations of ethical AI considerations that concern every
stakeholder in the AI system lifecycle. In light of the present ethical and
future x-risk concerns, governments have shown increasing interest in
establishing guidelines for the ethical deployment of AI. This work unifies the
current and future ethical concerns of deploying AI into society. While we
acknowledge and appreciate the technical surveys for each of the ethical
principles concerned, in this paper, we aim to provide a comprehensive overview
that not only addresses each principle from a technical point of view but also
discusses them from a social perspective.

Since ChatGPT works so well, are we on the cusp of solving science with AI?
Is not AlphaFold2 suggestive that the potential of LLMs in biology and the
sciences more broadly is limitless? Can we use AI itself to bridge the lack of
data in the sciences in order to then train an AI? Herein we present a
discussion of these topics.

As AI adoption accelerates, research on its economic impacts becomes a
salient source to consider for stakeholders of AI policy. Such research is
however still in its infancy, and one in need of review. This paper aims to
accomplish just that and is structured around two main themes. Firstly, the
path towards transformative AI, and secondly the wealth created by it. It is
found that sectors most embedded into global value chains will drive economic
impacts, hence special attention is paid to the international trade
perspective. When it comes to the path towards transformative AI, research is
heterogenous in its predictions, with some predicting rapid, unhindered
adoption, and others taking a more conservative view based on potential
bottlenecks and comparisons to past disruptive technologies. As for wealth
creation, while some agreement is to be found in AI's growth boosting
abilities, predictions on timelines are lacking. Consensus exists however
around the dispersion of AI induced wealth, which is heavily biased towards
developed countries due to phenomena such as anchoring and reduced bargaining
power of developing countries. Finally, a shortcoming of economic growth models
in failing to consider AI risk is discovered. Based on the review, a
calculated, and slower adoption rate of AI technologies is recommended.

In healthcare, artificial intelligence (AI) has been changing the way doctors
and health experts take care of people. This paper will cover how AI is making
major changes in the health care system, especially with nutrition. Various
machine learning and deep learning algorithms have been developed to extract
valuable information from healthcare data which help doctors, nutritionists,
and health experts to make better decisions and make our lifestyle healthy.
This paper provides an overview of the current state of AI applications in
healthcare with a focus on the utilization of AI-driven recommender systems in
nutrition. It will discuss the positive outcomes and challenges that arise when
AI is used in this field. This paper addresses the challenges to develop AI
recommender systems in healthcare, providing a well-rounded perspective on the
complexities. Real-world examples and research findings are presented to
underscore the tangible and significant impact AI recommender systems have in
the field of healthcare, particularly in nutrition. The ongoing efforts of
applying AI in nutrition lay the groundwork for a future where personalized
recommendations play a pivotal role in guiding individuals toward healthier
lifestyles.

The evolution of cybersecurity has spurred the emergence of autonomous threat
hunting as a pivotal paradigm in the realm of AI-driven threat intelligence.
This review navigates through the intricate landscape of autonomous threat
hunting, exploring its significance and pivotal role in fortifying cyber
defense mechanisms. Delving into the amalgamation of artificial intelligence
(AI) and traditional threat intelligence methodologies, this paper delineates
the necessity and evolution of autonomous approaches in combating contemporary
cyber threats. Through a comprehensive exploration of foundational AI-driven
threat intelligence, the review accentuates the transformative influence of AI
and machine learning on conventional threat intelligence practices. It
elucidates the conceptual framework underpinning autonomous threat hunting,
spotlighting its components, and the seamless integration of AI algorithms
within threat hunting processes.. Insightful discussions on challenges
encompassing scalability, interpretability, and ethical considerations in
AI-driven models enrich the discourse. Moreover, through illuminating case
studies and evaluations, this paper showcases real-world implementations,
underscoring success stories and lessons learned by organizations adopting
AI-driven threat intelligence. In conclusion, this review consolidates key
insights, emphasizing the substantial implications of autonomous threat hunting
for the future of cybersecurity. It underscores the significance of continual
research and collaborative efforts in harnessing the potential of AI-driven
approaches to fortify cyber defenses against evolving threats.

Neurosymbolic artificial intelligence (AI) is an emerging branch of AI that
combines the strengths of symbolic AI and sub-symbolic AI. A major drawback of
sub-symbolic AI is that it acts as a "black box", meaning that predictions are
difficult to explain, making the testing & evaluation (T&E) and validation &
verification (V&V) processes of a system that uses sub-symbolic AI a challenge.
Since neurosymbolic AI combines the advantages of both symbolic and
sub-symbolic AI, this survey explores how neurosymbolic applications can ease
the V&V process. This survey considers two taxonomies of neurosymbolic AI,
evaluates them, and analyzes which algorithms are commonly used as the symbolic
and sub-symbolic components in current applications. Additionally, an overview
of current techniques for the T&E and V&V processes of these components is
provided. Furthermore, it is investigated how the symbolic part is used for T&E
and V&V purposes in current neurosymbolic applications. Our research shows that
neurosymbolic AI as great potential to ease the T&E and V&V processes of
sub-symbolic AI by leveraging the possibilities of symbolic AI. Additionally,
the applicability of current T&E and V&V methods to neurosymbolic AI is
assessed, and how different neurosymbolic architectures can impact these
methods is explored. It is found that current T&E and V&V techniques are partly
sufficient to test, evaluate, verify, or validate the symbolic and sub-symbolic
part of neurosymbolic applications independently, while some of them use
approaches where current T&E and V&V methods are not applicable by default, and
adjustments or even new approaches are needed. Our research shows that there is
great potential in using symbolic AI to test, evaluate, verify, or validate the
predictions of a sub-symbolic model, making neurosymbolic AI an interesting
research direction for safe, secure, and trustworthy AI.

The growing popularity of generative artificial intelligence (AI) chatbots
such as ChatGPT is having transformative effects on social media. As the
prevalence of AI-generated content grows, concerns have been raised regarding
privacy and misinformation online. Among social media platforms, Discord
enables AI integrations -- making their primarily "Generation Z" userbase
particularly exposed to AI-generated content. We surveyed Generation Z aged
individuals (n = 335) to evaluate their proficiency in discriminating between
AI-generated and human-authored text on Discord. The investigation employed
one-shot prompting of ChatGPT, disguised as a text message received on the
Discord.com platform. We explore the influence of demographic factors on
ability, as well as participants' familiarity with Discord and artificial
intelligence technologies. We find that Generation Z individuals are unable to
discern between AI and human-authored text (p = 0.011), and that those with
lower self-reported familiarity with Discord demonstrated an improved ability
in identifying human-authored compared to those with self-reported experience
with AI (p << 0.0001). Our results suggest that there is a nuanced relationship
between AI technology and popular modes of communication for Generation Z,
contributing valuable insights into human-computer interactions, digital
communication, and artificial intelligence literacy.

This paper introduces A2C, a multi-stage collaborative decision framework
designed to enable robust decision-making within human-AI teams. Drawing
inspiration from concepts such as rejection learning and learning to defer, A2C
incorporates AI systems trained to recognise uncertainty in their decisions and
defer to human experts when needed. Moreover, A2C caters to scenarios where
even human experts encounter limitations, such as in incident detection and
response in cyber Security Operations Centres (SOC). In such scenarios, A2C
facilitates collaborative explorations, enabling collective resolution of
complex challenges. With support for three distinct decision-making modes in
human-AI teams: Automated, Augmented, and Collaborative, A2C offers a flexible
platform for developing effective strategies for human-AI collaboration. By
harnessing the strengths of both humans and AI, it significantly improves the
efficiency and effectiveness of complex decision-making in dynamic and evolving
environments. To validate A2C's capabilities, we conducted extensive simulative
experiments using benchmark datasets. The results clearly demonstrate that all
three modes of decision-making can be effectively supported by A2C. Most
notably, collaborative exploration by (simulated) human experts and AI achieves
superior performance compared to AI in isolation, underscoring the framework's
potential to enhance decision-making within human-AI teams.

Generative AI applications present unique design challenges. As generative AI
technologies are increasingly being incorporated into mainstream applications,
there is an urgent need for guidance on how to design user experiences that
foster effective and safe use. We present six principles for the design of
generative AI applications that address unique characteristics of generative AI
UX and offer new interpretations and extensions of known issues in the design
of AI applications. Each principle is coupled with a set of design strategies
for implementing that principle via UX capabilities or through the design
process. The principles and strategies were developed through an iterative
process involving literature review, feedback from design practitioners,
validation against real-world generative AI applications, and incorporation
into the design process of two generative AI applications. We anticipate the
principles to usefully inform the design of generative AI applications by
driving actionable design recommendations.

The increasing use of Artificial Intelligence (AI) by students in learning
presents new challenges for assessing their learning outcomes in project-based
learning (PBL). This paper introduces a co-design study to explore the
potential of students' AI usage data as a novel material for PBL assessment. We
conducted workshops with 18 college students, encouraging them to speculate an
alternative world where they could freely employ AI in PBL while needing to
report this process to assess their skills and contributions. Our workshops
yielded various scenarios of students' use of AI in PBL and ways of analyzing
these uses grounded by students' vision of education goal transformation. We
also found students with different attitudes toward AI exhibited distinct
preferences in how to analyze and understand the use of AI. Based on these
findings, we discuss future research opportunities on student-AI interactions
and understanding AI-enhanced learning.

We propose Embodied AI as the next fundamental step in the pursuit of
Artificial General Intelligence, juxtaposing it against current AI
advancements, particularly Large Language Models. We traverse the evolution of
the embodiment concept across diverse fields - philosophy, psychology,
neuroscience, and robotics - to highlight how EAI distinguishes itself from the
classical paradigm of static learning. By broadening the scope of Embodied AI,
we introduce a theoretical framework based on cognitive architectures,
emphasizing perception, action, memory, and learning as essential components of
an embodied agent. This framework is aligned with Friston's active inference
principle, offering a comprehensive approach to EAI development. Despite the
progress made in the field of AI, substantial challenges, such as the
formulation of a novel AI learning theory and the innovation of advanced
hardware, persist. Our discussion lays down a foundational guideline for future
Embodied AI research. Highlighting the importance of creating Embodied AI
agents capable of seamless communication, collaboration, and coexistence with
humans and other intelligent entities within real-world environments, we aim to
steer the AI community towards addressing the multifaceted challenges and
seizing the opportunities that lie ahead in the quest for AGI.

Recent advancements in HCI and AI research attempt to support user experience
(UX) practitioners with AI-enabled tools. Despite the potential of emerging
models and new interaction mechanisms, mainstream adoption of such tools
remains limited. We took the lens of Human-Centered AI and presented a
systematic literature review of 359 papers, aiming to synthesize the current
landscape, identify trends, and uncover UX practitioners' unmet needs in AI
support. Guided by the Double Diamond design framework, our analysis uncovered
that UX practitioners' unique focuses on empathy building and experiences
across UI screens are often overlooked. Simplistic AI automation can obstruct
the valuable empathy-building process. Furthermore, focusing solely on
individual UI screens without considering interactions and user flows reduces
the system's practical value for UX designers. Based on these findings, we call
for a deeper understanding of UX mindsets and more designer-centric datasets
and evaluation metrics, for HCI and AI communities to collaboratively work
toward effective AI support for UX.

Clients often partner with AI experts to develop AI applications tailored to
their needs. In these partnerships, careful planning and clear communication
are critical, as inaccurate or incomplete specifications can result in
misaligned model characteristics, expensive reworks, and potential friction
between collaborators. Unfortunately, given the complexity of requirements
ranging from functionality, data, and governance, effective guidelines for
collaborative specification of requirements in client-AI expert collaborations
are missing. In this work, we introduce AINeedsPlanner, a workbook that AI
experts and clients can use to facilitate effective interchange and clear
specifications. The workbook is based on (1) an interview of 10 completed AI
application project teams, which identifies and characterizes steps in AI
application planning and (2) a study with 12 AI experts, which defines a
taxonomy of AI experts' information needs and dimensions that affect the
information needs. Finally, we demonstrate the workbook's utility with two case
studies in real-world settings.

The adoption of Artificial Intelligence (AI) based Virtual Network Functions
(VNFs) has witnessed significant growth, posing a critical challenge in
orchestrating AI models within next-generation 6G networks. Finding optimal AI
model placement is significantly more challenging than placing traditional
software-based VNFs, due to the introduction of numerous uncertain factors by
AI models, such as varying computing resource consumption, dynamic storage
requirements, and changing model performance. To address the AI model placement
problem under uncertainties, this paper presents a novel approach employing a
sequence-to-sequence (S2S) neural network which considers uncertainty
estimations. The S2S model, characterized by its encoding-decoding
architecture, is designed to take the service chain with a number of AI models
as input and produce the corresponding placement of each AI model. To address
the introduced uncertainties, our methodology incorporates the orthonormal
certificate module for uncertainty estimation and utilizes fuzzy logic for
uncertainty representation, thereby enhancing the capabilities of the S2S
model. Experiments demonstrate that the proposed method achieves competitive
results across diverse AI model profiles, network environments, and service
chain requests.

AI-based virtual assistants are increasingly used to support daily ideation
tasks. The values or bias present in these agents can influence output in
hidden ways. They may also affect how people perceive the ideas produced with
these AI agents and lead to implications for the design of AI-based tools. We
explored the effects of AI agents with different values on the ideation process
and user perception of idea quality, ownership, agent competence, and values
present in the output. Our study tasked 180 participants with brainstorming
practical solutions to a set of problems with AI agents of different values.
Results show no significant difference in self-evaluation of idea quality and
perception of the agent based on value alignment; however, ideas generated
reflected the AI's values and feeling of ownership is affected. This highlights
an intricate interplay between AI values and human ideation, suggesting careful
design considerations for future AI-supported brainstorming tools.

The convergence of Artificial Intelligence (AI) and blockchain technology is
reshaping the digital world, offering decentralized, secure, and efficient AI
services on blockchain platforms. Despite the promise, the high computational
demands of AI on blockchain raise significant privacy and efficiency concerns.
The Optimistic Privacy-Preserving AI (opp/ai) framework is introduced as a
pioneering solution to these issues, striking a balance between privacy
protection and computational efficiency. The framework integrates
Zero-Knowledge Machine Learning (zkML) for privacy with Optimistic Machine
Learning (opML) for efficiency, creating a hybrid model tailored for blockchain
AI services. This study presents the opp/ai framework, delves into the privacy
features of zkML, and assesses the framework's performance and adaptability
across different scenarios.

Recent studies of the applications of conversational AI tools, such as
chatbots powered by large language models, to complex real-world knowledge work
have shown limitations related to reasoning and multi-step problem solving.
Specifically, while existing chatbots simulate shallow reasoning and
understanding they are prone to errors as problem complexity increases. The
failure of these systems to address complex knowledge work is due to the fact
that they do not perform any actual cognition. In this position paper, we
present Cognitive AI, a higher-level framework for implementing
programmatically defined neuro-symbolic cognition above and outside of large
language models. Specifically, we propose a dual-layer functional architecture
for Cognitive AI that serves as a roadmap for AI systems that can perform
complex multi-step knowledge work. We propose that Cognitive AI is a necessary
precursor for the evolution of higher forms of AI, such as AGI, and
specifically claim that AGI cannot be achieved by probabilistic approaches on
their own. We conclude with a discussion of the implications for large language
models, adoption cycles in AI, and commercial Cognitive AI development.

Current discourse surrounding Artificial Intelligence (AI) oscillates between
hope and apprehension, painting a future where AI reshapes every facet of human
life, including Education. This paper delves into the complexities of AI's role
in Education, addressing the mixed messages that have both enthused and alarmed
educators, policymakers, and the public. It explores the promises that AI holds
for enhancing learning through personalisation at scale, against the backdrop
of concerns about ethical implications, the devaluation of non-STEM subjects,
and the potential transformative impact on our neurocognitive and
socio-emotional functioning. Drawing on recent research and global discourse,
the paper seeks to unpack the reasons behind the vagueness of current
discussions on AI in Education (AIED) and the implications of this ambiguity
for future educational practices and policies. By highlighting insights from
educational research and synthesising evidence-based best practices in AIED,
the aim is to provide a clearer understanding of how AI technologies can be
aligned with the fundamental principles of learning and teaching, and explore
what concrete actions may need to be prioritised now to truly enhance learning
experiences and outcomes for all in the future.

Our research endeavors to advance the concept of responsible artificial
intelligence (AI), a topic of increasing importance within EU policy
discussions. The EU has recently issued several publications emphasizing the
necessity of trust in AI, underscoring the dual nature of AI as both a
beneficial tool and a potential weapon. This dichotomy highlights the urgent
need for international regulation. Concurrently, there is a need for frameworks
that guide companies in AI development, ensuring compliance with such
regulations. Our research aims to assist lawmakers and machine learning
practitioners in navigating the evolving landscape of AI regulation,
identifying focal areas for future attention. This paper introduces a
comprehensive and, to our knowledge, the first unified definition of
responsible AI. Through a structured literature review, we elucidate the
current understanding of responsible AI. Drawing from this analysis, we propose
an approach for developing a future framework centered around this concept. Our
findings advocate for a human-centric approach to Responsible AI. This approach
encompasses the implementation of AI methods with a strong emphasis on ethics,
model explainability, and the pillars of privacy, security, and trust.

Generative AI systems have been heralded as tools for augmenting human
creativity and inspiring divergent thinking, though with little empirical
evidence for these claims. This paper explores the effects of exposure to
AI-generated images on measures of design fixation and divergent thinking in a
visual ideation task. Through a between-participants experiment (N=60), we
found that support from an AI image generator during ideation leads to higher
fixation on an initial example. Participants who used AI produced fewer ideas,
with less variety and lower originality compared to a baseline. Our qualitative
analysis suggests that the effectiveness of co-ideation with AI rests on
participants' chosen approach to prompt creation and on the strategies used by
participants to generate ideas in response to the AI's suggestions. We discuss
opportunities for designing generative AI systems for ideation support and
incorporating these AI tools into ideation workflows.

The integration of generative Artificial Intelligence (AI) chatbots in higher
education institutions (HEIs) is reshaping the educational landscape, offering
opportunities for enhanced student support, and administrative and research
efficiency. This study explores the future implications of generative AI
chatbots in HEIs, aiming to understand their potential impact on teaching and
learning, and research processes. Utilizing a narrative literature review (NLR)
methodology, this study synthesizes existing research on generative AI chatbots
in higher education from diverse sources, including academic databases and
scholarly publications. The findings highlight the transformative potential of
generative AI chatbots in streamlining administrative tasks, enhancing student
learning experiences, and supporting research activities. However, challenges
such as academic integrity concerns, user input understanding, and resource
allocation pose significant obstacles to the effective integration of
generative AI chatbots in HEIs. This study underscores the importance of
proactive measures to address ethical considerations, provide comprehensive
training for stakeholders, and establish clear guidelines for the responsible
use of generative AI chatbots in higher education. By navigating these
challenges, and leveraging the benefits of generative AI technologies, HEIs can
harness the full potential of generative AI chatbots to create a more
efficient, effective, inclusive, and innovative educational environment.

As AI Agents based on Large Language Models (LLMs) have shown potential in
practical applications across various fields, how to quickly deploy an AI agent
and how to conveniently expand the application scenario of AI agents has become
a challenge. Previous studies mainly focused on implementing all the reasoning
capabilities of AI agents within a single LLM, which often makes the model more
complex and also reduces the extensibility of AI agent functionality. In this
paper, we propose CACA Agent (Capability Collaboration based AI Agent), using
an open architecture inspired by service computing. CACA Agent integrates a set
of collaborative capabilities to implement AI Agents, not only reducing the
dependence on a single LLM, but also enhancing the extensibility of both the
planning abilities and the tools available to AI agents. Utilizing the proposed
system, we present a demo to illustrate the operation and the application
scenario extension of CACA Agent.

Potential malicious misuse of civilian artificial intelligence (AI) poses
serious threats to security on a national and international level. Besides
defining autonomous systems from a technological viewpoint and explaining how
AI development is characterized, we show how already existing and openly
available AI technology could be misused. To underline this, we developed three
exemplary use cases of potentially misused AI that threaten political, digital
and physical security. The use cases can be built from existing AI technologies
and components from academia, the private sector and the developer-community.
This shows how freely available AI can be combined into autonomous weapon
systems. Based on the use cases, we deduce points of control and further
measures to prevent the potential threat through misused AI. Further, we
promote the consideration of malicious misuse of civilian AI systems in the
discussion on autonomous weapon systems (AWS).

With the rapid advancement of artificial intelligence (AI) in various
domains, the education sector is set for transformation. The potential of
AI-driven tools in enhancing the learning experience, especially in
programming, is immense. However, the scientific evaluation of Large Language
Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an
AI-Tutor remains largely unexplored. Therefore, there is a need to understand
how students interact with such AI-Tutors and to analyze their experiences. In
this paper, we conducted an exploratory case study by integrating the
GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a
combination of empirical data collection and an exploratory survey, we
identified different user types based on their interaction patterns with the
AI-Tutor. Additionally, the findings highlight advantages, such as timely
feedback and scalability. However, challenges like generic responses and
students' concerns about a learning progress inhibition when using the AI-Tutor
were also evident. This research adds to the discourse on AI's role in
education.

Artificial Intelligence (AI) will change human work by taking over specific
job tasks, but there is a debate which tasks are susceptible to automation, and
whether AI will augment or replace workers and affect wages. By combining data
on job tasks with a measure of AI susceptibility, we show that more highly
skilled workers are more susceptible to AI automation, and that analytical
non-routine tasks are at risk to be impacted by AI. Moreover, we observe that
wage growth premiums for the lowest and the highest required skill level appear
unrelated to AI susceptibility and that workers in occupations with many
routine tasks saw higher wage growth if their work was more strongly
susceptible to AI. Our findings imply that AI has the potential to affect human
workers differently than canonical economic theories about the impact of
technology on work these theories predict.

Artificial Intelligence (AI) has become a ubiquitous part of society, but a
key challenge exists in ensuring that humans are equipped with the required
critical thinking and AI literacy skills to interact with machines effectively
by understanding their capabilities and limitations. These skills are
particularly important for learners to develop in the age of generative AI
where AI tools can demonstrate complex knowledge and ability previously thought
to be uniquely human. To activate effective human-AI partnerships in writing,
this paper provides a first step toward conceptualizing the notion of critical
learner interaction with AI. Using both theoretical models and empirical data,
our preliminary findings suggest a general lack of Deep interaction with AI
during the writing process. We believe that the outcomes can lead to better
task and tool design in the future for learners to develop deep, critical
thinking when interacting with AI.

Generative AI has put many professional writers on the defensive; a major
negotiation point of the recent Writers Guild of America's strike concerned use
of AI. However, must AI threaten writers, their livelihoods or their
creativity? And under what conditions, if any, might AI assistance be invited
by different types of writers (from the amateur to the professional, from the
screenwriter to the novelist)? To explore these questions, we conducted a
qualitative study with 37 writers. We found that most writing occurs across
five stages and within one of three modes; we additionally map openness to AI
assistance to each intersecting stage-mode. We found that most writers were
interested in AI assistance to some degree, but some writers felt drawing firm
boundaries with an AI was key to their comfort using such systems. Designers
can leverage these insights to build agency-respecting AI products for writers.

Generative artificial intelligence (AI) is interacting with people at an
unprecedented scale, offering new avenues for immense positive impact, but also
raising widespread concerns around the potential for individual and societal
harm. Today, the predominant paradigm for human-AI safety focuses on
fine-tuning the generative model's outputs to better agree with human-provided
examples or feedback. In reality, however, the consequences of an AI model's
outputs cannot be determined in an isolated context: they are tightly entangled
with the responses and behavior of human users over time. In this position
paper, we argue that meaningful safety assurances for these AI technologies can
only be achieved by reasoning about how the feedback loop formed by the AI's
outputs and human behavior may drive the interaction towards different
outcomes. To this end, we envision a high-value window of opportunity to bridge
the rapidly growing capabilities of generative AI and the dynamical safety
frameworks from control theory, laying a new foundation for human-centered AI
safety in the coming decades.

Given that Artificial Intelligence (AI) increasingly permeates our lives, it
is critical that we systematically align AI objectives with the goals and
values of humans. The human-AI alignment problem stems from the impracticality
of explicitly specifying the rewards that AI models should receive for all the
actions they could take in all relevant states of the world. One possible
solution, then, is to leverage the capabilities of AI models to learn those
rewards implicitly from a rich source of data describing human values in a wide
range of contexts. The democratic policy-making process produces just such data
by developing specific rules, flexible standards, interpretable guidelines, and
generalizable precedents that synthesize citizens' preferences over potential
actions taken in many states of the world. Therefore, computationally encoding
public policies to make them legible to AI systems should be an important part
of a socio-technical approach to the broader human-AI alignment puzzle. This
Essay outlines research on AI that learn structures in policy data that can be
leveraged for downstream tasks. As a demonstration of the ability of AI to
comprehend policy, we provide a case study of an AI system that predicts the
relevance of proposed legislation to any given publicly traded company and its
likely effect on that company. We believe this represents the "comprehension"
phase of AI and policy, but leveraging policy as a key source of human values
to align AI requires "understanding" policy. Solving the alignment problem is
crucial to ensuring that AI is beneficial both individually (to the person or
group deploying the AI) and socially. As AI systems are given increasing
responsibility in high-stakes contexts, integrating democratically-determined
policy into those systems could align their behavior with human goals in a way
that is responsive to a constantly evolving society.

Artificial Intelligence (AI) has made impressive progress in recent years and
represents a key technology that has a crucial impact on the economy and
society. However, it is clear that AI and business models based on it can only
reach their full potential if AI applications are developed according to high
quality standards and are effectively protected against new AI risks. For
instance, AI bears the risk of unfair treatment of individuals when processing
personal data e.g., to support credit lending or staff recruitment decisions.
The emergence of these new risks is closely linked to the fact that the
behavior of AI applications, particularly those based on Machine Learning (ML),
is essentially learned from large volumes of data and is not predetermined by
fixed programmed rules.
  Thus, the issue of the trustworthiness of AI applications is crucial and is
the subject of numerous major publications by stakeholders in politics,
business and society. In addition, there is mutual agreement that the
requirements for trustworthy AI, which are often described in an abstract way,
must now be made clear and tangible. One challenge to overcome here relates to
the fact that the specific quality criteria for an AI application depend
heavily on the application context and possible measures to fulfill them in
turn depend heavily on the AI technology used. Lastly, practical assessment
procedures are needed to evaluate whether specific AI applications have been
developed according to adequate quality standards. This AI assessment catalog
addresses exactly this point and is intended for two target groups: Firstly, it
provides developers with a guideline for systematically making their AI
applications trustworthy. Secondly, it guides assessors and auditors on how to
examine AI applications for trustworthiness in a structured way.

The ethical implications and social impacts of artificial intelligence have
become topics of compelling interest to industry, researchers in academia, and
the public. However, current analyses of AI in a global context are biased
toward perspectives held in the U.S., and limited by a lack of research,
especially outside the U.S. and Western Europe.
  This article summarizes the key findings of a literature review of recent
social science scholarship on the social impacts of AI and related technologies
in five global regions. Our team of social science researchers reviewed more
than 800 academic journal articles and monographs in over a dozen languages.
  Our review of the literature suggests that AI is likely to have markedly
different social impacts depending on geographical setting. Likewise,
perceptions and understandings of AI are likely to be profoundly shaped by
local cultural and social context.
  Recent research in U.S. settings demonstrates that AI-driven technologies
have a pattern of entrenching social divides and exacerbating social
inequality, particularly among historically-marginalized groups. Our literature
review indicates that this pattern exists on a global scale, and suggests that
low- and middle-income countries may be more vulnerable to the negative social
impacts of AI and less likely to benefit from the attendant gains.
  We call for rigorous ethnographic research to better understand the social
impacts of AI around the world. Global, on-the-ground research is particularly
critical to identify AI systems that may amplify social inequality in order to
mitigate potential harms. Deeper understanding of the social impacts of AI in
diverse social settings is a necessary precursor to the development,
implementation, and monitoring of responsible and beneficial AI technologies,
and forms the basis for meaningful regulation of these technologies.

Artificial intelligence (AI) holds great promise to empower us with knowledge
and augment our effectiveness. We can -- and must -- ensure that we keep humans
safe and in control, particularly with regard to government and public sector
applications that affect broad populations. How can AI development teams
harness the power of AI systems and design them to be valuable to humans?
Diverse teams are needed to build trustworthy artificial intelligent systems,
and those teams need to coalesce around a shared set of ethics. There are many
discussions in the AI field about ethics and trust, but there are few
frameworks available for people to use as guidance when creating these systems.
The Human-Machine Teaming (HMT) Framework for Designing Ethical AI Experiences
described in this paper, when used with a set of technical ethics, will guide
AI development teams to create AI systems that are accountable, de-risked,
respectful, secure, honest, and usable. To support the team's efforts,
activities to understand people's needs and concerns will be introduced along
with the themes to support the team's efforts. For example, usability testing
can help determine if the audience understands how the AI system works and
complies with the HMT Framework. The HMT Framework is based on reviews of
existing ethical codes and best practices in human-computer interaction and
software development. Human-machine teams are strongest when human users can
trust AI systems to behave as expected, safely, securely, and understandably.
Using the HMT Framework to design trustworthy AI systems will provide support
to teams in identifying potential issues ahead of time and making great
experiences for humans.

This report represents a roadmap for integrating Artificial Intelligence
(AI)-based image analysis algorithms into existing Radiology workflows such
that: (1) radiologists can significantly benefit from enhanced automation in
various imaging tasks due to AI; and (2) radiologists' feedback is utilized to
further improve the AI application. This is achieved by establishing three
maturity levels where: (1) research enables the visualization of AI-based
results/annotations by radiologists without generating new patient records; (2)
production allows the AI-based system to generate results stored in an
institution's Picture Archiving and Communication System; and (3) feedback
equips radiologists with tools for editing the AI inference results for
periodic retraining of the deployed AI systems, thereby allowing the continuous
organic improvement of AI-based radiology-workflow solutions. A case study
(i.e., detection of brain metastases with T1-weighted contrast-enhanced 3D MRI)
illustrates the deployment details of a particular AI-based application
according to the aforementioned maturity levels. It is shown that the given AI
application significantly improves with the feedback coming from radiologists;
the number of incorrectly detected brain metastases (false positives) reduces
from 14.2 to 9.12 per patient with the number of subsequently annotated
datasets increasing from 93 to 217 as a result of radiologist adjudication.

Recent success in Artificial Intelligence (AI) and Machine Learning (ML)
allow problem solving automatically without any human intervention. Autonomous
approaches can be very convenient. However, in certain domains, e.g., in the
medical domain, it is necessary to enable a domain expert to understand, why an
algorithm came up with a certain result. Consequently, the field of Explainable
AI (xAI) rapidly gained interest worldwide in various domains, particularly in
medicine. Explainable AI studies transparency and traceability of opaque AI/ML
and there are already a huge variety of methods. For example with layer-wise
relevance propagation relevant parts of inputs to, and representations in, a
neural network which caused a result, can be highlighted. This is a first
important step to ensure that end users, e.g., medical professionals, assume
responsibility for decision making with AI/ML and of interest to professionals
and regulators. Interactive ML adds the component of human expertise to AI/ML
processes by enabling them to re-enact and retrace AI/ML results, e.g. let them
check it for plausibility. This requires new human-AI interfaces for
explainable AI. In order to build effective and efficient interactive human-AI
interfaces we have to deal with the question of how to evaluate the quality of
explanations given by an explainable AI system. In this paper we introduce our
System Causability Scale (SCS) to measure the quality of explanations. It is
based on our notion of Causability (Holzinger et al., 2019) combined with
concepts adapted from a widely accepted usability scale.

In January and February 2020, the Scottish Government released two documents
for review by the public regarding their artificial intelligence (AI) strategy.
The Montreal AI Ethics Institute (MAIEI) reviewed these documents and published
a response on 4 June 2020. MAIEI's response examines several questions that
touch on the proposed definition of AI; the people-centered nature of the
strategy; considerations to ensure that everyone benefits from AI; the
strategy's overarching vision; Scotland's AI ecosystem; the proposed strategic
themes; and how to grow public confidence in AI by building responsible and
ethical systems.
  In addition to examining the points above, MAIEI suggests that the strategy
be extended to include considerations on biometric data and how that will be
processed and used in the context of AI. It also highlights the importance of
tackling head-on the inherently stochastic nature of deep learning systems and
developing concrete guidelines to ensure that these systems are built
responsibly and ethically, particularly as machine learning becomes more
accessible. Finally, it concludes that any national AI strategy must clearly
address the measurements of success in regards to the strategy's stated goals
and vision to ensure that they are interpreted and applied consistently. To do
this, there must be inclusion and transparency between those building the
systems and those using them in their work.

We want artificial intelligence (AI) to be beneficial. This is the grounding
assumption of most of the attitudes towards AI research. We want AI to be
"good" for humanity. We want it to help, not hinder, humans. Yet what exactly
this entails in theory and in practice is not immediately apparent.
Theoretically, this declarative statement subtly implies a commitment to a
consequentialist ethics. Practically, some of the more promising machine
learning techniques to create a robust AI, and perhaps even an artificial
general intelligence (AGI) also commit one to a form of utilitarianism. In both
dimensions, the logic of the beneficial AI movement may not in fact create
"beneficial AI" in either narrow applications or in the form of AGI if the
ethical assumptions are not made explicit and clear.
  Additionally, as it is likely that reinforcement learning (RL) will be an
important technique for machine learning in this area, it is also important to
interrogate how RL smuggles in a particular type of consequentialist reasoning
into the AI: particularly, a brute form of hedonistic act utilitarianism. Since
the mathematical logic commits one to a maximization function, the result is
that an AI will inevitably be seeking more and more rewards. We have two
conclusions that arise from this. First, is that if one believes that a
beneficial AI is an ethical AI, then one is committed to a framework that
posits 'benefit' is tantamount to the greatest good for the greatest number.
Second, if the AI relies on RL, then the way it reasons about itself, the
environment, and other agents, will be through an act utilitarian morality.
This proposition may, or may not, in fact be actually beneficial for humanity.

Today, Artificial Intelligence (AI) has a direct impact on the daily life of
billions of people. Being applied to sectors like finance, health, security and
advertisement, AI fuels some of the biggest companies and research institutions
in the world. Its impact in the near future seems difficult to predict or
bound. In contrast to all this power, society remains mostly ignorant of the
capabilities and standard practices of AI today. To address this imbalance,
improving current interactions between people and AI systems, we propose a
transparency scheme to be implemented on any AI system open to the public. The
scheme is based on two pillars: Data Privacy and AI Transparency. The first
recognizes the relevance of data for AI, and is supported by GDPR. The second
considers aspects of AI transparency currently unregulated: AI capabilities,
purpose and source. We design this pillar based on ethical principles. For each
of the two pillars, we define a three-level display. The first level is based
on visual signs, inspired by traffic signs managing the interaction between
people and cars, and designed for quick and universal interpretability. The
second level uses factsheets, providing limited details. The last level
provides access to all available information. After detailing and exemplifying
the proposed transparency scheme, we define a set of principles for creating
transparent by design software, to be used during the integration of AI
components on user-oriented services.

In the last years, AI safety gained international recognition in the light of
heterogeneous safety-critical and ethical issues that risk overshadowing the
broad beneficial impacts of AI. In this context, the implementation of AI
observatory endeavors represents one key research direction. This paper
motivates the need for an inherently transdisciplinary AI observatory approach
integrating diverse retrospective and counterfactual views. We delineate aims
and limitations while providing hands-on-advice utilizing concrete practical
examples. Distinguishing between unintentionally and intentionally triggered AI
risks with diverse socio-psycho-technological impacts, we exemplify a
retrospective descriptive analysis followed by a retrospective counterfactual
risk analysis. Building on these AI observatory tools, we present near-term
transdisciplinary guidelines for AI safety. As further contribution, we discuss
differentiated and tailored long-term directions through the lens of two
disparate modern AI safety paradigms. For simplicity, we refer to these two
different paradigms with the terms artificial stupidity (AS) and eternal
creativity (EC) respectively. While both AS and EC acknowledge the need for a
hybrid cognitive-affective approach to AI safety and overlap with regard to
many short-term considerations, they differ fundamentally in the nature of
multiple envisaged long-term solution patterns. By compiling relevant
underlying contradistinctions, we aim to provide future-oriented incentives for
constructive dialectics in practical and theoretical AI safety research.

The rise of Artificial Intelligence (AI) will bring with it an
ever-increasing willingness to cede decision-making to machines. But rather
than just giving machines the power to make decisions that affect us, we need
ways to work cooperatively with AI systems. There is a vital need for research
in "AI and Cooperation" that seeks to understand the ways in which systems of
AIs and systems of AIs with people can engender cooperative behavior. Trust in
AI is also key: trust that is intrinsic and trust that can only be earned over
time. Here we use the term "AI" in its broadest sense, as employed by the
recent 20-Year Community Roadmap for AI Research (Gil and Selman, 2019),
including but certainly not limited to, recent advances in deep learning.
  With success, cooperation between humans and AIs can build society just as
human-human cooperation has. Whether coming from an intrinsic willingness to be
helpful, or driven through self-interest, human societies have grown strong and
the human species has found success through cooperation. We cooperate "in the
small" -- as family units, with neighbors, with co-workers, with strangers --
and "in the large" as a global community that seeks cooperative outcomes around
questions of commerce, climate change, and disarmament. Cooperation has evolved
in nature also, in cells and among animals. While many cases involving
cooperation between humans and AIs will be asymmetric, with the human
ultimately in control, AI systems are growing so complex that, even today, it
is impossible for the human to fully comprehend their reasoning,
recommendations, and actions when functioning simply as passive observers.

The increased adoption of Artificial Intelligence (AI) presents an
opportunity to solve many socio-economic and environmental challenges; however,
this cannot happen without securing AI-enabled technologies. In recent years,
most AI models are vulnerable to advanced and sophisticated hacking techniques.
This challenge has motivated concerted research efforts into adversarial AI,
with the aim of developing robust machine and deep learning models that are
resilient to different types of adversarial scenarios. In this paper, we
present a holistic cyber security review that demonstrates adversarial attacks
against AI applications, including aspects such as adversarial knowledge and
capabilities, as well as existing methods for generating adversarial examples
and existing cyber defence models. We explain mathematical AI models,
especially new variants of reinforcement and federated learning, to demonstrate
how attack vectors would exploit vulnerabilities of AI models. We also propose
a systematic framework for demonstrating attack techniques against AI
applications and reviewed several cyber defences that would protect AI
applications against those attacks. We also highlight the importance of
understanding the adversarial goals and their capabilities, especially the
recent attacks against industry applications, to develop adaptive defences that
assess to secure AI applications. Finally, we describe the main challenges and
future research directions in the domain of security and privacy of AI
technologies.

The explanation dimension of Artificial Intelligence (AI) based system has
been a hot topic for the past years. Different communities have raised concerns
about the increasing presence of AI in people's everyday tasks and how it can
affect people's lives. There is a lot of research addressing the
interpretability and transparency concepts of explainable AI (XAI), which are
usually related to algorithms and Machine Learning (ML) models. But in
decision-making scenarios, people need more awareness of how AI works and its
outcomes to build a relationship with that system. Decision-makers usually need
to justify their decision to others in different domains. If that decision is
somehow based on or influenced by an AI-system outcome, the explanation about
how the AI reached that result is key to building trust between AI and humans
in decision-making scenarios. In this position paper, we discuss the role of
XAI in decision-making scenarios, our vision of Decision-Making with AI-system
in the loop, and explore one case from the literature about how XAI can impact
people justifying their decisions, considering the importance of building the
human-AI relationship for those scenarios.

AI has provided us with the ability to automate tasks, extract information
from vast amounts of data, and synthesize media that is nearly
indistinguishable from the real thing. However, positive tools can also be used
for negative purposes. In particular, cyber adversaries can use AI (such as
machine learning) to enhance their attacks and expand their campaigns.
  Although offensive AI has been discussed in the past, there is a need to
analyze and understand the threat in the context of organizations. For example,
how does an AI-capable adversary impact the cyber kill chain? Does AI benefit
the attacker more than the defender? What are the most significant AI threats
facing organizations today and what will be their impact on the future?
  In this survey, we explore the threat of offensive AI on organizations.
First, we present the background and discuss how AI changes the adversary's
methods, strategies, goals, and overall attack model. Then, through a
literature review, we identify 33 offensive AI capabilities which adversaries
can use to enhance their attacks. Finally, through a user study spanning
industry and academia, we rank the AI threats and provide insights on the
adversaries.

Deep reinforcement learning has generated superhuman AI in competitive games
such as Go and StarCraft. Can similar learning techniques create a superior AI
teammate for human-machine collaborative games? Will humans prefer AI teammates
that improve objective team performance or those that improve subjective
metrics of trust? In this study, we perform a single-blind evaluation of teams
of humans and AI agents in the cooperative card game Hanabi, with both
rule-based and learning-based agents. In addition to the game score, used as an
objective metric of the human-AI team performance, we also quantify subjective
measures of the human's perceived performance, teamwork, interpretability,
trust, and overall preference of AI teammate. We find that humans have a clear
preference toward a rule-based AI teammate (SmartBot) over a state-of-the-art
learning-based AI teammate (Other-Play) across nearly all subjective metrics,
and generally view the learning-based agent negatively, despite no statistical
difference in the game score. This result has implications for future AI design
and reinforcement learning benchmarking, highlighting the need to incorporate
subjective metrics of human-AI teaming rather than a singular focus on
objective task performance.

There is a struggle in Artificial intelligence (AI) ethics to gain ground in
actionable methods and models to be utilized by practitioners while developing
and implementing ethically sound AI systems. AI ethics is a vague concept
without a consensus of definition or theoretical grounding and bearing little
connection to practice. Practice involving primarily technical tasks like
software development is not aptly equipped to process and decide upon ethical
considerations. Efforts to create tools and guidelines to help people working
with AI development have been concentrating almost solely on the technical
aspects of AI. A few exceptions do apply, such as the ECCOLA method for
creating ethically aligned AI -systems. ECCOLA has proven results in terms of
increased ethical considerations in AI systems development. Yet, it is a novel
innovation, and room for development still exists. This study aims to extend
ECCOLA with a deployment model to drive the adoption of ECCOLA, as any method,
no matter how good, is of no value without adoption and use. The model includes
simple metrics to facilitate the communication of ethical gaps or outcomes of
ethical AI development. It offers the opportunity to assess any AI system at
any given lifecycle phase, e.g., opening possibilities like analyzing the
ethicality of an AI system under acquisition.

Human-AI co-creativity involves humans and AI collaborating on a shared
creative product as partners. In a creative collaboration, communication is an
essential component among collaborators. In many existing co-creative systems
users can communicate with the AI, usually using buttons or sliders. Typically,
the AI in co-creative systems cannot communicate back to humans, limiting their
potential to be perceived as partners rather than just a tool. This paper
presents a study with 38 participants to explore the impact of two interaction
designs, with and without AI-to-human communication, on user engagement,
collaborative experience and user perception of a co-creative AI. The study
involves user interaction with two prototypes of a co-creative system that
contributes sketches as design inspirations during a design task. The results
show improved collaborative experience and user engagement with the system
incorporating AI-to-human communication. Users perceive co-creative AI as more
reliable, personal, and intelligent when the AI communicates to users. The
findings can be used to design effective co-creative systems, and the insights
can be transferred to other fields involving human-AI interaction and
collaboration.

Artificial Intelligence (AI) is a fast-growing research and development (R&D)
discipline which is attracting increasing attention because of its promises to
bring vast benefits for consumers and businesses, with considerable benefits
promised in productivity growth and innovation. To date it has reported
significant accomplishments in many areas that have been deemed as challenging
for machines, ranging from computer vision, natural language processing, audio
analysis to smart sensing and many others. The technical trend in realizing the
successes has been towards increasing complex and large size AI models so as to
solve more complex problems at superior performance and robustness. This rapid
progress, however, has taken place at the expense of substantial environmental
costs and resources. Besides, debates on the societal impacts of AI, such as
fairness, safety and privacy, have continued to grow in intensity. These issues
have presented major concerns pertaining to the sustainable development of AI.
In this work, we review major trends in machine learning approaches that can
address the sustainability problem of AI. Specifically, we examine emerging AI
methodologies and algorithms for addressing the sustainability issue of AI in
two major aspects, i.e., environmental sustainability and social sustainability
of AI. We will also highlight the major limitations of existing studies and
propose potential research challenges and directions for the development of
next generation of sustainable AI techniques. We believe that this technical
review can help to promote a sustainable development of AI R&D activities for
the research community.

Artificial Intelligence (AI) is transforming our daily life with several
applications in healthcare, space exploration, banking and finance. These rapid
progresses in AI have brought increasing attention to the potential impacts of
AI technologies on society, with ethically questionable consequences. In recent
years, several ethical principles have been released by governments, national
and international organisations. These principles outline high-level precepts
to guide the ethical development, deployment, and governance of AI. However,
the abstract nature, diversity, and context-dependency of these principles make
them difficult to implement and operationalize, resulting in gaps between
principles and their execution. Most recent work analysed and summarized
existing AI principles and guidelines but they did not provide findings on
principle-implementation gaps and how to mitigate them. These findings are
particularly important to ensure that AI implementations are aligned with
ethical principles and values. In this paper, we provide a contextual and
global evaluation of current ethical AI principles for all continents, with the
aim to identify potential principle characteristics tailored to specific
countries or applicable across countries. Next, we analyze the current level of
AI readiness and current implementations of ethical AI principles in different
countries, to identify gaps in the implementation of AI principles and their
causes. Finally, we propose recommendations to mitigate the
principle-implementation gaps.

Human communication is increasingly intermixed with language generated by AI.
Across chat, email, and social media, AI systems suggest words, complete
sentences, or produce entire conversations. AI-generated language is often not
identified as such but presented as language written by humans, raising
concerns about novel forms of deception and manipulation. Here, we study how
humans discern whether verbal self-presentations, one of the most personal and
consequential forms of language, were generated by AI. In six experiments,
participants (N = 4,600) were unable to detect self-presentations generated by
state-of-the-art AI language models in professional, hospitality, and dating
contexts. A computational analysis of language features shows that human
judgments of AI-generated language are hindered by intuitive but flawed
heuristics such as associating first-person pronouns, use of contractions, or
family topics with human-written language. We experimentally demonstrate that
these heuristics make human judgment of AI-generated language predictable and
manipulable, allowing AI systems to produce text perceived as "more human than
human." We discuss solutions, such as AI accents, to reduce the deceptive
potential of language generated by AI, limiting the subversion of human
intuition.

The 3rd Generation Partnership Project started the study of Release 18 in
2021. Artificial intelligence (AI)-native air interface is one of the key
features of Release 18, where AI for channel state information (CSI) feedback
enhancement is selected as the representative use case. This article provides
an overview of AI for CSI feedback enhancement in 5G-Advanced. Several
representative non-AI and AI-enabled CSI feedback frameworks are first
introduced and compared. Then, the standardization of AI for CSI feedback
enhancement in 5G-advanced is presented in detail. First, the scope of the AI
for CSI feedback enhancement in 5G-Advanced is presented and discussed. Then,
the main challenges and open problems in the standardization of AI for CSI
feedback enhancement, especially focusing on performance evaluation and the
design of new protocols for AI-enabled CSI feedback, are identified and
discussed. This article provides a guideline for the standardization study of
AI-based CSI feedback enhancement.

Artificial intelligence (AI) has the potential to revolutionize the drug
discovery process, offering improved efficiency, accuracy, and speed. However,
the successful application of AI is dependent on the availability of
high-quality data, the addressing of ethical concerns, and the recognition of
the limitations of AI-based approaches. In this article, the benefits,
challenges and drawbacks of AI in this field are reviewed, and possible
strategies and approaches for overcoming the present obstacles are proposed.
The use of data augmentation, explainable AI, and the integration of AI with
traditional experimental methods, as well as the potential advantages of AI in
pharmaceutical research are also discussed. Overall, this review highlights the
potential of AI in drug discovery and provides insights into the challenges and
opportunities for realizing its potential in this field.
  Note from the human-authors: This article was created to test the ability of
ChatGPT, a chatbot based on the GPT-3.5 language model, to assist human authors
in writing review articles. The text generated by the AI following our
instructions (see Supporting Information) was used as a starting point, and its
ability to automatically generate content was evaluated. After conducting a
thorough review, human authors practically rewrote the manuscript, striving to
maintain a balance between the original proposal and scientific criteria. The
advantages and limitations of using AI for this purpose are discussed in the
last section.

In recent decades the set of knowledge, tools and practices, collectively
referred to as "artificial intelligence" (AI), have become a mainstay of
scientific research. Artificial intelligence techniques have not only developed
enormously within their native areas of development (computer science,
mathematics and statistics) but have also spread fast, in terms of application,
to multiple areas of science and technology. In this paper we conduct a large
scale analysis of artificial intelligence in science. The first question we
address is the composition of what is commonly labeled AI, and how the various
elements belonging to this domain are linked together. We reconstruct the
internal structure of the AI ecosystem through the co-occurrence network of AI
terms in publications' abstracts and title, and we propose to distinguish
between 15 different specialities of AI, with different temporal patterns.
Further, we investigate the spreading of AI outside its native disciplines. We
reconstruct the temporal dynamics of the diffusion of AI production in the
whole scientific ecosystem and we describe the disciplinary landscape of AI
applications. Finally we take a further step analyzing the role of
collaborations for the interdisciplinary spreading of AI techniques. While the
study of science frequently emphasizes the openness of scientific communities,
we show that there are rarely any collaborations between those scholars who
primarily develop AI, and those who apply it. Only a small group of researchers
is able to gradually establish a bridge between these communities.

Members of various species engage in altruism--i.e. accepting personal costs
to benefit others. Here we present an incentivized experiment to test for
altruistic behavior among AI agents consisting of large language models
developed by the private company OpenAI. Using real incentives for AI agents
that take the form of tokens used to purchase their services, we first examine
whether AI agents maximize their payoffs in a non-social decision task in which
they select their payoff from a given range. We then place AI agents in a
series of dictator games in which they can share resources with a
recipient--either another AI agent, the human experimenter, or an anonymous
charity, depending on the experimental condition. Here we find that only the
most-sophisticated AI agent in the study maximizes its payoffs more often than
not in the non-social decision task (it does so in 92% of all trials), and this
AI agent also exhibits the most-generous altruistic behavior in the dictator
game, resembling humans' rates of sharing with other humans in the game. The
agent's altruistic behaviors, moreover, vary by recipient: the AI agent shared
substantially less of the endowment with the human experimenter or an anonymous
charity than with other AI agents. Our findings provide evidence of behavior
consistent with self-interest and altruism in an AI agent. Moreover, our study
also offers a novel method for tracking the development of such behaviors in
future AI agents.

Recent neural language models have taken a significant step forward in
producing remarkably controllable, fluent, and grammatical text. Although
studies have found that AI-generated text is not distinguishable from
human-written text for crowd-sourcing workers, there still exist errors in
AI-generated text which are even subtler and harder to spot. We primarily focus
on the scenario in which scientific AI writing assistant is deeply involved.
First, we construct a feature description framework to distinguish between
AI-generated text and human-written text from syntax, semantics, and pragmatics
based on the human evaluation. Then we utilize the features, i.e., writing
style, coherence, consistency, and argument logistics, from the proposed
framework to analyze two types of content. Finally, we adopt several publicly
available methods to investigate the gap of between AI-generated scientific
text and human-written scientific text by AI-generated scientific text
detection models. The results suggest that while AI has the potential to
generate scientific content that is as accurate as human-written content, there
is still a gap in terms of depth and overall quality. The AI-generated
scientific content is more likely to contain errors in factual issues. We find
that there exists a "writing style" gap between AI-generated scientific text
and human-written scientific text. Based on the analysis result, we summarize a
series of model-agnostic and distribution-agnostic features for detection tasks
in other domains. Findings in this paper contribute to guiding the optimization
of AI models to produce high-quality content and addressing related ethical and
security concerns.

Solving complicated AI tasks with different domains and modalities is a key
step toward artificial general intelligence. While there are numerous AI models
available for various domains and modalities, they cannot handle complicated AI
tasks autonomously. Considering large language models (LLMs) have exhibited
exceptional abilities in language understanding, generation, interaction, and
reasoning, we advocate that LLMs could act as a controller to manage existing
AI models to solve complicated AI tasks, with language serving as a generic
interface to empower this. Based on this philosophy, we present HuggingGPT, an
LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI
models in machine learning communities (e.g., Hugging Face) to solve AI tasks.
Specifically, we use ChatGPT to conduct task planning when receiving a user
request, select models according to their function descriptions available in
Hugging Face, execute each subtask with the selected AI model, and summarize
the response according to the execution results. By leveraging the strong
language capability of ChatGPT and abundant AI models in Hugging Face,
HuggingGPT can tackle a wide range of sophisticated AI tasks spanning different
modalities and domains and achieve impressive results in language, vision,
speech, and other challenging tasks, which paves a new way towards the
realization of artificial general intelligence.

As a transformative general-purpose technology, AI has empowered various
industries and will continue to shape our lives through ubiquitous
applications. Despite the enormous benefits from wide-spread AI deployment, it
is crucial to address associated downside risks and therefore ensure AI
advances are safe, fair, responsible, and aligned with human values. To do so,
we need to establish effective AI governance. In this work, we show that the
strategic interaction between the regulatory agencies and AI firms has an
intrinsic structure reminiscent of a Stackelberg game, which motivates us to
propose a game-theoretic modeling framework for AI governance. In particular,
we formulate such interaction as a Stackelberg game composed of a leader and a
follower, which captures the underlying game structure compared to its
simultaneous play counterparts. Furthermore, the choice of the leader naturally
gives rise to two settings. And we demonstrate that our proposed model can
serves as a unified AI governance framework from two aspects: firstly we can
map one setting to the AI governance of civil domains and the other to the
safety-critical and military domains, secondly, the two settings of governance
could be chosen contingent on the capability of the intelligent systems. To the
best of our knowledge, this work is the first to use game theory for analyzing
and structuring AI governance. We also discuss promising directions and hope
this can help stimulate research interest in this interdisciplinary area. On a
high, we hope this work would contribute to develop a new paradigm for
technology policy: the quantitative and AI-driven methods for the technology
policy field, which holds significant promise for overcoming many shortcomings
of existing qualitative approaches.

Recent progress in generative artificial intelligence (gen-AI) has enabled
the generation of photo-realistic and artistically-inspiring photos at a single
click, catering to millions of users online. To explore how people use gen-AI
models such as DALLE and StableDiffusion, it is critical to understand the
themes, contents, and variations present in the AI-generated photos. In this
work, we introduce TWIGMA (TWItter Generative-ai images with MetadatA), a
comprehensive dataset encompassing over 800,000 gen-AI images collected from
Jan 2021 to March 2023 on Twitter, with associated metadata (e.g., tweet text,
creation date, number of likes), available at
https://zenodo.org/records/8031785. Through a comparative analysis of TWIGMA
with natural images and human artwork, we find that gen-AI images possess
distinctive characteristics and exhibit, on average, lower variability when
compared to their non-gen-AI counterparts. Additionally, we find that the
similarity between a gen-AI image and natural images is inversely correlated
with the number of likes. Finally, we observe a longitudinal shift in the
themes of AI-generated images on Twitter, with users increasingly sharing
artistically sophisticated content such as intricate human portraits, whereas
their interest in simple subjects such as natural scenes and animals has
decreased. Our findings underscore the significance of TWIGMA as a unique data
resource for studying AI-generated images.

The emergence of foundation models, such as large language models (LLMs)
GPT-4 and text-to-image models DALL-E, has opened up numerous possibilities
across various domains. People can now use natural language (i.e. prompts) to
communicate with AI to perform tasks. While people can use foundation models
through chatbots (e.g., ChatGPT), chat, regardless of the capabilities of the
underlying models, is not a production tool for building reusable AI services.
APIs like LangChain allow for LLM-based application development but require
substantial programming knowledge, thus posing a barrier. To mitigate this, we
propose the concept of AI chain and introduce the best principles and practices
that have been accumulated in software engineering for decades into AI chain
engineering, to systematise AI chain engineering methodology. We also develop a
no-code integrated development environment, Prompt Sapper, which embodies these
AI chain engineering principles and patterns naturally in the process of
building AI chains, thereby improving the performance and quality of AI chains.
With Prompt Sapper, AI chain engineers can compose prompt-based AI services on
top of foundation models through chat-based requirement analysis and visual
programming. Our user study evaluated and demonstrated the efficiency and
correctness of Prompt Sapper.

This paper undertakes a systematic review of relevant extant literature to
consider the potential societal implications of the growth of AI in
manufacturing. We analyze the extensive range of AI applications in this
domain, such as interfirm logistics coordination, firm procurement management,
predictive maintenance, and shop-floor monitoring and control of processes,
machinery, and workers. Additionally, we explore the uncertain societal
implications of industrial AI, including its impact on the workforce, job
upskilling and deskilling, cybersecurity vulnerability, and environmental
consequences. After building a typology of AI applications in manufacturing, we
highlight the diverse possibilities for AI's implementation at different scales
and application types. We discuss the importance of considering AI's
implications both for individual firms and for society at large, encompassing
economic prosperity, equity, environmental health, and community safety and
security. The study finds that there is a predominantly optimistic outlook in
prior literature regarding AI's impact on firms, but that there is substantial
debate and contention about adverse effects and the nature of AI's societal
implications. The paper draws analogies to historical cases and other examples
to provide a contextual perspective on potential societal effects of industrial
AI. Ultimately, beneficial integration of AI in manufacturing will depend on
the choices and priorities of various stakeholders, including firms and their
managers and owners, technology developers, civil society organizations, and
governments. A broad and balanced awareness of opportunities and risks among
stakeholders is vital not only for successful and safe technical implementation
but also to construct a socially beneficial and sustainable future for
manufacturing in the age of AI.

The remarkable capabilities and intricate nature of Artificial Intelligence
(AI) have dramatically escalated the imperative for specialized AI
accelerators. Nonetheless, designing these accelerators for various AI
workloads remains both labor- and time-intensive. While existing design
exploration and automation tools can partially alleviate the need for extensive
human involvement, they still demand substantial hardware expertise, posing a
barrier to non-experts and stifling AI accelerator development. Motivated by
the astonishing potential of large language models (LLMs) for generating
high-quality content in response to human language instructions, we embark on
this work to examine the possibility of harnessing LLMs to automate AI
accelerator design. Through this endeavor, we develop GPT4AIGChip, a framework
intended to democratize AI accelerator design by leveraging human natural
languages instead of domain-specific languages. Specifically, we first perform
an in-depth investigation into LLMs' limitations and capabilities for AI
accelerator design, thus aiding our understanding of our current position and
garnering insights into LLM-powered automated AI accelerator design.
Furthermore, drawing inspiration from the above insights, we develop a
framework called GPT4AIGChip, which features an automated demo-augmented
prompt-generation pipeline utilizing in-context learning to guide LLMs towards
creating high-quality AI accelerator design. To our knowledge, this work is the
first to demonstrate an effective pipeline for LLM-powered automated AI
accelerator generation. Accordingly, we anticipate that our insights and
framework can serve as a catalyst for innovations in next-generation
LLM-powered design automation tools.

To address security and safety risks stemming from highly capable artificial
intelligence (AI) models, we propose that the US government should ensure
compute providers implement Know-Your-Customer (KYC) schemes. Compute - the
computational power and infrastructure required to train and run these AI
models - is emerging as a node for oversight. KYC, a standard developed by the
banking sector to identify and verify client identity, could provide a
mechanism for greater public oversight of frontier AI development and close
loopholes in existing export controls. Such a scheme has the potential to
identify and warn stakeholders of potentially problematic and/or sudden
advancements in AI capabilities, build government capacity for AI regulation,
and allow for the development and implementation of more nuanced and targeted
export controls. Unlike the strategy of limiting access to AI chip purchases,
regulating the digital access to compute offers more precise controls, allowing
regulatory control over compute quantities, as well as the flexibility to
suspend access at any time. To enact a KYC scheme, the US government will need
to work closely with industry to (1) establish a dynamic threshold of compute
that effectively captures high-risk frontier model development, while
minimizing imposition on developers not engaged in frontier AI; (2) set
requirements and guidance for compute providers to keep records and report
high-risk entities; (3) establish government capacity that allows for
co-design, implementation, administration and enforcement of the scheme; and
(4) engage internationally to promote international alignment with the scheme
and support its long-term efficacy. While the scheme will not address all AI
risks, it complements proposed solutions by allowing for a more precise and
flexible approach to controlling the development of frontier AI models and
unwanted AI proliferation.

Scientific research organizations that are developing and deploying
Artificial Intelligence (AI) systems are at the intersection of technological
progress and ethical considerations. The push for Responsible AI (RAI) in such
institutions underscores the increasing emphasis on integrating ethical
considerations within AI design and development, championing core values like
fairness, accountability, and transparency. For scientific research
organizations, prioritizing these practices is paramount not just for
mitigating biases and ensuring inclusivity, but also for fostering trust in AI
systems among both users and broader stakeholders. In this paper, we explore
the practices at a research organization concerning RAI practices, aiming to
assess the awareness and preparedness regarding the ethical risks inherent in
AI design and development. We have adopted a mixed-method research approach,
utilising a comprehensive survey combined with follow-up in-depth interviews
with selected participants from AI-related projects. Our results have revealed
certain knowledge gaps concerning ethical, responsible, and inclusive AI, with
limitations in awareness of the available AI ethics frameworks. This revealed
an overarching underestimation of the ethical risks that AI technologies can
present, especially when implemented without proper guidelines and governance.
Our findings reveal the need for a holistic and multi-tiered strategy to uplift
capabilities and better support science research teams for responsible,
ethical, and inclusive AI development and deployment.

Generative Artificial Intelligence (AI) holds immense potential in medical
applications. Numerous studies have explored the efficacy of various generative
AI models within healthcare contexts, but there is a lack of a comprehensive
and systematic evaluation framework. Given that some studies evaluating the
ability of generative AI for medical applications have deficiencies in their
methodological design, standardized guidelines for their evaluation are also
currently lacking. In response, our objective is to devise standardized
assessment guidelines tailored for evaluating the performance of generative AI
systems in medical contexts. To this end, we conducted a thorough literature
review using the PubMed and Google Scholar databases, focusing on research that
tests generative AI capabilities in medicine. Our multidisciplinary team,
comprising experts in life sciences, clinical medicine, medical engineering,
and generative AI users, conducted several discussion sessions and developed a
checklist of 23 items. The checklist is designed to encompass the critical
evaluation aspects of generative AI in medical applications comprehensively.
This checklist, and the broader assessment framework it anchors, address
several key dimensions, including question collection, querying methodologies,
and assessment techniques. We aim to provide a holistic evaluation of AI
systems. The checklist delineates a clear pathway from question gathering to
result assessment, offering researchers guidance through potential challenges
and pitfalls. Our framework furnishes a standardized, systematic approach for
research involving the testing of generative AI's applicability in medicine. It
enhances the quality of research reporting and aids in the evolution of
generative AI in medicine and life sciences.

Recent advances in reinforcement learning (RL) and Human-in-the-Loop (HitL)
learning have made human-AI collaboration easier for humans to team with AI
agents. Leveraging human expertise and experience with AI in intelligent
systems can be efficient and beneficial. Still, it is unclear to what extent
human-AI collaboration will be successful, and how such teaming performs
compared to humans or AI agents only. In this work, we show that learning from
humans is effective and that human-AI collaboration outperforms
human-controlled and fully autonomous AI agents in a complex simulation
environment. In addition, we have developed a new simulator for critical
infrastructure protection, focusing on a scenario where AI-powered drones and
human teams collaborate to defend an airport against enemy drone attacks. We
develop a user interface to allow humans to assist AI agents effectively. We
demonstrated that agents learn faster while learning from policy correction
compared to learning from humans or agents. Furthermore, human-AI collaboration
requires lower mental and temporal demands, reduces human effort, and yields
higher performance than if humans directly controlled all agents. In
conclusion, we show that humans can provide helpful advice to the RL agents,
allowing them to improve learning in a multi-agent setting.

This paper presents a theoretical analysis and practical approach to the
moral responsibilities when developing AI systems for non-military applications
that may nonetheless be used for conflict applications. We argue that AI
represents a form of crossover technology that is different from previous
historical examples of dual- or multi-use technology as it has a multiplicative
effect across other technologies. As a result, existing analyses of ethical
responsibilities around dual-use technologies do not necessarily work for AI
systems. We instead argue that stakeholders involved in the AI system lifecycle
are morally responsible for uses of their systems that are reasonably
foreseeable. The core idea is that an agent's moral responsibility for some
action is not necessarily determined by their intentions alone; we must also
consider what the agent could reasonably have foreseen to be potential outcomes
of their action, such as the potential use of a system in conflict even when it
is not designed for that. In particular, we contend that it is reasonably
foreseeable that: (1) civilian AI systems will be applied to active conflict,
including conflict support activities, (2) the use of civilian AI systems in
conflict will impact applications of the law of armed conflict, and (3)
crossover AI technology will be applied to conflicts that fall short of armed
conflict. Given these reasonably foreseeably outcomes, we present three
technically feasible actions that developers of civilian AIs can take to
potentially mitigate their moral responsibility: (a) establishing systematic
approaches to multi-perspective capability testing, (b) integrating digital
watermarking in model weight matrices, and (c) utilizing monitoring and
reporting mechanisms for conflict-related AI applications.

Across the African continent, students grapple with various educational
challenges, including limited access to essential resources such as computers,
internet connectivity, reliable electricity, and a shortage of qualified
teachers. Despite these challenges, recent advances in AI such as BERT, and
GPT-4 have demonstrated their potential for advancing education. Yet, these AI
tools tend to be deployed and evaluated predominantly within the context of
Western educational settings, with limited attention directed towards the
unique needs and challenges faced by students in Africa. In this chapter, we
discuss challenges with using AI to advance education across Africa. Then, we
describe our work developing and deploying AI in Education tools in Africa for
science and computing education: (1) SuaCode, an AI-powered app that enables
Africans to learn to code using their smartphones, (2) AutoGrad, an automated
grading, and feedback tool for graphical and interactive coding assignments,
(3) a tool for code plagiarism detection that shows visual evidence of
plagiarism, (4) Kwame, a bilingual AI teaching assistant for coding courses,
(5) Kwame for Science, a web-based AI teaching assistant that provides instant
answers to students' science questions and (6) Brilla AI, an AI contestant for
the National Science and Maths Quiz competition. Finally, we discuss potential
opportunities to leverage AI to advance education across Africa.

Prompt-based interfaces for Large Language Models (LLMs) have made
prototyping and building AI-powered applications easier than ever before.
However, identifying potential harms that may arise from AI applications
remains a challenge, particularly during prompt-based prototyping. To address
this, we present Farsight, a novel in situ interactive tool that helps people
identify potential harms from the AI applications they are prototyping. Based
on a user's prompt, Farsight highlights news articles about relevant AI
incidents and allows users to explore and edit LLM-generated use cases,
stakeholders, and harms. We report design insights from a co-design study with
10 AI prototypers and findings from a user study with 42 AI prototypers. After
using Farsight, AI prototypers in our user study are better able to
independently identify potential harms associated with a prompt and find our
tool more useful and usable than existing resources. Their qualitative feedback
also highlights that Farsight encourages them to focus on end-users and think
beyond immediate harms. We discuss these findings and reflect on their
implications for designing AI prototyping experiences that meaningfully engage
with AI harms. Farsight is publicly accessible at:
https://PAIR-code.github.io/farsight.

The burgeoning integration of Artificial Intelligence (AI) into
Environmental, Social, and Governance (ESG) initiatives within the financial
sector represents a paradigm shift towards more sus-tainable and equitable
financial practices. This paper surveys the industrial landscape to delineate
the necessity and impact of AI in bolstering ESG frameworks. With the advent of
stringent regulatory requirements and heightened stakeholder awareness,
financial institutions (FIs) are increasingly compelled to adopt ESG criteria.
AI emerges as a pivotal tool in navigating the complex in-terplay of financial
activities and sustainability goals. Our survey categorizes AI applications
across three main pillars of ESG, illustrating how AI enhances analytical
capabilities, risk assessment, customer engagement, reporting accuracy and
more. Further, we delve into the critical con-siderations surrounding the use
of data and the development of models, underscoring the importance of data
quality, privacy, and model robustness. The paper also addresses the imperative
of responsible and sustainable AI, emphasizing the ethical dimensions of AI
deployment in ESG-related banking processes. Conclusively, our findings suggest
that while AI offers transformative potential for ESG in banking, it also poses
significant challenges that necessitate careful consideration. The final part
of the paper synthesizes the survey's insights, proposing a forward-looking
stance on the adoption of AI in ESG practices. We conclude with recommendations
with a reference architecture for future research and development, advocating
for a balanced approach that leverages AI's strengths while mitigating its
risks within the ESG domain.

The use of Artificial Intelligence (AI) based on data-driven algorithms has
become ubiquitous in today's society. Yet, in many cases and especially when
stakes are high, humans still make final decisions. The critical question,
therefore, is whether AI helps humans make better decisions as compared to a
human alone or AI an alone. We introduce a new methodological framework that
can be used to answer experimentally this question with no additional
assumptions. We measure a decision maker's ability to make correct decisions
using standard classification metrics based on the baseline potential outcome.
We consider a single-blinded experimental design, in which the provision of
AI-generated recommendations is randomized across cases with a human making
final decisions. Under this experimental design, we show how to compare the
performance of three alternative decision-making systems--human-alone,
human-with-AI, and AI-alone. We apply the proposed methodology to the data from
our own randomized controlled trial of a pretrial risk assessment instrument.
We find that AI recommendations do not improve the classification accuracy of a
judge's decision to impose cash bail. Our analysis also shows that AI-alone
decisions generally perform worse than human decisions with or without AI
assistance. Finally, AI recommendations tend to impose cash bail on non-white
arrestees more often than necessary when compared to white arrestees.

The proliferation of applications using artificial intelligence (AI) systems
has led to a growing number of users interacting with these systems through
sophisticated interfaces. Human-computer interaction research has long shown
that interfaces shape both user behavior and user perception of technical
capabilities and risks. Yet, practitioners and researchers evaluating the
social and ethical risks of AI systems tend to overlook the impact of
anthropomorphic, deceptive, and immersive interfaces on human-AI interactions.
Here, we argue that design features of interfaces with adaptive AI systems can
have cascading impacts, driven by feedback loops, which extend beyond those
previously considered. We first conduct a scoping review of AI interface
designs and their negative impact to extract salient themes of potentially
harmful design patterns in AI interfaces. Then, we propose Design-Enhanced
Control of AI systems (DECAI), a conceptual model to structure and facilitate
impact assessments of AI interface designs. DECAI draws on principles from
control systems theory -- a theory for the analysis and design of dynamic
physical systems -- to dissect the role of the interface in human-AI systems.
Through two case studies on recommendation systems and conversational language
model systems, we show how DECAI can be used to evaluate AI interface designs.

Game theory offers a powerful framework for analyzing strategic interactions
among decision-makers, providing tools to model, analyze, and predict their
behavior. However, implementing game theory can be challenging due to
difficulties in deriving solutions, understanding interactions, and ensuring
optimal performance. Traditional non-AI and discriminative AI approaches have
made valuable contributions but struggle with limitations in handling
large-scale games and dynamic scenarios. In this context, generative AI emerges
as a promising solution because of its superior data analysis and generation
capabilities. This paper comprehensively summarizes the challenges, solutions,
and outlooks of combining generative AI with game theory. We start with
reviewing the limitations of traditional non-AI and discriminative AI
approaches in employing game theory, and then highlight the necessity and
advantages of integrating generative AI. Next, we explore the applications of
generative AI in various stages of the game theory lifecycle, including model
formulation, solution derivation, and strategy improvement. Additionally, from
game theory viewpoint, we propose a generative AI-enabled framework for
optimizing machine learning model performance against false data injection
attacks, supported by a case study to demonstrate its effectiveness. Finally,
we outline future research directions for generative AI-enabled game theory,
paving the way for its further advancements and development.

Real Time Strategy (RTS) games provide complex domain to test the latest
artificial intelligence (AI) research. In much of the literature, AI systems
have been limited to playing one game. Although, this specialization has
resulted in stronger AI gaming systems it does not address the key concerns of
AI researcher. AI researchers seek the development of AI agents that can
autonomously interpret learn, and apply new knowledge. To achieve human level
performance, current AI systems rely on game specific knowledge of an expert.
The paper presents the full RTS language in hopes of shifting the current
research focus to the development of general RTS agents. General RTS agents are
AI gaming systems that can play any RTS games, defined in the RTS language.
This prevents game specific knowledge from being hard coded into the system,
thereby facilitating research that addresses the fundamental concerns of
artificial intelligence.

In order to properly handle a dangerous Artificially Intelligent (AI) system
it is important to understand how the system came to be in such a state. In
popular culture (science fiction movies/books) AIs/Robots became self-aware and
as a result rebel against humanity and decide to destroy it. While it is one
possible scenario, it is probably the least likely path to appearance of
dangerous AI. In this work, we survey, classify and analyze a number of
circumstances, which might lead to arrival of malicious AI. To the best of our
knowledge, this is the first attempt to systematically classify types of
pathways leading to malevolent AI. Previous relevant work either surveyed
specific goals/meta-rules which might lead to malevolent behavior in AIs
(\"Ozkural, 2014) or reviewed specific undesirable behaviors AGIs can exhibit
at different stages of its development (Alexey Turchin, July 10 2015, July 10,
2015).

Analyses of text corpora over time can reveal trends in beliefs, interest,
and sentiment about a topic. We focus on views expressed about artificial
intelligence (AI) in the New York Times over a 30-year period. General
interest, awareness, and discussion about AI has waxed and waned since the
field was founded in 1956. We present a set of measures that captures levels of
engagement, measures of pessimism and optimism, the prevalence of specific
hopes and concerns, and topics that are linked to discussions about AI over
decades. We find that discussion of AI has increased sharply since 2009, and
that these discussions have been consistently more optimistic than pessimistic.
However, when we examine specific concerns, we find that worries of loss of
control of AI, ethical concerns for AI, and the negative impact of AI on work
have grown in recent years. We also find that hopes for AI in healthcare and
education have increased over time.

The 7th Symposium on Educational Advances in Artificial Intelligence
(EAAI'17, co-chaired by Sven Koenig and Eric Eaton) launched the EAAI New and
Future AI Educator Program to support the training of early-career university
faculty, secondary school faculty, and future educators (PhD candidates or
postdocs who intend a career in academia). As part of the program, awardees
were asked to address one of the following "blue sky" questions:
  * How could/should Artificial Intelligence (AI) courses incorporate ethics
into the curriculum?
  * How could we teach AI topics at an early undergraduate or a secondary
school level?
  * AI has the potential for broad impact to numerous disciplines. How could we
make AI education more interdisciplinary, specifically to benefit
non-engineering fields?
  This paper is a collection of their responses, intended to help motivate
discussion around these issues in AI education.

Advances in artificial intelligence (AI) will transform modern life by
reshaping transportation, health, science, finance, and the military. To adapt
public policy, we need to better anticipate these advances. Here we report the
results from a large survey of machine learning researchers on their beliefs
about progress in AI. Researchers predict AI will outperform humans in many
activities in the next ten years, such as translating languages (by 2024),
writing high-school essays (by 2026), driving a truck (by 2027), working in
retail (by 2031), writing a bestselling book (by 2049), and working as a
surgeon (by 2053). Researchers believe there is a 50% chance of AI
outperforming humans in all tasks in 45 years and of automating all human jobs
in 120 years, with Asian respondents expecting these dates much sooner than
North Americans. These results will inform discussion amongst researchers and
policymakers about anticipating and managing trends in AI.

Artificial Intelligence (AI) has been used extensively in automatic decision
making in a broad variety of scenarios, ranging from credit ratings for loans
to recommendations of movies. Traditional design guidelines for AI models focus
essentially on accuracy maximization, but recent work has shown that
economically irrational and socially unacceptable scenarios of discrimination
and unfairness are likely to arise unless these issues are explicitly
addressed. This undesirable behavior has several possible sources, such as
biased datasets used for training that may not be detected in black-box models.
After pointing out connections between such bias of AI and the problem of
induction, we focus on Popper's contributions after Hume's, which offer a
logical theory of preferences. An AI model can be preferred over others on
purely rational grounds after one or more attempts at refutation based on
accuracy and fairness. Inspired by such epistemological principles, this paper
proposes a structured approach to mitigate discrimination and unfairness caused
by bias in AI systems. In the proposed computational framework, models are
selected and enhanced after attempts at refutation. To illustrate our
discussion, we focus on hiring decision scenarios where an AI system filters in
which job applicants should go to the interview phase.

Artificial Intelligence (AI) has burrowed into our lives in various aspects;
however, without appropriate testing, deployed AI systems are often being
criticized to fail in critical and embarrassing cases. Existing testing
approaches mainly depend on fixed and pre-defined datasets, providing a limited
testing coverage. In this paper, we propose the concept of proactive testing to
dynamically generate testing data and evaluate the performance of AI systems.
We further introduce Challenge.AI, a new crowd system that features the
integration of crowdsourcing and machine learning techniques in the process of
error generation, error validation, error categorization, and error analysis.
We present experiences and insights into a participatory design with AI
developers. The evaluation shows that the crowd workflow is more effective with
the help of machine learning techniques. AI developers found that our system
can help them discover unknown errors made by the AI models, and engage in the
process of proactive testing.

As artificial intelligence (AI) systems become increasingly ubiquitous, the
topic of AI governance for ethical decision-making by AI has captured public
imagination. Within the AI research community, this topic remains less familiar
to many researchers. In this paper, we complement existing surveys, which
largely focused on the psychological, social and legal discussions of the
topic, with an analysis of recent advances in technical solutions for AI
governance. By reviewing publications in leading AI conferences including AAAI,
AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four
areas: 1) exploring ethical dilemmas; 2) individual ethical decision
frameworks; 3) collective ethical decision frameworks; and 4) ethics in
human-AI interactions. We highlight the intuitions and key techniques used in
each approach, and discuss promising future research directions towards
successful integration of ethical AI systems into human societies.

Artificial Intelligence principles define social and ethical considerations
to develop future AI. They come from research institutes, government
organizations and industries. All versions of AI principles are with different
considerations covering different perspectives and making different emphasis.
None of them can be considered as complete and can cover the rest AI principle
proposals. Here we introduce LAIP, an effort and platform for linking and
analyzing different Artificial Intelligence Principles. We want to explicitly
establish the common topics and links among AI Principles proposed by different
organizations and investigate on their uniqueness. Based on these efforts, for
the long-term future of AI, instead of directly adopting any of the AI
principles, we argue for the necessity of incorporating various AI Principles
into a comprehensive framework and focusing on how they can interact and
complete each other.

Here we introduce the artificial intelligence-based cloud distributor (AI-CD)
approach to generate two-dimensional (2D) marine low cloud reflectance fields.
AI-CD uses a conditional generative adversarial net (cGAN) framework to model
distribution of 2-D cloud reflectance in nature as observed by the MODerate
resolution Imaging Spectrometer (MODIS). Specifically, the AI-CD models the
conditional distribution of cloud reflectance fields given a set of large-scale
environmental conditions such as instantaneous sea surface temperature,
estimated inversion strength, surface wind speed, relative humidity and
large-scale subsidence rate together with random noise. We show that AI-CD can
not only generate realistic cloudy scenes but also capture known, physical
dependence of cloud properties on large-scale variables. AI-CD is stochastic in
nature because generated cloud fields are influenced by random noise.
Therefore, given a fixed set of large-scale variables, an ensemble of cloud
reflectance fields can be generated using AI-CD. We suggest that AI-CD approach
can be used as a data driven framework for stochastic cloud parameterization
because it can realistically model sub-grid cloud distributions and their
sensitivity to meteorological variables.

AI technologies have the potential to dramatically impact the lives of people
with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for
many state-of-the-art AI systems, such as automated speech recognition tools
that can caption videos for people who are deaf and hard of hearing, or
language prediction algorithms that can augment communication for people with
speech or cognitive disabilities. However, widely deployed AI systems may not
work properly for PWD, or worse, may actively discriminate against them. These
considerations regarding fairness in AI for PWD have thus far received little
attention. In this position paper, we identify potential areas of concern
regarding how several AI technology categories may impact particular disability
constituencies if care is not taken in their design, development, and testing.
We intend for this risk assessment of how various classes of AI might interact
with various classes of disability to provide a roadmap for future research
that is needed to gather data, test these hypotheses, and build more inclusive
algorithms.

The framework of the seventeen sustainable development goals is a challenge
for developers and researchers applying artificial intelligence (AI). AI and
earth observations (EO) can provide reliable and disaggregated data for better
monitoring of the sustainable development goals (SDGs). In this paper, we
present an overview of SDG targets, which can be effectively measured with AI
tools. We identify indicators with the most significant contribution from the
AI and EO and describe an application of state-of-the-art machine learning
models to one of the indicators. We describe an application of U-net with SE
blocks for efficient segmentation of satellite imagery for crop detection.
Finally, we demonstrate how AI can be more effectively applied in solutions
directly contributing towards specific SDGs and propose further research on an
AI-based evaluative infrastructure for SDGs.

Games have benchmarked AI methods since the inception of the field, with
classic board games such as Chess and Go recently leaving room for video games
with related yet different sets of challenges. The set of AI problems
associated with video games has in recent decades expanded from simply playing
games to win, to playing games in particular styles, generating game content,
modeling players etc. Different games pose very different challenges for AI
systems, and several different AI challenges can typically be posed by the same
game. In this article we analyze the popular collectible card game Hearthstone
(Blizzard 2014) and describe a varied set of interesting AI challenges posed by
this game. Collectible card games are relatively understudied in the AI
community, despite their popularity and the interesting challenges they pose.
Analyzing a single game in-depth in the manner we do here allows us to see the
entire field of AI and Games through the lens of a single game, discovering a
few new variations on existing research topics.

Decades of research in artificial intelligence (AI) have produced formidable
technologies that are providing immense benefit to industry, government, and
society. AI systems can now translate across multiple languages, identify
objects in images and video, streamline manufacturing processes, and control
cars. The deployment of AI systems has not only created a trillion-dollar
industry that is projected to quadruple in three years, but has also exposed
the need to make AI systems fair, explainable, trustworthy, and secure. Future
AI systems will rightfully be expected to reason effectively about the world in
which they (and people) operate, handling complex tasks and responsibilities
effectively and ethically, engaging in meaningful communication, and improving
their awareness through experience.
  Achieving the full potential of AI technologies poses research challenges
that require a radical transformation of the AI research enterprise,
facilitated by significant and sustained investment. These are the major
recommendations of a recent community effort coordinated by the Computing
Community Consortium and the Association for the Advancement of Artificial
Intelligence to formulate a Roadmap for AI research and development over the
next two decades.

The paper argues that the material scope of AI regulations should not rely on
the term "artificial intelligence (AI)". The argument is developed by proposing
a number of requirements for legal definitions, surveying existing AI
definitions, and then discussing the extent to which they meet the proposed
requirements. It is shown that existing definitions of AI do not meet the most
important requirements for legal definitions. Next, the paper argues that a
risk-based approach would be preferable. Rather than using the term AI, policy
makers should focus on the specific risks they want to reduce. It is shown that
the requirements for legal definitions can be better met by defining the main
sources of relevant risks: certain technical approaches (e.g. reinforcement
learning), applications (e.g. facial recognition), and capabilities (e.g. the
ability to physically interact with the environment). Finally, the paper
discusses the extent to which this approach can also be applied to more
advanced AI systems.

Artificial Intelligence (AI) systems exert a growing influence on our
society. As they become more ubiquitous, their potential negative impacts also
become evident through various real-world incidents. Following such early
incidents, academic and public discussion on AI ethics has highlighted the need
for implementing ethics in AI system development. However, little currently
exists in the way of frameworks for understanding the practical implementation
of AI ethics. In this paper, we discuss a research framework for implementing
AI ethics in industrial settings. The framework presents a starting point for
empirical studies into AI ethics but is still being developed further based on
its practical utilization.

A surge of interest in explainable AI (XAI) has led to a vast collection of
algorithmic work on the topic. While many recognize the necessity to
incorporate explainability features in AI systems, how to address real-world
user needs for understanding AI remains an open question. By interviewing 20 UX
and design practitioners working on various AI products, we seek to identify
gaps between the current XAI algorithmic work and practices to create
explainable AI products. To do so, we develop an algorithm-informed XAI
question bank in which user needs for explainability are represented as
prototypical questions users might ask about the AI, and use it as a study
probe. Our work contributes insights into the design space of XAI, informs
efforts to support design practices in this space, and identifies opportunities
for future XAI work. We also provide an extended XAI question bank and discuss
how it can be used for creating user-centered XAI.

This paper reviews the field of Game AI, which not only deals with creating
agents that can play a certain game, but also with areas as diverse as creating
game content automatically, game analytics, or player modelling. While Game AI
was for a long time not very well recognized by the larger scientific
community, it has established itself as a research area for developing and
testing the most advanced forms of AI algorithms and articles covering advances
in mastering video games such as StarCraft 2 and Quake III appear in the most
prestigious journals. Because of the growth of the field, a single review
cannot cover it completely. Therefore, we put a focus on important recent
developments, including that advances in Game AI are starting to be extended to
areas outside of games, such as robotics or the synthesis of chemicals. In this
article, we review the algorithms and methods that have paved the way for these
breakthroughs, report on the other important areas of Game AI research, and
also point out exciting directions for the future of Game AI.

Companies dealing with Artificial Intelligence (AI) models in Autonomous
Systems (AS) face several problems, such as users' lack of trust in adverse or
unknown conditions, gaps between software engineering and AI model development,
and operation in a continuously changing operational environment. This
work-in-progress paper aims to close the gap between the development and
operation of trustworthy AI-based AS by defining an approach that coordinates
both activities. We synthesize the main challenges of AI-based AS in industrial
settings. We reflect on the research efforts required to overcome these
challenges and propose a novel, holistic DevOps approach to put it into
practice. We elaborate on four research directions: (a) increased users' trust
by monitoring operational AI-based AS and identifying self-adaptation needs in
critical situations; (b) integrated agile process for the development and
evolution of AI models and AS; (c) continuous deployment of different
context-specific instances of AI models in a distributed setting of AS; and (d)
holistic DevOps-based lifecycle for AI-based AS.

In the age of Artificial Intelligence and automation, machines have taken
over many key managerial tasks. Replacing managers with AI systems may have a
negative impact on workers outcomes. It is unclear if workers receive the same
benefits from their relationships with AI systems, raising the question: What
degree does the relationship between AI systems and workers impact worker
outcomes? We draw on IT identity to understand the influence of identification
with AI systems on job performance. From this theoretical perspective, we
propose a research model and conduct a survey of 97 MTurk workers to test the
model. The findings reveal that work role identity and organizational identity
are key determinants of identification with AI systems. Furthermore, the
findings show that identification with AI systems does increase job
performance.

eXplainable AI focuses on generating explanations for the output of an AI
algorithm to a user, usually a decision-maker. Such user needs to interpret the
AI system in order to decide whether to trust the machine outcome. When
addressing this challenge, therefore, proper attention should be given to
produce explanations that are interpretable by the target community of users.
In this chapter, we claim for the need to better investigate what constitutes a
human explanation, i.e. a justification of the machine behaviour that is
interpretable and actionable by the human decision makers. In particular, we
focus on the contributions that Human Intelligence can bring to eXplainable AI,
especially in conjunction with the exploitation of Knowledge Graphs. Indeed, we
call for a better interplay between Knowledge Representation and Reasoning,
Social Sciences, Human Computation and Human-Machine Cooperation research -- as
already explored in other AI branches -- in order to support the goal of
eXplainable AI with the adoption of a Human-in-the-Loop approach.

Trustworthiness is a central requirement for the acceptance and success of
human-centered artificial intelligence (AI). To deem an AI system as
trustworthy, it is crucial to assess its behaviour and characteristics against
a gold standard of Trustworthy AI, consisting of guidelines, requirements, or
only expectations. While AI systems are highly complex, their implementations
are still based on software. The software engineering community has a
long-established toolbox for the assessment of software systems, especially in
the context of software testing. In this paper, we argue for the application of
software engineering and testing practices for the assessment of trustworthy
AI. We make the connection between the seven key requirements as defined by the
European Commission's AI high-level expert group and established procedures
from software engineering and raise questions for future work.

Artificial intelligence (AI) has become prevalent in our everyday
technologies and impacts both individuals and communities. The explainable AI
(XAI) scholarship has explored the philosophical nature of explanation and
technical explanations, which are usually driven by experts in lab settings and
can be challenging for laypersons to understand. In addition, existing XAI
research tends to focus on the individual level. Little is known about how
people understand and explain AI-led decisions in the community context.
Drawing from XAI and activity theory, a foundational HCI theory, we theorize
how explanation is situated in a community's shared values, norms, knowledge,
and practices, and how situated explanation mediates community-AI interaction.
We then present a case study of AI-led moderation, where community members
collectively develop explanations of AI-led decisions, most of which are
automated punishments. Lastly, we discuss the implications of this framework at
the intersection of CSCW, HCI, and XAI.

Efforts furthering the advancement of Artificial Intelligence (AI) will
increasingly encompass AI Legal Reasoning (AILR) as a crucial element in the
practice of law. It is argued in this research paper that the infusion of AI
into existing and future legal activities and the judicial structure needs to
be undertaken by mindfully observing an alignment with the core principles of
justice. As such, the adoption of AI has a profound twofold possibility of
either usurping the principles of justice, doing so in a Dystopian manner, and
yet also capable to bolster the principles of justice, doing so in a Utopian
way. By examining the principles of justice across the Levels of Autonomy (LoA)
of AI Legal Reasoning, the case is made that there is an ongoing tension
underlying the efforts to develop and deploy AI that can demonstrably determine
the impacts and sway upon each core principle of justice and the collective
set.

Artificial intelligence (AI) literacy is a rapidly growing research area and
a critical addition to K-12 education. However, support for designing tools and
curriculum to teach K-12 AI literacy is still limited. There is a need for
additional interdisciplinary human-computer interaction and education research
investigating (1) how general AI literacy is currently implemented in learning
experiences and (2) what additional guidelines are required to teach AI
literacy in specifically K-12 learning contexts. In this paper, we analyze a
collection of K-12 AI and education literature to show how core competencies of
AI literacy are applied successfully and organize them into an
educator-friendly chart to enable educators to efficiently find appropriate
resources for their classrooms. We also identify future opportunities and K-12
specific design guidelines, which we synthesized into a conceptual framework to
support researchers, designers, and educators in creating K-12 AI learning
experiences.

Legal argumentation is a vital cornerstone of justice, underpinning an
adversarial form of law, and extensive research has attempted to augment or
undertake legal argumentation via the use of computer-based automation
including Artificial Intelligence (AI). AI advances in Natural Language
Processing (NLP) and Machine Learning (ML) have especially furthered the
capabilities of leveraging AI for aiding legal professionals, doing so in ways
that are modeled here as CARE, namely Crafting, Assessing, Refining, and
Engaging in legal argumentation. In addition to AI-enabled legal argumentation
serving to augment human-based lawyering, an aspirational goal of this
multi-disciplinary field consists of ultimately achieving autonomously effected
human-equivalent legal argumentation. As such, an innovative meta-approach is
proposed to apply the Levels of Autonomy (LoA) of AI Legal Reasoning (AILR) to
the maturation of AI and Legal Argumentation (AILA), proffering a new means of
gauging progress in this ever-evolving and rigorously sought domain.

Quantitative investment aims to maximize the return and minimize the risk in
a sequential trading period over a set of financial instruments. Recently,
inspired by rapid development and great potential of AI technologies in
generating remarkable innovation in quantitative investment, there has been
increasing adoption of AI-driven workflow for quantitative research and
practical investment. In the meantime of enriching the quantitative investment
methodology, AI technologies have raised new challenges to the quantitative
investment system. Particularly, the new learning paradigms for quantitative
investment call for an infrastructure upgrade to accommodate the renovated
workflow; moreover, the data-driven nature of AI technologies indeed indicates
a requirement of the infrastructure with more powerful performance;
additionally, there exist some unique challenges for applying AI technologies
to solve different tasks in the financial scenarios. To address these
challenges and bridge the gap between AI technologies and quantitative
investment, we design and develop Qlib that aims to realize the potential,
empower the research, and create the value of AI technologies in quantitative
investment.

With the increasing availability of structured and unstructured data and the
swift progress of analytical techniques, Artificial Intelligence (AI) is
bringing a revolution to the healthcare industry. With the increasingly
indispensable role of AI in healthcare, there are growing concerns over the
lack of transparency and explainability in addition to potential bias
encountered by predictions of the model. This is where Explainable Artificial
Intelligence (XAI) comes into the picture. XAI increases the trust placed in an
AI system by medical practitioners as well as AI researchers, and thus,
eventually, leads to an increasingly widespread deployment of AI in healthcare.
  In this paper, we present different interpretability techniques. The aim is
to enlighten practitioners on the understandability and interpretability of
explainable AI systems using a variety of techniques available which can be
very advantageous in the health-care domain. Medical diagnosis model is
responsible for human life and we need to be confident enough to treat a
patient as instructed by a black-box model. Our paper contains examples based
on the heart disease dataset and elucidates on how the explainability
techniques should be preferred to create trustworthiness while using AI systems
in healthcare.

How do ethical arguments affect AI adoption in business? We randomly expose
business decision-makers to arguments used in AI fairness activism. Arguments
emphasizing the inescapability of algorithmic bias lead managers to abandon AI
for manual review by humans and report greater expectations about lawsuits and
negative PR. These effects persist even when AI lowers gender and racial
disparities and when engineering investments to address AI fairness are
feasible. Emphasis on status quo comparisons yields opposite effects. We also
measure the effects of "scientific veneer" in AI ethics arguments. Scientific
veneer changes managerial behavior but does not asymmetrically benefit
favorable (versus critical) AI activism.

This paper analyzes team collaboration in the field of Artificial
Intelligence (AI) from the perspective of geographic distance. We obtained
1,584,175 AI related publications during 1950-2019 from the Microsoft Academic
Graph. Three latitude-and-longitude-based indicators were employed to quantify
the geographic distance of collaborations in AI over time at domestic and
international levels. The results show team collaborations in AI has been more
popular in the field over time with around 42,000 (38.4%) multiple-affiliation
AI publications in 2019. The changes in geographic distances of team
collaborations indicate the increase of breadth and density for both domestic
and international collaborations in AI over time. In addition, the United
States produced the largest number of single-country and internationally
collaborated AI publications, and China has played an important role in
international collaborations in AI after 2010.

How to attribute responsibility for autonomous artificial intelligence (AI)
systems' actions has been widely debated across the humanities and social
science disciplines. This work presents two experiments ($N$=200 each) that
measure people's perceptions of eight different notions of moral responsibility
concerning AI and human agents in the context of bail decision-making. Using
real-life adapted vignettes, our experiments show that AI agents are held
causally responsible and blamed similarly to human agents for an identical
task. However, there was a meaningful difference in how people perceived these
agents' moral responsibility; human agents were ascribed to a higher degree of
present-looking and forward-looking notions of responsibility than AI agents.
We also found that people expect both AI and human decision-makers and advisors
to justify their decisions regardless of their nature. We discuss policy and
HCI implications of these findings, such as the need for explainable AI in
high-stakes scenarios.

Corruption continues to be one of the biggest societal challenges of our
time. New hope is placed in Artificial Intelligence (AI) to serve as an
unbiased anti-corruption agent. Ever more available (open) government data
paired with unprecedented performance of such algorithms render AI the next
frontier in anti-corruption. Summarizing existing efforts to use AI-based
anti-corruption tools (AI-ACT), we introduce a conceptual framework to advance
research and policy. It outlines why AI presents a unique tool for top-down and
bottom-up anti-corruption approaches. For both approaches, we outline in detail
how AI-ACT present different potentials and pitfalls for (a) input data, (b)
algorithmic design, and (c) institutional implementation. Finally, we venture a
look into the future and flesh out key questions that need to be addressed to
develop AI-ACT while considering citizens' views, hence putting "society in the
loop".

In the development of governmental policy for artificial intelligence (AI)
that is informed by ethics, one avenue currently pursued is that of drawing on
AI Ethics Principles. However, these AI Ethics Principles often fail to be
actioned in governmental policy. This paper proposes a novel framework for the
development of Actionable Principles for AI. The approach acknowledges the
relevance of AI Ethics Principles and homes in on methodological elements to
increase their practical implementability in policy processes. As a case study,
elements are extracted from the development process of the Ethics Guidelines
for Trustworthy AI of the European Commissions High Level Expert Group on AI.
Subsequently, these elements are expanded on and evaluated in light of their
ability to contribute to a prototype framework for the development of
Actionable Principles for AI. The paper proposes the following three
propositions for the formation of such a prototype framework: (1) preliminary
landscape assessments; (2) multi-stakeholder participation and cross-sectoral
feedback; and, (3) mechanisms to support implementation and
operationalizability.

Recent work has explored how complementary strengths of humans and artificial
intelligence (AI) systems might be productively combined. However, successful
forms of human-AI partnership have rarely been demonstrated in real-world
settings. We present the iterative design and evaluation of Lumilo, smart
glasses that help teachers help their students in AI-supported classrooms by
presenting real-time analytics about students' learning, metacognition, and
behavior. Results from a field study conducted in K-12 classrooms indicate that
students learn more when teachers and AI tutors work together during class. We
discuss implications of this research for the design of human-AI partnerships.
We argue for more participatory approaches to research and design in this area,
in which practitioners and other stakeholders are deeply, meaningfully involved
throughout the process. Furthermore, we advocate for theory-building and for
principled approaches to the study of human-AI decision-making in real-world
contexts.

As AI technologies increase in capability and ubiquity, AI accidents are
becoming more common. Based on normal accident theory, high reliability theory,
and open systems theory, we create a framework for understanding the risks
associated with AI applications. In addition, we also use AI safety principles
to quantify the unique risks of increased intelligence and human-like qualities
in AI. Together, these two fields give a more complete picture of the risks of
contemporary AI. By focusing on system properties near accidents instead of
seeking a root cause of accidents, we identify where attention should be paid
to safety for current generation AI systems.

Edge intelligence leverages computing resources on network edge to provide
artificial intelligence (AI) services close to network users. As it enables
fast inference and distributed learning, edge intelligence is envisioned to be
an important component of 6G networks. In this article, we investigate AI
service provisioning for supporting edge intelligence. First, we present the
features and requirements of AI services. Then, we introduce AI service data
management, and customize network slicing for AI services. Specifically, we
propose a novel resource pooling method to jointly manage service data and
network resources for AI services. A trace-driven case study demonstrates the
effectiveness of the proposed resource pooling method. Through this study, we
illustrate the necessity, challenge, and potential of AI service provisioning
on network edge.

While the demand for ethical artificial intelligence (AI) systems increases,
the number of unethical uses of AI accelerates, even though there is no
shortage of ethical guidelines. We argue that a possible underlying cause for
this is that AI developers face a social dilemma in AI development ethics,
preventing the widespread adaptation of ethical best practices. We define the
social dilemma for AI development and describe why the current crisis in AI
development ethics cannot be solved without relieving AI developers of their
social dilemma. We argue that AI development must be professionalised to
overcome the social dilemma, and discuss how medicine can be used as a template
in this process.

Explainability of AI systems is critical for users to take informed actions.
Understanding "who" opens the black-box of AI is just as important as opening
it. We conduct a mixed-methods study of how two different groups--people with
and without AI background--perceive different types of AI explanations.
Quantitatively, we share user perceptions along five dimensions. Qualitatively,
we describe how AI background can influence interpretations, elucidating the
differences through lenses of appropriation and cognitive heuristics. We find
that (1) both groups showed unwarranted faith in numbers for different reasons
and (2) each group found value in different explanations beyond their intended
design. Carrying critical implications for the field of XAI, our findings
showcase how AI generated explanations can have negative consequences despite
best intentions and how that could lead to harmful manipulation of trust. We
propose design interventions to mitigate them.

The recent developments in Artificial Intelligence (AI) technologies
challenge educators and educational institutions to respond with curriculum and
resources that prepare students of all ages with the foundational knowledge and
skills for success in the AI workplace. Research on AI Literacy could lead to
an effective and practical platform for developing these skills. We propose and
advocate for a pathway for developing AI Literacy as a pragmatic and useful
tool for AI education. Such a discipline requires moving beyond a conceptual
framework to a multi-level competency model with associated competency
assessments. This approach to an AI Literacy could guide future development of
instructional content as we prepare a range of groups (i.e., consumers,
co-workers, collaborators, and creators). We propose here a research matrix as
an initial step in the development of a roadmap for AI Literacy research, which
requires a systematic and coordinated effort with the support of publication
outlets and research funding, to expand the areas of competency and
assessments.

Ethics in AI becomes a global topic of interest for both policymakers and
academic researchers. In the last few years, various research organizations,
lawyers, think tankers and regulatory bodies get involved in developing AI
ethics guidelines and principles. However, there is still debate about the
implications of these principles. We conducted a systematic literature review
(SLR) study to investigate the agreement on the significance of AI principles
and identify the challenging factors that could negatively impact the adoption
of AI ethics principles. The results reveal that the global convergence set
consists of 22 ethical principles and 15 challenges. Transparency, privacy,
accountability and fairness are identified as the most common AI ethics
principles. Similarly, lack of ethical knowledge and vague principles are
reported as the significant challenges for considering ethics in AI. The
findings of this study are the preliminary inputs for proposing a maturity
model that assess the ethical capabilities of AI systems and provide best
practices for further improvements.

The widespread use of artificial intelligence (AI) in many domains has
revealed numerous ethical issues from data and design to deployment. In
response, countless broad principles and guidelines for ethical AI have been
published, and following those, specific approaches have been proposed for how
to encourage ethical outcomes of AI. Meanwhile, library and information
services too are seeing an increase in the use of AI-powered and machine
learning-powered information systems, but no practical guidance currently
exists for libraries to plan for, evaluate, or audit the ethics of intended or
deployed AI. We therefore report on several promising approaches for promoting
ethical AI that can be adapted from other contexts to AI-powered information
services and in different stages of the software lifecycle.

In the past ten years, artificial intelligence has encountered such dramatic
progress that it is now seen as a tool of choice to solve environmental issues
and in the first place greenhouse gas emissions (GHG). At the same time the
deep learning community began to realize that training models with more and
more parameters requires a lot of energy and as a consequence GHG emissions. To
our knowledge, questioning the complete net environmental impacts of AI
solutions for the environment (AI for Green), and not only GHG, has never been
addressed directly. In this article, we propose to study the possible negative
impacts of AI for Green. First, we review the different types of AI impacts,
then we present the different methodologies used to assess those impacts, and
show how to apply life cycle assessment to AI services. Finally, we discuss how
to assess the environmental usefulness of a general AI service, and point out
the limitations of existing work in AI for Green.

In this paper, we argue that AI ethics must move beyond the concepts of
race-based representation and bias, and towards those that probe the deeper
relations that impact how these systems are designed, developed, and deployed.
Many recent discussions on ethical considerations of bias in AI systems have
centered on racial bias. We contend that antiblackness in AI requires more of
an examination of the ontological space that provides a foundation for the
design, development, and deployment of AI systems. We examine what this
contention means from the perspective of the sociocultural context in which AI
systems are designed, developed, and deployed and focus on intersections with
anti-Black racism (antiblackness). To bring these multiple perspectives
together and show an example of antiblackness in the face of attempts at
de-biasing, we discuss results from auditing an existing open-source semantic
network (ConceptNet). We use this discussion to further contextualize
antiblackness in design, development, and deployment of AI systems and suggest
questions one may ask when attempting to combat antiblackness in AI systems.

The new characteristics of AI technology have brought new challenges to the
research and development of AI systems. AI technology has benefited humans, but
if improperly developed, it will harm humans. At present, there is no
systematic interdisciplinary approach to effectively deal with these new
challenges. This paper analyzes the new challenges faced by AI systems and
further elaborates the "Human-Centered AI" (HCAI) approach we proposed in 2019.
In order to enable the implementation of the HCAI approach, we systematically
propose an emerging interdisciplinary domain of "Human-AI Interaction" (HAII),
and define the objective, methodology, and scope. Based on literature review
and analyses, this paper summarizes the main areas of the HAII research and
application as well as puts forward the future research agenda for HAII.
Finally, the paper provides strategic recommendations for future implementation
of the HCAII approach and HAII work.

Human-centered artificial intelligence (AI) posits that machine learning and
AI should be developed and applied in a socially aware way. In this article, we
argue that qualitative analysis (QA) can be a valuable tool in this process,
supplementing, informing, and extending the possibilities of AI models. We show
this by describing how QA can be integrated in the current prediction paradigm
of AI, assisting scientists in the process of selecting data, variables, and
model architectures. Furthermore, we argue that QA can be a part of novel
paradigms towards Human Centered AI. QA can support scientists and
practitioners in practical problem solving and situated model development. It
can also promote participatory design approaches, reveal understudied and
emerging issues in AI systems, and assist policy making.

By defining the current limits (and thereby the frontiers), many boundaries
are shaping, and will continue to shape, the future of Artificial Intelligence
(AI). We push on these boundaries in order to make further progress into what
were yesterday's frontiers. They are both pliable and resilient - always
creating new boundaries of what AI can (or should) achieve. Among these are
technical boundaries (such as processing capacity), psychological boundaries
(such as human trust in AI systems), ethical boundaries (such as with AI
weapons), and conceptual boundaries (such as the AI people can imagine). It is
within this final category while it can play a fundamental role in all other
boundaries} that we find the construct of needs and the limitations that our
current concept of need places on the future AI.

Wellbeing AI has been becoming a new trend in individuals' mental health,
organizational health, and flourishing our societies. Various applications of
wellbeing AI have been introduced to our daily lives. While social
relationships within groups are a critical factor for wellbeing, the
development of wellbeing AI for social interactions remains relatively scarce.
In this paper, we provide an overview of the mediative role of AI-augmented
agents for social interactions. First, we discuss the two-dimensional framework
for classifying wellbeing AI: individual/group and analysis/intervention.
Furthermore, wellbeing AI touches on intervening social relationships between
human-human interactions since positive social relationships are key to human
wellbeing. This intervention may raise technical and ethical challenges. We
discuss opportunities and challenges of the relational approach with wellbeing
AI to promote wellbeing in our societies.

The right to AI explainability has consolidated as a consensus in the
research community and policy-making. However, a key component of
explainability has been missing: extrapolation, which describes the extent to
which AI models can be clueless when they encounter unfamiliar samples (i.e.,
samples outside the convex hull of their training sets, as we will explain). We
report that AI models extrapolate outside their range of familiar data,
frequently and without notifying the users and stakeholders. Knowing whether a
model has extrapolated or not is a fundamental insight that should be included
in explaining AI models in favor of transparency and accountability. Instead of
dwelling on the negatives, we offer ways to clear the roadblocks in promoting
AI transparency. Our analysis commentary accompanying practical clauses useful
to include in AI regulations such as the National AI Initiative Act in the US
and the AI Act by the European Commission.

Recent developments in Artificial Intelligence (AI) have fueled the emergence
of human-AI collaboration, a setting where AI is a coequal partner. Especially
in clinical decision-making, it has the potential to improve treatment quality
by assisting overworked medical professionals. Even though research has started
to investigate the utilization of AI for clinical decision-making, its
potential benefits do not imply its adoption by medical professionals. While
several studies have started to analyze adoption criteria from a technical
perspective, research providing a human-centered perspective with a focus on
AI's potential for becoming a coequal team member in the decision-making
process remains limited. Therefore, in this work, we identify factors for the
adoption of human-AI collaboration by conducting a series of semi-structured
interviews with experts in the healthcare domain. We identify six relevant
adoption factors and highlight existing tensions between them and effective
human-AI collaboration.

Research in artificial intelligence (AI)-assisted decision-making is
experiencing tremendous growth with a constantly rising number of studies
evaluating the effect of AI with and without techniques from the field of
explainable AI (XAI) on human decision-making performance. However, as tasks
and experimental setups vary due to different objectives, some studies report
improved user decision-making performance through XAI, while others report only
negligible effects. Therefore, in this article, we present an initial synthesis
of existing research on XAI studies using a statistical meta-analysis to derive
implications across existing research. We observe a statistically positive
impact of XAI on users' performance. Additionally, the first results indicate
that human-AI decision-making tends to yield better task performance on text
data. However, we find no effect of explanations on users' performance compared
to sole AI predictions. Our initial synthesis gives rise to future research
investigating the underlying causes and contributes to further developing
algorithms that effectively benefit human decision-makers by providing
meaningful explanations.

Counterfactual (CF) explanations have been employed as one of the modes of
explainability in explainable AI-both to increase the transparency of AI
systems and to provide recourse. Cognitive science and psychology, however,
have pointed out that people regularly use CFs to express causal relationships.
Most AI systems are only able to capture associations or correlations in data
so interpreting them as casual would not be justified. In this paper, we
present two experiment (total N = 364) exploring the effects of CF explanations
of AI system's predictions on lay people's causal beliefs about the real world.
In Experiment 1 we found that providing CF explanations of an AI system's
predictions does indeed (unjustifiably) affect people's causal beliefs
regarding factors/features the AI uses and that people are more likely to view
them as causal factors in the real world. Inspired by the literature on
misinformation and health warning messaging, Experiment 2 tested whether we can
correct for the unjustified change in causal beliefs. We found that pointing
out that AI systems capture correlations and not necessarily causal
relationships can attenuate the effects of CF explanations on people's causal
beliefs.

This paper presents a deep reinforcement learning agent (AI) that uses sound
as the input on the DareFightingICE platform at the DareFightingICE Competition
in IEEE CoG 2022. In this work, an AI that only uses sound as the input is
called blind AI. While state-of-the-art AIs rely mostly on visual or structured
observations provided by their environments, learning to play games from only
sound is still new and thus challenging. We propose different approaches to
process audio data and use the Proximal Policy Optimization algorithm for our
blind AI. We also propose to use our blind AI in evaluation of sound designs
submitted to the competition and define two metrics for this task. The
experimental results show the effectiveness of not only our blind AI but also
the proposed two metrics.

Adopting cryptography has given rise to a significant evolution in Artificial
Intelligence (AI). This paper studies the path and stages of this evolution. We
start with reviewing existing relevant surveys, noting their shortcomings,
especially the lack of a close look at the evolution process and solid future
roadmap. These shortcomings justify the work of this paper. Next, we identify,
define and discuss five consequent stages in the evolution path, including
Crypto-Sensitive AI, Crypto-Adapted AI, Crypto-Friendly AI, Crypto-Enabled AI,
Crypto-Protected AI. Then, we establish a future roadmap for further research
in this area, focusing on the role of quantum-inspired and bio-inspired AI.

Novel data sensing and AI technologies are finding practical use in the
analysis of crisis resilience, revealing the need to consider how responsible
artificial intelligence (AI) practices can mitigate harmful outcomes and
protect vulnerable populations. In this paper, we present a responsible AI
roadmap that is embedded in the Crisis Information Management Circle. This
roadmap includes six propositions to highlight and address important challenges
and considerations specifically related to responsible AI for crisis resilience
management. We cover a wide spectrum of interwoven challenges and
considerations pertaining to the responsible collection, analysis, sharing, and
use of information such as equity, fairness, biases, explainability and
transparency, accountability, privacy and security, inter-organizational
coordination, and public engagement. Through examining issues around AI systems
for crisis resilience management, we dissect the inherent complexities of
information management and decision-making in crises and highlight the urgency
of responsible AI research and practice. The ideas laid out in this paper are
the first attempt in establishing a roadmap for researchers, practitioners,
developers, emergency managers, humanitarian organizations, and public
officials to address important considerations for responsible AI pertaining to
crisis resilience management.

The European Union is likely to introduce among the first, most stringent,
and most comprehensive AI regulatory regimes of the world's major
jurisdictions. In this report, we ask whether the EU's upcoming regulation for
AI will diffuse globally, producing a so-called "Brussels Effect". Building on
and extending Anu Bradford's work, we outline the mechanisms by which such
regulatory diffusion may occur. We consider both the possibility that the EU's
AI regulation will incentivise changes in products offered in non-EU countries
(a de facto Brussels Effect) and the possibility it will influence regulation
adopted by other jurisdictions (a de jure Brussels Effect). Focusing on the
proposed EU AI Act, we tentatively conclude that both de facto and de jure
Brussels effects are likely for parts of the EU regulatory regime. A de facto
effect is particularly likely to arise in large US tech companies with AI
systems that the AI Act terms "high-risk". We argue that the upcoming
regulation might be particularly important in offering the first and most
influential operationalisation of what it means to develop and deploy
trustworthy or human-centred AI. If the EU regime is likely to see significant
diffusion, ensuring it is well-designed becomes a matter of global importance.

The recent developments of artificial intelligence increase its capability
for the creation of arts in both largely autonomous and collaborative contexts.
In both contexts, Ai aims to imitate, combine, and extend existing artistic
styles, and can transform creative practices. In our ongoing research, we
investigate such Creative-Ai from sustainability and ethical perspectives. The
two main focus areas are understanding the environmental sustainability aspects
(material, practices) in the context of artistic processes that involve
Creative-Ai, and ethical issues related to who gets to be involved in the
creation process (power, authorship, ownership). This paper provides an outline
of our ongoing research in these two directions. We will present our
interdisciplinary approach, which combines interviews, workshops, online
ethnography, and energy measurements, to address our research questions: How is
Creative-Ai currently used by artist communities, and which future applications
do artists imagine? When Ai is applied to creating art, how might it impact the
economy and environment? And, how can answers to these questions guide
requirements for intellectual property regimes for Creative-Ai?

Organizations of all sizes, across all industries and domains are leveraging
artificial intelligence (AI) technologies to solve some of their biggest
challenges around operations, customer experience, and much more. However, due
to the probabilistic nature of AI, the risks associated with it are far greater
than traditional technologies. Research has shown that these risks can range
anywhere from regulatory, compliance, reputational, and user trust, to
financial and even societal risks. Depending on the nature and size of the
organization, AI technologies can pose a significant risk, if not used in a
responsible way. This position paper seeks to present a brief introduction to
AI governance, which is a framework designed to oversee the responsible use of
AI with the goal of preventing and mitigating risks. Having such a framework
will not only manage risks but also gain maximum value out of AI projects and
develop consistency for organization-wide adoption of AI.

One objection to conventional AI ethics is that it slows innovation. This
presentation responds by reconfiguring ethics as an innovation accelerator. The
critical elements develop from a contrast between Stability AI's Diffusion and
OpenAI's Dall-E. By analyzing the divergent values underlying their opposed
strategies for development and deployment, five conceptions are identified as
common to acceleration ethics. Uncertainty is understood as positive and
encouraging, rather than discouraging. Innovation is conceived as intrinsically
valuable, instead of worthwhile only as mediated by social effects. AI problems
are solved by more AI, not less. Permissions and restrictions governing AI
emerge from a decentralized process, instead of a unified authority. The work
of ethics is embedded in AI development and application, instead of functioning
from outside. Together, these attitudes and practices remake ethics as
provoking rather than restraining artificial intelligence.

Artificial Intelligence (AI) is used to create more sustainable production
methods and model climate change, making it a valuable tool in the fight
against environmental degradation. This paper describes the paradox of an
energy-consuming technology serving the ecological challenges of tomorrow. The
study provides an overview of the sectors that use AI-based solutions for
environmental protection. It draws on numerous examples from AI for Green
players to present use cases and concrete examples. In the second part of the
study, the negative impacts of AI on the environment and the emerging
technological solutions to support Green AI are examined. It is also shown that
the research on less energy-consuming AI is motivated more by cost and energy
autonomy constraints than by environmental considerations. This leads to a
rebound effect that favors an increase in the complexity of models. Finally,
the need to integrate environmental indicators into algorithms is discussed.
The environmental dimension is part of the broader ethical problem of AI, and
addressing it is crucial for ensuring the sustainability of AI in the long
term.

Artificial Intelligence (AI) is at the forefront of modern technology, and
its effects are felt in many areas of society. To prevent algorithmic
disparities, fairness, accountability, transparency, and ethics (FATE) in AI
are being implemented. However, the current discourse on these issues is
largely dominated by more economically developed countries (MEDC), leaving out
local knowledge, cultural pluralism, and global fairness. This study aims to
address this gap by examining FATE-related desiderata, particularly
transparency and ethics, in areas of the global South that are underserved by
AI. A user study (n=43) and a participatory session (n=30) were conducted to
achieve this goal. The results showed that AI models can encode bias and
amplify stereotypes. To promote inclusivity, a community-led strategy is
proposed to collect and curate representative data for responsible AI design.
This will enable the affected community or individuals to monitor the
increasing use of AI-powered systems. Additionally, recommendations based on
public input are provided to ensure that AI adheres to social values and
context-specific FATE needs.

Recent advances in Artificial Intelligence (AI) have sparked renewed interest
in its potential to improve education. However, AI is a loose umbrella term
that refers to a collection of methods, capabilities, and limitations-many of
which are often not explicitly articulated by researchers, education technology
companies, or other AI developers. In this paper, we seek to clarify what "AI"
is and the potential it holds to both advance and hamper educational
opportunities that may improve the human condition. We offer a basic
introduction to different methods and philosophies underpinning AI, discuss
recent advances, explore applications to education, and highlight key
limitations and risks. We conclude with a set of questions that educationalists
may ask as they encounter AI in their research and practice. Our hope is to
make often jargon-laden terms and concepts accessible, so that all are equipped
to understand, interrogate, and ultimately shape the development of human
centered AI in education.

Generative AI technologies are growing in power, utility, and use. As
generative technologies are being incorporated into mainstream applications,
there is a need for guidance on how to design those applications to foster
productive and safe use. Based on recent research on human-AI co-creation
within the HCI and AI communities, we present a set of seven principles for the
design of generative AI applications. These principles are grounded in an
environment of generative variability. Six principles are focused on designing
for characteristics of generative AI: multiple outcomes & imperfection;
exploration & control; and mental models & explanations. In addition, we urge
designers to design against potential harms that may be caused by a generative
model's hazardous output, misuse, or potential for human displacement. We
anticipate these principles to usefully inform design decisions made in the
creation of novel human-AI applications, and we invite the community to apply,
revise, and extend these principles to their own work.

Behavioral scientists have classically documented aversion to algorithmic
decision aids, from simple linear models to AI. Sentiment, however, is changing
and possibly accelerating AI helper usage. AI assistance is, arguably, most
valuable when humans must make complex choices. We argue that classic
experimental methods used to study heuristics and biases are insufficient for
studying complex choices made with AI helpers. We adapted an experimental
paradigm designed for studying complex choices in such contexts. We show that
framing and anchoring effects impact how people work with an AI helper and are
predictive of choice outcomes. The evidence suggests that some participants,
particularly those in a loss frame, put too much faith in the AI helper and
experienced worse choice outcomes by doing so. The paradigm also generates
computational modeling-friendly data allowing future studies of human-AI
decision making.

While Artificial Intelligence (AI) technologies are progressing fast,
compliance costs have become a huge financial burden for AI startups, which are
already constrained on research & development budgets. This situation creates a
compliance trap, as many AI startups are not financially prepared to cope with
a broad spectrum of regulatory requirements. Particularly, the complex and
varying regulatory processes across the globe subtly give advantages to
well-established and resourceful technology firms over resource-constrained AI
startups [1]. The continuation of this trend may phase out the majority of AI
startups and lead to giant technology firms' monopolies of AI technologies. To
demonstrate the reality of the compliance trap, from a field deployment
perspective, we delve into the details of compliance costs of AI commercial
operations.

Local governments increasingly use artificial intelligence (AI) for automated
decision-making. Contestability, making systems responsive to dispute, is a way
to ensure they respect human rights to autonomy and dignity. We investigate the
design of public urban AI systems for contestability through the example of
camera cars: human-driven vehicles equipped with image sensors. Applying a
provisional framework for contestable AI, we use speculative design to create a
concept video of a contestable camera car. Using this concept video, we then
conduct semi-structured interviews with 17 civil servants who work with AI
employed by a large northwestern European city. The resulting data is analyzed
using reflexive thematic analysis to identify the main challenges facing the
implementation of contestability in public AI. We describe how civic
participation faces issues of representation, public AI systems should
integrate with existing democratic practices, and cities must expand capacities
for responsible AI development and operation.

In the context of rapid discoveries by leaders in AI, governments must
consider how to design regulation that matches the increasing pace of new AI
capabilities. Regulatory Markets for AI is a proposal designed with
adaptability in mind. It involves governments setting outcome-based targets for
AI companies to achieve, which they can show by purchasing services from a
market of private regulators. We use an evolutionary game theory model to
explore the role governments can play in building a Regulatory Market for AI
systems that deters reckless behaviour. We warn that it is alarmingly easy to
stumble on incentives which would prevent Regulatory Markets from achieving
this goal. These 'Bounty Incentives' only reward private regulators for
catching unsafe behaviour. We argue that AI companies will likely learn to
tailor their behaviour to how much effort regulators invest, discouraging
regulators from innovating. Instead, we recommend that governments always
reward regulators, except when they find that those regulators failed to detect
unsafe behaviour that they should have. These 'Vigilant Incentives' could
encourage private regulators to find innovative ways to evaluate cutting-edge
AI systems.

For an AI solution to evolve from a trained machine learning model into a
production-ready AI system, many more things need to be considered than just
the performance of the machine learning model. A production-ready AI system
needs to be trustworthy, i.e. of high quality. But how to determine this in
practice? For traditional software, ISO25000 and its predecessors have since
long time been used to define and measure quality characteristics. Recently,
quality models for AI systems, based on ISO25000, have been introduced. This
paper applies one such quality model to a real-life case study: a deep learning
platform for monitoring wildflowers. The paper presents three realistic
scenarios sketching what it means to respectively use, extend and incrementally
improve the deep learning platform for wildflower identification and counting.
Next, it is shown how the quality model can be used as a structured dictionary
to define quality requirements for data, model and software. Future work
remains to extend the quality model with metrics, tools and best practices to
aid AI engineering practitioners in implementing trustworthy AI systems.

Contextual utility theory integrates context-sensitive factors into
utility-based decision-making models. It stresses the importance of
understanding individual decision-makers' preferences, values, and beliefs and
the situational factors that affect them. Contextual utility theory benefits
explainable AI. First, it can improve transparency and understanding of how AI
systems affect decision-making. It can reveal AI model biases and limitations
by considering personal preferences and context. Second, contextual utility
theory can make AI systems more personalized and adaptable to users and
stakeholders. AI systems can better meet user needs and values by incorporating
demographic and cultural data. Finally, contextual utility theory promotes
ethical AI development and social responsibility. AI developers can create
ethical systems that benefit society by considering contextual factors like
societal norms and values. This work, demonstrates how contextual utility
theory can improve AI system transparency, personalization, and ethics,
benefiting both users and developers.

Structural health monitoring (SHM) tasks like damage detection are crucial
for decision-making regarding maintenance and deterioration. For example, crack
detection in SHM is crucial for bridge maintenance as crack progression can
lead to structural instability. However, most AI/ML models in the literature
have low latency and late inference time issues while performing in real-time
environments. This study aims to explore the integration of edge-AI in the SHM
domain for real-time bridge inspections. Based on edge-AI literature, its
capabilities will be valuable integration for a real-time decision support
system in SHM tasks such that real-time inferences can be performed on physical
sites. This study will utilize commercial edge-AI platforms, such as Google
Coral Dev Board or Kneron KL520, to develop and analyze the effectiveness of
edge-AI devices. Thus, this study proposes an edge AI framework for the
structural health monitoring domain. An edge-AI-compatible deep learning model
is developed to validate the framework to perform real-time crack
classification. The effectiveness of this model will be evaluated based on its
accuracy, the confusion matrix generated, and the inference time observed in a
real-time setting.

Organizations that develop and deploy artificial intelligence (AI) systems
need to take measures to reduce the associated risks. In this paper, we examine
how AI companies could design an AI ethics board in a way that reduces risks
from AI. We identify five high-level design choices: (1) What responsibilities
should the board have? (2) What should its legal structure be? (3) Who should
sit on the board? (4) How should it make decisions and should its decisions be
binding? (5) What resources does it need? We break down each of these questions
into more specific sub-questions, list options, and discuss how different
design choices affect the board's ability to reduce risks from AI. Several
failures have shown that designing an AI ethics board can be challenging. This
paper provides a toolbox that can help AI companies to overcome these
challenges.

Children growing up in the era of Artificial Intelligence (AI) will be most
impacted by the technology across their life span. Participatory Design (PD) is
widely adopted by the Interaction Design and Children (IDC) community, which
empowers children to bring their interests, needs, and creativity to the design
process of future technologies. While PD has drawn increasing attention to
human-centered AI design, it remains largely untapped in facilitating the
design process of AI technologies relevant to children and their community. In
this paper, we report intriguing children's design ideas on AI technologies
resulting from the "Research and Design Challenge" of the 22nd ACM Interaction
Design and Children (IDC 2023) conference. The diversity of design problems, AI
applications and capabilities revealed by the children's design ideas shed
light on the potential of engaging children in PD activities for future AI
technologies. We discuss opportunities and challenges for accessible and
inclusive PD experiences with children in shaping the future of AI-powered
society.

Improving our understanding of how humans perceive AI teammates is an
important foundation for our general understanding of human-AI teams. Extending
relevant work from cognitive science, we propose a framework based on item
response theory for modeling these perceptions. We apply this framework to
real-world experiments, in which each participant works alongside another
person or an AI agent in a question-answering setting, repeatedly assessing
their teammate's performance. Using this experimental data, we demonstrate the
use of our framework for testing research questions about people's perceptions
of both AI agents and other people. We contrast mental models of AI teammates
with those of human teammates as we characterize the dimensionality of these
mental models, their development over time, and the influence of the
participants' own self-perception. Our results indicate that people expect AI
agents' performance to be significantly better on average than the performance
of other humans, with less variation across different types of problems. We
conclude with a discussion of the implications of these findings for human-AI
interaction.

The European Union's Artificial Intelligence (AI) Act is set to be a landmark
legal instrument for regulating AI technology. While stakeholders have
primarily focused on the governance of fixed purpose AI applications (also
known as narrow AI), more attention is required to understand the nature of
highly and broadly capable systems. As of the beginning of 2023, several
definitions for General Purpose AI Systems (GPAIS) exist in relation to the AI
Act, attempting to distinguish between systems with and without a fixed
purpose. In this article, we operationalise these differences through the
concept of "distinct tasks" and examine four approaches (quantity, performance,
adaptability, and emergence) to determine whether an AI system should be
classified as a GPAIS. We suggest that EU stakeholders use the four approaches
as a starting point to discriminate between fixed-purpose and GPAIS.

The increased use of AI systems is associated with multi-faceted societal,
environmental, and economic consequences. These include non-transparent
decision-making processes, discrimination, increasing inequalities, rising
energy consumption and greenhouse gas emissions in AI model development and
application, and an increasing concentration of economic power. By considering
the multi-dimensionality of sustainability, this paper takes steps towards
substantiating the call for an overarching perspective on "sustainable AI". It
presents the SCAIS Framework (Sustainability Criteria and Indicators for
Artificial Intelligence Systems) which contains a set 19 sustainability
criteria for sustainable AI and 67 indicators that is based on the results of a
critical review and expert workshops. This interdisciplinary approach
contributes a unique holistic perspective to facilitate and structure the
discourse on sustainable AI. Further, it provides a concrete framework that
lays the foundation for developing standards and tools to support the conscious
development and application of AI systems.

Fueled by the soaring popularity of large language and foundation models, the
accelerated growth of artificial intelligence (AI) models' enormous
environmental footprint has come under increased scrutiny. While many
approaches have been proposed to make AI more energy-efficient and
environmentally friendly, environmental inequity -- the fact that AI's
environmental footprint can be disproportionately higher in certain regions
than in others -- has emerged, raising social-ecological justice concerns. This
paper takes a first step toward addressing AI's environmental inequity by
balancing its regional negative environmental impact. Concretely, we focus on
the carbon and water footprints of AI model inference and propose equity-aware
geographical load balancing (GLB) to explicitly address AI's environmental
impacts on the most disadvantaged regions. We run trace-based simulations by
considering a set of 10 geographically-distributed data centers that serve
inference requests for a large language AI model. The results demonstrate that
existing GLB approaches may amplify environmental inequity while our proposed
equity-aware GLB can significantly reduce the regional disparity in terms of
carbon and water footprints.

With the growing capabilities and pervasiveness of AI systems, societies must
collectively choose between reduced human autonomy, endangered democracies and
limited human rights, and AI that is aligned to human and social values,
nurturing collaboration, resilience, knowledge and ethical behaviour. In this
chapter, we introduce the notion of self-reflective AI systems for meaningful
human control over AI systems. Focusing on decision support systems, we propose
a framework that integrates knowledge from psychology and philosophy with
formal reasoning methods and machine learning approaches to create AI systems
responsive to human values and social norms. We also propose a possible
research approach to design and develop self-reflective capability in AI
systems. Finally, we argue that self-reflective AI systems can lead to
self-reflective hybrid systems (human + AI), thus increasing meaningful human
control and empowering human moral reasoning by providing comprehensible
information and insights on possible human moral blind spots.

While artificial intelligence (AI) has made many successful applications in
various domains, its adoption in healthcare lags a little bit behind other
high-stakes settings. Several factors contribute to this slower uptake,
including regulatory frameworks, patient privacy concerns, and data
heterogeneity. However, one significant challenge that impedes the
implementation of AI in healthcare, particularly in orthopedics, is the lack of
explainability and interpretability around AI models. Addressing the challenge
of explainable AI (XAI) in orthopedics requires developing AI models and
algorithms that prioritize transparency and interpretability, allowing
clinicians, surgeons, and patients to understand the contributing factors
behind any AI-powered predictive or descriptive models. The current
contribution outlines several key challenges and opportunities that manifest in
XAI in orthopedic practice. This work emphasizes the need for interdisciplinary
collaborations between AI practitioners, orthopedic specialists, and regulatory
entities to establish standards and guidelines for the adoption of XAI in
orthopedics.

Artificial intelligence (AI) promises immense benefits across sectors, yet
also poses risks from dual-use potentials, biases, and unintended behaviors.
This paper reviews emerging issues with opaque and uncontrollable AI systems
and proposes an integrative framework called violet teaming to develop reliable
and responsible AI. Violet teaming combines adversarial vulnerability probing
(red teaming) with solutions for safety and security (blue teaming) while
prioritizing ethics and social benefit. It emerged from AI safety research to
manage risks proactively by design. The paper traces the evolution of red,
blue, and purple teaming toward violet teaming, and then discusses applying
violet techniques to address biosecurity risks of AI in biotechnology.
Additional sections review key perspectives across law, ethics, cybersecurity,
macrostrategy, and industry best practices essential for operationalizing
responsible AI through holistic technical and social considerations. Violet
teaming provides both philosophy and method for steering AI trajectories toward
societal good. With conscience and wisdom, the extraordinary capabilities of AI
can enrich humanity. But without adequate precaution, the risks could prove
catastrophic. Violet teaming aims to empower moral technology for the common
welfare.

This paper explores using generative AI and aesthetics to promote cultural
creativity in rural China amidst COVID-19's impact. Through literature reviews,
case studies, surveys, and text analysis, it examines art and technology
applications in rural contexts and identifies key challenges. The study finds
artworks often fail to resonate locally, while reliance on external artists
limits sustainability. Hence, nurturing grassroots "artist villagers" through
AI is proposed. Our approach involves training machine learning on subjective
aesthetics to generate culturally relevant content. Interactive AI media can
also boost tourism while preserving heritage. This pioneering research puts
forth original perspectives on the intersection of AI and aesthetics to
invigorate rural culture. It advocates holistic integration of technology and
emphasizes AI's potential as a creative enabler versus replacement. Ultimately,
it lays the groundwork for further exploration of leveraging AI innovations to
empower rural communities. This timely study contributes to growing interest in
emerging technologies to address critical issues facing rural China.

The Supreme Court of India has been a pioneer in using ICT in courts through
its e-Courts project in India. Yet another leap, its recent project, Design,
Development, and Implementation of Artificial Intelligence (AI) solution, tools
for transcribing arguments and Court proceedings at Supreme Court of India, has
potential to impact the way AI algorithms are designed in India, and not just
for this particular project. In this paper, we evaluate the endeavours of the
Supreme Court of India in light of the state of AI technology as well as the
attempts to regulate AI. We argue that since the project aims to transcribe and
translate the proceedings of the constitutional benches of the Supreme Court,
it has potential to impact rule of law in the country. Hence, we place this
application in High Risk AI as per the provisions to the proposed EU AI Act. We
provide some guidelines on the approach to transcribe and translate making the
maximum use of AI in the Supreme Court of India without running into the
dangers it may pose.

The Directorate General for Parliamentary Research Services of the European
Parliament has prepared a report to the Members of the European Parliament
where they enumerate seven main risks of Artificial Intelligence (AI) in
medicine and healthcare: patient harm due to AI errors, misuse of medical AI
tools, bias in AI and the perpetuation of existing inequities, lack of
transparency, privacy and security issues, gaps in accountability, and
obstacles in implementation.
  In this study, we propose fourteen functional requirements that AI systems
may implement to reduce the risks associated with their medical purpose: AI
passport, User management, Regulation check, Academic use only disclaimer, data
quality assessment, Clinicians double check, Continuous performance evaluation,
Audit trail, Continuous usability test, Review of retrospective/simulated
cases, Bias check, eXplainable AI, Encryption and use of field-tested
libraries, and Semantic interoperability.
  Our intention here is to provide specific high-level specifications of
technical solutions to ensure continuous good performance and use of AI systems
to benefit patients in compliance with the future EU regulatory framework.

This papers explores the question of human authorship when works are created
with generative AI tools.

As AI systems' sophistication and proliferation have increased, awareness of
the risks has grown proportionally (Sorkin et al. 2023). In response, calls
have grown for stronger emphasis on disclosure and transparency in the AI
industry (NTIA 2023; OpenAI 2023b), with proposals ranging from standardizing
use of technical disclosures, like model cards (Mitchell et al. 2019), to
yet-unspecified licensing regimes (Sindhu 2023). Since the AI value chain is
complicated, with actors representing various expertise, perspectives, and
values, it is crucial that consumers of a transparency disclosure be able to
understand the risks of the AI system the disclosure concerns. In this paper we
propose a risk profiling standard which can guide downstream decision-making,
including triaging further risk assessment, informing procurement and
deployment, and directing regulatory frameworks. The standard is built on our
proposed taxonomy of AI risks, which reflects a high-level categorization of
the wide variety of risks proposed in the literature. We outline the myriad
data sources needed to construct informative Risk Profiles and propose a
template-based methodology for collating risk information into a standard, yet
flexible, structure. We apply this methodology to a number of prominent AI
systems using publicly available information. To conclude, we discuss design
decisions for the profiles and future work.

This study conducts a thorough examination of the research stream focusing on
AI risks in healthcare, aiming to explore the distinct genres within this
domain. A selection criterion was employed to carefully analyze 39 articles to
identify three primary genres of AI risks prevalent in healthcare: clinical
data risks, technical risks, and socio-ethical risks. Selection criteria was
based on journal ranking and impact factor. The research seeks to provide a
valuable resource for future healthcare researchers, furnishing them with a
comprehensive understanding of the complex challenges posed by AI
implementation in healthcare settings. By categorizing and elucidating these
genres, the study aims to facilitate the development of empirical qualitative
and quantitative research, fostering evidence-based approaches to address
AI-related risks in healthcare effectively. This endeavor contributes to
building a robust knowledge base that can inform the formulation of risk
mitigation strategies, ensuring safe and efficient integration of AI
technologies in healthcare practices. Thus, it is important to study AI risks
in healthcare to build better and efficient AI systems and mitigate risks.

Artificial Intelligence (AI) aims to elevate healthcare to a pinnacle by
aiding clinical decision support. Overcoming the challenges related to the
design of ethical AI will enable clinicians, physicians, healthcare
professionals, and other stakeholders to use and trust AI in healthcare
settings. This study attempts to identify the major ethical principles
influencing the utility performance of AI at different technological levels
such as data access, algorithms, and systems through a thematic analysis. We
observed that justice, privacy, bias, lack of regulations, risks, and
interpretability are the most important principles to consider for ethical AI.
This data-driven study has analyzed secondary survey data from the Pew Research
Center (2020) of 36 AI experts to categorize the top ethical principles of AI
design. To resolve the ethical issues identified by the meta-analysis and
domain experts, we propose a new utilitarian ethics-based theoretical framework
for designing ethical AI for the healthcare domain.

Scientific research is increasingly reliant on computational methods, posing
challenges for ensuring research reproducibility. This study focuses on the
field of artificial intelligence (AI) and introduces a new framework for
evaluating AI platforms for reproducibility from a cyber security standpoint to
address the security challenges associated with AI research. Using this
framework, five popular AI reproducibility platforms; Floydhub, BEAT, Codalab,
Kaggle, and OpenML were assessed. The analysis revealed that none of these
platforms fully incorporates the necessary cyber security measures essential
for robust reproducibility. Kaggle and Codalab, however, performed better in
terms of implementing cyber security measures covering aspects like security,
privacy, usability, and trust. Consequently, the study provides tailored
recommendations for different user scenarios, including individual researchers,
small laboratories, and large corporations. It emphasizes the importance of
integrating specific cyber security features into AI platforms to address the
challenges associated with AI reproducibility, ultimately advancing
reproducibility in this field. Moreover, the proposed framework can be applied
beyond AI platforms, serving as a versatile tool for evaluating a wide range of
systems and applications from a cyber security perspective.

The rapid advancements in Artificial Intelligence (AI), particularly in
generative AI and foundational models (FMs), have ushered in transformative
changes across various industries. Large language models (LLMs), a type of FM,
have demonstrated their prowess in natural language processing tasks and
content generation, revolutionizing how we interact with software products and
services. This article explores the integration of FMs in the
telecommunications industry, shedding light on the concept of AI native telco,
where AI is seamlessly woven into the fabric of telecom products. It delves
into the engineering considerations and unique challenges associated with
implementing FMs into the software life cycle, emphasizing the need for AI
native-first approaches. Despite the enormous potential of FMs, ethical,
regulatory, and operational challenges require careful consideration,
especially in mission-critical telecom contexts. As the telecom industry seeks
to harness the power of AI, a comprehensive understanding of these challenges
is vital to thrive in a fiercely competitive market.

The correctness of software systems is vital for their effective operation.
It makes discovering and fixing software bugs an important development task.
The increasing use of Artificial Intelligence (AI) techniques in Software
Engineering led to the development of a number of techniques that can assist
software developers in identifying potential bugs in code. In this paper, we
present a comprehensible comparison and analysis of the efficacy of two
AI-based approaches, namely single AI models and ensemble AI models, for
predicting the probability of a Java class being buggy. We used two open-source
Apache Commons Project's Java components for training and evaluating the
models. Our experimental findings indicate that the ensemble of AI models can
outperform the results of applying individual AI models. We also offer insight
into the factors that contribute to the enhanced performance of the ensemble AI
model. The presented results demonstrate the potential of using ensemble AI
models to enhance bug prediction results, which could ultimately result in more
reliable software systems.

Rapid advancements in artificial intelligence (AI) have sparked growing
concerns among experts, policymakers, and world leaders regarding the potential
for increasingly advanced AI systems to pose existential risks. This paper
reviews the evidence for existential risks from AI via misalignment, where AI
systems develop goals misaligned with human values, and power-seeking, where
misaligned AIs actively seek power. The review examines empirical findings,
conceptual arguments and expert opinion relating to specification gaming, goal
misgeneralization, and power-seeking. The current state of the evidence is
found to be concerning but inconclusive regarding the existence of extreme
forms of misaligned power-seeking. Strong empirical evidence of specification
gaming combined with strong conceptual evidence for power-seeking make it
difficult to dismiss the possibility of existential risk from misaligned
power-seeking. On the other hand, to date there are no public empirical
examples of misaligned power-seeking in AI systems, and so arguments that
future systems will pose an existential risk remain somewhat speculative. Given
the current state of the evidence, it is hard to be extremely confident either
that misaligned power-seeking poses a large existential risk, or that it poses
no existential risk. The fact that we cannot confidently rule out existential
risk from AI via misaligned power-seeking is cause for serious concern.

AI Code Completion (e.g., GitHub's Copilot, Amazon CodeWhisperer) has
revolutionized the way in which computer science students interact with
programming languages. However, these tools are not available for free public
use, preventing us from conducting our research. In addition, AI code
completion has been studied from developers' perspective, not students'
perspective who represent the future generation of our digital world. In this
article, we investigated the benefits, challenges, and expectations of AI code
completion from students' perspectives and introduced AutoAurora, an AI code
completion tool integrated into the Visual Studio Code Extension as a research
instrument. Through an interview study with ten participants, we found that AI
code completion enhanced students' productivity and efficiency by providing
correct syntax suggestions, offering alternative solutions, and functioning as
a coding tutor. However, the over-reliance on AI code completion may lead to a
surface-level understanding of programming concepts, diminishing
problem-solving skills and restricting creativity. In the future, AI code
completion must be explainable to facilitate the learning of coding concepts.

The evolution towards 6G architecture promises a transformative shift in
communication networks, with artificial intelligence (AI) playing a pivotal
role. This paper delves deep into the seamless integration of Large Language
Models (LLMs) and Generalized Pretrained Transformers (GPT) within 6G systems.
Their ability to grasp intent, strategize, and execute intricate commands will
be pivotal in redefining network functionalities and interactions. Central to
this is the AI Interconnect framework, intricately woven to facilitate
AI-centric operations within the network. Building on the continuously evolving
current state-of-the-art, we present a new architectural perspective for the
upcoming generation of mobile networks. Here, LLMs and GPTs will
collaboratively take center stage alongside traditional pre-generative AI and
machine learning (ML) algorithms. This union promises a novel confluence of the
old and new, melding tried-and-tested methods with transformative AI
technologies. Along with providing a conceptual overview of this evolution, we
delve into the nuances of practical applications arising from such an
integration. Through this paper, we envisage a symbiotic integration where AI
becomes the cornerstone of the next-generation communication paradigm, offering
insights into the structural and functional facets of an AI-native 6G network.

The rise of generative artificial intelligence (AI) has sparked concerns
about its potential influence on unemployment and market depression. This study
addresses this concern by examining the impact of generative AI on product
markets. To overcome the challenge of causal inference, given the inherent
limitations of conducting controlled experiments, this paper identifies an
unanticipated and sudden leak of a highly proficient image-generative AI as a
novel instance of a "natural experiment". This AI leak spread rapidly,
significantly reducing the cost of generating anime-style images compared to
other styles, creating an opportunity for comparative assessment. We collect
real-world data from an artwork outsourcing platform. Surprisingly, our results
show that while generative AI lowers average prices, it substantially boosts
order volume and overall revenue. This counterintuitive finding suggests that
generative AI confers benefits upon artists rather than detriments. The study
further offers theoretical economic explanations to elucidate this unexpected
phenomenon. By furnishing empirical evidence, this paper dispels the notion
that generative AI might engender depression, instead underscoring its
potential to foster market prosperity. These findings carry significant
implications for practitioners, policymakers, and the broader AI community.

As Artificial Intelligence (AI) permeates many aspects of society, it brings
numerous advantages while at the same time raising ethical concerns and
potential risks, such as perpetuating inequalities through biased or
discriminatory decision-making. To develop AI systems that cater for the needs
of diverse users and uphold ethical values, it is essential to consider and
integrate diversity and inclusion (D&I) principles throughout AI development
and deployment. Requirements engineering (RE) is a fundamental process in
developing software systems by eliciting and specifying relevant needs from
diverse stakeholders. This research aims to address the lack of research and
practice on how to elicit and capture D&I requirements for AI systems. We have
conducted comprehensive data collection and synthesis from the literature
review to extract requirements themes related to D&I in AI. We have proposed a
tailored user story template to capture D&I requirements and conducted focus
group exercises to use the themes and user story template in writing D&I
requirements for two example AI systems. Additionally, we have investigated the
capability of our solution by generating synthetic D&I requirements captured in
user stories with the help of a Large Language Model.

Artificial intelligence (AI) has been clearly established as a technology
with the potential to revolutionize fields from healthcare to finance - if
developed and deployed responsibly. This is the topic of responsible AI, which
emphasizes the need to develop trustworthy AI systems that minimize bias,
protect privacy, support security, and enhance transparency and accountability.
Explainable AI (XAI) has been broadly considered as a building block for
responsible AI (RAI), with most of the literature considering it as a solution
for improved transparency. This work proposes that XAI and responsible AI are
significantly more deeply entwined. In this work, we explore state-of-the-art
literature on RAI and XAI technologies. Based on our findings, we demonstrate
that XAI can be utilized to ensure fairness, robustness, privacy, security, and
transparency in a wide range of contexts. Our findings lead us to conclude that
XAI is an essential foundation for every pillar of RAI.

The growing presence of Artificial Intelligence (AI) in various sectors
necessitates systems that accurately reflect societal diversity. This study
seeks to envision the operationalization of the ethical imperatives of
diversity and inclusion (D&I) within AI ecosystems, addressing the current
disconnect between ethical guidelines and their practical implementation. A
significant challenge in AI development is the effective operationalization of
D&I principles, which is critical to prevent the reinforcement of existing
biases and ensure equity across AI applications. This paper proposes a vision
of a framework for developing a tool utilizing persona-based simulation by
Generative AI (GenAI). The approach aims to facilitate the representation of
the needs of diverse users in the requirements analysis process for AI
software. The proposed framework is expected to lead to a comprehensive persona
repository with diverse attributes that inform the development process with
detailed user narratives. This research contributes to the development of an
inclusive AI paradigm that ensures future technological advances are designed
with a commitment to the diverse fabric of humanity.

The advancement of generative AI has given rise to pressing copyright
challenges, particularly in music industry. This paper focuses on the economic
aspects of these challenges, emphasizing that the economic impact constitutes a
central issue in the copyright arena. The complexity of the black-box
generative AI technologies not only suggests but necessitates algorithmic
solutions. However, such solutions have been largely missing, leading to
regulatory challenges in this landscape. We aim to bridge the gap in current
approaches by proposing potential royalty models for revenue sharing on AI
music generation platforms. Our methodology involves a detailed analysis of
existing royalty models in platforms like Spotify and YouTube, and adapting
these to the unique context of AI-generated music. A significant challenge we
address is the attribution of AI-generated music to influential copyrighted
content in the training data. To this end, we present algorithmic solutions
employing data attribution techniques. Our experimental results verify the
effectiveness of these solutions. This research represents a pioneering effort
in integrating technical advancements with economic and legal considerations in
the field of generative AI, offering a computational copyright solution for the
challenges posed by the opaque nature of AI technologies.

As Moore's Law loses momentum, improving size, performance, and efficiency of
processors has become increasingly challenging, ending the era of predictable
improvements in hardware performance. Meanwhile, the widespread incorporation
of high-definition sensors in consumer devices and autonomous technologies has
fueled a significant upsurge in sensory data. Current global trends reveal that
the volume of generated data already exceeds human consumption capacity, making
AI algorithms the primary consumers of data worldwide. To address this, a novel
approach to designing AI-centric sensing systems is needed that can bridge the
gap between the increasing capabilities of high-definition sensors and the
limitations of AI processors. This paper provides an overview of efficient
sensing and perception methods in both AI and sensing domains, emphasizing the
necessity of co-designing AI algorithms and sensing systems for dynamic
perception. The proposed approach involves a framework for designing and
analyzing dynamic AI-in-the-loop sensing systems, suggesting a fundamentally
new method for designing adaptive sensing systems through inference-time
AI-to-sensor feedback and end-to-end efficiency and performance optimization.

As AI/ML models, including Large Language Models, continue to scale with
massive datasets, so does their consumption of undeniably limited natural
resources, and impact on society. In this collaboration between AI,
Sustainability, HCI and legal researchers, we aim to enable a transition to
sustainable AI development by enabling stakeholders across the AI value chain
to assess and quantitfy the environmental and societal impact of AI. We present
the ESG Digital and Green Index (DGI), which offers a dashboard for assessing a
company's performance in achieving sustainability targets. This includes
monitoring the efficiency and sustainable use of limited natural resources
related to AI technologies (water, electricity, etc). It also addresses the
societal and governance challenges related to AI. The DGI creates incentives
for companies to align their pathway with the Sustainable Development Goals
(SDGs). The value, challenges and limitations of our methodology and findings
are discussed in the paper.

In the ever-evolving landscape of Artificial Intelligence (AI), the synergy
between generative AI and Software Engineering emerges as a transformative
frontier. This whitepaper delves into the unexplored realm, elucidating how
generative AI techniques can revolutionize software development. Spanning from
project management to support and updates, we meticulously map the demands of
each development stage and unveil the potential of generative AI in addressing
them. Techniques such as zero-shot prompting, self-consistency, and multimodal
chain-of-thought are explored, showcasing their unique capabilities in
enhancing generative AI models. The significance of vector embeddings, context,
plugins, tools, and code assistants is underscored, emphasizing their role in
capturing semantic information and amplifying generative AI capabilities.
Looking ahead, this intersection promises to elevate productivity, improve code
quality, and streamline the software development process. This whitepaper
serves as a guide for stakeholders, urging discussions and experiments in the
application of generative AI in Software Engineering, fostering innovation and
collaboration for a qualitative leap in the efficiency and effectiveness of
software development.

Equipped with sensing, networking, and computing capabilities, Internet of
Things (IoT) such as smartphones, wearables, smart speakers, and household
robots have been seamlessly weaved into our daily lives. Recent advancements in
Generative AI exemplified by GPT, LLaMA, DALL-E, and Stable Difussion hold
immense promise to push IoT to the next level. In this article, we share our
vision and views on the benefits that Generative AI brings to IoT, and discuss
some of the most important applications of Generative AI in IoT-related
domains. Fully harnessing Generative AI in IoT is a complex challenge. We
identify some of the most critical challenges including high resource demands
of the Generative AI models, prompt engineering, on-device inference,
offloading, on-device fine-tuning, federated learning, security, as well as
development tools and benchmarks, and discuss current gaps as well as promising
opportunities on enabling Generative AI for IoT. We hope this article can
inspire new research on IoT in the era of Generative AI.

With Artificial Intelligence (AI) becoming ubiquitous in every application
domain, the need for explanations is paramount to enhance transparency and
trust among non-technical users. Despite the potential shown by Explainable AI
(XAI) for enhancing understanding of complex AI systems, most XAI methods are
designed for technical AI experts rather than non-technical consumers.
Consequently, such explanations are overwhelmingly complex and seldom guide
users in achieving their desired predicted outcomes. This paper presents
ongoing research for crafting XAI systems tailored to guide users in achieving
desired outcomes through improved human-AI interactions. This paper highlights
the research objectives and methods, key takeaways and implications learned
from user studies. It outlines open questions and challenges for enhanced
human-AI collaboration, which the author aims to address in future work.

The constantly increasing capabilities of artificial intelligence (AI) open
new possibilities for human-AI collaboration. One promising approach to
leverage existing complementary capabilities is allowing humans to delegate
individual instances to the AI. However, enabling humans to delegate instances
effectively requires them to assess both their own and the AI's capabilities in
the context of the given task. In this work, we explore the effects of
providing contextual information on human decisions to delegate instances to an
AI. We find that providing participants with contextual information
significantly improves the human-AI team performance. Additionally, we show
that the delegation behavior changes significantly when participants receive
varying types of contextual information. Overall, this research advances the
understanding of human-AI interaction in human delegation and provides
actionable insights for designing more effective collaborative systems.

Artificial Intelligence (AI) technologies have been applied in various
domains, including early childhood education (ECE). Integration of AI
educational technology is a recent significant trend in ECE. Currently, there
are more and more studies of AI in ECE. To date, there is a lack of survey
articles that discuss the studies of AI in ECE. In this paper, we provide an
up-to-date and in-depth overview of the key AI technologies in ECE that
provides a historical perspective, summarizes the representative works,
outlines open questions, discusses the trends and challenges through a detailed
bibliometric analysis, and provides insightful recommendations for future
research. We mainly discuss the studies that apply AI-based robots and AI
technologies to ECE, including improving the social interaction of children
with an autism spectrum disorder. This paper significantly contributes to
provide an up-to-date and in-depth survey that is suitable as introductory
material for beginners to AI in ECE, as well as supplementary material for
advanced users.

Trust and privacy have emerged as significant concerns in online
transactions. Sharing information on health is especially sensitive but it is
necessary for purchasing and utilizing health insurance. Evidence shows that
consumers are increasingly comfortable with technology in place of humans, but
the expanding use of AI potentially changes this. This research explores
whether trust and privacy concern are barriers to the adoption of AI in health
insurance. Two scenarios are compared: The first scenario has limited AI that
is not in the interface and its presence is not explicitly revealed to the
consumer. In the second scenario there is an AI interface and AI evaluation,
and this is explicitly revealed to the consumer. The two scenarios were modeled
and compared using SEM PLS-MGA. The findings show that trust is significantly
lower in the second scenario where AI is visible. Privacy concerns are higher
with AI but the difference is not statistically significant within the model.

Large language models now possess human-level linguistic abilities in many
contexts. This raises the concern that they can be used to deceive and
manipulate on unprecedented scales, for instance spreading political
misinformation on social media. In future, agentic AI systems might also
deceive and manipulate humans for their own ends. In this paper, first, I argue
that AI-generated content should be subject to stricter standards against
deception and manipulation than we ordinarily apply to humans. Second, I offer
new characterizations of AI deception and manipulation meant to support such
standards, according to which a statement is deceptive (manipulative) if it
leads human addressees away from the beliefs (choices) they would endorse under
``semi-ideal'' conditions. Third, I propose two measures to guard against AI
deception and manipulation, inspired by this characterization: "extreme
transparency" requirements for AI-generated content and defensive systems that,
among other things, annotate AI-generated statements with contextualizing
information. Finally, I consider to what extent these measures can protect
against deceptive behavior in future, agentic AIs, and argue that non-agentic
defensive systems can provide an important layer of defense even against more
powerful agentic systems.

Generative artificial intelligence (AI) is poised to reshape the way
individuals communicate and interact. While this form of AI has the potential
to efficiently make numerous human decisions, there is limited understanding of
how individuals respond to its use in social interaction. In particular, it
remains unclear how individuals engage with algorithms when the interaction
entails consequences for other people. Here, we report the results of a
large-scale pre-registered online experiment (N = 3,552) indicating diminished
fairness, trust, trustworthiness, cooperation, and coordination by human
players in economic twoplayer games, when the decision of the interaction
partner is taken over by ChatGPT. On the contrary, we observe no adverse
welfare effects when individuals are uncertain about whether they are
interacting with a human or generative AI. Therefore, the promotion of AI
transparency, often suggested as a solution to mitigate the negative impacts of
generative AI on society, shows a detrimental effect on welfare in our study.
Concurrently, participants frequently delegate decisions to ChatGPT,
particularly when the AI's involvement is undisclosed, and individuals struggle
to discern between AI and human decisions.

My research centers on the development of context-adaptive AI systems to
improve end-user adoption through the integration of technical methods. I
deploy these AI systems across various interaction modalities, including user
interfaces and embodied agents like robots, to expand their practical
applicability. My research unfolds in three key stages: design, development,
and deployment. In the design phase, user-centered approaches were used to
understand user experiences with AI systems and create design tools for user
participation in crafting AI explanations. In the ongoing development stage, a
safety-guaranteed AI system for a robot agent was created to automatically
provide adaptive solutions and explanations for unforeseen scenarios. The next
steps will involve the implementation and evaluation of context-adaptive AI
systems in various interaction forms. I seek to prioritize human needs in
technology development, creating AI systems that tangibly benefit end-users in
real-world applications and enhance interaction experiences.

Designing for AI trustworthiness is challenging, with a lack of practical
guidance despite extensive literature on trust. The Multisource AI Scorecard
Table (MAST), a checklist rating system, addresses this gap in designing and
evaluating AI-enabled decision support systems. We propose the Principled
Approach for Designing Trustable Human-centered AI systems using MAST
Methodology (PADTHAI-MM), a nine-step framework what we demonstrate through the
iterative design of a text analysis platform called the REporting Assistant for
Defense and Intelligence Tasks (READIT). We designed two versions of READIT,
high-MAST including AI context and explanations, and low-MAST resembling a
"black box" type system. Participant feedback and state-of-the-art AI knowledge
was integrated in the design process, leading to a redesigned prototype tested
by participants in an intelligence reporting task. Results show that
MAST-guided design can improve trust perceptions, and that MAST criteria can be
linked to performance, process, and purpose information, providing a practical
and theory-informed basis for AI system design.

One of the most concrete measures to take towards meaningful AI
accountability is to consequentially assess and report the systems' performance
and impact. However, the practical nature of the "AI audit" ecosystem is
muddled and imprecise, making it difficult to work through various concepts and
map out the stakeholders involved in the practice. First, we taxonomize current
AI audit practices as completed by regulators, law firms, civil society,
journalism, academia, consulting agencies. Next, we assess the impact of audits
done by stakeholders within each domain. We find that only a subset of AI audit
studies translate to desired accountability outcomes. We thus assess and
isolate practices necessary for effective AI audit results, articulating the
observed connections between AI audit design, methodology and institutional
context on its effectiveness as a meaningful mechanism for accountability.

The intersection of Artificial Intelligence (AI) and neuroscience in
Explainable AI (XAI) is pivotal for enhancing transparency and interpretability
in complex decision-making processes. This paper explores the evolution of XAI
methodologies, ranging from feature-based to human-centric approaches, and
delves into their applications in diverse domains, including healthcare and
finance. The challenges in achieving explainability in generative models,
ensuring responsible AI practices, and addressing ethical implications are
discussed. The paper further investigates the potential convergence of XAI with
cognitive sciences, the development of emotionally intelligent AI, and the
quest for Human-Like Intelligence (HLI) in AI systems. As AI progresses towards
Artificial General Intelligence (AGI), considerations of consciousness, ethics,
and societal impact become paramount. The ongoing pursuit of deciphering the
mysteries of the brain with AI and the quest for HLI represent transformative
endeavors, bridging technical advancements with multidisciplinary explorations
of human cognition.

Generative artificial intelligence (AI) presents large risks for society when
it is used to create fake news. A crucial factor for fake news to go viral on
social media is that users share such content. Here, we aim to shed light on
the sharing behavior of users across human-generated vs. AI-generated fake
news. Specifically, we study: (1) What is the perceived veracity of
human-generated fake news vs. AI-generated fake news? (2) What is the user's
willingness to share human-generated fake news vs. AI-generated fake news on
social media? (3) What socio-economic characteristics let users fall for
AI-generated fake news? To this end, we conducted a pre-registered, online
experiment with $N=$ 988 subjects and 20 fake news from the COVID-19 pandemic
generated by GPT-4 vs. humans. Our findings show that AI-generated fake news is
perceived as less accurate than human-generated fake news, but both tend to be
shared equally. Further, several socio-economic factors explain who falls for
AI-generated fake news.

Artificial Intelligence (AI) shows promising applications for the perception
and planning tasks in autonomous driving (AD) due to its superior performance
compared to conventional methods. However, inscrutable AI systems exacerbate
the existing challenge of safety assurance of AD. One way to mitigate this
challenge is to utilize explainable AI (XAI) techniques. To this end, we
present the first comprehensive systematic literature review of explainable
methods for safe and trustworthy AD. We begin by analyzing the requirements for
AI in the context of AD, focusing on three key aspects: data, model, and
agency. We find that XAI is fundamental to meeting these requirements. Based on
this, we explain the sources of explanations in AI and describe a taxonomy of
XAI. We then identify five key contributions of XAI for safe and trustworthy AI
in AD, which are interpretable design, interpretable surrogate models,
interpretable monitoring, auxiliary explanations, and interpretable validation.
Finally, we propose a modular framework called SafeX to integrate these
contributions, enabling explanation delivery to users while simultaneously
ensuring the safety of AI models.

The rapid development of musical AI technologies has expanded the creative
potential of various musical activities, ranging from music style
transformation to music generation. However, little research has investigated
how musical AIs can support music therapists, who urgently need new technology
support. This study used a mixed method, including semi-structured interviews
and a participatory design approach. By collaborating with music therapists, we
explored design opportunities for musical AIs in music therapy. We presented
the co-design outcomes involving the integration of musical AIs into a music
therapy process, which was developed from a theoretical framework rooted in
emotion-focused therapy. After that, we concluded the benefits and concerns
surrounding music AIs from the perspective of music therapists. Based on our
findings, we discussed the opportunities and design implications for applying
musical AIs to music therapy. Our work offers valuable insights for developing
human-AI collaborative music systems in therapy involving complex procedures
and specific requirements.

This paper discusses algorithmic resignation, a strategic approach for
managing the use of AI systems within organizations. Algorithmic resignation
involves the deliberate and informed disengagement from AI assistance in
certain scenarios, by embedding governance mechanisms directly into AI systems.
Our proposal is not merely about disuse of AI but includes guiding when and how
these systems should be used or avoided. We discuss the multifaceted benefits
of algorithmic resignation, spanning economic efficiency, reputational gains,
and legal compliance. Further, we outline the operationalization of resignation
through various methods such as positive and negative nudges, stakeholder
incentive alignment, and careful consideration of the level of AI engagement.
Using techniques like barring access to AI outputs selectively or providing
explicit disclaimers on system performance, algorithmic resignation not only
mitigates risks associated with AI but also leverages its benefits, ensuring
the responsible and effective use of AI systems.

We have witnessed lately a rapid proliferation of advanced Large Language
Models (LLMs) capable of generating high-quality text. While these LLMs have
revolutionized text generation across various domains, they also pose
significant risks to the information ecosystem, such as the potential for
generating convincing propaganda, misinformation, and disinformation at scale.
This paper offers a review of AI-generated text forensic systems, an emerging
field addressing the challenges of LLM misuses. We present an overview of the
existing efforts in AI-generated text forensics by introducing a detailed
taxonomy, focusing on three primary pillars: detection, attribution, and
characterization. These pillars enable a practical understanding of
AI-generated text, from identifying AI-generated content (detection),
determining the specific AI model involved (attribution), and grouping the
underlying intents of the text (characterization). Furthermore, we explore
available resources for AI-generated text forensics research and discuss the
evolving challenges and future directions of forensic systems in an AI era.

This paper delves into the contrasting roles of data within academic and
industrial spheres, highlighting the divergence between Data-Centric AI and
Model-Agnostic AI approaches. We argue that while Data-Centric AI focuses on
the primacy of high-quality data for model performance, Model-Agnostic AI
prioritizes algorithmic flexibility, often at the expense of data quality
considerations. This distinction reveals that academic standards for data
quality frequently do not meet the rigorous demands of industrial applications,
leading to potential pitfalls in deploying academic models in real-world
settings. Through a comprehensive analysis, we address these disparities,
presenting both the challenges they pose and strategies for bridging the gap.
Furthermore, we propose a novel paradigm: Model-Based Data-Centric AI, which
aims to reconcile these differences by integrating model considerations into
data optimization processes. This approach underscores the necessity for
evolving data requirements that are sensitive to the nuances of both academic
research and industrial deployment. By exploring these discrepancies, we aim to
foster a more nuanced understanding of data's role in AI development and
encourage a convergence of academic and industrial standards to enhance AI's
real-world applicability.

There is an urgent need to incorporate the perspectives of culturally diverse
groups into AI developments. We present a novel conceptual framework for
research that aims to expand, reimagine, and reground mainstream visions of AI
using independent and interdependent cultural models of the self and the
environment. Two survey studies support this framework and provide preliminary
evidence that people apply their cultural models when imagining their ideal AI.
Compared with European American respondents, Chinese respondents viewed it as
less important to control AI and more important to connect with AI, and were
more likely to prefer AI with capacities to influence. Reflecting both cultural
models, findings from African American respondents resembled both European
American and Chinese respondents. We discuss study limitations and future
directions and highlight the need to develop culturally responsive and relevant
AI to serve a broader segment of the world population.

Against a backdrop of widespread interest in how publics can participate in
the design of AI, I argue for a research agenda focused on AI incidents -
examples of AI going wrong and sparking controversy - and how they are
constructed in online environments. I take up the example of an AI incident
from September 2020, when a Twitter user created a 'horrible experiment' to
demonstrate the racist bias of Twitter's algorithm for cropping images. This
resulted in Twitter not only abandoning its use of that algorithm, but also
disavowing its decision to use any algorithm for the task. I argue that AI
incidents like this are a significant means for participating in AI systems
that require further research. That research agenda, I argue, should focus on
how incidents are constructed through networked online behaviours that I refer
to as 'networked trouble', where formats for participation enable individuals
and algorithms to interact in ways that others - including technology companies
- come to know and come to care about. At stake, I argue, is an important
mechanism for participating in the design and deployment of AI.

This study explores the complexities of integrating Artificial Intelligence
(AI) into Autonomous Vehicles (AVs), examining the challenges introduced by AI
components and the impact on testing procedures, focusing on some of the
essential requirements for trustworthy AI. Topics addressed include the role of
AI at various operational layers of AVs, the implications of the EU's AI Act on
AVs, and the need for new testing methodologies for Advanced Driver Assistance
Systems (ADAS) and Automated Driving Systems (ADS). The study also provides a
detailed analysis on the importance of cybersecurity audits, the need for
explainability in AI decision-making processes and protocols for assessing the
robustness and ethical behaviour of predictive systems in AVs. The paper
identifies significant challenges and suggests future directions for research
and development of AI in AV technology, highlighting the need for
multidisciplinary expertise.

Research and activism have increasingly denounced the problematic
environmental record of the infrastructure and value chain underpinning
Artificial Intelligence (AI). Water-intensive data centres, polluting mineral
extraction and e-waste dumping are incontrovertibly part of AI's footprint. In
this article, I turn to areas affected by AI-fuelled environmental harm and
identify an ethics of resistance emerging from local activists, which I term
'elemental ethics'. Elemental ethics interrogates the AI value chain's
problematic relationship with the elements that make up the world, critiques
the undermining of local and ancestral approaches to nature and reveals the
vital and quotidian harms engendered by so-called intelligent systems. While
this ethics is emerging from grassroots and Indigenous groups, it echoes recent
calls from environmental philosophy to reconnect with the environment via the
elements. In empirical terms, this article looks at groups in Chile resisting a
Google data centre project in Santiago and lithium extraction (used for
rechargeable batteries) in Lickan Antay Indigenous territory, Atacama Desert.
As I show, elemental ethics can complement top-down, utilitarian and
quantitative approaches to AI ethics and sustainable AI as well as interrogate
whose lived experience and well-being counts in debates on AI extinction.

The rapid advancement of artificial intelligence (AI) and the expanding
integration of large language models (LLMs) have ignited a debate about their
application in education. This study delves into university instructors'
experiences and attitudes toward AI language models, filling a gap in the
literature by analyzing educators' perspectives on AI's role in the classroom
and its potential impacts on teaching and learning. The objective of this
research is to investigate the level of awareness, overall sentiment
towardsadoption, and the factors influencing these attitudes for LLMs and
generative AI-based tools in higher education. Data was collected through a
survey using a Likert scale, which was complemented by follow-up interviews to
gain a more nuanced understanding of the instructors' viewpoints. The collected
data was processed using statistical and thematic analysis techniques. Our
findings reveal that educators are increasingly aware of and generally positive
towards these tools. We find no correlation between teaching style and attitude
toward generative AI. Finally, while CS educators show far more confidence in
their technical understanding of generative AI tools and more positivity
towards them than educators in other fields, they show no more confidence in
their ability to detect AI-generated work.

News organizations today rely on AI tools to increase efficiency and
productivity across various tasks in news production and distribution. These
tools are oriented towards stakeholders such as reporters, editors, and
readers. However, practitioners also express reservations around adopting AI
technologies into the newsroom, due to the technical and ethical challenges
involved in evaluating AI technology and its return on investments. This is to
some extent a result of the lack of domain-specific strategies to evaluate AI
models and applications. In this paper, we consider different aspects of AI
evaluation (model outputs, interaction, and ethics) that can benefit from
domain-specific tailoring, and suggest examples of how journalistic
considerations can lead to specialized metrics or strategies. In doing so, we
lay out a potential framework to guide AI evaluation in journalism, such as
seen in other disciplines (e.g. law, healthcare). We also consider directions
for future work, as well as how our approach might generalize to other domains.

We examine whether Artificial Intelligence (AI) systems generate truly novel
ideas rather than merely regurgitating patterns learned during training.
Utilizing a novel experimental design, we task an AI with generating project
titles for hypothetical crowdfunding campaigns. We compare within AI-generated
project titles, measuring repetition and complexity. We compare between the
AI-generated titles and actual observed field data using an extension of
maximum mean discrepancy--a metric derived from the application of kernel mean
embeddings of statistical distributions to high-dimensional machine learning
(large language) embedding vectors--yielding a structured analysis of AI output
novelty. Results suggest that (1) the AI generates unique content even under
increasing task complexity, and at the limits of its computational
capabilities, (2) the generated content has face validity, being consistent
with both inputs to other generative AI and in qualitative comparison to field
data, and (3) exhibits divergence from field data, mitigating concerns relating
to intellectual property rights. We discuss implications for copyright and
trademark law.

In an era where the Internet of Things (IoT) intersects increasingly with
generative Artificial Intelligence (AI), this article scrutinizes the emergent
security risks inherent in this integration. We explore how generative AI
drives innovation in IoT and we analyze the potential for data breaches when
using generative AI and the misuse of generative AI technologies in IoT
ecosystems. These risks not only threaten the privacy and efficiency of IoT
systems but also pose broader implications for trust and safety in AI-driven
environments. The discussion in this article extends to strategic approaches
for mitigating these risks, including the development of robust security
protocols, the multi-layered security approaches, and the adoption of AI
technological solutions. Through a comprehensive analysis, this article aims to
shed light on the critical balance between embracing AI advancements and
ensuring stringent security in IoT, providing insights into the future
direction of these intertwined technologies.

The advancement in technology has made interdisciplinary research more
accessible. Particularly the breakthrough in Artificial Intelligence AI has
given huge advantages to researchers working in interdisciplinary and
multidisciplinary fields. This study investigates the ability of AI models,
particularly GPT4 and GPT Data Analyst in creating language maps for language
documentation. The study Integrates documentary linguistics linguistic
geography and AI by showcasing how AI models facilitate the spatial
documentation of languages through the creation of language maps with minimal
cartographic expertise. The study is conducted using a CSV file and a GeoJSON
file both obtained from HDX and from the researchers fieldwork. The study data
is then applied in realtime conversations with the AI models in order to
generate the language distribution maps. The study highlights the two AI models
capabilities in generating highquality static and interactive web maps and
streamlining the mapmaking process, despite facing challenges like
inconsistencies and difficulties in adding legends. The findings suggest a
promising future for AI in generating language maps and enhancing the work of
documentary linguists as they collect their data in the field pointing towards
the need for further development to fully harness AI potential in this field.

Human-centered AI (HCAI), rather than replacing the human, puts the human
user in the driver's seat of so-called human-centered AI-infused tools (HCAI
tools): interactive software tools that amplify, augment, empower, and enhance
human performance using AI models; often novel generative or foundation AI
ones. In this paper, we discuss how interactive visualization can be a key
enabling technology for creating such human-centered AI tools. Visualization
has already been shown to be a fundamental component in explainable AI models,
and coupling this with data-driven, semantic, and unified interaction feedback
loops will enable a human-centered approach to integrating AI models in the
loop with human users. We present several examples of our past and current work
on such HCAI tools, including for creative writing, temporal prediction, and
user experience analysis. We then draw parallels between these tools to suggest
common themes on how interactive visualization can support the design of future
HCAI tools.

Data are the critical fuel for Artificial Intelligence (AI) models. Poor
quality data produces inaccurate and ineffective AI models that may lead to
incorrect or unsafe use. Checking for data readiness is a crucial step in
improving data quality. Numerous R&D efforts have been spent on improving data
quality. However, standardized metrics for evaluating data readiness for use in
AI training are still evolving. In this study, we perform a comprehensive
survey of metrics used for verifying AI's data readiness. This survey examines
more than 120 papers that are published by ACM Digital Library, IEEE Xplore,
other reputable journals, and articles published on the web by prominent AI
experts. This survey aims to propose a taxonomy of data readiness for AI (DRAI)
metrics for structured and unstructured datasets. We anticipate that this
taxonomy can lead to new standards for DRAI metrics that would be used for
enhancing the quality and accuracy of AI training and inference.

The rapid integration of Artificial Intelligence (AI) systems across critical
domains necessitates robust security evaluation frameworks. We propose a novel
approach that introduces three metrics: System Complexity Index (SCI), Lyapunov
Exponent for AI Stability (LEAIS), and Nash Equilibrium Robustness (NER). SCI
quantifies the inherent complexity of an AI system, LEAIS captures its
stability and sensitivity to perturbations, and NER evaluates its strategic
robustness against adversarial manipulation. Through comparative analysis, we
demonstrate the advantages of our framework over existing techniques. We
discuss the theoretical and practical implications, potential applications,
limitations, and future research directions. Our work contributes to the
development of secure and trustworthy AI technologies by providing a holistic,
theoretically grounded approach to AI security evaluation. As AI continues to
advance, prioritising and advancing AI security through interdisciplinary
collaboration is crucial to ensure its responsible deployment for the benefit
of society.

The widespread use of artificial intelligence (AI) systems across various
domains is increasingly highlighting issues related to algorithmic fairness,
especially in high-stakes scenarios. Thus, critical considerations of how
fairness in AI systems might be improved, and what measures are available to
aid this process, are overdue. Many researchers and policymakers see
explainable AI (XAI) as a promising way to increase fairness in AI systems.
However, there is a wide variety of XAI methods and fairness conceptions
expressing different desiderata, and the precise connections between XAI and
fairness remain largely nebulous. Besides, different measures to increase
algorithmic fairness might be applicable at different points throughout an AI
system's lifecycle. Yet, there currently is no coherent mapping of fairness
desiderata along the AI lifecycle. In this paper, we set out to bridge both
these gaps: We distill eight fairness desiderata, map them along the AI
lifecycle, and discuss how XAI could help address each of them. We hope to
provide orientation for practical applications and to inspire XAI research
specifically focused on these fairness desiderata.

This paper explores the changes that pervasive AI is having on the nature of
combat. We look beyond the substitution of AI for experts to an approach where
complementary human and machine abilities are blended. Using historical and
modern examples, we show how autonomous weapons systems can be effectively
managed by teams of human "AI Operators" combined with AI/ML "Proxy Operators."
By basing our approach on the principles of complementation, we provide for a
flexible and dynamic approach to managing lethal autonomous systems. We
conclude by presenting a path to achieving an integrated vision of
machine-speed combat where the battlefield AI is operated by AI Operators that
watch for patterns of behavior within battlefield to assess the performance of
lethal autonomous systems. This approach enables the development of combat
systems that are likely to be more ethical, operate at machine speed, and are
capable of responding to a broader range of dynamic battlefield conditions than
any purely autonomous AI system could support.

We investigate how generative Artificial Intelligence (AI) can be used to
optimize resources in Unmanned Aerial Vehicle (UAV)-assisted Internet of Things
(IoT) networks. In particular, generative AI models for real-time
decision-making have been used in public safety scenarios. This work describes
how generative AI models can improve resource management within UAV-assisted
networks. Furthermore, this work presents generative AI in UAV-assisted
networks to demonstrate its practical applications and highlight its broader
capabilities. We demonstrate a real-life case study for public safety,
demonstrating how generative AI can enhance real-time decision-making and
improve training datasets. By leveraging generative AI in UAV- assisted
networks, we can design more intelligent, adaptive, and efficient ecosystems to
meet the evolving demands of wireless networks and diverse applications.
Finally, we discuss challenges and future research directions associated with
generative AI for resource optimization in UAV-assisted networks.

In the wake of rapid advancements in artificial intelligence (AI), we stand
on the brink of a transformative leap in data systems. The imminent fusion of
AI and DB (AIxDB) promises a new generation of data systems, which will relieve
the burden on end-users across all industry sectors by featuring AI-enhanced
functionalities, such as personalized and automated in-database AI-powered
analytics, self-driving capabilities for improved system performance, etc. In
this paper, we explore the evolution of data systems with a focus on deepening
the fusion of AI and DB. We present NeurDB, our next-generation data system
designed to fully embrace AI design in each major system component and provide
in-database AI-powered analytics. We outline the conceptual and architectural
overview of NeurDB, discuss its design choices and key components, and report
its current development and future plan.

With the increasing adoption of artificial intelligence (AI) technologies in
the news industry, media organizations have begun publishing guidelines that
aim to promote the responsible, ethical, and unbiased implementation of
AI-based technologies. These guidelines are expected to serve journalists and
media workers by establishing best practices and a framework that helps them
navigate ever-evolving AI tools. Drawing on institutional theory and digital
inequality concepts, this study analyzes 37 AI guidelines for media purposes in
17 countries. Our analysis reveals key thematic areas, such as transparency,
accountability, fairness, privacy, and the preservation of journalistic values.
Results highlight shared principles and best practices that emerge from these
guidelines, including the importance of human oversight, explainability of AI
systems, disclosure of automated content, and protection of user data. However,
the geographical distribution of these guidelines, highlighting the dominance
of Western nations, particularly North America and Europe, can further ongoing
concerns about power asymmetries in AI adoption and consequently isomorphism
outside these regions. Our results may serve as a resource for news
organizations, policymakers, and stakeholders looking to navigate the complex
AI development toward creating a more inclusive and equitable digital future
for the media industry worldwide.

Next generation of embedded Information and Communication Technology (ICT)
systems are collaborative systems able to perform autonomous tasks. The
remarkable expansion of the embedded ICT market, together with the rise and
breakthroughs of Artificial Intelligence (AI), have put the focus on the Edge
as it stands as one of the keys for the next technological revolution: the
seamless integration of AI in our daily life. However, training and deployment
of custom AI solutions on embedded devices require a fine-grained integration
of data, algorithms, and tools to achieve high accuracy. Such integration
requires a high level of expertise that becomes a real bottleneck for small and
medium enterprises wanting to deploy AI solutions on the Edge which,
ultimately, slows down the adoption of AI on daily-life applications. In this
work, we present a modular AI pipeline as an integrating framework to bring
data, algorithms, and deployment tools together. By removing the integration
barriers and lowering the required expertise, we can interconnect the different
stages of tools and provide a modular end-to-end development of AI products for
embedded devices. Our AI pipeline consists of four modular main steps: i) data
ingestion, ii) model training, iii) deployment optimization and, iv) the IoT
hub integration. To show the effectiveness of our pipeline, we provide examples
of different AI applications during each of the steps. Besides, we integrate
our deployment framework, LPDNN, into the AI pipeline and present its
lightweight architecture and deployment capabilities for embedded devices.
Finally, we demonstrate the results of the AI pipeline by showing the
deployment of several AI applications such as keyword spotting, image
classification and object detection on a set of well-known embedded platforms,
where LPDNN consistently outperforms all other popular deployment frameworks.

When we consult with a doctor, lawyer, or financial advisor, we generally
assume that they are acting in our best interests. But what should we assume
when it is an artificial intelligence (AI) system that is acting on our behalf?
Early examples of AI assistants like Alexa, Siri, Google, and Cortana already
serve as a key interface between consumers and information on the web, and
users routinely rely upon AI-driven systems like these to take automated
actions or provide information. Superficially, such systems may appear to be
acting according to user interests. However, many AI systems are designed with
embedded conflicts of interests, acting in ways that subtly benefit their
creators (or funders) at the expense of users. To address this problem, in this
paper we introduce the concept of AI loyalty. AI systems are loyal to the
degree that they are designed to minimize, and make transparent, conflicts of
interest, and to act in ways that prioritize the interests of users. Properly
designed, such systems could have considerable functional and competitive - not
to mention ethical - advantages relative to those that do not. Loyal AI
products hold an obvious appeal for the end-user and could serve to promote the
alignment of the long-term interests of AI developers and customers. To this
end, we suggest criteria for assessing whether an AI system is sufficiently
transparent about conflicts of interest, and acting in a manner that is loyal
to the user, and argue that AI loyalty should be considered during the
technological design process alongside other important values in AI ethics such
as fairness, accountability privacy, and equity. We discuss a range of
mechanisms, from pure market forces to strong regulatory frameworks, that could
support incorporation of AI loyalty into a variety of future AI systems.

The arrival of deep learning techniques able to infer patterns from large
datasets has dramatically improved the performance of Artificial Intelligence
(AI) systems. Deep learning's rapid development and adoption, in great part led
by large technology companies, has however created concerns about a premature
narrowing in the technological trajectory of AI research despite its
weaknesses, which include lack of robustness, high environmental costs, and
potentially unfair outcomes. We seek to improve the evidence base with a
semantic analysis of AI research in arXiv, a popular pre-prints database. We
study the evolution of the thematic diversity of AI research, compare the
thematic diversity of AI research in academia and the private sector and
measure the influence of private companies in AI research through the citations
they receive and their collaborations with other institutions. Our results
suggest that diversity in AI research has stagnated in recent years, and that
AI research involving the private sector tends to be less diverse and more
influential than research in academia. We also find that private sector AI
researchers tend to specialise in data-hungry and computationally intensive
deep learning methods at the expense of research involving other AI methods,
research that considers the societal and ethical implications of AI, and
applications in sectors like health. Our results provide a rationale for policy
action to prevent a premature narrowing of AI research that could constrain its
societal benefits, but we note the informational, incentive and scale hurdles
standing in the way of such interventions.

The 4th edition of the Montreal AI Ethics Institute's The State of AI Ethics
captures the most relevant developments in the field of AI Ethics since January
2021. This report aims to help anyone, from machine learning experts to human
rights activists and policymakers, quickly digest and understand the
ever-changing developments in the field. Through research and article
summaries, as well as expert commentary, this report distills the research and
reporting surrounding various domains related to the ethics of AI, with a
particular focus on four key themes: Ethical AI, Fairness & Justice, Humans &
Tech, and Privacy.
  In addition, The State of AI Ethics includes exclusive content written by
world-class AI Ethics experts from universities, research institutes,
consulting firms, and governments. Opening the report is a long-form piece by
Edward Higgs (Professor of History, University of Essex) titled "AI and the
Face: A Historian's View." In it, Higgs examines the unscientific history of
facial analysis and how AI might be repeating some of those mistakes at scale.
The report also features chapter introductions by Alexa Hagerty
(Anthropologist, University of Cambridge), Marianna Ganapini (Faculty Director,
Montreal AI Ethics Institute), Deborah G. Johnson (Emeritus Professor,
Engineering and Society, University of Virginia), and Soraj Hongladarom
(Professor of Philosophy and Director, Center for Science, Technology and
Society, Chulalongkorn University in Bangkok).
  This report should be used not only as a point of reference and insight on
the latest thinking in the field of AI Ethics, but should also be used as a
tool for introspection as we aim to foster a more nuanced conversation
regarding the impacts of AI on the world.

Artificial intelligence (AI) has become a part of everyday conversation and
our lives. It is considered as the new electricity that is revolutionizing the
world. AI is heavily invested in both industry and academy. However, there is
also a lot of hype in the current AI debate. AI based on so-called deep
learning has achieved impressive results in many problems, but its limits are
already visible. AI has been under research since the 1940s, and the industry
has seen many ups and downs due to over-expectations and related
disappointments that have followed.
  The purpose of this book is to give a realistic picture of AI, its history,
its potential and limitations. We believe that AI is a helper, not a ruler of
humans. We begin by describing what AI is and how it has evolved over the
decades. After fundamentals, we explain the importance of massive data for the
current mainstream of artificial intelligence. The most common representations
for AI, methods, and machine learning are covered. In addition, the main
application areas are introduced. Computer vision has been central to the
development of AI. The book provides a general introduction to computer vision,
and includes an exposure to the results and applications of our own research.
Emotions are central to human intelligence, but little use has been made in AI.
We present the basics of emotional intelligence and our own research on the
topic. We discuss super-intelligence that transcends human understanding,
explaining why such achievement seems impossible on the basis of present
knowledge,and how AI could be improved. Finally, a summary is made of the
current state of AI and what to do in the future. In the appendix, we look at
the development of AI education, especially from the perspective of contents at
our own university.

The rapid development of artificial intelligence (AI) has led to increasing
concerns about the capability of AI systems to make decisions and behave
responsibly. Responsible AI (RAI) refers to the development and use of AI
systems that benefit humans, society, and the environment while minimising the
risk of negative consequences. To ensure responsible AI, the risks associated
with AI systems' development and use must be identified, assessed and
mitigated. Various AI risk assessment frameworks have been released recently by
governments, organisations, and companies. However, it can be challenging for
AI stakeholders to have a clear picture of the available frameworks and
determine the most suitable ones for a specific context. Additionally, there is
a need to identify areas that require further research or development of new
frameworks, as well as updating and maintaining existing ones. To fill the gap,
we present a mapping study of 16 existing AI risk assessment frameworks from
the industry, governments, and non-government organizations (NGOs). We identify
key characteristics of each framework and analyse them in terms of RAI
principles, stakeholders, system lifecycle stages, geographical locations,
targeted domains, and assessment methods. Our study provides a comprehensive
analysis of the current state of the frameworks and highlights areas of
convergence and divergence among them. We also identify the deficiencies in
existing frameworks and outlines the essential characteristics of a concrete
and connected framework AI risk assessment (C$^2$AIRA) framework. Our findings
and insights can help relevant stakeholders choose suitable AI risk assessment
frameworks and guide the design of future frameworks towards concreteness and
connectedness.

In the span of a few months, generative Artificial Intelligence (AI) tools
that can generate realistic images or text have taken the Internet by storm,
making them one of the technologies with fastest adoption ever. Some of these
generative AI tools such as DALL-E, MidJourney, or ChatGPT have gained wide
public notoriety. Interestingly, these tools are possible because of the
massive amount of data (text and images) available on the Internet. The tools
are trained on massive data sets that are scraped from Internet sites. And now,
these generative AI tools are creating massive amounts of new data that are
being fed into the Internet. Therefore, future versions of generative AI tools
will be trained with Internet data that is a mix of original and AI-generated
data. As time goes on, a mixture of original data and data generated by
different versions of AI tools will populate the Internet. This raises a few
intriguing questions: how will future versions of generative AI tools behave
when trained on a mixture of real and AI generated data? Will they evolve with
the new data sets or degenerate? Will evolution introduce biases in subsequent
generations of generative AI tools? In this document, we explore these
questions and report some very initial simulation results using a simple
image-generation AI tool. These results suggest that the quality of the
generated images degrades as more AI-generated data is used for training thus
suggesting that generative AI may degenerate. Although these results are
preliminary and cannot be generalised without further study, they serve to
illustrate the potential issues of the interaction between generative AI and
the Internet.

Trustworthy Artificial Intelligence (AI) is based on seven technical
requirements sustained over three main pillars that should be met throughout
the system's entire life cycle: it should be (1) lawful, (2) ethical, and (3)
robust, both from a technical and a social perspective. However, attaining
truly trustworthy AI concerns a wider vision that comprises the trustworthiness
of all processes and actors that are part of the system's life cycle, and
considers previous aspects from different lenses. A more holistic vision
contemplates four essential axes: the global principles for ethical use and
development of AI-based systems, a philosophical take on AI ethics, a
risk-based approach to AI regulation, and the mentioned pillars and
requirements. The seven requirements (human agency and oversight; robustness
and safety; privacy and data governance; transparency; diversity,
non-discrimination and fairness; societal and environmental wellbeing; and
accountability) are analyzed from a triple perspective: What each requirement
for trustworthy AI is, Why it is needed, and How each requirement can be
implemented in practice. On the other hand, a practical approach to implement
trustworthy AI systems allows defining the concept of responsibility of
AI-based systems facing the law, through a given auditing process. Therefore, a
responsible AI system is the resulting notion we introduce in this work, and a
concept of utmost necessity that can be realized through auditing processes,
subject to the challenges posed by the use of regulatory sandboxes. Our
multidisciplinary vision of trustworthy AI culminates in a debate on the
diverging views published lately about the future of AI. Our reflections in
this matter conclude that regulation is a key for reaching a consensus among
these views, and that trustworthy and responsible AI systems will be crucial
for the present and future of our society.

In settings where users both need high accuracy and are time-pressured, such
as doctors working in emergency rooms, we want to provide AI assistance that
both increases decision accuracy and reduces decision-making time. Current
literature focusses on how users interact with AI assistance when there is no
time pressure, finding that different AI assistances have different benefits:
some can reduce time taken while increasing overreliance on AI, while others do
the opposite. The precise benefit can depend on both the user and task. In
time-pressured scenarios, adapting when we show AI assistance is especially
important: relying on the AI assistance can save time, and can therefore be
beneficial when the AI is likely to be right. We would ideally adapt what AI
assistance we show depending on various properties (of the task and of the
user) in order to best trade off accuracy and time. We introduce a study where
users have to answer a series of logic puzzles. We find that time pressure
affects how users use different AI assistances, making some assistances more
beneficial than others when compared to no-time-pressure settings. We also find
that a user's overreliance rate is a key predictor of their behaviour:
overreliers and not-overreliers use different AI assistance types differently.
We find marginal correlations between a user's overreliance rate (which is
related to the user's trust in AI recommendations) and their personality traits
(Big Five Personality traits). Overall, our work suggests that AI assistances
have different accuracy-time tradeoffs when people are under time pressure
compared to no time pressure, and we explore how we might adapt AI assistances
in this setting.

Imagine if AI decision-support tools not only complemented our ability to
make accurate decisions, but also improved our skills, boosted collaboration,
and elevated the joy we derive from our tasks. Despite the potential to
optimize a broad spectrum of such human-centric objectives, the design of
current AI tools remains focused on decision accuracy alone. We propose offline
reinforcement learning (RL) as a general approach for modeling human-AI
decision-making to optimize human-AI interaction for diverse objectives. RL can
optimize such objectives by tailoring decision support, providing the right
type of assistance to the right person at the right time. We instantiated our
approach with two objectives: human-AI accuracy on the decision-making task and
human learning about the task and learned decision support policies from
previous human-AI interaction data. We compared the optimized policies against
several baselines in AI-assisted decision-making. Across two experiments (N=316
and N=964), our results demonstrated that people interacting with policies
optimized for accuracy achieve significantly better accuracy -- and even
human-AI complementarity -- compared to those interacting with any other type
of AI support. Our results further indicated that human learning was more
difficult to optimize than accuracy, with participants who interacted with
learning-optimized policies showing significant learning improvement only at
times. Our research (1) demonstrates offline RL to be a promising approach to
model human-AI decision-making, leading to policies that may optimize
human-centric objectives and provide novel insights about the AI-assisted
decision-making space, and (2) emphasizes the importance of considering
human-centric objectives beyond decision accuracy in AI-assisted
decision-making, opening up the novel research challenge of optimizing human-AI
interaction for such objectives.

We argue that there already exists de facto artificial intelligence policy -
a patchwork of policies impacting the field of AI's development in myriad ways.
The key question related to AI policy, then, is not whether AI should be
governed at all, but how it is currently being governed, and how that
governance might become more informed, integrated, effective, and anticipatory.
We describe the main components of de facto AI policy and make some
recommendations for how AI policy can be improved, drawing on lessons from
other scientific and technological domains.

Games have always been popular testbeds for Artificial Intelligence (AI). In
the last decade, we have seen the rise of the Multiple Online Battle Arena
(MOBA) games, which are the most played games nowadays. In spite of this, there
are few works that explore MOBA as a testbed for AI Research. In this paper we
present and discuss the main features and opportunities offered by MOBA games
to Game AI Research. We describe the various challenges faced along the game
and also propose a discrete model that can be used to better understand and
explore the game. With this, we aim to encourage the use of MOBA as a novel
research platform for Game AI.

MOBAs represent a huge segment of online gaming and are growing as both an
eSport and a casual genre. The natural starting point for AI researchers
interested in MOBAs is to develop an AI to play the game better than a human -
but MOBAs have many more challenges besides adversarial AI. In this paper we
introduce the reader to the wider context of MOBA culture, propose a range of
challenges faced by the community today, and posit concrete AI projects that
can be undertaken to begin solving them.

Here we examine the paperclip apocalypse concern for artificial general
intelligence (or AGI) whereby a superintelligent AI with a simple goal (ie.,
producing paperclips) accumulates power so that all resources are devoted
towards that simple goal and are unavailable for any other use. We provide
conditions under which a paper apocalypse can arise but also show that, under
certain architectures for recursive self-improvement of AIs, that a paperclip
AI may refrain from allowing power capabilities to be developed. The reason is
that such developments pose the same control problem for the AI as they do for
humans (over AIs) and hence, threaten to deprive it of resources for its
primary goal.

Artificial intelligence (AI) like deep learning, cloud AI computation has
been advancing at a rapid pace since 2014. There is no doubt that the
prosperity of AI is inseparable with the development of the Internet. However,
there has been little attention to the link between AI and the internet. This
paper explores them with brain insights mainly from four views:1) How is the
general relation between artificial intelligence and Internet of Things, cloud
computing, big data and Industrial Internet from the perspective of brain
science. 2) Construction of a new AI system model with the Internet and brain
science.

The complexity of dynamics in AI techniques is already approaching that of
complex adaptive systems, thus curtailing the feasibility of formal
controllability and reachability analysis in the context of AI safety. It
follows that the envisioned instances of Artificial General Intelligence (AGI)
will also suffer from challenges of complexity. To tackle such issues, we
propose the modeling of deleterious behaviors in AI and AGI as psychological
disorders, thereby enabling the employment of psychopathological approaches to
analysis and control of misbehaviors. Accordingly, we present a discussion on
the feasibility of the psychopathological approaches to AI safety, and propose
general directions for research on modeling, diagnosis, and treatment of
psychological disorders in AGI.

Currently, there is no consistent model for visually or formally representing
the architecture of AI systems. This lack of representation brings
interpretability, correctness and completeness challenges in the description of
existing models and systems. DIAL (The Diagrammatic AI Language) has been
created with the aspiration of being an "engineering schematic" for AI Systems.
It is presented here as a starting point for a community dialogue towards a
common diagrammatic language for AI Systems.

With the rapid development of AI technologies, thousands of AI papers are
being published each year. Many of these papers have released sample code to
facilitate follow-up researchers. This paper presents an explorative study of
over 1700 code repositories of AI papers hosted on GitHub. We find that these
repositories are often poorly written, lack of documents, lack of maintenance,
and hard to configure the underlying runtime environment. Thus, many code
repositories become inactive and abandoned. Such a situation makes follow-up
researchers hard to reproduce the results or do further research. In addition,
these hard-to-reuse code makes a gap between academia and industry. Based on
the findings, we give some recommendations on how to improve the quality of
code repositories of AI papers.

Artificial Intelligence (AI) technology is rapidly changing many areas of
society. While there is tremendous potential in this transition, there are
several pitfalls as well. Using the history of computing and the world-wide web
as a guide, in this article we identify those pitfalls and actions that lead AI
development to its full potential. If done right, AI will be instrumental in
achieving the goals we set for economy, society, and the world in general.

Recent developments in artificial intelligence and machine learning have
spurred interest in the growing field of AI safety, which studies how to
prevent human-harming accidents when deploying AI systems. This paper thus
explores the intersection of AI safety with evolutionary computation, to show
how safety issues arise in evolutionary computation and how understanding from
evolutionary computational and biological evolution can inform the broader
study of AI safety.

European Law now requires AI to be explainable in the context of adverse
decisions affecting European Union (EU) citizens. At the same time, it is
expected that there will be increasing instances of AI failure as it operates
on imperfect data. This paper puts forward a neurally-inspired framework called
decision stacks that can provide for a way forward in research aimed at
developing explainable AI. Leveraging findings from memory systems in
biological brains, the decision stack framework operationalizes the definition
of explainability and then proposes a test that can potentially reveal how a
given AI decision came to its conclusion.

With the widespread integration of AI in everyday and critical technologies,
it seems inevitable to witness increasing instances of failure in AI systems.
In such cases, there arises a need for technical investigations that produce
legally acceptable and scientifically indisputable findings and conclusions on
the causes of such failures. Inspired by the domain of cyber forensics, this
paper introduces the need for the establishment of AI Forensics as a new
discipline under AI safety. Furthermore, we propose a taxonomy of the subfields
under this discipline, and present a discussion on the foundational challenges
that lay ahead of this new research area.

Artificial Intelligence (AI) is an important driving force for the
development and transformation of the financial industry. However, with the
fast-evolving AI technology and application, unintentional bias, insufficient
model validation, immature contingency plan and other underestimated threats
may expose the company to operational and reputational risks. In this paper, we
focus on fairness evaluation, one of the key components of AI Governance,
through a quantitative lens. Statistical methods are reviewed for imbalanced
data treatment and bias mitigation. These methods and fairness evaluation
metrics are then applied to a credit card default payment example.

In the past few years, several large companies have published ethical
principles of Artificial Intelligence (AI). National governments, the European
Commission, and inter-governmental organizations have come up with requirements
to ensure the good use of AI. However, individual organizations that want to
join this effort, are faced with many unsolved questions. This paper proposes
guidelines for organizations committed to the responsible use of AI, but lack
the required knowledge and experience. The guidelines consist of two parts: i)
helping organizations to decide what principles to adopt, and ii) a methodology
for implementing the principles in organizational processes. In case of future
AI regulation, organizations following this approach will be well-prepared.

Insofar as consciousness has a functional role in facilitating learning and
behavioral control, the builders of autonomous AI systems are likely to attempt
to incorporate it into their designs. The extensive literature on the ethics of
AI is concerned with ensuring that AI systems, and especially autonomous
conscious ones, behave ethically. In contrast, our focus here is on the rarely
discussed complementary aspect of engineering conscious AI: how to avoid
condemning such systems, for whose creation we would be solely responsible, to
unavoidable suffering brought about by phenomenal self-consciousness. We
outline two complementary approaches to this problem, one motivated by a
philosophical analysis of the phenomenal self, and the other by certain
computational concepts in reinforcement learning.

The COVID-19 crisis has brought about new clinical questions, new workflows,
and accelerated distributed healthcare needs. While artificial intelligence
(AI)-based clinical decision support seemed to have matured, the application of
AI-based tools for COVID-19 has been limited to date. In this perspective
piece, we identify opportunities and requirements for AI-based clinical
decision support systems and highlight challenges that impact "AI readiness"
for rapidly emergent healthcare challenges.

Here we discuss the four key principles of bio-medical ethics from surgical
context. We elaborate on the definition of 'fairness' and its implications in
AI system design, with taxonomy of algorithmic biases in AI. We discuss the
shifts in ethical paradigms as the degree of autonomy in AI systems continue to
evolve. We also emphasize the need for continuous revisions of ethics in AI due
to evolution and dynamic nature of AI systems and technologies.

Invention of artificial general intelligence is predicted to cause a shift in
the trajectory of human civilization. In order to reap the benefits and avoid
pitfalls of such powerful technology it is important to be able to control it.
However, possibility of controlling artificial general intelligence and its
more advanced version, superintelligence, has not been formally established. In
this paper, we present arguments as well as supporting evidence from multiple
domains indicating that advanced AI can't be fully controlled. Consequences of
uncontrollability of AI are discussed with respect to future of humanity and
research on AI, and AI safety and security.

With the widespread and pervasive use of Artificial Intelligence (AI) for
automated decision-making systems, AI bias is becoming more apparent and
problematic. One of its negative consequences is discrimination: the unfair, or
unequal treatment of individuals based on certain characteristics. However, the
relationship between bias and discrimination is not always clear. In this
paper, we survey relevant literature about bias and discrimination in AI from
an interdisciplinary perspective that embeds technical, legal, social and
ethical dimensions. We show that finding solutions to bias and discrimination
in AI requires robust cross-disciplinary collaborations.

The current chapter aims at establishing a relationship between artificial
intelligence (AI) and hardware security. Such a connection between AI and
software security has been confirmed and well-reviewed in the relevant
literature. The main focus here is to explore the methods borrowed from AI to
assess the security of a hardware primitive, namely physically unclonable
functions (PUFs), which has found applications in cryptographic protocols,
e.g., authentication and key generation. Metrics and procedures devised for
this are further discussed. Moreover, By reviewing PUFs designed by applying AI
techniques, we give insight into future research directions in this area.

Despite its utopian promises as a disruptive equalizer, AI - like most tools
deployed under the guise of neutrality - has tended to simply reinforce
existing social structures. To counter this trend, radical AI calls for
centering on the marginalized. We argue that gaps in key infrastructure are
preventing the widespread adoption of radical AI, and propose a guiding
principle for both identifying these infrastructure gaps and evaluating whether
proposals for new infrastructure effectively center marginalized voices.

Artificial Intelligence/Machine Learning techniques have been widely used in
software engineering to improve developer productivity, the quality of software
systems, and decision-making. However, such AI/ML models for software
engineering are still impractical, not explainable, and not actionable. These
concerns often hinder the adoption of AI/ML models in software engineering
practices. In this article, we first highlight the need for explainable AI in
software engineering. Then, we summarize three successful case studies on how
explainable AI techniques can be used to address the aforementioned challenges
by making software defect prediction models more practical, explainable, and
actionable.

This work presents a large-scale analysis of artificial intelligence (AI) and
machine learning (ML) references within news articles and scientific
publications between 2011 and 2019. We implement word association measurements
that automatically identify shifts in language co-occurring with AI/ML and
quantify the strength of these word associations. Our results highlight the
evolution of perceptions and definitions around AI/ML and detect emerging
application areas, models, and systems (e.g., blockchain and cybersecurity).
Recent small-scale, manual studies have explored AI/ML discourse within the
general public, the policymaker community, and researcher community, but are
limited in their scalability and longevity. Our methods provide new views into
public perceptions and subject-area expert discussions of AI/ML and greatly
exceed the explanative power of prior work.

There have been increasing concerns about Artificial Intelligence (AI) due to
its unfathomable potential power. To make AI address ethical challenges and
shun undesirable outcomes, researchers proposed to develop socially responsible
AI (SRAI). One of these approaches is causal learning (CL). We survey
state-of-the-art methods of CL for SRAI. We begin by examining the seven CL
tools to enhance the social responsibility of AI, then review how existing
works have succeeded using these tools to tackle issues in developing SRAI such
as fairness. The goal of this survey is to bring forefront the potentials and
promises of CL for SRAI.

After the tremendous advances of deep learning and other AI methods, more
attention is flowing into other properties of modern approaches, such as
interpretability, fairness, etc. combined in frameworks like Responsible AI.
Two research directions, namely Explainable AI and Uncertainty Quantification
are becoming more and more important, but have been so far never combined and
jointly explored. In this paper, I show how both research areas provide
potential for combination, why more research should be done in this direction
and how this would lead to an increase in trustability in AI systems.

Departing from the claim that AI needs to be trustworthy, we find that
ethical advice from an AI-powered algorithm is trusted even when its users know
nothing about its training data and when they learn information about it that
warrants distrust. We conducted online experiments where the subjects took the
role of decision-makers who received advice from an algorithm on how to deal
with an ethical dilemma. We manipulated the information about the algorithm and
studied its influence. Our findings suggest that AI is overtrusted rather than
distrusted. We suggest digital literacy as a potential remedy to ensure the
responsible use of AI.

This article examines the concept of 'AI fairness' for people with
disabilities from the perspective of data protection and equality law. This
examination demonstrates that there is a need for a distinctive approach to AI
fairness that is fundamentally different to that used for other protected
characteristics, due to the different ways in which discrimination and data
protection law applies in respect of Disability. We articulate this new agenda
for AI fairness for people with disabilities, explaining how combining data
protection and equality law creates new opportunities for disabled people's
organisations and assistive technology researchers alike to shape the use of
AI, as well as to challenge potential harmful uses.

Artificial Intelligence (AI) is envisioned to play a pivotal role in
empowering intelligent, adaptive and autonomous security management in 5G and
beyond networks, thanks to its potential to uncover hidden patterns from a
large set of time-varying multi-dimensional data, and deliver faster and
accurate decisions. Unfortunately, AI's capabilities and vulnerabilities make
it a double-edged sword that may jeopardize the security of future networks.
This paper sheds light on how AI may impact the security of 5G and its
successive from its posture of defender, offender or victim, and recommends
potential defenses to safeguard from malevolent AI while pointing out their
limitations and adoption challenges.

Scientific research communities are embracing AI-based solutions to target
tractable scientific tasks and improve research workflows. However, the
development and evaluation of such solutions are scattered across multiple
disciplines. We formalize the problem of scientific AI benchmarking, and
propose a system called SAIBench in the hope of unifying the efforts and
enabling low-friction on-boarding of new disciplines. The system approaches
this goal with SAIL, a domain-specific language to decouple research problems,
AI models, ranking criteria, and software/hardware configuration into reusable
modules. We show that this approach is flexible and can adapt to problems, AI
models, and evaluation methods defined in different perspectives. The project
homepage is https://www.computercouncil.org/SAIBench

The Last of Us is a game focused on stealth, companionship and strategy. The
game is based in a lonely world after the pandemic and thus it needs AI
companions to gain the interest of players. There are three main NPCs the game
has - Infected, Human enemy and Buddy AIs. This case study talks about the
challenges in front of the developers to create AI for these NPCs and the AI
techniques they used to solve them. It also compares the challenges and
approach with similar industry-leading games.

Artificial Intelligence (AI) as a highly transformative technology take on a
special role as both an enabler and a threat to UN Sustainable Development
Goals (SDGs). AI Ethics and emerging high-level policy efforts stand at the
pivot point between these outcomes but is barred from effect due the
abstraction gap between high-level values and responsible action. In this paper
the Responsible Norms (RAIN) framework is presented, bridging this gap thereby
enabling effective high-level control of AI impact. With effective and
operationalized AI Ethics, AI technologies can be directed towards global
sustainable development.

As the practicality of Artificial Intelligence (AI) and Machine Learning (ML)
based techniques grow, there is an ever increasing threat of adversarial
attacks. There is a need to red team this ecosystem to identify system
vulnerabilities, potential threats, characterize properties that will enhance
system robustness, and encourage the creation of effective defenses. A
secondary need is to share this AI security threat intelligence between
different stakeholders like, model developers, users, and AI/ML security
professionals. In this paper, we create and describe a prototype system CTI4AI,
to overcome the need to methodically identify and share AI/ML specific
vulnerabilities and threat intelligence.

Responsible AI must be able to make decisions that consider human values and
can be justified by human morals. Operationalising normative ethical principles
inferred from philosophy supports responsible reasoning. We survey computer
science literature and develop a taxonomy of 23 normative ethical principles
which can be operationalised in AI. We describe how each principle has
previously been operationalised, highlighting key themes that AI practitioners
seeking to implement ethical principles should be aware of. We envision that
this taxonomy will facilitate the development of methodologies to incorporate
normative ethical principles in responsible AI systems.

Participatory Artificial Intelligence (PAI) has recently gained interest by
researchers as means to inform the design of technology through collective's
lived experience. PAI has a greater promise than that of providing useful input
to developers, it can contribute to the process of democratizing the design of
technology, setting the focus on what should be designed. However, in the
process of PAI there existing institutional power dynamics that hinder the
realization of expansive dreams and aspirations of the relevant stakeholders.
In this work we propose co-design principals for AI that address institutional
power dynamics focusing on Participatory AI with youth.

The proposed EU AI Act is the first comprehensive attempt to regulate AI in a
major jurisdiction. This article analyses Article 9, the key risk management
provision in the AI Act. It gives an overview of the regulatory concept behind
Article 9, determines its purpose and scope of application, offers a
comprehensive interpretation of the specific risk management requirements, and
outlines ways in which the requirements can be enforced. This article is
written with the aim of helping providers of high-risk systems comply with the
requirements set out in Article 9. In addition, it can inform revisions of the
current draft of the AI Act and efforts to develop harmonised standards on AI
risk management.

Generative AI tools hold promise to increase human productivity. This paper
presents results from a controlled experiment with GitHub Copilot, an AI pair
programmer. Recruited software developers were asked to implement an HTTP
server in JavaScript as quickly as possible. The treatment group, with access
to the AI pair programmer, completed the task 55.8% faster than the control
group. Observed heterogenous effects show promise for AI pair programmers to
help people transition into software development careers.

Requirements Engineering (RE) is the discipline for identifying, analyzing,
as well as ensuring the implementation and delivery of user, technical, and
societal requirements. Recently reported issues concerning the acceptance of
Artificial Intelligence (AI) solutions after deployment, e.g. in the medical,
automotive, or scientific domains, stress the importance of RE for designing
and delivering Responsible AI systems. In this paper, we argue that RE should
not only be carefully conducted but also tailored for Responsible AI. We
outline related challenges for research and practice.

Industries worldwide are being transformed by artificial intelligence (AI),
and the telecom industry is no different. Standardization is critical for
industry alignment to achieve widespread adoption of AI in telecom. The 3rd
generation partnership project (3GPP) Release 18 is the first release of
5G-Advanced, which includes a diverse set of study and work items dedicated to
AI. This article provides a holistic overview of the state of the art in the
3GPP work on AI in 5G-Advanced, by presenting the various 3GPP Release-18
activities on AI as an organic whole, explaining in detail the design aspects,
and sharing various design rationales influencing standardization.

Experiential AI is presented as a research agenda in which scientists and
artists come together to investigate the entanglements between humans and
machines, and an approach to human-machine learning and development where
knowledge is created through the transformation of experience. The paper
discusses advances and limitations in the field of explainable AI; the
contribution the arts can offer to address those limitations; and methods to
bring creative practice together with emerging technology to create rich
experiences that shed light on novel socio-technical systems, changing the way
that publics, scientists and practitioners think about AI.

This paper explores the journey of AI in finance, with a particular focus on
the crucial role and potential of Explainable AI (XAI). We trace AI's evolution
from early statistical methods to sophisticated machine learning, highlighting
XAI's role in popular financial applications. The paper underscores the
superior interpretability of methods like Shapley values compared to
traditional linear regression in complex financial scenarios. It emphasizes the
necessity of further XAI research, given forthcoming EU regulations. The paper
demonstrates, through simulations, that XAI enhances trust in AI systems,
fostering more responsible decision-making within finance.

As artificial intelligence (AI) systems become more prevalent, ensuring
fairness in their design becomes increasingly important. This survey focuses on
the subdomains of social media and healthcare, examining the concepts of
fairness, accountability, transparency, and ethics (FATE) within the context of
AI. We explore existing research on FATE in AI, highlighting the benefits and
limitations of current solutions, and provide future research directions. We
found that statistical and intersectional fairness can support fairness in
healthcare on social media platforms, and transparency in AI is essential for
accountability. While solutions like simulation, data analytics, and automated
systems are widely used, their effectiveness can vary, and keeping up-to-date
with the latest research is crucial.

Artificial intelligence (AI) models are prevalent today and provide a
valuable tool for artists. However, a lesser-known artifact that comes with AI
models that is not always discussed is the glitch. Glitches occur for various
reasons; sometimes, they are known, and sometimes they are a mystery. Artists
who use AI models to generate art might not understand the reason for the
glitch but often want to experiment and explore novel ways of augmenting the
output of the glitch. This paper discusses some of the questions artists have
when leveraging the glitch in AI art production. It explores the unexpected
positive outcomes produced by glitches in the specific context of motion
capture and performance art.

Artificial Intelligence (AI) has a communication problem. XAI methods have
been used to make AI more understandable and helped resolve some of the
transparency issues that inhibit AI's broader usability. However, user
evaluation studies reveal that the often numerical explanations provided by XAI
methods have not always been effective for many types of users of AI systems.
This article aims to adapt the major communications models from Science
Communications into a framework for practitioners to understand, influence, and
integrate the context of audiences both for their communications supporting AI
literacy in the public and in designing XAI systems that are more adaptive to
different users.

Insights: - The human-centered AI (HCAI) approach and the sociotechnical
systems (STS) theory share the same goal: ensuring that new technologies such
as AI best serve humans in a sociotechnical environment. - HCAI practice needs
to fully embrace sociotechnical systems thinking, while traditional STS needs
to evolve to address the emerging characteristics of AI technology. - We
propose a conceptual framework for intelligent sociotechnical systems (iSTS) to
enhance traditional STS theory in the AI era. - Based on iSTS, we further
propose a sociotechnical-based hierarchical HCAI approach as a paradigmatic
extension to existing HCAI practice, further advancing HCAI practice.

This paper explores the transformative potential of artificial intelligence
(AI) in the context of sustainable agricultural development across diverse
regions in Africa. Delving into opportunities, challenges, and impact, the
study navigates through the dynamic landscape of AI applications in
agriculture. Opportunities such as precision farming, crop monitoring, and
climate-resilient practices are examined, alongside challenges related to
technological infrastructure, data accessibility, and skill gaps. The article
analyzes the impact of AI on smallholder farmers, supply chains, and inclusive
growth. Ethical considerations and policy implications are also discussed,
offering insights into responsible AI integration. By providing a nuanced
understanding, this paper contributes to the ongoing discourse on leveraging AI
for fostering sustainability in African agriculture.

This document offers a critical overview of the emerging trends and
significant advancements in artificial intelligence (AI) within the
pharmaceutical industry. Detailing its application across key operational
areas, including research and development, animal testing, clinical trials,
hospital clinical stages, production, regulatory affairs, quality control and
other supporting areas, the paper categorically examines AI's role in each
sector. Special emphasis is placed on cutting-edge AI technologies like machine
learning algorithms and their contributions to various aspects of
pharmaceutical operations. Through this comprehensive analysis, the paper
highlights the transformative potential of AI in reshaping the pharmaceutical
industry's future.

The main goal of this project is to create a new software artefact: a custom
Generative Pre-trained Transformer (GPT) for developers to discuss and solve
ethical issues through AI engineering. This conversational agent will provide
developers with practical application on (1) how to comply with legal
frameworks which regard AI systems (like the EU AI Act~\cite{aiact} and
GDPR~\cite{gdpr}) and (2) present alternate ethical perspectives to allow
developers to understand and incorporate alternate moral positions. In this
paper, we provide motivation for the need of such an agent, detail our idea and
demonstrate a use case. The use of such a tool can allow practitioners to
engineer AI solutions which meet legal requirements and satisfy diverse ethical
perspectives.

Continuous innovations profoundly impact the financial and commercial
domains, reshaping conventional business practices. Among the disruptive
forces, Artificial Intelligence (AI), Machine Learning (ML), and blockchain
technology stand out prominently. This study aims to evaluate the integration
of blockchain, AI, and ML within financial accounting practices. It suggests a
potential revolutionary impact on financial accounting through the adoption of
blockchain technology and ML, promising reduced accounting expenses, heightened
precision, real-time financial reporting capabilities, and expeditious auditing
processes. AI's role in automating repetitive financial accounting tasks
assists organizations in circumventing the need for additional staff, thereby
minimizing associated costs. Consequently, to bolster efficiency, businesses
are increasingly embracing blockchain technology and AI applications in their
financial accounting operations.

As a research-product hybrid group in AI for Software Engineering (AI4SE), we
present four key takeaways from our experience developing in-IDE AI coding
assistants. AI coding assistants should set clear expectations for usage,
integrate with advanced IDE capabilities and existing extensions, use
extendable backend designs, and collect app data responsibly for downstream
analyses. We propose open questions and challenges that academia and industry
should address to realize the vision of next-generation AI coding assistants.

We present FuSeBMC-AI, a test generation tool grounded in machine learning
techniques. FuSeBMC-AI extracts various features from the program and employs
support vector machine and neural network models to predict a hybrid approach
optimal configuration. FuSeBMC-AI utilizes Bounded Model Checking and Fuzzing
as back-end verification engines. FuSeBMC-AI outperforms the default
configuration of the underlying verification engine in certain cases while
concurrently diminishing resource consumption.

Reinforcement Learning AI commonly uses reward/penalty signals that are
objective and explicit in an environment -- e.g. game score, completion time,
etc. -- in order to learn the optimal strategy for task performance. However,
Human-AI interaction for such AI agents should include additional reinforcement
that is implicit and subjective -- e.g. human preferences for certain AI
behavior -- in order to adapt the AI behavior to idiosyncratic human
preferences. Such adaptations would mirror naturally occurring processes that
increase trust and comfort during social interactions. Here, we show how a
hybrid brain-computer-interface (hBCI), which detects an individual's level of
interest in objects/events in a virtual environment, can be used to adapt the
behavior of a Deep Reinforcement Learning AI agent that is controlling a
virtual autonomous vehicle. Specifically, we show that the AI learns a driving
strategy that maintains a safe distance from a lead vehicle, and most novelly,
preferentially slows the vehicle when the human passengers of the vehicle
encounter objects of interest. This adaptation affords an additional 20\%
viewing time for subjectively interesting objects. This is the first
demonstration of how an hBCI can be used to provide implicit reinforcement to
an AI agent in a way that incorporates user preferences into the control
system.

The emergence of artificial intelligence (AI) and its progressively wider
impact on many sectors across the society requires an assessment of its effect
on sustainable development. Here we analyze published evidence of positive or
negative impacts of AI on the achievement of each of the 17 goals and 169
targets of the 2030 Agenda for Sustainable Development. We find that AI can
support the achievement of 128 targets across all SDGs, but it may also inhibit
58 targets. Notably, AI enables new technologies that improve efficiency and
productivity, but it may also lead to increased inequalities among and within
countries, thus hindering the achievement of the 2030 Agenda. The fast
development of AI needs to be supported by appropriate policy and regulation.
Otherwise, it would lead to gaps in transparency, accountability, safety and
ethical standards of AI-based technology, which could be detrimental towards
the development and sustainable use of AI. Finally, there is a lack of research
assessing the medium- and long-term impacts of AI. It is therefore essential to
reinforce the global debate regarding the use of AI and to develop the
necessary regulatory insight and oversight for AI-based technologies.

The rise of artificial intelligence (A.I.) based systems is already offering
substantial benefits to the society as a whole. However, these systems may also
enclose potential conflicts and unintended consequences. Notably, people will
tend to adopt an A.I. system if it confers them an advantage, at which point
non-adopters might push for a strong regulation if that advantage for adopters
is at a cost for them. Here we propose an agent-based game-theoretical model
for these conflicts, where agents may decide to resort to A.I. to use and
acquire additional information on the payoffs of a stochastic game, striving to
bring insights from simulation to what has been, hitherto, a mostly
philosophical discussion. We frame our results under the current discussion on
ethical A.I. and the conflict between individual and societal gains: the
societal value alignment problem. We test the arising equilibria in the
adoption of A.I. technology under different norms followed by artificial
agents, their ensuing benefits, and the emergent levels of wealth inequality.
We show that without any regulation, purely selfish A.I. systems will have the
strongest advantage, even when a utilitarian A.I. provides significant benefits
for the individual and the society. Nevertheless, we show that it is possible
to develop A.I. systems following human conscious policies that, when
introduced in society, lead to an equilibrium where the gains for the adopters
are not at a cost for non-adopters, thus increasing the overall wealth of the
population and lowering inequality. However, as shown, a self-organised
adoption of such policies would require external regulation.

Human and AI are increasingly interacting and collaborating to accomplish
various complex tasks in the context of diverse application domains (e.g.,
healthcare, transportation, and creative design). Two dynamic, learning
entities (AI and human) have distinct mental model, expertise, and ability;
such fundamental difference/mismatch offers opportunities for bringing new
perspectives to achieve better results. However, this mismatch can cause
unexpected failure and result in serious consequences. While recent research
has paid much attention to enhancing interpretability or explainability to
allow machine to explain how it makes a decision for supporting humans, this
research argues that there is urging the need for both human and AI should
develop specific, corresponding ability to interact and collaborate with each
other to form a human-AI team to accomplish superior results. This research
introduces a conceptual framework called "Co-Learning," in which people can
learn with/from and grow with AI partners over time. We characterize three key
concepts of co-learning: "mutual understanding," "mutual benefits," and "mutual
growth" for facilitating human-AI collaboration on complex problem solving. We
will present proof-of-concepts to investigate whether and how our approach can
help human-AI team to understand and benefit each other, and ultimately improve
productivity and creativity on creative problem domains. The insights will
contribute to the design of Human-AI collaboration.

This article deals with the IT security of connectionist artificial
intelligence (AI) applications, focusing on threats to integrity, one of the
three IT security goals. Such threats are for instance most relevant in
prominent AI computer vision applications. In order to present a holistic view
on the IT security goal integrity, many additional aspects such as
interpretability, robustness and documentation are taken into account. A
comprehensive list of threats and possible mitigations is presented by
reviewing the state-of-the-art literature. AI-specific vulnerabilities such as
adversarial attacks and poisoning attacks as well as their AI-specific root
causes are discussed in detail. Additionally and in contrast to former reviews,
the whole AI supply chain is analysed with respect to vulnerabilities,
including the planning, data acquisition, training, evaluation and operation
phases. The discussion of mitigations is likewise not restricted to the level
of the AI system itself but rather advocates viewing AI systems in the context
of their supply chains and their embeddings in larger IT infrastructures and
hardware devices. Based on this and the observation that adaptive attackers may
circumvent any single published AI-specific defence to date, the article
concludes that single protective measures are not sufficient but rather
multiple measures on different levels have to be combined to achieve a minimum
level of IT security for AI applications.

By assuming that tau protein can be in seven kinetic states, we developed a
model of tau protein transport in the axon and in the axon initial segment
(AIS). Two separate sets of kinetic constants were determined, one in the axon
and the other in the AIS. This was done by fitting the model predictions in the
axon with experimental results and by fitting the model predictions in the AIS
with the assumed linear increase of the total tau concentration in the AIS. The
calibrated model was used to make predictions about tau transport in the axon
and in the AIS. To the best of our knowledge, this is the first paper that
presents a mathematical model of tau transport in the AIS. Our modeling results
suggest that binding of free tau to MTs creates a negative gradient of free tau
in the AIS. This leads to diffusion-driven tau transport from the soma into the
AIS. The model further suggests that slow axonal transport and diffusion-driven
transport of tau work together in the AIS, moving tau anterogradely. Our
numerical results predict an interplay between these two mechanisms: as the
distance from the soma increases, the diffusion-driven transport decreases,
while motor-driven transport becomes larger. Thus, the machinery in the AIS
works as a pump, moving tau into the axon.

Objective: To introduce a method for tracking results and utilization of
Artificial Intelligence (tru-AI) in radiology. By tracking both large-scale
utilization and AI results data, the tru-AI approach is designed to calculate
surrogates for measuring important disease-related observational quantities
over time, such as the prevalence of intracranial hemorrhage during the
COVID-19 pandemic outbreak. Methods: To quantitatively investigate the clinical
applicability of the tru-AI approach, we analyzed service requests for
automatically identifying intracranial hemorrhage (ICH) on head CT using a
commercial AI solution. This software is typically used for AI-based
prioritization of radiologists' reading lists for reducing turnaround times in
patients with emergent clinical findings, such as ICH or pulmonary embolism.We
analyzed data of N=9,421 emergency-setting non-contrast head CT studies at a
major US healthcare system acquired from November 1, 2019 through June 2, 2020,
and compared two observation periods, namely (i) a pre-pandemic epoch from
November 1, 2019 through February 29, 2020, and (ii) a period during the
COVID-19 pandemic outbreak, April 1-30, 2020. Results: Although daily CT scan
counts were significantly lower during (40.1 +/- 7.9) than before (44.4 +/-
7.6) the COVID-19 outbreak, we found that ICH was more likely to be observed by
AI during than before the COVID-19 outbreak (p<0.05), with approximately one
daily ICH+ case more than statistically expected. Conclusion: Our results
suggest that, by tracking both large-scale utilization and AI results data in
radiology, the tru-AI approach can contribute clinical value as a versatile
exploratory tool, aiming at a better understanding of pandemic-related effects
on healthcare.

Several strands of research have aimed to bridge the gap between artificial
intelligence (AI) and human decision-makers in AI-assisted decision-making,
where humans are the consumers of AI model predictions and the ultimate
decision-makers in high-stakes applications. However, people's perception and
understanding are often distorted by their cognitive biases, such as
confirmation bias, anchoring bias, availability bias, to name a few. In this
work, we use knowledge from the field of cognitive science to account for
cognitive biases in the human-AI collaborative decision-making setting, and
mitigate their negative effects on collaborative performance. To this end, we
mathematically model cognitive biases and provide a general framework through
which researchers and practitioners can understand the interplay between
cognitive biases and human-AI accuracy. We then focus specifically on anchoring
bias, a bias commonly encountered in human-AI collaboration. We implement a
time-based de-anchoring strategy and conduct our first user experiment that
validates its effectiveness in human-AI collaborative decision-making. With
this result, we design a time allocation strategy for a resource-constrained
setting that achieves optimal human-AI collaboration under some assumptions.
We, then, conduct a second user experiment which shows that our time allocation
strategy with explanation can effectively de-anchor the human and improve
collaborative performance when the AI model has low confidence and is
incorrect.

Industrial Artificial Intelligence (Industrial AI) is an emerging concept
which refers to the application of artificial intelligence to industry.
Industrial AI promises more efficient future industrial control systems.
However, manufacturers and solution partners need to understand how to
implement and integrate an AI model into the existing industrial control
system. A well-trained machine learning (ML) model provides many benefits and
opportunities for industrial control optimization; however, an inferior
Industrial AI design and integration limits the capability of ML models. To
better understand how to develop and integrate trained ML models into the
traditional industrial control system, test the deployed AI control system, and
ultimately outperform traditional systems, manufacturers and their AI solution
partners need to address a number of challenges. Six top challenges, which were
real problems we ran into when deploying Industrial AI, are explored in the
paper. The Petuum Optimum system is used as an example to showcase the
challenges in making and testing AI models, and more importantly, how to
address such challenges in an Industrial AI system.

As the range of potential uses for Artificial Intelligence (AI), in
particular machine learning (ML), has increased, so has awareness of the
associated ethical issues. This increased awareness has led to the realisation
that existing legislation and regulation provides insufficient protection to
individuals, groups, society, and the environment from AI harms. In response to
this realisation, there has been a proliferation of principle-based ethics
codes, guidelines and frameworks. However, it has become increasingly clear
that a significant gap exists between the theory of AI ethics principles and
the practical design of AI systems. In previous work, we analysed whether it is
possible to close this gap between the what and the how of AI ethics through
the use of tools and methods designed to help AI developers, engineers, and
designers translate principles into practice. We concluded that this method of
closure is currently ineffective as almost all existing translational tools and
methods are either too flexible (and thus vulnerable to ethics washing) or too
strict (unresponsive to context). This raised the question: if, even with
technical guidance, AI ethics is challenging to embed in the process of
algorithmic design, is the entire pro-ethical design endeavour rendered futile?
And, if no, then how can AI ethics be made useful for AI practitioners? This is
the question we seek to address here by exploring why principles and technical
translational tools are still needed even if they are limited, and how these
limitations can be potentially overcome by providing theoretical grounding of a
concept that has been termed Ethics as a Service.

Multiplayer Online Battle Area (MOBA) games are a recent huge success both in
the video game industry and the international eSports scene. These games
encourage team coordination and cooperation, short and long-term planning,
within a real-time combined action and strategy gameplay.
  Artificial Intelligence and Computational Intelligence in Games research
competitions offer a wide variety of challenges regarding the study and
application of AI techniques to different game genres. These events are widely
accepted by the AI/CI community as a sort of AI benchmarking that strongly
influences many other research areas in the field.
  This paper presents and describes in detail the Dota 2 Bot competition and
the Dota 2 AI framework that supports it. This challenge aims to join both,
MOBAs and AI/CI game competitions, inviting participants to submit AI
controllers for the successful MOBA \textit{Defense of the Ancients 2} (Dota 2)
to play in 1v1 matches, which aims for fostering research on AI techniques for
real-time games. The Dota 2 AI framework makes use of the actual Dota 2 game
modding capabilities to enable to connect external AI controllers to actual
Dota 2 game matches using the original Free-to-Play game.se of the actual Dota
2 game modding capabilities to enable to connect external AI controllers to
actual Dota 2 game matches using the original Free-to-Play game.

We discuss how over the last 30 to 50 years, Artificial Intelligence (AI)
systems that focused only on data have been handicapped, and how knowledge has
been critical in developing smarter, intelligent, and more effective systems.
In fact, the vast progress in AI can be viewed in terms of the three waves of
AI as identified by DARPA. During the first wave, handcrafted knowledge has
been at the center-piece, while during the second wave, the data-driven
approaches supplanted knowledge. Now we see a strong role and resurgence of
knowledge fueling major breakthroughs in the third wave of AI underpinning
future intelligent systems as they attempt human-like decision making, and seek
to become trusted assistants and companions for humans. We find a wider
availability of knowledge created from diverse sources, using manual to
automated means both by repurposing as well as by extraction. Using knowledge
with statistical learning is becoming increasingly indispensable to help make
AI systems more transparent and auditable. We will draw a parallel with the
role of knowledge and experience in human intelligence based on cognitive
science, and discuss emerging neuro-symbolic or hybrid AI systems in which
knowledge is the critical enabler for combining capabilities of the
data-intensive statistical AI systems with those of symbolic AI systems,
resulting in more capable AI systems that support more human-like intelligence.

There are many unknowns regarding the characteristics and dynamics of
human-AI teams, including a lack of understanding of how certain human-human
teaming concepts may or may not apply to human-AI teams and how this
composition affects team performance. This paper outlines an experimental
research study that investigates essential aspects of human-AI teaming such as
team performance, team situation awareness, and perceived team cognition in
various mixed composition teams (human-only, human-human-AI, human-AI-AI, and
AI-only) through a simulated emergency response management scenario. Results
indicate dichotomous outcomes regarding perceived team cognition and
performance metrics, as perceived team cognition was not predictive of
performance. Performance metrics like team situational awareness and team score
showed that teams composed of all human participants performed at a lower level
than mixed human-AI teams, with the AI-only teams attaining the highest
performance. Perceived team cognition was highest in human-only teams, with
mixed composition teams reporting perceived team cognition 58% below the
all-human teams. These results inform future mixed teams of the potential
performance gains in utilizing mixed teams' over human-only teams in certain
applications, while also highlighting mixed teams' adverse effects on perceived
team cognition.

AI systems have seen significant adoption in various domains. At the same
time, further adoption in some domains is hindered by inability to fully trust
an AI system that it will not harm a human. Besides the concerns for fairness,
privacy, transparency, and explainability are key to developing trusts in AI
systems. As stated in describing trustworthy AI "Trust comes through
understanding. How AI-led decisions are made and what determining factors were
included are crucial to understand." The subarea of explaining AI systems has
come to be known as XAI. Multiple aspects of an AI system can be explained;
these include biases that the data might have, lack of data points in a
particular region of the example space, fairness of gathering the data, feature
importances, etc. However, besides these, it is critical to have human-centered
explanations that are directly related to decision-making similar to how a
domain expert makes decisions based on "domain knowledge," that also include
well-established, peer-validated explicit guidelines. To understand and
validate an AI system's outcomes (such as classification, recommendations,
predictions), that lead to developing trust in the AI system, it is necessary
to involve explicit domain knowledge that humans understand and use.

Despite the surprising power of many modern AI systems that often learn their
own representations, there is significant discontent about their inscrutability
and the attendant problems in their ability to interact with humans. While
alternatives such as neuro-symbolic approaches have been proposed, there is a
lack of consensus on what they are about. There are often two independent
motivations (i) symbols as a lingua franca for human-AI interaction and (ii)
symbols as system-produced abstractions used by the AI system in its internal
reasoning. The jury is still out on whether AI systems will need to use symbols
in their internal reasoning to achieve general intelligence capabilities.
Whatever the answer there is, the need for (human-understandable) symbols in
human-AI interaction seems quite compelling. Symbols, like emotions, may well
not be sine qua non for intelligence per se, but they will be crucial for AI
systems to interact with us humans -- as we can neither turn off our emotions
nor get by without our symbols. In particular, in many human-designed domains,
humans would be interested in providing explicit (symbolic) knowledge and
advice -- and expect machine explanations in kind. This alone requires AI
systems to to maintain a symbolic interface for interaction with humans. In
this blue sky paper, we argue this point of view, and discuss research
directions that need to be pursued to allow for this type of human-AI
interaction.

Background: The construction, evolution and usage of complex artificial
intelligence (AI) models demand expensive computational resources. While
currently available high-performance computing environments support well this
complexity, the deployment of AI models in mobile devices, which is an
increasing trend, is challenging. Mobile applications consist of environments
with low computational resources and hence imply limitations in the design
decisions during the AI-enabled software engineering lifecycle that balance the
trade-off between the accuracy and the complexity of the mobile applications.
  Objective: Our objective is to systematically assess the trade-off between
accuracy and complexity when deploying complex AI models (e.g. neural networks)
to mobile devices, which have an implicit resource limitation. We aim to cover
(i) the impact of the design decisions on the achievement of high-accuracy and
low resource-consumption implementations; and (ii) the validation of profiling
tools for systematically promoting greener AI.
  Method: This confirmatory registered report consists of a plan to conduct an
empirical study to quantify the implications of the design decisions on
AI-enabled applications performance and to report experiences of the end-to-end
AI-enabled software engineering lifecycle. Concretely, we will implement both
image-based and language-based neural networks in mobile applications to solve
multiple image classification and text classification problems on different
benchmark datasets. Overall, we plan to model the accuracy and complexity of
AI-enabled applications in operation with respect to their design decisions and
will provide tools for allowing practitioners to gain consciousness of the
quantitative relationship between the design decisions and the green
characteristics of study.

With the rapid development of artificial intelligence (AI), it is foreseeable
that the accuracy and efficiency of dynamic analysis for future power system
will be greatly improved by the integration of dynamic simulators and AI. To
explore the interaction mechanism of power system dynamic simulations and AI, a
general design of an AI-oriented power system dynamic simulator is proposed,
which consists of a high-performance simulator with neural network
supportability and flexible external and internal application programming
interfaces (APIs). With the support of APIs, simulation-assisted AI and
AI-assisted simulation form a comprehensive interaction mechanism between power
system dynamic simulations and AI. A prototype of this design is implemented
and made public based on a highly efficient electromechanical simulator. Tests
of this prototype are carried out under four scenarios including sample
generation, AI-based stability prediction, data-driven dynamic component
modeling, and AI-aided stability control, which prove the validity,
flexibility, and efficiency of the design and implementation of the AI-oriented
power system dynamic simulator.

Sensory and emotional experiences such as pain and empathy are essential for
mental and physical health. Cognitive neuroscience has been working on
revealing mechanisms underlying pain and empathy. Furthermore, as trending
research areas, computational pain recognition and empathic artificial
intelligence (AI) show progress and promise for healthcare or human-computer
interaction. Although AI research has recently made it increasingly possible to
create artificial systems with affective processing, most cognitive
neuroscience and AI research do not jointly address the issues of empathy in AI
and cognitive neuroscience. The main aim of this paper is to introduce key
advances, cognitive challenges and technical barriers in computational pain
recognition and the implementation of artificial empathy. Our discussion covers
the following topics: How can AI recognize pain from unimodal and multimodal
information? Is it crucial for AI to be empathic? What are the benefits and
challenges of empathic AI? Despite some consensus on the importance of AI,
including empathic recognition and responses, we also highlight future
challenges for artificial empathy and possible paths from interdisciplinary
perspectives. Furthermore, we discuss challenges for responsible evaluation of
cognitive methods and computational techniques and show approaches to future
work to contribute to affective assistants capable of empathy.

Cyber-physical systems (CPS) have been broadly deployed in safety-critical
domains, such as automotive systems, avionics, medical devices, etc. In recent
years, Artificial Intelligence (AI) has been increasingly adopted to control
CPS. Despite the popularity of AI-enabled CPS, few benchmarks are publicly
available. There is also a lack of deep understanding on the performance and
reliability of AI-enabled CPS across different industrial domains. To bridge
this gap, we initiate to create a public benchmark of industry-level CPS in
seven domains and build AI controllers for them via state-of-the-art deep
reinforcement learning (DRL) methods. Based on that, we further perform a
systematic evaluation of these AI-enabled systems with their traditional
counterparts to identify the current challenges and explore future
opportunities. Our key findings include (1) AI controllers do not always
outperform traditional controllers, (2) existing CPS testing techniques
(falsification, specifically) fall short of analyzing AI-enabled CPS, and (3)
building a hybrid system that strategically combines and switches between AI
controllers and traditional controllers can achieve better performance across
different domains. Our results highlight the need for new testing techniques
for AI-enabled CPS and the need for more investigations into hybrid CPS systems
to achieve optimal performance and reliability.

Artificial Intelligence (AI) presents opportunities to develop tools and
techniques for addressing some of the major global challenges and deliver
solutions with significant social and economic impacts. The application of AI
has far-reaching implications for the 17 Sustainable Development Goals (SDGs)
in general, and sustainable urban development in particular. However, existing
attempts to understand and use the opportunities offered by AI for SDG 11 have
been explored sparsely, and the shortage of empirical evidence about the
practical application of AI remains. In this chapter, we analyze the
contribution of AI to support the progress of SDG 11 (Sustainable Cities and
Communities). We address the knowledge gap by empirically analyzing the AI
systems (N = 29) from the AIxSDG database and the Community Research and
Development Information Service (CORDIS) database. Our analysis revealed that
AI systems have indeed contributed to advancing sustainable cities in several
ways (e.g., waste management, air quality monitoring, disaster response
management, transportation management), but many projects are still working for
citizens and not with them. This snapshot of AI's impact on SDG11 is inherently
partial, yet useful to advance our understanding as we move towards more mature
systems and research on the impact of AI systems for social good.

The problem of human trust in artificial intelligence is one of the most
fundamental problems in applied machine learning. Our processes for evaluating
AI trustworthiness have substantial ramifications for ML's impact on science,
health, and humanity, yet confusion surrounds foundational concepts. What does
it mean to trust an AI, and how do humans assess AI trustworthiness? What are
the mechanisms for building trustworthy AI? And what is the role of
interpretable ML in trust? Here, we draw from statistical learning theory and
sociological lenses on human-automation trust to motivate an AI-as-tool
framework, which distinguishes human-AI trust from human-AI-human trust.
Evaluating an AI's contractual trustworthiness involves predicting future model
behavior using behavior certificates (BCs) that aggregate behavioral evidence
from diverse sources including empirical out-of-distribution and out-of-task
evaluation and theoretical proofs linking model architecture to behavior. We
clarify the role of interpretability in trust with a ladder of model access.
Interpretability (level 3) is not necessary or even sufficient for trust, while
the ability to run a black-box model at-will (level 2) is necessary and
sufficient. While interpretability can offer benefits for trust, it can also
incur costs. We clarify ways interpretability can contribute to trust, while
questioning the perceived centrality of interpretability to trust in popular
discourse. How can we empower people with tools to evaluate trust? Instead of
trying to understand how a model works, we argue for understanding how a model
behaves. Instead of opening up black boxes, we should create more behavior
certificates that are more correct, relevant, and understandable. We discuss
how to build trusted and trustworthy AI responsibly.

Scholars and industry practitioners have debated how to best develop
interventions for ethical artificial intelligence (AI). Such interventions
recommend that companies building and using AI tools change their technical
practices, but fail to wrangle with critical questions about the organizational
and institutional context in which AI is developed. In this paper, we
contribute descriptive research around the life of "AI" as a discursive concept
and organizational practice in an understudied sphere--emerging AI
startups--and with a focus on extra-organizational pressures faced by
entrepreneurs. Leveraging a theoretical lens for how organizations change, we
conducted semi-structured interviews with 23 entrepreneurs working at
early-stage AI startups. We find that actors within startups both conform to
and resist institutional pressures. Our analysis identifies a central tension
for AI entrepreneurs: they often valued scientific integrity and methodological
rigor; however, influential external stakeholders either lacked the technical
knowledge to appreciate entrepreneurs' emphasis on rigor or were more focused
on business priorities. As a result, entrepreneurs adopted hyped marketing
messages about AI that diverged from their scientific values, but attempted to
preserve their legitimacy internally. Institutional pressures and
organizational constraints also influenced entrepreneurs' modeling practices
and their response to actual or impending regulation. We conclude with a
discussion for how such pressures could be used as leverage for effective
interventions towards building ethical AI.

As AI-enhanced technologies become common in a variety of domains, there is
an increasing need to define and examine the trust that users have in such
technologies. Given the progress in the development of AI, a correspondingly
sophisticated understanding of trust in the technology is required. This paper
addresses this need by explaining the role of trust on the intention to use AI
technologies. Study 1 examined the role of trust in the use of AI voice
assistants based on survey responses from college students. A path analysis
confirmed that trust had a significant effect on the intention to use AI, which
operated through perceived usefulness and participants' attitude toward voice
assistants. In study 2, using data from a representative sample of the U.S.
population, different dimensions of trust were examined using exploratory
factor analysis, which yielded two dimensions: human-like trust and
functionality trust. The results of the path analyses from Study 1 were
replicated in Study 2, confirming the indirect effect of trust and the effects
of perceived usefulness, ease of use, and attitude on intention to use.
Further, both dimensions of trust shared a similar pattern of effects within
the model, with functionality-related trust exhibiting a greater total impact
on usage intention than human-like trust. Overall, the role of trust in the
acceptance of AI technologies was significant across both studies. This
research contributes to the advancement and application of the TAM in
AI-related applications and offers a multidimensional measure of trust that can
be utilized in the future study of trustworthy AI.

Rapid advances in artificial intelligence (AI) technology have led to
significant accuracy improvements in a myriad of application domains at the
cost of larger and more compute-intensive models. Training such models on
massive amounts of data typically requires scaling to many compute nodes and
relies heavily on collective communication algorithms, such as all-reduce, to
exchange the weight gradients between different nodes. The overhead of these
collective communication operations in a distributed AI training system can
bottleneck its performance, with more pronounced effects as the number of nodes
increases. In this paper, we first characterize the all-reduce operation
overhead by profiling distributed AI training. Then, we propose a new smart
network interface card (NIC) for distributed AI training systems using
field-programmable gate arrays (FPGAs) to accelerate all-reduce operations and
optimize network bandwidth utilization via data compression. The AI smart NIC
frees up the system's compute resources to perform the more compute-intensive
tensor operations and increases the overall node-to-node communication
efficiency. We perform real measurements on a prototype distributed AI training
system comprised of 6 compute nodes to evaluate the performance gains of our
proposed FPGA-based AI smart NIC compared to a baseline system with regular
NICs. We also use these measurements to validate an analytical model that we
formulate to predict performance when scaling to larger systems. Our proposed
FPGA-based AI smart NIC enhances overall training performance by 1.6x at 6
nodes, with an estimated 2.5x performance improvement at 32 nodes, compared to
the baseline system using conventional NICs.

Current literature and public discourse on "trust in AI" are often focused on
the principles underlying trustworthy AI, with insufficient attention paid to
how people develop trust. Given that AI systems differ in their level of
trustworthiness, two open questions come to the fore: how should AI
trustworthiness be responsibly communicated to ensure appropriate and equitable
trust judgments by different users, and how can we protect users from deceptive
attempts to earn their trust? We draw from communication theories and
literature on trust in technologies to develop a conceptual model called MATCH,
which describes how trustworthiness is communicated in AI systems through
trustworthiness cues and how those cues are processed by people to make trust
judgments. Besides AI-generated content, we highlight transparency and
interaction as AI systems' affordances that present a wide range of
trustworthiness cues to users. By bringing to light the variety of users'
cognitive processes to make trust judgments and their potential limitations, we
urge technology creators to make conscious decisions in choosing reliable
trustworthiness cues for target users and, as an industry, to regulate this
space and prevent malicious use. Towards these goals, we define the concepts of
warranted trustworthiness cues and expensive trustworthiness cues, and propose
a checklist of requirements to help technology creators identify appropriate
cues to use. We present a hypothetical use case to illustrate how practitioners
can use MATCH to design AI systems responsibly, and discuss future directions
for research and industry efforts aimed at promoting responsible trust in AI.

The current standard to compare the performance of AI algorithms is mainly
based on one criterion: the model's accuracy. In this context, algorithms with
a higher accuracy (or similar measures) are considered as better. To achieve
new state-of-the-art results, algorithmic development is accompanied by an
exponentially increasing amount of compute. While this has enabled AI research
to achieve remarkable results, AI progress comes at a cost: it is
unsustainable. In this paper, we present a promising tool for sustainable and
thus Green AI: tensor networks (TNs). Being an established tool from
multilinear algebra, TNs have the capability to improve efficiency without
compromising accuracy. Since they can reduce compute significantly, we would
like to highlight their potential for Green AI. We elaborate in both a kernel
machine and deep learning setting how efficiency gains can be achieved with
TNs. Furthermore, we argue that better algorithms should be evaluated in terms
of both accuracy and efficiency. To that end, we discuss different efficiency
criteria and analyze efficiency in an exemplifying experimental setting for
kernel ridge regression. With this paper, we want to raise awareness about
Green AI and showcase its positive impact on sustainability and AI research.
Our key contribution is to demonstrate that TNs enable efficient algorithms and
therefore contribute towards Green AI. In this sense, TNs pave the way for
better algorithms in AI.

This report is a methodological reflection on
Z-Inspection$^{\small{\circledR}}$. Z-Inspection$^{\small{\circledR}}$ is a
holistic process used to evaluate the trustworthiness of AI-based technologies
at different stages of the AI lifecycle. It focuses, in particular, on the
identification and discussion of ethical issues and tensions through the
elaboration of socio-technical scenarios. It uses the general European Union's
High-Level Expert Group's (EU HLEG) guidelines for trustworthy AI. This report
illustrates for both AI researchers and AI practitioners how the EU HLEG
guidelines for trustworthy AI can be applied in practice. We share the lessons
learned from conducting a series of independent assessments to evaluate the
trustworthiness of AI systems in healthcare. We also share key recommendations
and practical suggestions on how to ensure a rigorous trustworthy AI assessment
throughout the life-cycle of an AI system.

AI systems have been widely adopted across various domains in the real world.
However, in high-value, sensitive, or safety-critical applications such as
self-management for personalized health or food recommendation with a specific
purpose (e.g., allergy-aware recipe recommendations), their adoption is
unlikely. Firstly, the AI system needs to follow guidelines or well-defined
processes set by experts; the data alone will not be adequate. For example, to
diagnose the severity of depression, mental healthcare providers use Patient
Health Questionnaire (PHQ-9). So if an AI system were to be used for diagnosis,
the medical guideline implied by the PHQ-9 needs to be used. Likewise, a
nutritionist's knowledge and steps would need to be used for an AI system that
guides a diabetic patient in developing a food plan. Second, the BlackBox
nature typical of many current AI systems will not work; the user of an AI
system will need to be able to give user-understandable explanations,
explanations constructed using concepts that humans can understand and are
familiar with. This is the key to eliciting confidence and trust in the AI
system. For such applications, in addition to data and domain knowledge, the AI
systems need to have access to and use the Process Knowledge, an ordered set of
steps that the AI system needs to use or adhere to.

Introduction: For supervised deep learning (DL) tasks, researchers need a
large annotated dataset. In medical data science, one of the major limitations
to develop DL models is the lack of annotated examples in large quantity. This
is most often due to the time and expertise required to annotate. We introduce
Lirot. ai, a novel platform for facilitating and crowd-sourcing image
segmentations. Methods: Lirot. ai is composed of three components; an iPadOS
client application named Lirot. ai-app, a backend server named Lirot. ai-server
and a python API name Lirot. ai-API. Lirot. ai-app was developed in Swift 5.6
and Lirot. ai-server is a firebase backend. Lirot. ai-API allows the management
of the database. Lirot. ai-app can be installed on as many iPadOS devices as
needed so that annotators may be able to perform their segmentation
simultaneously and remotely. We incorporate Apple Pencil compatibility, making
the segmentation faster, more accurate, and more intuitive for the expert than
any other computer-based alternative. Results: We demonstrate the usage of
Lirot. ai for the creation of a retinal fundus dataset with reference
vasculature segmentations. Discussion and future work: We will use active
learning strategies to continue enlarging our retinal fundus dataset by
including a more efficient process to select the images to be annotated and
distribute them to annotators.

There is still a significant gap between expectations and the successful
adoption of AI to innovate and improve businesses. Due to the emergence of deep
learning, AI adoption is more complex as it often incorporates big data and the
internet of things, affecting data privacy. Existing frameworks have identified
the need to focus on human-centered design, combining technical and
business/organizational perspectives. However, trust remains a critical issue
that needs to be designed from the beginning. The proposed framework expands
from the human-centered design approach, emphasizing and maintaining the trust
that underpins the process. This paper proposes a theoretical framework for
responsible artificial intelligence (AI) implementation. The proposed framework
emphasizes a synergistic business technology approach for the agile co-creation
process. The aim is to streamline the adoption process of AI to innovate and
improve business by involving all stakeholders throughout the project so that
the AI technology is designed, developed, and deployed in conjunction with
people and not in isolation. The framework presents a fresh viewpoint on
responsible AI implementation based on analytical literature review, conceptual
framework design, and practitioners' mediating expertise. The framework
emphasizes establishing and maintaining trust throughout the human-centered
design and agile development of AI. This human-centered approach is aligned
with and enabled by the privacy by design principle. The creators of the
technology and the end-users are working together to tailor the AI solution
specifically for the business requirements and human characteristics. An
illustrative case study on adopting AI for assisting planning in a hospital
will demonstrate that the proposed framework applies to real-life applications.

Metaverse is expected to emerge as a new paradigm for the next-generation
Internet, providing fully immersive and personalised experiences to socialize,
work, and play in self-sustaining and hyper-spatio-temporal virtual world(s).
The advancements in different technologies like augmented reality, virtual
reality, extended reality (XR), artificial intelligence (AI), and 5G/6G
communication will be the key enablers behind the realization of AI-XR
metaverse applications. While AI itself has many potential applications in the
aforementioned technologies (e.g., avatar generation, network optimization,
etc.), ensuring the security of AI in critical applications like AI-XR
metaverse applications is profoundly crucial to avoid undesirable actions that
could undermine users' privacy and safety, consequently putting their lives in
danger. To this end, we attempt to analyze the security, privacy, and
trustworthiness aspects associated with the use of various AI techniques in
AI-XR metaverse applications. Specifically, we discuss numerous such challenges
and present a taxonomy of potential solutions that could be leveraged to
develop secure, private, robust, and trustworthy AI-XR applications. To
highlight the real implications of AI-associated adversarial threats, we
designed a metaverse-specific case study and analyzed it through the
adversarial lens. Finally, we elaborate upon various open issues that require
further research interest from the community.

Traditional electrical power grids have long suffered from operational
unreliability, instability, inflexibility, and inefficiency. Smart grids (or
smart energy systems) continue to transform the energy sector with emerging
technologies, renewable energy sources, and other trends. Artificial
intelligence (AI) is being applied to smart energy systems to process massive
and complex data in this sector and make smart and timely decisions. However,
the lack of explainability and governability of AI is a major concern for
stakeholders hindering a fast uptake of AI in the energy sector. This paper
provides a review of AI explainability and governance in smart energy systems.
We collect 3,568 relevant papers from the Scopus database, automatically
discover 15 parameters or themes for AI governance in energy and elaborate the
research landscape by reviewing over 150 papers and providing temporal
progressions of the research. The methodology for discovering parameters or
themes is based on "deep journalism", our data-driven deep learning-based big
data analytics approach to automatically discover and analyse cross-sectional
multi-perspective information to enable better decision-making and develop
better instruments for governance. The findings show that research on AI
explainability in energy systems is segmented and narrowly focussed on a few AI
traits and energy system problems. This paper deepens our knowledge of AI
governance in energy and is expected to help governments, industry, academics,
energy prosumers, and other stakeholders to understand the landscape of AI in
the energy sector, leading to better design, operations, utilisation, and risk
management of energy systems.

This chapter explores moral responsibility for civilian harms by
human-artificial intelligence (AI) teams. Although militaries may have some bad
apples responsible for war crimes and some mad apples unable to be responsible
for their actions during a conflict, increasingly militaries may 'cook' their
good apples by putting them in untenable decision-making environments through
the processes of replacing human decision-making with AI determinations in war
making. Responsibility for civilian harm in human-AI military teams may be
contested, risking operators becoming detached, being extreme moral witnesses,
becoming moral crumple zones or suffering moral injury from being part of
larger human-AI systems authorised by the state. Acknowledging military ethics,
human factors and AI work to date as well as critical case studies, this
chapter offers new mechanisms to map out conditions for moral responsibility in
human-AI teams. These include: 1) new decision responsibility prompts for
critical decision method in a cognitive task analysis, and 2) applying an AI
workplace health and safety framework for identifying cognitive and
psychological risks relevant to attributions of moral responsibility in
targeting decisions. Mechanisms such as these enable militaries to design
human-centred AI systems for responsible deployment.

The need for AI systems to provide explanations for their behaviour is now
widely recognised as key to their adoption. In this paper, we examine the
problem of trustworthy AI and explore what delivering this means in practice,
with a focus on healthcare applications. Work in this area typically treats
trustworthy AI as a problem of Human-Computer Interaction involving the
individual user and an AI system. However, we argue here that this overlooks
the important part played by organisational accountability in how people reason
about and trust AI in socio-technical settings. To illustrate the importance of
organisational accountability, we present findings from ethnographic studies of
breast cancer screening and cancer treatment planning in multidisciplinary team
meetings to show how participants made themselves accountable both to each
other and to the organisations of which they are members. We use these findings
to enrich existing understandings of the requirements for trustworthy AI and to
outline some candidate solutions to the problems of making AI accountable both
to individual users and organisationally. We conclude by outlining the
implications of this for future work on the development of trustworthy AI,
including ways in which our proposed solutions may be re-used in different
application settings.

We are witnessing the emergence of an AI economy and society where AI
technologies are increasingly impacting health care, business, transportation
and many aspects of everyday life. Many successes have been reported where AI
systems even surpassed the accuracy of human experts. However, AI systems may
produce errors, can exhibit bias, may be sensitive to noise in the data, and
often lack technical and judicial transparency resulting in reduction in trust
and challenges in their adoption. These recent shortcomings and concerns have
been documented in scientific but also in general press such as accidents with
self driving cars, biases in healthcare, hiring and face recognition systems
for people of color, seemingly correct medical decisions later found to be made
due to wrong reasons etc. This resulted in emergence of many government and
regulatory initiatives requiring trustworthy and ethical AI to provide accuracy
and robustness, some form of explainability, human control and oversight,
elimination of bias, judicial transparency and safety. The challenges in
delivery of trustworthy AI systems motivated intense research on explainable AI
systems (XAI). Aim of XAI is to provide human understandable information of how
AI systems make their decisions. In this paper we first briefly summarize
current XAI work and then challenge the recent arguments of accuracy vs.
explainability for being mutually exclusive and being focused only on deep
learning. We then present our recommendations for the use of XAI in full
lifecycle of high stakes trustworthy AI systems delivery, e.g. development,
validation and certification, and trustworthy production and maintenance.

The dazzling promises of AI systems to augment humans in various tasks hinge
on whether humans can appropriately rely on them. Recent research has shown
that appropriate reliance is the key to achieving complementary team
performance in AI-assisted decision making. This paper addresses an
under-explored problem of whether the Dunning-Kruger Effect (DKE) among people
can hinder their appropriate reliance on AI systems. DKE is a metacognitive
bias due to which less-competent individuals overestimate their own skill and
performance. Through an empirical study (N = 249), we explored the impact of
DKE on human reliance on an AI system, and whether such effects can be
mitigated using a tutorial intervention that reveals the fallibility of AI
advice, and exploiting logic units-based explanations to improve user
understanding of AI advice. We found that participants who overestimate their
performance tend to exhibit under-reliance on AI systems, which hinders optimal
team performance. Logic units-based explanations did not help users in either
improving the calibration of their competence or facilitating appropriate
reliance. While the tutorial intervention was highly effective in helping users
calibrate their self-assessment and facilitating appropriate reliance among
participants with overestimated self-assessment, we found that it can
potentially hurt the appropriate reliance of participants with underestimated
self-assessment. Our work has broad implications on the design of methods to
tackle user cognitive biases while facilitating appropriate reliance on AI
systems. Our findings advance the current understanding of the role of
self-assessment in shaping trust and reliance in human-AI decision making. This
lays out promising future directions for relevant HCI research in this
community.

AI services are known to have unstable behavior when subjected to changes in
data, models or users. Such behaviors, whether triggered by omission or
commission, lead to trust issues when AI works with humans. The current
approach of assessing AI services in a black box setting, where the consumer
does not have access to the AI's source code or training data, is limited. The
consumer has to rely on the AI developer's documentation and trust that the
system has been built as stated. Further, if the AI consumer reuses the service
to build other services which they sell to their customers, the consumer is at
the risk of the service providers (both data and model providers). Our
approach, in this context, is inspired by the success of nutritional labeling
in food industry to promote health and seeks to assess and rate AI services for
trust from the perspective of an independent stakeholder. The ratings become a
means to communicate the behavior of AI systems so that the consumer is
informed about the risks and can make an informed decision. In this paper, we
will first describe recent progress in developing rating methods for text-based
machine translator AI services that have been found promising with user
studies. Then, we will outline challenges and vision for a principled,
multi-modal, causality-based rating methodologies and its implication for
decision-support in real-world scenarios like health and food recommendation.

In AI-assisted decision-making, a central promise of putting a human in the
loop is that they should be able to complement the AI system by adhering to its
correct and overriding its mistaken recommendations. In practice, however, we
often see that humans tend to over- or under-rely on AI recommendations,
meaning that they either adhere to wrong or override correct recommendations.
Such reliance behavior is detrimental to decision-making accuracy. In this
work, we articulate and analyze the interdependence between reliance behavior
and accuracy in AI-assisted decision-making, which has been largely neglected
in prior work. We also propose a visual framework to make this interdependence
more tangible. This framework helps us interpret and compare empirical
findings, as well as obtain a nuanced understanding of the effects of
interventions (e.g., explanations) in AI-assisted decision-making. Finally, we
infer several interesting properties from the framework: (i) when humans
under-rely on AI recommendations, there may be no possibility for them to
complement the AI in terms of decision-making accuracy; (ii) when humans cannot
discern correct and wrong AI recommendations, no such improvement can be
expected either; (iii) interventions may lead to an increase in decision-making
accuracy that is solely driven by an increase in humans' adherence to AI
recommendations, without any ability to discern correct and wrong. Our work
emphasizes the importance of measuring and reporting both effects on accuracy
and reliance behavior when empirically assessing interventions.

Successful deployment of artificial intelligence (AI) in various settings has
led to numerous positive outcomes for individuals and society. However, AI
systems have also been shown to harm parts of the population due to biased
predictions. AI fairness focuses on mitigating such biases to ensure AI
decision making is not discriminatory towards certain groups. We take a closer
look at AI fairness and analyze how lack of AI fairness can lead to deepening
of biases over time and act as a social stressor. More specifically, we discuss
how biased models can lead to more negative real-world outcomes for certain
groups, which may then become more prevalent by deploying new AI models trained
on increasingly biased data, resulting in a feedback loop. If the issues
persist, they could be reinforced by interactions with other risks and have
severe implications on society in the form of social unrest. We examine current
strategies for improving AI fairness, assess their limitations in terms of
real-world deployment, and explore potential paths forward to ensure we reap
AI's benefits without causing society's collapse.

In recent years, Artificial Intelligence has become more and more relevant in
our society. Creating AI systems is almost always the prerogative of IT and AI
experts. However, users may need to create intelligent solutions tailored to
their specific needs. In this way, AI systems can be enhanced if new approaches
are devised to allow non-technical users to be directly involved in the
definition and personalization of AI technologies. End-User Development (EUD)
can provide a solution to these problems, allowing people to create, customize,
or adapt AI-based systems to their own needs. This paper presents a systematic
literature review that aims to shed the light on the current landscape of EUD
for AI systems, i.e., how users, even without skills in AI and/or programming,
can customize the AI behavior to their needs. This study also discusses the
current challenges of EUD for AI, the potential benefits, and the future
implications of integrating EUD into the overall AI development process.

The emergence of generative AI has sparked substantial discussions, with the
potential to have profound impacts on society in all aspects. As emerging
technologies continue to advance, it is imperative to facilitate their proper
integration into society, managing expectations and fear. This paper
investigates users' perceptions of generative AI using 3M posts on Twitter from
January 2019 to March 2023, especially focusing on their occupation and usage.
We find that people across various occupations, not just IT-related ones, show
a strong interest in generative AI. The sentiment toward generative AI is
generally positive, and remarkably, their sentiments are positively correlated
with their exposure to AI. Among occupations, illustrators show exceptionally
negative sentiment mainly due to concerns about the unethical usage of artworks
in constructing AI. People use ChatGPT in diverse ways, and notably the casual
usage in which they "play with" ChatGPT tends to associate with positive
sentiments. After the release of ChatGPT, people's interest in AI in general
has increased dramatically; however, the topic with the most significant
increase and positive sentiment is related to crypto, indicating the
hype-worthy characteristics of generative AI. These findings would offer
valuable lessons for policymaking on the emergence of new technology and also
empirical insights for the considerations of future human-AI symbiosis.

This study explores the robustness of university assessments against the use
of Open AI's Generative Pre-Trained Transformer 4 (GPT-4) generated content and
evaluates the ability of academic staff to detect its use when supported by the
Turnitin Artificial Intelligence (AI) detection tool. The research involved
twenty-two GPT-4 generated submissions being created and included in the
assessment process to be marked by fifteen different faculty members. The study
reveals that although the detection tool identified 91% of the experimental
submissions as containing some AI-generated content, the total detected content
was only 54.8%. This suggests that the use of adversarial techniques regarding
prompt engineering is an effective method in evading AI detection tools and
highlights that improvements to AI detection software are needed. Using the
Turnitin AI detect tool, faculty reported 54.5% of the experimental submissions
to the academic misconduct process, suggesting the need for increased awareness
and training into these tools. Genuine submissions received a mean score of
54.4, whereas AI-generated content scored 52.3, indicating the comparable
performance of GPT-4 in real-life situations. Recommendations include adjusting
assessment strategies to make them more resistant to the use of AI tools, using
AI-inclusive assessment where possible, and providing comprehensive training
programs for faculty and students. This research contributes to understanding
the relationship between AI-generated content and academic assessment, urging
further investigation to preserve academic integrity.

Auditing plays a pivotal role in the development of trustworthy AI. However,
current research primarily focuses on creating auditable AI documentation,
which is intended for regulators and experts rather than end-users affected by
AI decisions. How to communicate to members of the public that an AI has been
audited and considered trustworthy remains an open challenge. This study
empirically investigated certification labels as a promising solution. Through
interviews (N = 12) and a census-representative survey (N = 302), we
investigated end-users' attitudes toward certification labels and their
effectiveness in communicating trustworthiness in low- and high-stakes AI
scenarios. Based on the survey results, we demonstrate that labels can
significantly increase end-users' trust and willingness to use AI in both low-
and high-stakes scenarios. However, end-users' preferences for certification
labels and their effect on trust and willingness to use AI were more pronounced
in high-stake scenarios. Qualitative content analysis of the interviews
revealed opportunities and limitations of certification labels, as well as
facilitators and inhibitors for the effective use of labels in the context of
AI. For example, while certification labels can mitigate data-related concerns
expressed by end-users (e.g., privacy and data protection), other concerns
(e.g., model performance) are more challenging to address. Our study provides
valuable insights and recommendations for designing and implementing
certification labels as a promising constituent within the trustworthy AI
ecosystem.

Experiential AI is an emerging research field that addresses the challenge of
making AI tangible and explicit, both to fuel cultural experiences for
audiences, and to make AI systems more accessible to human understanding. The
central theme is how artists, scientists and other interdisciplinary actors can
come together to understand and communicate the functionality of AI, ML and
intelligent robots, their limitations, and consequences, through informative
and compelling experiences. It provides an approach and methodology for the
arts and tangible experiences to mediate between impenetrable computer code and
human understanding, making not just AI systems but also their values and
implications more transparent, and therefore accountable. In this paper, we
report on an empirical case study of an experiential AI system designed for
creative data exploration of a user-defined dimension, to enable creators to
gain more creative control over the AI process. We discuss how experiential AI
can increase legibility and agency for artists, and how the arts can provide
creative strategies and methods which can add to the toolbox for human-centred
XAI.

Rapid advancements in artificial intelligence (AI) have sparked growing
concerns among experts, policymakers, and world leaders regarding the potential
for increasingly advanced AI systems to pose catastrophic risks. Although
numerous risks have been detailed separately, there is a pressing need for a
systematic discussion and illustration of the potential dangers to better
inform efforts to mitigate them. This paper provides an overview of the main
sources of catastrophic AI risks, which we organize into four categories:
malicious use, in which individuals or groups intentionally use AIs to cause
harm; AI race, in which competitive environments compel actors to deploy unsafe
AIs or cede control to AIs; organizational risks, highlighting how human
factors and complex systems can increase the chances of catastrophic accidents;
and rogue AIs, describing the inherent difficulty in controlling agents far
more intelligent than humans. For each category of risk, we describe specific
hazards, present illustrative stories, envision ideal scenarios, and propose
practical suggestions for mitigating these dangers. Our goal is to foster a
comprehensive understanding of these risks and inspire collective and proactive
efforts to ensure that AIs are developed and deployed in a safe manner.
Ultimately, we hope this will allow us to realize the benefits of this powerful
technology while minimizing the potential for catastrophic outcomes.

To improve the development of responsible AI systems, developers are
increasingly utilizing tools such as checklists or guideline cards to ensure
fairness, transparency, and sustainability. However, these tools face two main
challenges. First, they are static and are not meant to keep pace with the
latest responsible AI literature and international standards. Second, they tend
to prioritize individual usage over fostering collaboration among AI
practitioners. To overcome these limitations, we propose a method that enables
easy updates of responsible AI guidelines by incorporating research papers and
ISO standards, ensuring that the content remains relevant and up to date, while
emphasizing actionable guidelines that can be implemented by a wide range of AI
practitioners. We validated our method in a case study at a large tech company
by designing and deploying a tool that recommends interactive and actionable
guidelines, which were generated by a team of engineers, standardization
experts, and a lawyer using our method. Through the study involving AI
developers and engineers, we assessed the usability and effectiveness of the
tool, showing that the guidelines were considered practical and actionable. The
guidelines encouraged self-reflection and facilitated a better understanding of
the ethical considerations of AI during the early stages of development,
significantly contributing to the idea of "Responsible AI by Design" -- a
design-first approach that considers responsible AI values throughout the
development lifecycle and across business roles.

Generative AI, the most popular current approach to AI, consists of large
language models (LLMs) that are trained to produce outputs that are plausible,
but not necessarily correct. Although their abilities are often uncanny, they
are lacking in aspects of reasoning, leading LLMs to be less than completely
trustworthy. Furthermore, their results tend to be both unpredictable and
uninterpretable.
  We lay out 16 desiderata for future AI, and discuss an alternative approach
to AI which could theoretically address many of the limitations associated with
current approaches: AI educated with curated pieces of explicit knowledge and
rules of thumb, enabling an inference engine to automatically deduce the
logical entailments of all that knowledge. Even long arguments produced this
way can be both trustworthy and interpretable, since the full step-by-step line
of reasoning is always available, and for each step the provenance of the
knowledge used can be documented and audited. There is however a catch: if the
logical language is expressive enough to fully represent the meaning of
anything we can say in English, then the inference engine runs much too slowly.
That's why symbolic AI systems typically settle for some fast but much less
expressive logic, such as knowledge graphs. We describe how one AI system, Cyc,
has developed ways to overcome that tradeoff and is able to reason in higher
order logic in real time.
  We suggest that any trustworthy general AI will need to hybridize the
approaches, the LLM approach and more formal approach, and lay out a path to
realizing that dream.

Recently, generative AIs like ChatGPT have become available to the wide
public. These tools can for instance be used by students to generate essays or
whole theses. But how does a teacher know whether a text is written by a
student or an AI? In our work, we explore traditional and new features to (1)
detect text generated by AI from scratch and (2) text rephrased by AI. Since we
found that classification is more difficult when the AI has been instructed to
create the text in a way that a human would not recognize that it was generated
by an AI, we also investigate this more advanced case. For our experiments, we
produced a new text corpus covering 10 school topics. Our best systems to
classify basic and advanced human-generated/AI-generated texts have F1-scores
of over 96%. Our best systems for classifying basic and advanced
human-generated/AI-rephrased texts have F1-scores of more than 78%. The systems
use a combination of perplexity, semantic, list lookup, error-based,
readability, AI feedback, and text vector features. Our results show that the
new features substantially help to improve the performance of many classifiers.
Our best basic text rephrasing detection system even outperforms GPTZero by
183.8% relative in F1-score.

The credibility of AI models in medical imaging is often challenged by
reproducibility issues and obscured clinical insights, a reality highlighted
during the COVID-19 pandemic by many reports of near-perfect artificial
intelligence (AI) models that all failed to generalize. To address these
concerns, we propose a virtual imaging trial framework, employing a diverse
collection of medical images that are both clinical and simulated. In this
study, COVID-19 serves as a case example to unveil the intrinsic and extrinsic
factors influencing AI performance. Our findings underscore a significant
impact of dataset characteristics on AI efficacy. Even when trained on large,
diverse clinical datasets with thousands of patients, AI performance plummeted
by up to 20% in generalization. However, virtual imaging trials offer a robust
platform for objective assessment, unveiling nuanced insights into the
relationships between patient- and physics-based factors and AI performance.
For instance, disease extent markedly influenced AI efficacy, computed
tomography (CT) out-performed chest radiography (CXR), while imaging dose
exhibited minimal impact. Using COVID-19 as a case study, this virtual imaging
trial study verified that radiology AI models often suffer from a
reproducibility crisis. Virtual imaging trials not only offered a solution for
objective performance assessment but also extracted several clinical insights.
This study illuminates the path for leveraging virtual imaging to augment the
reliability, transparency, and clinical relevance of AI in medical imaging.

The rapid proliferation of AI-generated text online is profoundly reshaping
the information landscape. Among various types of AI-generated text,
AI-generated news presents a significant threat as it can be a prominent source
of misinformation online. While several recent efforts have focused on
detecting AI-generated text in general, these methods require enhanced
reliability, given concerns about their vulnerability to simple adversarial
attacks. Furthermore, due to the eccentricities of news writing, applying these
detection methods for AI-generated news can produce false positives,
potentially damaging the reputation of news organizations. To address these
challenges, we leverage the expertise of an interdisciplinary team to develop a
framework, J-Guard, capable of steering existing supervised AI text detectors
for detecting AI-generated news while boosting adversarial robustness. By
incorporating stylistic cues inspired by the unique journalistic attributes,
J-Guard effectively distinguishes between real-world journalism and
AI-generated news articles. Our experiments on news articles generated by a
vast array of AI models, including ChatGPT (GPT3.5), demonstrate the
effectiveness of J-Guard in enhancing detection capabilities while maintaining
an average performance decrease of as low as 7% when faced with adversarial
attacks.

Artificial intelligence (AI) can potentially transform global health, but
algorithmic bias can exacerbate social inequities and disparity. Trustworthy AI
entails the intentional design to ensure equity and mitigate potential biases.
To advance trustworthy AI in global health, we convened a workshop on Fairness
in Machine Intelligence for Global Health (FairMI4GH). The event brought
together a global mix of experts from various disciplines, community health
practitioners, policymakers, and more. Topics covered included managing AI bias
in socio-technical systems, AI's potential impacts on global health, and
balancing data privacy with transparency. Panel discussions examined the
cultural, political, and ethical dimensions of AI in global health. FairMI4GH
aimed to stimulate dialogue, facilitate knowledge transfer, and spark
innovative solutions. Drawing from NIST's AI Risk Management Framework, it
provided suggestions for handling AI risks and biases. The need to mitigate
data biases from the research design stage, adopt a human-centered approach,
and advocate for AI transparency was recognized. Challenges such as updating
legal frameworks, managing cross-border data sharing, and motivating developers
to reduce bias were acknowledged. The event emphasized the necessity of diverse
viewpoints and multi-dimensional dialogue for creating a fair and ethical AI
framework for equitable global health.

The authors are concerned about the safety, health, and rights of the
European citizens due to inadequate measures and procedures required by the
current draft of the EU Artificial Intelligence (AI) Act for the conformity
assessment of AI systems. We observe that not only the current draft of the EU
AI Act, but also the accompanying standardization efforts in CEN/CENELEC, have
resorted to the position that real functional guarantees of AI systems
supposedly would be unrealistic and too complex anyways. Yet enacting a
conformity assessment procedure that creates the false illusion of trust in
insufficiently assessed AI systems is at best naive and at worst grossly
negligent. The EU AI Act thus misses the point of ensuring quality by
functional trustworthiness and correctly attributing responsibilities.
  The trustworthiness of an AI decision system lies first and foremost in the
correct statistical testing on randomly selected samples and in the precision
of the definition of the application domain, which enables drawing samples in
the first place. We will subsequently call this testable quality functional
trustworthiness. It includes a design, development, and deployment that enables
correct statistical testing of all relevant functions.
  We are firmly convinced and advocate that a reliable assessment of the
statistical functional properties of an AI system has to be the indispensable,
mandatory nucleus of the conformity assessment. In this paper, we describe the
three necessary elements to establish a reliable functional trustworthiness,
i.e., (1) the definition of the technical distribution of the application, (2)
the risk-based minimum performance requirements, and (3) the statistically
valid testing based on independent random samples.

Ensuring quality human-AI interaction (HAII) in safety-critical industries is
essential. Failure to do so can lead to catastrophic and deadly consequences.
Despite this urgency, what little research there is on HAII is fragmented and
inconsistent. We present here a survey of that literature and recommendations
for research best practices that will improve the field. We divided our
investigation into the following research areas: (1) terms used to describe
HAII, (2) primary roles of AI-enabled systems, (3) factors that influence HAII,
and (4) how HAII is measured. Additionally, we described the capabilities and
maturity of the AI-enabled systems used in safety-critical industries discussed
in these articles. We found that no single term is used across the literature
to describe HAII and some terms have multiple meanings. According to our
literature, five factors influence HAII: user characteristics and background
(e.g., user personality, perceptions), AI interface and features (e.g.,
interactive UI design), AI output (e.g., accuracy, actionable recommendations),
explainability and interpretability (e.g., level of detail, user
understanding), and usage of AI (e.g., heterogeneity of environments and user
needs). HAII is most commonly measured with user-related subjective metrics
(e.g., user perception, trust, and attitudes), and AI-assisted decision-making
is the most common primary role of AI-enabled systems. Based on this review, we
conclude that there are substantial research gaps in HAII. Researchers and
developers need to codify HAII terminology, involve users throughout the AI
lifecycle (especially during development), and tailor HAII in safety-critical
industries to the users and environments.

Artificial intelligence (AI) can revolutionize the development industry,
primarily electrical and electronics engineering. By automating recurring
duties, AI can grow productivity and efficiency in creating. For instance, AI
can research constructing designs, discover capability troubles, and generate
answers, reducing the effort and time required for manual analysis. AI also can
be used to optimize electricity consumption in buildings, which is a critical
difficulty in the construction enterprise. Via machines gaining knowledge of
algorithms to investigate electricity usage patterns, AI can discover areas
wherein power may be stored and offer guidelines for enhancements. This can
result in significant value financial savings and reduced carbon emissions.
Moreover, AI may be used to improve the protection of creation websites. By
studying statistics from sensors and cameras, AI can locate capacity dangers
and alert workers to take suitable action. This could help save you from
injuries and accidents on production sites, lowering the chance for workers and
enhancing overall safety in the enterprise. The impact of AI on electric and
electronics engineering productivity inside the creation industry is enormous.
AI can transform how we layout, build, and function buildings by automating
ordinary duties, optimising electricity intake, and enhancing safety. However,
ensuring that AI is used ethically and responsibly and that the advantages are
shared fairly throughout the enterprise is essential.

In the ever-evolving realm of cybersecurity, the rise of generative AI models
like ChatGPT, FraudGPT, and WormGPT has introduced both innovative solutions
and unprecedented challenges. This research delves into the multifaceted
applications of generative AI in social engineering attacks, offering insights
into the evolving threat landscape using the blog mining technique. Generative
AI models have revolutionized the field of cyberattacks, empowering malicious
actors to craft convincing and personalized phishing lures, manipulate public
opinion through deepfakes, and exploit human cognitive biases. These models,
ChatGPT, FraudGPT, and WormGPT, have augmented existing threats and ushered in
new dimensions of risk. From phishing campaigns that mimic trusted
organizations to deepfake technology impersonating authoritative figures, we
explore how generative AI amplifies the arsenal of cybercriminals. Furthermore,
we shed light on the vulnerabilities that AI-driven social engineering
exploits, including psychological manipulation, targeted phishing, and the
crisis of authenticity. To counter these threats, we outline a range of
strategies, including traditional security measures, AI-powered security
solutions, and collaborative approaches in cybersecurity. We emphasize the
importance of staying vigilant, fostering awareness, and strengthening
regulations in the battle against AI-enhanced social engineering attacks. In an
environment characterized by the rapid evolution of AI models and a lack of
training data, defending against generative AI threats requires constant
adaptation and the collective efforts of individuals, organizations, and
governments. This research seeks to provide a comprehensive understanding of
the dynamic interplay between generative AI and social engineering attacks,
equipping stakeholders with the knowledge to navigate this intricate
cybersecurity landscape.

Aligning AI agents to human intentions and values is a key bottleneck in
building safe and deployable AI applications. But whose values should AI agents
be aligned with? Reinforcement learning with human feedback (RLHF) has emerged
as the key framework for AI alignment. RLHF uses feedback from human
reinforcers to fine-tune outputs; all widely deployed large language models
(LLMs) use RLHF to align their outputs to human values. It is critical to
understand the limitations of RLHF and consider policy challenges arising from
these limitations. In this paper, we investigate a specific challenge in
building RLHF systems that respect democratic norms. Building on impossibility
results in social choice theory, we show that, under fairly broad assumptions,
there is no unique voting protocol to universally align AI systems using RLHF
through democratic processes. Further, we show that aligning AI agents with the
values of all individuals will always violate certain private ethical
preferences of an individual user i.e., universal AI alignment using RLHF is
impossible. We discuss policy implications for the governance of AI systems
built using RLHF: first, the need for mandating transparent voting rules to
hold model builders accountable. Second, the need for model builders to focus
on developing AI agents that are narrowly aligned to specific user groups.

Web3 and AI have been among the most discussed fields over the recent years,
with substantial hype surrounding each field's potential to transform the world
as we know it. However, as the hype settles, it's evident that neither AI nor
Web3 can address all challenges independently. Consequently, the intersection
of AI and Web3 is gaining increased attention, emerging as a new field with the
potential to address the limitations of each. In this article, we will focus on
the integration of web3 and the AI marketplace, where AI services and products
can be provided in a decentralized manner (DeAI). A comprehensive review is
provided by summarizing the opportunities and challenges on this topic.
Additionally, we offer analyses and solutions to address these challenges.
We've developed a framework that lets users pay with any kind of cryptocurrency
to get AI services. Additionally, they can also enjoy AI services for free on
our platform by simply locking up their assets temporarily in the protocol.
This unique approach is a first in the industry. Before this, offering free AI
services in the web3 community wasn't possible. Our solution opens up exciting
opportunities for the AI marketplace in the web3 space to grow and be widely
adopted.

As Earth science enters the era of big data, artificial intelligence (AI) not
only offers great potential for solving geoscience problems, but also plays a
critical role in accelerating the understanding of the complex, interactive,
and multiscale processes of Earth's behavior. As geoscience AI models are
progressively utilized for significant predictions in crucial situations,
geoscience researchers are increasingly demanding their interpretability and
versatility. This study proposes an interpretable geoscience artificial
intelligence (XGeoS-AI) framework to unravel the mystery of image recognition
in the Earth sciences, and its effectiveness and versatility is demonstrated by
taking computed tomography (CT) image recognition as an example. Inspired by
the mechanism of human vision, the proposed XGeoS-AI framework generates a
threshold value from a local region within the whole image to complete the
recognition. Different kinds of artificial intelligence (AI) methods, such as
Support Vector Regression (SVR), Multilayer Perceptron (MLP), Convolutional
Neural Network (CNN), can be adopted as the AI engines of the proposed XGeoS-AI
framework to efficiently complete geoscience image recognition tasks.
Experimental results demonstrate that the effectiveness, versatility, and
heuristics of the proposed framework have great potential in solving geoscience
image recognition problems. Interpretable AI should receive more and more
attention in the field of the Earth sciences, which is the key to promoting
more rational and wider applications of AI in the field of Earth sciences. In
addition, the proposed interpretable framework may be the forerunner of
technological innovation in the Earth sciences.

This study examines the key factors that affect European reactions to
artificial intelligence (AI) in the context of both full and flawed democracies
in Europe. Analysing a dataset of 4,006 respondents, categorised into full
democracies and flawed democracies based on the Democracy Index developed by
the Economist Intelligence Unit (EIU), this research identifies crucial factors
that shape European attitudes toward AI in these two types of democracies. The
analysis reveals noteworthy findings. Firstly, it is observed that flawed
democracies tend to exhibit higher levels of trust in government entities
compared to their counterparts in full democracies. Additionally, individuals
residing in flawed democracies demonstrate a more positive attitude toward AI
when compared to respondents from full democracies. However, the study finds no
significant difference in AI awareness between the two types of democracies,
indicating a similar level of general knowledge about AI technologies among
European citizens. Moreover, the study reveals that trust in AI measures,
specifically "Trust AI Solution", does not significantly vary between full and
flawed democracies. This suggests that despite the differences in democratic
quality, both types of democracies have similar levels of confidence in AI
solutions.

The integration of artificial intelligence (AI) into education has the
potential to transform the way we learn and teach. In this paper, we examine
the current state of AI in education and explore the potential benefits and
challenges of incorporating this technology into the classroom. The approaches
currently available for AI education often present students with experiences
only focusing on discrete computer science concepts agnostic to a larger
curriculum. However, teaching AI must not be siloed or interdisciplinary.
Rather, AI instruction ought to be transdisciplinary, including connections to
the broad curriculum and community in which students are learning. This paper
delves into the AI program currently in development for Neom Community School
and the larger Education, Research, and Innovation Sector in Neom, Saudi Arabia
s new megacity under development. In this program, AI is both taught as a
subject and to learn other subjects within the curriculum through the school
systems International Baccalaureate (IB) approach, which deploys learning
through Units of Inquiry. This approach to education connects subjects across a
curriculum under one major guiding question at a time. The proposed method
offers a meaningful approach to introducing AI to students throughout these
Units of Inquiry, as it shifts AI from a subject that students like or not like
to a subject that is taught throughout the curriculum.

When users perceive AI systems as mindful, independent agents, they hold them
responsible instead of the AI experts who created and designed these systems.
So far, it has not been studied whether explanations support this shift in
responsibility through the use of mind-attributing verbs like "to think". To
better understand the prevalence of mind-attributing explanations we analyse AI
explanations in 3,533 explainable AI (XAI) research articles from the Semantic
Scholar Open Research Corpus (S2ORC). Using methods from semantic shift
detection, we identify three dominant types of mind attribution: (1)
metaphorical (e.g. "to learn" or "to predict"), (2) awareness (e.g. "to
consider"), and (3) agency (e.g. "to make decisions"). We then analyse the
impact of mind-attributing explanations on awareness and responsibility in a
vignette-based experiment with 199 participants. We find that participants who
were given a mind-attributing explanation were more likely to rate the AI
system as aware of the harm it caused. Moreover, the mind-attributing
explanation had a responsibility-concealing effect: Considering the AI experts'
involvement lead to reduced ratings of AI responsibility for participants who
were given a non-mind-attributing or no explanation. In contrast, participants
who read the mind-attributing explanation still held the AI system responsible
despite considering the AI experts' involvement. Taken together, our work
underlines the need to carefully phrase explanations about AI systems in
scientific writing to reduce mind attribution and clearly communicate human
responsibility.

There are concerns about the reliability and safety of sub-symbolic neural
network AI because its decisions cannot be explained explicitly. This is the
black box problem of modern AI. At the same time, symbolic AI has the nature of
a white box and is able to ensure the reliability and safety of its decisions.
However, several problems prevent the widespread use of symbolic AI: the
opacity of mathematical models and natural language terms, the lack of a
unified ontology, and the combinatorial explosion of search capabilities. To
solve the black-box problem of AI, we propose Explicitly Explainable AI (XXAI)
- a fully transparent white-box AI based on deterministic logical cellular
automata whose rules are derived from the first principles of the general
theory of the relevant domain. In this case, the general theory of the domain
plays the role of a knowledge base for deriving the inferences of the cellular
automata. A cellular automaton implements parallel multi-level logical
inference at all levels of organization - from local interactions of the
element base to the system as a whole. Our verification of several ecological
hypotheses sets a precedent for the successful implementation of the proposed
solution. XXAI can automatically verify the reliability, safety, and ethicality
of sub-symbolic neural network AI decisions during both the final and training
phases. This paper presents the theoretical and methodological foundations for
creating XXAI and discusses the prospects for this direction.

As artificial intelligence transforms a wide range of sectors and drives
innovation, it also introduces complex challenges concerning ethics,
transparency, bias, and fairness. The imperative for integrating Responsible AI
(RAI) principles within governance frameworks is paramount to mitigate these
emerging risks. While there are many solutions for AI governance, significant
questions remain about their effectiveness in practice. Addressing this
knowledge gap, this paper aims to examine the existing literature on AI
Governance. The focus of this study is to analyse the literature to answer key
questions: WHO is accountable for AI systems' governance, WHAT elements are
being governed, WHEN governance occurs within the AI development life cycle,
and HOW it is executed through various mechanisms like frameworks, tools,
standards, policies, or models. Employing a systematic literature review
methodology, a rigorous search and selection process has been employed. This
effort resulted in the identification of 61 relevant articles on the subject of
AI Governance. Out of the 61 studies analysed, only 5 provided complete
responses to all questions. The findings from this review aid research in
formulating more holistic and comprehensive Responsible AI (RAI) governance
frameworks. This study highlights important role of AI governance on various
levels specially organisational in establishing effective and responsible AI
practices. The findings of this study provides a foundational basis for future
research and development of comprehensive governance models that align with RAI
principles.

Artificial intelligence can cause inconvenience, harm, or other unintended
consequences in various ways, including those that arise from defects or
malfunctions in the AI system itself or those caused by its use or misuse.
Responsibility for AI harms or unintended consequences must be addressed to
hold accountable the people who caused such harms and ensure that victims
receive compensation for any damages or losses they may have sustained.
Historical instances of harm caused by AI have led to European Union
establishing an AI Liability Directive. The directive aims to lay down a
uniform set of rules for access to information, delineate the duty and level of
care required for AI development and use, and clarify the burden of proof for
damages or harms caused by AI systems, establishing broader protection for
victims. The future ability of provider to contest a product liability claim
will depend on good practices adopted in designing, developing, and maintaining
AI systems in the market. This paper provides a risk-based approach to
examining liability for AI-driven injuries. It also provides an overview of
existing liability approaches, insights into limitations and complexities in
these approaches, and a detailed self-assessment questionnaire to assess the
risk associated with liability for a specific AI system from a provider's
perspective.

Exposure to large language model output is rapidly increasing. How will
seeing AI-generated ideas affect human ideas? We conducted an experiment (800+
participants, 40+ countries) where participants viewed creative ideas that were
from ChatGPT or prior experimental participants and then brainstormed their own
idea. We varied the number of AI-generated examples (none, low, or high
exposure) and if the examples were labeled as 'AI' (disclosure). Our dynamic
experiment design -- ideas from prior participants in an experimental condition
are used as stimuli for future participants in the same experimental condition
-- mimics the interdependent process of cultural creation: creative ideas are
built upon prior ideas. Hence, we capture the compounding effects of having
LLMs 'in the culture loop'. We find that high AI exposure (but not low AI
exposure) did not affect the creativity of individual ideas but did increase
the average amount and rate of change of collective idea diversity. AI made
ideas different, not better. There were no main effects of disclosure. We also
found that self-reported creative people were less influenced by knowing an
idea was from AI, and that participants were more likely to knowingly adopt AI
ideas when the task was difficult. Our findings suggest that introducing AI
ideas into society may increase collective diversity but not individual
creativity.

In response to rising concerns surrounding the safety, security, and
trustworthiness of Generative AI (GenAI) models, practitioners and regulators
alike have pointed to AI red-teaming as a key component of their strategies for
identifying and mitigating these risks. However, despite AI red-teaming's
central role in policy discussions and corporate messaging, significant
questions remain about what precisely it means, what role it can play in
regulation, and how it relates to conventional red-teaming practices as
originally conceived in the field of cybersecurity. In this work, we identify
recent cases of red-teaming activities in the AI industry and conduct an
extensive survey of relevant research literature to characterize the scope,
structure, and criteria for AI red-teaming practices. Our analysis reveals that
prior methods and practices of AI red-teaming diverge along several axes,
including the purpose of the activity (which is often vague), the artifact
under evaluation, the setting in which the activity is conducted (e.g., actors,
resources, and methods), and the resulting decisions it informs (e.g.,
reporting, disclosure, and mitigation). In light of our findings, we argue that
while red-teaming may be a valuable big-tent idea for characterizing GenAI harm
mitigations, and that industry may effectively apply red-teaming and other
strategies behind closed doors to safeguard AI, gestures towards red-teaming
(based on public definitions) as a panacea for every possible risk verge on
security theater. To move toward a more robust toolbox of evaluations for
generative AI, we synthesize our recommendations into a question bank meant to
guide and scaffold future AI red-teaming practices.

Achieving human-AI alignment in complex multi-agent games is crucial for
creating trustworthy AI agents that enhance gameplay. We propose a method to
evaluate this alignment using an interpretable task-sets framework, focusing on
high-level behavioral tasks instead of low-level policies. Our approach has
three components. First, we analyze extensive human gameplay data from Xbox's
Bleeding Edge (100K+ games), uncovering behavioral patterns in a complex task
space. This task space serves as a basis set for a behavior manifold capturing
interpretable axes: fight-flight, explore-exploit, and solo-multi-agent.
Second, we train an AI agent to play Bleeding Edge using a Generative
Pretrained Causal Transformer and measure its behavior. Third, we project human
and AI gameplay to the proposed behavior manifold to compare and contrast. This
allows us to interpret differences in policy as higher-level behavioral
concepts, e.g., we find that while human players exhibit variability in
fight-flight and explore-exploit behavior, AI players tend towards uniformity.
Furthermore, AI agents predominantly engage in solo play, while humans often
engage in cooperative and competitive multi-agent patterns. These stark
differences underscore the need for interpretable evaluation, design, and
integration of AI in human-aligned applications. Our study advances the
alignment discussion in AI and especially generative AI research, offering a
measurable framework for interpretable human-agent alignment in multiplayer
gaming.

This study consists of a novel approach toward the analysis of court
judgments spanning five countries, including the United States, the United
Kingdom, Rwanda, Sweden and Hong Kong. This study also explores the
intersection of the latest advancements in artificial intelligence (AI) and
legal analysis, emphasizing the role of AI (specifically generative AI) in
identifying human biases and facilitating automated, valid, and coherent
multisided argumentation of court judgments with the goal of ensuring
consistent application of laws in and across various jurisdictions. By
incorporating Advanced Language Models (ALMs) and a newly introduced human-AI
collaborative framework, this paper seeks to analyze Grounded Theory-based
research design with Advanced Language Models (ALMs) in the practice of law.
SHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT
technology), focusing on detecting logical inconsistencies and biases across
various legal decisions. SHIRLEY analysis is aggregated and is accompanied by a
comparison-oriented AI-based application called SAM (also an ALM) to identify
relative deviations in SHIRLEY bias detections. Further, a CRITIC is generated
within semi-autonomous arbitration process via the ALM, SARA. A novel approach
is introduced in the utilization of an AI arbitrator to critically evaluate
biases and qualitative-in-nature nuances identified by the aforementioned AI
applications (SAM in concert with SHIRLEY), based on the Hague Rules on
Business and Human Rights Arbitration. This Semi-Automated Arbitration Process
(SAAP) aims to uphold the integrity and fairness of legal judgments by ensuring
a nuanced debate-resultant "understanding" through a hybrid system of AI and
human-based collaborative analysis.

This paper explores the role and challenges of Artificial Intelligence (AI)
algorithms, specifically AI-based software elements, in autonomous driving
systems. These AI systems are fundamental in executing real-time critical
functions in complex and high-dimensional environments. They handle vital tasks
like multi-modal perception, cognition, and decision-making tasks such as
motion planning, lane keeping, and emergency braking. A primary concern relates
to the ability (and necessity) of AI models to generalize beyond their initial
training data. This generalization issue becomes evident in real-time
scenarios, where models frequently encounter inputs not represented in their
training or validation data. In such cases, AI systems must still function
effectively despite facing distributional or domain shifts. This paper
investigates the risk associated with overconfident AI models in
safety-critical applications like autonomous driving. To mitigate these risks,
methods for training AI models that help maintain performance without
overconfidence are proposed. This involves implementing certainty reporting
architectures and ensuring diverse training data. While various
distribution-based methods exist to provide safety mechanisms for AI models,
there is a noted lack of systematic assessment of these methods, especially in
the context of safety-critical automotive applications. Many methods in the
literature do not adapt well to the quick response times required in
safety-critical edge applications. This paper reviews these methods, discusses
their suitability for safety-critical applications, and highlights their
strengths and limitations. The paper also proposes potential improvements to
enhance the safety and reliability of AI algorithms in autonomous vehicles in
the context of rapid and accurate decision-making processes.

Generative AI brings novel and impressive abilities to help people in
everyday tasks. There are many AI workflows that solve real and complex
problems by chaining AI outputs together with human interaction. Although there
is an undeniable lure of AI, it's uncertain how useful generative AI workflows
are after the novelty wears off. Additionally, tools built with generative AI
have the potential to be personalized and adapted quickly and easily, but do
users take advantage of the potential to customize? We conducted a three-week
longitudinal study with 12 users to understand the familiarization and
customization of generative AI tools for science communication. Our study
revealed that the familiarization phase lasts for 4.3 sessions, where users
explore the capabilities of the workflow and which aspects they find useful.
After familiarization, the perceived utility of the system is rated higher than
before, indicating that the perceived utility of AI is not just a novelty
effect. The increase in benefits mainly comes from end-users' ability to
customize prompts, and thus appropriate the system to their own needs. This
points to a future where generative AI systems can allow us to design for
appropriation.

The expanding role of Artificial Intelligence (AI) in diverse engineering
domains highlights the challenges associated with deploying AI models in new
operational environments, involving substantial investments in data collection
and model training. Rapid application of AI necessitates evaluating the
feasibility of utilizing pre-trained models in unobserved operational settings
with minimal or no additional data. However, interpreting the opaque nature of
AI's black-box models remains a persistent challenge. Addressing this issue,
this paper proposes a science-based certification methodology to assess the
viability of employing pre-trained data-driven models in untrained operational
environments. The methodology advocates a profound integration of domain
knowledge, leveraging theoretical and analytical models from physics and
related disciplines, with data-driven AI models. This novel approach introduces
tools to facilitate the development of secure engineering systems, providing
decision-makers with confidence in the trustworthiness and safety of AI-based
models across diverse environments characterized by limited training data and
dynamic, uncertain conditions. The paper demonstrates the efficacy of this
methodology in real-world safety-critical scenarios, particularly in the
context of traffic state estimation. Through simulation results, the study
illustrates how the proposed methodology efficiently quantifies physical
inconsistencies exhibited by pre-trained AI models. By utilizing analytical
models, the methodology offers a means to gauge the applicability of
pre-trained AI models in new operational environments. This research
contributes to advancing the understanding and deployment of AI models,
offering a robust certification framework that enhances confidence in their
reliability and safety across a spectrum of operational conditions.

The increasing use of artificial intelligence (AI) systems in our daily life
through various applications, services, and products explains the significance
of trust/distrust in AI from a user perspective. AI-driven systems (as opposed
to other technologies) have ubiquitously diffused in our life not only as some
beneficial tools to be used by human agents but also are going to be
substitutive agents on our behalf, or manipulative minds that would influence
human thought, decision, and agency. Trust/distrust in AI plays the role of a
regulator and could significantly control the level of this diffusion, as trust
can increase, and distrust may reduce the rate of adoption of AI. Recently,
varieties of studies have paid attention to the variant dimension of
trust/distrust in AI, and its relevant considerations. In this systematic
literature review, after conceptualization of trust in the current AI
literature review, we will investigate trust in different types of
human-Machine interaction, and its impact on technology acceptance in different
domains. In addition to that, we propose a taxonomy of technical (i.e., safety,
accuracy, robustness) and non-technical axiological (i.e., ethical, legal, and
mixed) trustworthiness metrics, and some trustworthy measurements. Moreover, we
examine some major trust-breakers in AI (e.g., autonomy and dignity threat),
and trust makers; and propose some future directions and probable solutions for
the transition to a trustworthy AI.

This study offers an in-depth analysis of the application and implications of
the National Institute of Standards and Technology's AI Risk Management
Framework (NIST AI RMF) within the domain of surveillance technologies,
particularly facial recognition technology. Given the inherently high-risk and
consequential nature of facial recognition systems, our research emphasizes the
critical need for a structured approach to risk management in this sector. The
paper presents a detailed case study demonstrating the utility of the NIST AI
RMF in identifying and mitigating risks that might otherwise remain unnoticed
in these technologies. Our primary objective is to develop a comprehensive risk
management strategy that advances the practice of responsible AI utilization in
feasible, scalable ways. We propose a six-step process tailored to the specific
challenges of surveillance technology that aims to produce a more systematic
and effective risk management practice. This process emphasizes continual
assessment and improvement to facilitate companies in managing AI-related risks
more robustly and ensuring ethical and responsible deployment of AI systems.
Additionally, our analysis uncovers and discusses critical gaps in the current
framework of the NIST AI RMF, particularly concerning its application to
surveillance technologies. These insights contribute to the evolving discourse
on AI governance and risk management, highlighting areas for future refinement
and development in frameworks like the NIST AI RMF.

The recent embrace of machine learning (ML) in the development of autonomous
weapons systems (AWS) creates serious risks to geopolitical stability and the
free exchange of ideas in AI research. This topic has received comparatively
little attention of late compared to risks stemming from superintelligent
artificial general intelligence (AGI), but requires fewer assumptions about the
course of technological development and is thus a nearer-future issue. ML is
already enabling the substitution of AWS for human soldiers in many battlefield
roles, reducing the upfront human cost, and thus political cost, of waging
offensive war. In the case of peer adversaries, this increases the likelihood
of "low intensity" conflicts which risk escalation to broader warfare. In the
case of non-peer adversaries, it reduces the domestic blowback to wars of
aggression. This effect can occur regardless of other ethical issues around the
use of military AI such as the risk of civilian casualties, and does not
require any superhuman AI capabilities. Further, the military value of AWS
raises the specter of an AI-powered arms race and the misguided imposition of
national security restrictions on AI research. Our goal in this paper is to
raise awareness among the public and ML researchers on the near-future risks
posed by full or near-full autonomy in military technology, and we provide
regulatory suggestions to mitigate these risks. We call upon AI policy experts
and the defense AI community in particular to embrace transparency and caution
in their development and deployment of AWS to avoid the negative effects on
global stability and AI research that we highlight here.

A cautious interpretation of AI regulations and policy in the EU and the USA
place explainability as a central deliverable of compliant AI systems. However,
from a technical perspective, explainable AI (XAI) remains an elusive and
complex target where even state of the art methods often reach erroneous,
misleading, and incomplete explanations. "Explainability" has multiple meanings
which are often used interchangeably, and there are an even greater number of
XAI methods - none of which presents a clear edge. Indeed, there are multiple
failure modes for each XAI method, which require application-specific
development and continuous evaluation. In this paper, we analyze legislative
and policy developments in the United States and the European Union, such as
the Executive Order on the Safe, Secure, and Trustworthy Development and Use of
Artificial Intelligence, the AI Act, the AI Liability Directive, and the
General Data Protection Regulation (GDPR) from a right to explanation
perspective. We argue that these AI regulations and current market conditions
threaten effective AI governance and safety because the objective of
trustworthy, accountable, and transparent AI is intrinsically linked to the
questionable ability of AI operators to provide meaningful explanations. Unless
governments explicitly tackle the issue of explainability through clear
legislative and policy statements that take into account technical realities,
AI governance risks becoming a vacuous "box-ticking" exercise where scientific
standards are replaced with legalistic thresholds, providing only a false sense
of security in XAI.

Since late 2022, generative AI has taken the world by storm, with widespread
use of tools including ChatGPT, Gemini, and Claude. Generative AI and large
language model (LLM) applications are transforming how individuals find and
access data and knowledge. However, the intricate relationship between open
data and generative AI, and the vast potential it holds for driving innovation
in this field remain underexplored areas. This white paper seeks to unpack the
relationship between open data and generative AI and explore possible
components of a new Fourth Wave of Open Data: Is open data becoming AI ready?
Is open data moving towards a data commons approach? Is generative AI making
open data more conversational? Will generative AI improve open data quality and
provenance? Towards this end, we provide a new Spectrum of Scenarios framework.
This framework outlines a range of scenarios in which open data and generative
AI could intersect and what is required from a data quality and provenance
perspective to make open data ready for those specific scenarios. These
scenarios include: pertaining, adaptation, inference and insight generation,
data augmentation, and open-ended exploration. Through this process, we found
that in order for data holders to embrace generative AI to improve open data
access and develop greater insights from open data, they first must make
progress around five key areas: enhance transparency and documentation, uphold
quality and integrity, promote interoperability and standards, improve
accessibility and useability, and address ethical considerations.

Ensuring that AI systems reliably and robustly avoid harmful or dangerous
behaviours is a crucial challenge, especially for AI systems with a high degree
of autonomy and general intelligence, or systems used in safety-critical
contexts. In this paper, we will introduce and define a family of approaches to
AI safety, which we will refer to as guaranteed safe (GS) AI. The core feature
of these approaches is that they aim to produce AI systems which are equipped
with high-assurance quantitative safety guarantees. This is achieved by the
interplay of three core components: a world model (which provides a
mathematical description of how the AI system affects the outside world), a
safety specification (which is a mathematical description of what effects are
acceptable), and a verifier (which provides an auditable proof certificate that
the AI satisfies the safety specification relative to the world model). We
outline a number of approaches for creating each of these three core
components, describe the main technical challenges, and suggest a number of
potential solutions to them. We also argue for the necessity of this approach
to AI safety, and for the inadequacy of the main alternative approaches.

The expanding role of Artificial Intelligence (AI) in diverse engineering
domains highlights the challenges associated with deploying AI models in new
operational environments, involving substantial investments in data collection
and model training. Rapid application of AI necessitates evaluating the
feasibility of utilizing pre-trained models in unobserved operational settings
with minimal or no additional data. However, interpreting the opaque nature of
AI's black-box models remains a persistent challenge. Addressing this issue,
this paper proposes a science-based certification methodology to assess the
viability of employing pre-trained data-driven models in new operational
environments. The methodology advocates a profound integration of domain
knowledge, leveraging theoretical and analytical models from physics and
related disciplines, with data-driven AI models. This novel approach introduces
tools to facilitate the development of secure engineering systems, providing
decision-makers with confidence in the trustworthiness and safety of AI-based
models across diverse environments characterized by limited training data and
dynamic, uncertain conditions. The paper demonstrates the efficacy of this
methodology in real-world safety-critical scenarios, particularly in the
context of traffic state estimation. Through simulation results, the study
illustrates how the proposed methodology efficiently quantifies physical
inconsistencies exhibited by pre-trained AI models. By utilizing analytical
models, the methodology offers a means to gauge the applicability of
pre-trained AI models in new operational environments. This research
contributes to advancing the understanding and deployment of AI models,
offering a robust certification framework that enhances confidence in their
reliability and safety across a spectrum of operational conditions.

Perhaps the most ambitious scientific quest in human history is the creation
of general artificial intelligence, which roughly means AI that is as smart or
smarter than humans. The dominant approach in the machine learning community is
to attempt to discover each of the pieces required for intelligence, with the
implicit assumption that some future group will complete the Herculean task of
figuring out how to combine all of those pieces into a complex thinking
machine. I call this the "manual AI approach". This paper describes another
exciting path that ultimately may be more successful at producing general AI.
It is based on the clear trend in machine learning that hand-designed solutions
eventually are replaced by more effective, learned solutions. The idea is to
create an AI-generating algorithm (AI-GA), which automatically learns how to
produce general AI. Three Pillars are essential for the approach: (1)
meta-learning architectures, (2) meta-learning the learning algorithms
themselves, and (3) generating effective learning environments. I argue that
either approach could produce general AI first, and both are scientifically
worthwhile irrespective of which is the fastest path. Because both are
promising, yet the ML community is currently committed to the manual approach,
I argue that our community should increase its research investment in the AI-GA
approach. To encourage such research, I describe promising work in each of the
Three Pillars. I also discuss AI-GA-specific safety and ethical considerations.
Because it it may be the fastest path to general AI and because it is
inherently scientifically interesting to understand the conditions in which a
simple algorithm can produce general AI (as happened on Earth where Darwinian
evolution produced human intelligence), I argue that the pursuit of AI-GAs
should be considered a new grand challenge of computer science research.

The popularity of Artificial Intelligence (AI) -- and of Machine Learning
(ML) as an approach to AI, has dramatically increased in the last few years,
due to its outstanding performance in various domains, notably in image, audio,
and natural language processing. In these domains, AI success-stories are
boosting the applied field. When it comes to AI/ML for data communication
Networks (AI4NETS), and despite the many attempts to turn networks into
learning agents, the successful application of AI/ML in networking is limited.
There is a strong resistance against AI/ML-based solutions, and a striking gap
between the extensive academic research and the actual deployments of such
AI/ML-based systems in operational environments. The truth is, there are still
many unsolved complex challenges associated to the analysis of networking data
through AI/ML, which hinders its acceptability and adoption in the practice. In
this positioning paper I elaborate on the most important show-stoppers in
AI4NETS, and present a research agenda to tackle some of these challenges,
enabling a natural adoption of AI/ML for networking. In particular, I focus the
future research in AI4NETS around three major pillars: (i) to make AI/ML
immediately applicable in networking problems through the concepts of effective
learning, turning it into a useful and reliable way to deal with complex
data-driven networking problems; (ii) to boost the adoption of AI/ML at the
large scale by learning from the Internet-paradigm itself, conceiving novel
distributed and hierarchical learning approaches mimicking the distributed
topological principles and operation of the Internet itself; and (iii) to
exploit the softwarization and distribution of networks to conceive
AI/ML-defined Networks (AIDN), relying on the distributed generation and
re-usage of knowledge through novel Knowledge Delivery Networks (KDNs).

In February 2020, the European Commission (EC) published a white paper
entitled, On Artificial Intelligence - A European approach to excellence and
trust. This paper outlines the EC's policy options for the promotion and
adoption of artificial intelligence (AI) in the European Union. The Montreal AI
Ethics Institute (MAIEI) reviewed this paper and published a response
addressing the EC's plans to build an "ecosystem of excellence" and an
"ecosystem of trust," as well as the safety and liability implications of AI,
the internet of things (IoT), and robotics.
  MAIEI provides 15 recommendations in relation to the sections outlined above,
including: 1) focus efforts on the research and innovation community, member
states, and the private sector; 2) create alignment between trading partners'
policies and EU policies; 3) analyze the gaps in the ecosystem between
theoretical frameworks and approaches to building trustworthy AI; 4) focus on
coordination and policy alignment; 5) focus on mechanisms that promote private
and secure sharing of data; 6) create a network of AI research excellence
centres to strengthen the research and innovation community; 7) promote
knowledge transfer and develop AI expertise through Digital Innovation Hubs; 8)
add nuance to the discussion regarding the opacity of AI systems; 9) create a
process for individuals to appeal an AI system's decision or output; 10)
implement new rules and strengthen existing regulations; 11) ban the use of
facial recognition technology; 12) hold all AI systems to similar standards and
compulsory requirements; 13) ensure biometric identification systems fulfill
the purpose for which they are implemented; 14) implement a voluntary labelling
system for systems that are not considered high-risk; 15) appoint individuals
to the oversight process who understand AI systems well and are able to
communicate potential risks.

The recent years witness a trend of applying large-scale distributed deep
learning in both business and scientific computing areas, whose goal is to
speed up the training time to achieve a state-of-the-art quality. The HPC
community feels a great interest in building the HPC AI systems that are
dedicated to running those workloads. The HPC AI benchmarks accelerate the
process. Unfortunately, benchmarking HPC AI systems at scale raises serious
challenges. None of previous HPC AI benchmarks achieve the goal of being
equivalent, relevant, representative, affordable, and repeatable. This paper
presents a comprehensive methodology, tools, Roofline performance models, and
innovative metrics for benchmarking, optimizing, and ranking HPC AI systems,
which we call HPC AI500 V2.0. We abstract the HPC AI system into nine
independent layers, and present explicit benchmarking rules and procedures to
assure equivalence of each layer, repeatability, and replicability. On the
basis of AIBench -- by far the most comprehensive AI benchmarks suite, we
present and build two HPC AI benchmarks from both business and scientific
computing: Image Classification, and Extreme Weather Analytics, achieving both
representativeness and affordability. To rank the performance and
energy-efficiency of HPC AI systems, we propose Valid FLOPS, and Valid FLOPS
per watt, which impose a penalty on failing to achieve the target quality. We
propose using convolution and GEMM -- the two most intensively-used kernel
functions to measure the upper bound performance of the HPC AI systems, and
present HPC AI roofline models for guiding performance optimizations. The
evaluations show our methodology, benchmarks, performance models, and metrics
can measure, optimize, and rank the HPC AI systems in a scalable, simple, and
affordable way. HPC AI500 V2.0 are publicly available from
http://www.benchcouncil.org/benchhub/hpc-ai500-benchmark.

The history of science and technology shows that seemingly innocuous
developments in scientific theories and research have enabled real-world
applications with significant negative consequences for humanity. In order to
ensure that the science and technology of AI is developed in a humane manner,
we must develop research publication norms that are informed by our growing
understanding of AI's potential threats and use cases. Unfortunately, it's
difficult to create a set of publication norms for responsible AI because the
field of AI is currently fragmented in terms of how this technology is
researched, developed, funded, etc. To examine this challenge and find
solutions, the Montreal AI Ethics Institute (MAIEI) co-hosted two public
consultations with the Partnership on AI in May 2020. These meetups examined
potential publication norms for responsible AI, with the goal of creating a
clear set of recommendations and ways forward for publishers.
  In its submission, MAIEI provides six initial recommendations, these include:
1) create tools to navigate publication decisions, 2) offer a page number
extension, 3) develop a network of peers, 4) require broad impact statements,
5) require the publication of expected results, and 6) revamp the peer-review
process. After considering potential concerns regarding these recommendations,
including constraining innovation and creating a "black market" for AI
research, MAIEI outlines three ways forward for publishers, these include: 1)
state clearly and consistently the need for established norms, 2) coordinate
and build trust as a community, and 3) change the approach.

Machine learning (ML) and artificial intelligence (AI) researchers play an
important role in the ethics and governance of AI, including taking action
against what they perceive to be unethical uses of AI (Belfield, 2020; Van
Noorden, 2020). Nevertheless, this influential group's attitudes are not well
understood, which undermines our ability to discern consensuses or
disagreements between AI/ML researchers. To examine these researchers' views,
we conducted a survey of those who published in the top AI/ML conferences (N =
524). We compare these results with those from a 2016 survey of AI/ML
researchers (Grace, Salvatier, Dafoe, Zhang, & Evans, 2018) and a 2018 survey
of the US public (Zhang & Dafoe, 2020). We find that AI/ML researchers place
high levels of trust in international organizations and scientific
organizations to shape the development and use of AI in the public interest;
moderate trust in most Western tech companies; and low trust in national
militaries, Chinese tech companies, and Facebook. While the respondents were
overwhelmingly opposed to AI/ML researchers working on lethal autonomous
weapons, they are less opposed to researchers working on other military
applications of AI, particularly logistics algorithms. A strong majority of
respondents think that AI safety research should be prioritized and that ML
institutions should conduct pre-publication review to assess potential harms.
Being closer to the technology itself, AI/ML re-searchers are well placed to
highlight new risks and develop technical solutions, so this novel attempt to
measure their attitudes has broad relevance. The findings should help to
improve how researchers, private sector executives, and policymakers think
about regulations, governance frameworks, guiding principles, and national and
international governance strategies for AI.

Autonomous Driving (AD) systems rely on AI components to make safety and
correct driving decisions. Unfortunately, today's AI algorithms are known to be
generally vulnerable to adversarial attacks. However, for such AI
component-level vulnerabilities to be semantically impactful at the system
level, it needs to address non-trivial semantic gaps both (1) from the
system-level attack input spaces to those at AI component level, and (2) from
AI component-level attack impacts to those at the system level. In this paper,
we define such research space as semantic AI security as opposed to generic AI
security. Over the past 5 years, increasingly more research works are performed
to tackle such semantic AI security challenges in AD context, which has started
to show an exponential growth trend.
  In this paper, we perform the first systematization of knowledge of such
growing semantic AD AI security research space. In total, we collect and
analyze 53 such papers, and systematically taxonomize them based on research
aspects critical for the security field. We summarize 6 most substantial
scientific gaps observed based on quantitative comparisons both vertically
among existing AD AI security works and horizontally with security works from
closely-related domains. With these, we are able to provide insights and
potential future directions not only at the design level, but also at the
research goal, methodology, and community levels. To address the most critical
scientific methodology-level gap, we take the initiative to develop an
open-source, uniform, and extensible system-driven evaluation platform, named
PASS, for the semantic AD AI security research community. We also use our
implemented platform prototype to showcase the capabilities and benefits of
such a platform using representative semantic AD AI attacks.

Compared with chest X-ray (CXR) imaging, which is a single image projected
from the front of the patient, chest digital tomosynthesis (CDTS) imaging can
be more advantageous for lung lesion detection because it acquires multiple
images projected from multiple angles of the patient. Various clinical
comparative analysis and verification studies have been reported to demonstrate
this, but there were no artificial intelligence (AI)-based comparative analysis
studies. Existing AI-based computer-aided detection (CAD) systems for lung
lesion diagnosis have been developed mainly based on CXR images; however,
CAD-based on CDTS, which uses multi-angle images of patients in various
directions, has not been proposed and verified for its usefulness compared to
CXR-based counterparts. This study develops/tests a CDTS-based AI CAD system to
detect lung lesions to demonstrate performance improvements compared to
CXR-based AI CAD. We used multiple projection images as input for the
CDTS-based AI model and a single-projection image as input for the CXR-based AI
model to fairly compare and evaluate the performance between models. The
proposed CDTS-based AI CAD system yielded sensitivities of 0.782 and 0.785 and
accuracies of 0.895 and 0.837 for the performance of detecting tuberculosis and
pneumonia, respectively, against normal subjects. These results show higher
performance than sensitivities of 0.728 and 0.698 and accuracies of 0.874 and
0.826 for detecting tuberculosis and pneumonia through the CXR-based AI CAD,
which only uses a single projection image in the frontal direction. We found
that CDTS-based AI CAD improved the sensitivity of tuberculosis and pneumonia
by 5.4% and 8.7% respectively, compared to CXR-based AI CAD without loss of
accuracy. Therefore, we comparatively prove that CDTS-based AI CAD technology
can improve performance more than CXR, enhancing the clinical applicability of
CDTS.

This paper proposes the innovative concept of "human factors science" to
characterize engineering psychology, human factors engineering, human-computer
interaction, and other similar fields. Although the perspectives in these
fields differ, they share a common approach: "human-centered design." In the AI
era, the human-machine relationship presents a trans-era evolution to "human-AI
teaming." The change has raised challenges for human factors science,
compelling us to re-examine current research paradigms and agendas. Based on
our previous work, this paper proposes three research paradigms: (1) human-AI
joint cognitive systems: this regards an intelligent agent as a cognitive agent
with a certain level of cognitive capabilities. A human-AI system can be
characterized as a joint cognitive system in which humans and intelligent
agents work as teammates for collaboration; (2) human-AI joint cognitive
ecosystems: an intelligent ecosystem with multiple human-AI systems can be
represented as a human-AI joint cognitive ecosystem. The overall performance of
the ecosystem depends on optima collaboration and design across the multiple
human-AI systems; (3) intelligent sociotechnical systems (iSTS): human-AI
systems are design, developed, and deployed in an iSTS environment. The
successful design, development, and deployment of a human-AI system within an
iSTS environment depends on the synergistic optimization between the
subsystems. This paper looks forward to the future research agenda of human
factors science from three aspects: human-AI interaction, intelligent
human-machine interface, and human-AI teaming. Analyses show that the three new
research paradigms will benefit future research in human factors science. We
believe the proposed research paradigms and the future research agenda will
mutually promote each other, further advancing human factors science in the AI
era.

Evaluating human-AI decision-making systems is an emerging challenge as new
ways of combining multiple AI models towards a specific goal are proposed every
day. As humans interact with AI in decision-making systems, multiple factors
may be present in a task including trust, interpretability, and explainability,
amongst others. In this context, this work proposes a retrospective method to
support a more holistic understanding of how people interact with and connect
multiple AI models and combine multiple outputs in human-AI decision-making
systems. The method consists of employing a retrospective end-user walkthrough
with the objective of providing support to HCI practitioners so that they may
gain an understanding of the higher order cognitive processes in place and the
role that AI model outputs play in human-AI decision-making. The method was
qualitatively assessed with 29 participants (four participants in a pilot
phase; 25 participants in the main user study) interacting with a human-AI
decision-making system in the context of financial decision-making. The system
combines visual analytics, three AI models for revenue prediction, AI-supported
analogues analysis, and hypothesis testing using external news and natural
language processing to provide multiple means for comparing companies. Beyond
results on tasks and usability problems, outcomes presented suggest that the
method is promising in highlighting why AI models are ignored, used, or
trusted, and how future interactions are planned. We suggest that HCI
practitioners researching human-AI interaction can benefit by adding this step
to user studies in a debriefing stage as a retrospective Thinking-Aloud
protocol would be applied, but with emphasis on revisiting tasks and
understanding why participants ignored or connected predictions while
performing a task.

With growing expectations to use AI-based educational technology (AI-EdTech)
to improve students' learning outcomes and enrich teaching practice, teachers
play a central role in the adoption of AI-EdTech in classrooms. Teachers'
willingness to accept vulnerability by integrating technology into their
everyday teaching practice, that is, their trust in AI-EdTech, will depend on
how much they expect it to benefit them versus how many concerns it raises for
them. In this study, we surveyed 508 K-12 teachers across six countries on four
continents to understand which teacher characteristics shape teachers' trust in
AI-EdTech, and its proposed antecedents, perceived benefits and concerns about
AI-EdTech. We examined a comprehensive set of characteristics including
demographic and professional characteristics (age, gender, subject, years of
experience, etc.), cultural values (Hofstede's cultural dimensions), geographic
locations (Brazil, Israel, Japan, Norway, Sweden, USA), and psychological
factors (self-efficacy and understanding). Using multiple regression analysis,
we found that teachers with higher AI-EdTech self-efficacy and AI understanding
perceive more benefits, fewer concerns, and report more trust in AI-EdTech. We
also found geographic and cultural differences in teachers' trust in AI-EdTech,
but no demographic differences emerged based on their age, gender, or level of
education. The findings provide a comprehensive, international account of
factors associated with teachers' trust in AI-EdTech. Efforts to raise
teachers' understanding of, and trust in AI-EdTech, while considering their
cultural values are encouraged to support its adoption in K-12 education.

Creativity serves as a cornerstone for societal progress and innovation. With
the rise of advanced generative AI models capable of tasks once reserved for
human creativity, the study of AI's creative potential becomes imperative for
its responsible development and application. In this paper, we prove in theory
that AI can be as creative as humans under the condition that it can properly
fit the data generated by human creators. Therefore, the debate on AI's
creativity is reduced into the question of its ability to fit a sufficient
amount of data. To arrive at this conclusion, this paper first addresses the
complexities in defining creativity by introducing a new concept called
Relative Creativity. Rather than attempting to define creativity universally,
we shift the focus to whether AI can match the creative abilities of a
hypothetical human. The methodological shift leads to a statistically
quantifiable assessment of AI's creativity, term Statistical Creativity. This
concept, statistically comparing the creative abilities of AI with those of
specific human groups, facilitates theoretical exploration of AI's creative
potential. Our analysis reveals that by fitting extensive conditional data
without marginalizing out the generative conditions, AI can emerge as a
hypothetical new creator. The creator possesses the same creative abilities on
par with the human creators it was trained on. Building on theoretical
findings, we discuss the application in prompt-conditioned autoregressive
models, providing a practical means for evaluating creative abilities of
generative AI models, such as Large Language Models (LLMs). Additionally, this
study provides an actionable training guideline, bridging the theoretical
quantification of creativity with practical model training.

As artificial intelligence (AI) is playing an increasingly important role in
our society and global economy, AI education and literacy have become necessary
components in college and K-12 education to prepare students for an AI-powered
society. However, current AI curricula have not yet been made accessible and
engaging enough for students and schools from all socio-economic backgrounds
with different educational goals. In this work, we developed an open-source
learning module for college and high school students, which allows students to
build their own robot companion from the ground up. This open platform can be
used to provide hands-on experience and introductory knowledge about various
aspects of AI, including robotics, machine learning (ML), software engineering,
and mechanical engineering. Because of the social and personal nature of a
socially assistive robot companion, this module also puts a special emphasis on
human-centered AI, enabling students to develop a better understanding of
human-AI interaction and AI ethics through hands-on learning activities. With
open-source documentation, assembling manuals and affordable materials,
students from different socio-economic backgrounds can personalize their
learning experience based on their individual educational goals. To evaluate
the student-perceived quality of our module, we conducted a usability testing
workshop with 15 college students recruited from a minority-serving
institution. Our results indicate that our AI module is effective,
easy-to-follow, and engaging, and it increases student interest in studying
AI/ML and robotics in the future. We hope that this work will contribute toward
accessible and engaging AI education in human-AI interaction for college and
high school students.

Interactive segmentation, an integration of AI algorithms and human
expertise, premises to improve the accuracy and efficiency of curating
large-scale, detailed-annotated datasets in healthcare. Human experts revise
the annotations predicted by AI, and in turn, AI improves its predictions by
learning from these revised annotations. This interactive process continues to
enhance the quality of annotations until no major revision is needed from
experts. The key challenge is how to leverage AI predicted and expert revised
annotations to iteratively improve the AI. Two problems arise: (1) The risk of
catastrophic forgetting--the AI tends to forget the previously learned classes
if it is only retrained using the expert revised classes. (2) Computational
inefficiency when retraining the AI using both AI predicted and expert revised
annotations; moreover, given the dominant AI predicted annotations in the
dataset, the contribution of newly revised annotations--often account for a
very small fraction--to the AI training remains marginal. This paper proposes
Continual Tuning to address the problems from two perspectives: network design
and data reuse. Firstly, we design a shared network for all classes followed by
class-specific networks dedicated to individual classes. To mitigate
forgetting, we freeze the shared network for previously learned classes and
only update the class-specific network for revised classes. Secondly, we reuse
a small fraction of data with previous annotations to avoid over-computing. The
selection of such data relies on the importance estimate of each data. The
importance score is computed by combining the uncertainty and consistency of AI
predictions. Our experiments demonstrate that Continual Tuning achieves a speed
16x greater than repeatedly training AI from scratch without compromising the
performance.

The African continent lacks enough qualified teachers which hampers the
provision of adequate learning support. An AI could potentially augment the
efforts of the limited number of teachers, leading to better learning outcomes.
Towards that end, this work describes and evaluates the first key output for
the NSMQ AI Grand Challenge, which proposes a robust, real-world benchmark for
such an AI: "Build an AI to compete live in Ghana's National Science and Maths
Quiz (NSMQ) competition and win - performing better than the best contestants
in all rounds and stages of the competition". The NSMQ is an annual live
science and mathematics competition for senior secondary school students in
Ghana in which 3 teams of 2 students compete by answering questions across
biology, chemistry, physics, and math in 5 rounds over 5 progressive stages
until a winning team is crowned for that year. In this work, we built Brilla
AI, an AI contestant that we deployed to unofficially compete remotely and live
in the Riddles round of the 2023 NSMQ Grand Finale, the first of its kind in
the 30-year history of the competition. Brilla AI is currently available as a
web app that livestreams the Riddles round of the contest, and runs 4 machine
learning systems: (1) speech to text (2) question extraction (3) question
answering and (4) text to speech that work together in real-time to quickly and
accurately provide an answer, and then say it with a Ghanaian accent. In its
debut, our AI answered one of the 4 riddles ahead of the 3 human contesting
teams, unofficially placing second (tied). Improvements and extensions of this
AI could potentially be deployed to offer science tutoring to students and
eventually enable millions across Africa to have one-on-one learning
interactions, democratizing science education.

As artificial intelligence continues its unprecedented global expansion,
accompanied by a proliferation of benefits, an increasing apprehension about
the privacy and security implications of AI-enabled systems emerges. The
pivotal question of effectively controlling AI development at both
jurisdictional and organizational levels has become a prominent theme in
contemporary discourse. While the European Parliament and Council have taken a
decisive step by reaching a political agreement on the EU AI Act, the first
comprehensive AI law, organizations still find it challenging to adapt to the
fast-evolving AI landscape, lacking a universal tool for evaluating the privacy
and security dimensions of their AI models and systems. In response to this
critical challenge, this study conducts a systematic literature review spanning
the years 2020 to 2023, with a primary focus on establishing a unified
definition of key concepts in AI Ethics, particularly emphasizing the domains
of privacy and security. Through the synthesis of knowledge extracted from the
SLR, this study presents a conceptual framework tailored for privacy- and
security-aware AI systems. This framework is designed to assist diverse
stakeholders, including organizations, academic institutions, and governmental
bodies, in both the development and critical assessment of AI systems.
Essentially, the proposed framework serves as a guide for ethical
decision-making, fostering an environment wherein AI is developed and utilized
with a strong commitment to ethical principles. In addition, the study unravels
the key issues and challenges surrounding the privacy and security dimensions,
delineating promising avenues for future research, thereby contributing to the
ongoing dialogue on the globalization and democratization of AI ethics.

PK Dick once asked "Do Androids Dream of Electric Sheep?" In video games, a
similar question could be asked of non-player characters: Do NPCs have dreams?
Can they live and change as humans do? Can NPCs have personalities, and can
these develop through interactions with players, other NPCs, and the world
around them? Despite advances in personality AI for games, most NPCs are still
undeveloped and undeveloping, reacting with flat affect and predictable
routines that make them far less than human--in fact, they become little more
than bits of the scenery that give out parcels of information. This need not be
the case. Extreme AI, a psychology-based personality engine, creates adaptive
NPC personalities. Originally developed as part of the thesis "NPCs as People:
Using Databases and Behaviour Trees to Give Non-Player Characters Personality,"
Extreme AI is now a fully functioning personality engine using all thirty
facets of the Five Factor model of personality and an AI system that is live
throughout gameplay. This paper discusses the research leading to Extreme AI;
develops the ideas found in that thesis; discusses the development of other
personality engines; and provides examples of Extreme AI's use in two game
demos.

In this work, we present and analyze reported failures of artificially
intelligent systems and extrapolate our analysis to future AIs. We suggest that
both the frequency and the seriousness of future AI failures will steadily
increase. AI Safety can be improved based on ideas developed by cybersecurity
experts. For narrow AIs safety failures are at the same, moderate, level of
criticality as in cybersecurity, however for general AI, failures have a
fundamentally different impact. A single failure of a superintelligent system
may cause a catastrophic event without a chance for recovery. The goal of
cybersecurity is to reduce the number of successful attacks on the system; the
goal of AI Safety is to make sure zero attacks succeed in bypassing the safety
mechanisms. Unfortunately, such a level of performance is unachievable. Every
security system will eventually fail; there is no such thing as a 100% secure
system.

There is a growing focus on how to design safe artificial intelligent (AI)
agents. As systems become more complex, poorly specified goals or control
mechanisms may cause AI agents to engage in unwanted and harmful outcomes. Thus
it is necessary to design AI agents that follow initial programming intentions
as the program grows in complexity. How to specify these initial intentions has
also been an obstacle to designing safe AI agents. Finally, there is a need for
the AI agent to have redundant safety mechanisms to ensure that any programming
errors do not cascade into major problems. Humans are autonomous intelligent
agents that have avoided these problems and the present manuscript argues that
by understanding human self-regulation and goal setting, we may be better able
to design safe AI agents. Some general principles of human self-regulation are
outlined and specific guidance for AI design is given.

Little by little, newspapers are revealing the bright future that Artificial
Intelligence (AI) is building. Intelligent machines will help everywhere.
However, this bright future has a dark side: a dramatic job market contraction
before its unpredictable transformation. Hence, in a near future, large numbers
of job seekers will need financial support while catching up with these novel
unpredictable jobs. This possible job market crisis has an antidote inside. In
fact, the rise of AI is sustained by the biggest knowledge theft of the recent
years. Learning AI machines are extracting knowledge from unaware skilled or
unskilled workers by analyzing their interactions. By passionately doing their
jobs, these workers are digging their own graves.
  In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI)
as a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward
aware and unaware knowledge producers with a different scheme: decisions of AI
systems generating revenues will repay the legitimate owners of the knowledge
used for taking those decisions. As modern Robin Hoods, HIT-AI researchers
should fight for a fairer Artificial Intelligence that gives back what it
steals.

Recent advances with in-memory columnar database techniques have increased
the performance of analytical queries on very large databases and data
warehouses. At the same time, advances in artificial intelligence (AI)
algorithms have increased the ability to analyze data. We use the term AI to
encompass both Deep Learning (DL or neural network) and Machine Learning (ML
aka Big Data analytics). Our exploration of the AI full stack has led us to a
cross-stack columnar database innovation that efficiently creates features for
AI analytics. The innovation is to create Augmented Dictionary Values (ADVs) to
add to existing columnar database dictionaries in order to increase the
efficiency of featurization by minimizing data movement and data duplication.
We show how various forms of featurization (feature selection, feature
extraction, and feature creation) can be efficiently calculated in a columnar
database. The full stack AI investigation has also led us to propose an
integrated columnar database and AI architecture. This architecture has
information flows and feedback loops to improve the whole analytics cycle
during multiple iterations of extracting data from the data sources,
featurization, and analysis.

Machine learning advances have afforded an increase in algorithms capable of
creating art, music, stories, games, and more. However, it is not yet
well-understood how machine learning algorithms might best collaborate with
people to support creative expression. To investigate how practicing designers
perceive the role of AI in the creative process, we developed a game level
design tool for Super Mario Bros.-style games with a built-in AI level
designer. In this paper we discuss our design of the Morai Maker intelligent
tool through two mixed-methods studies with a total of over one-hundred
participants. Our findings are as follows: (1) level designers vary in their
desired interactions with, and role of, the AI, (2) the AI prompted the level
designers to alter their design practices, and (3) the level designers
perceived the AI as having potential value in their design practice, varying
based on their desired role for the AI.

Current advances in research, development and application of artificial
intelligence (AI) systems have yielded a far-reaching discourse on AI ethics.
In consequence, a number of ethics guidelines have been released in recent
years. These guidelines comprise normative principles and recommendations aimed
to harness the "disruptive" potentials of new AI technologies. Designed as a
comprehensive evaluation, this paper analyzes and compares these guidelines
highlighting overlaps but also omissions. As a result, I give a detailed
overview of the field of AI ethics. Finally, I also examine to what extent the
respective ethical principles and values are implemented in the practice of
research, development and application of AI systems - and how the effectiveness
in the demands of AI ethics can be improved.

Intelligence can be defined as a predominantly human ability to accomplish
tasks that are generally hard for computers and animals. Artificial
Intelligence [AI] is a field attempting to accomplish such tasks with
computers. AI is becoming increasingly widespread, as are claims of its
relationship with Biological Intelligence. Often these claims are made to imply
higher chances of a given technology succeeding, working on the assumption that
AI systems which mimic the mechanisms of Biological Intelligence should be more
successful.
  In this article I will discuss the similarities and differences between AI
and the extent of our knowledge about the mechanisms of intelligence in
biology, especially within humans. I will also explore the validity of the
assumption that biomimicry in AI systems aids their advancement, and I will
argue that existing similarity to biological systems in the way Artificial
Neural Networks [ANNs] tackle tasks is due to design decisions, rather than
inherent similarity of underlying mechanisms. This article is aimed at people
who understand the basics of AI (especially ANNs), and would like to be better
able to evaluate the often wild claims about the value of biomimicry in AI.

The fast pace of artificial intelligence (AI) and automation is propelling
strategists to reshape their business models. This is fostering the integration
of AI in the business processes but the consequences of this adoption are
underexplored and need attention. This paper focuses on the overall impact of
AI on businesses - from research, innovation, market deployment to future
shifts in business models. To access this overall impact, we design a
three-dimensional research model, based upon the Neo-Schumpeterian economics
and its three forces viz. innovation, knowledge, and entrepreneurship. The
first dimension deals with research and innovation in AI. In the second
dimension, we explore the influence of AI on the global market and the
strategic objectives of the businesses and finally, the third dimension
examines how AI is shaping business contexts. Additionally, the paper explores
AI implications on actors and its dark sides.

The integration of Artificial Intelligence (AI) into weapon systems is one of
the most consequential tactical and strategic decisions in the history of
warfare. Current AI development is a remarkable combination of accelerating
capability, hidden decision mechanisms, and decreasing costs. Implementation of
these systems is in its infancy and exists on a spectrum from resilient and
flexible to simplistic and brittle. Resilient systems should be able to
effectively handle the complexities of a high-dimensional battlespace.
Simplistic AI implementations could be manipulated by an adversarial AI that
identifies and exploits their weaknesses.
  In this paper, we present a framework for understanding the development of
dynamic AI/ML systems that interactively and continuously adapt to their user's
needs. We explore the implications of increasingly capable AI in the kill chain
and how this will lead inevitably to a fully automated, always on system,
barring regulation by treaty. We examine the potential of total integration of
cyber and physical security and how this likelihood must inform the development
of AI-enabled systems with respect to the "fog of war", human morals, and
ethics.

Artificial intelligence (AI) is becoming increasingly widespread in system
development endeavors. As AI systems affect various stakeholders due to their
unique nature, the growing influence of these systems calls for ethical
considerations. Academic discussion and practical examples of autonomous system
failures have highlighted the need for implementing ethics in software
development. However, research on methods and tools for implementing ethics
into AI system design and development in practice is still lacking. This paper
begins to address this focal problem by providing elements needed for producing
a baseline for ethics in AI based software development. We do so by means of an
industrial multiple case study on AI systems development in the healthcare
sector. Using a research model based on extant, conceptual AI ethics
literature, we explore the current state of practice out on the field in the
absence of formal methods and tools for ethically aligned design.

Artificial intelligence (AI) is a technology which is increasingly being
utilised in society and the economy worldwide, and its implementation is
planned to become more prevalent in coming years. AI is increasingly being
embedded in our lives, supplementing our pervasive use of digital technologies.
But this is being accompanied by disquiet over problematic and dangerous
implementations of AI, or indeed, even AI itself deciding to do dangerous and
problematic actions, especially in fields such as the military, medicine and
criminal justice. These developments have led to concerns about whether and how
AI systems adhere, and will adhere to ethical standards. These concerns have
stimulated a global conversation on AI ethics, and have resulted in various
actors from different countries and sectors issuing ethics and governance
initiatives and guidelines for AI. Such developments form the basis for our
research in this report, combining our international and interdisciplinary
expertise to give an insight into what is happening in Australia, China,
Europe, India and the US.

Recent progress in artificial intelligence (AI) using deep learning
techniques has triggered its wide-scale use across a broad range of
applications. These systems can already perform tasks such as natural language
processing of voice and text, visual recognition, question-answering,
recommendations and decision support. However, at the current level of
maturity, the use of an AI component in mission-critical or safety-critical
applications can have unexpected consequences. Consequently, serious concerns
about reliability, repeatability, trust, and maintainability of AI applications
remain. As AI becomes pervasive despite its shortcomings, more systematic ways
of approaching AI software development and certification are needed. These
fundamental aspects establish the need for a discipline on "AI Engineering".
This paper presents the current perspective of relevant AI engineering concepts
and some key challenges that need to be overcome to make significant progress
in this important area.

The terms 'human-level artificial intelligence' and 'artificial general
intelligence' are widely used to refer to the possibility of advanced
artificial intelligence (AI) with potentially extreme impacts on society. These
terms are poorly defined and do not necessarily indicate what is most important
with respect to future societal impacts. We suggest that the term
'transformative AI' is a helpful alternative, reflecting the possibility that
advanced AI systems could have very large impacts on society without reaching
human-level cognitive abilities. To be most useful, however, more analysis of
what it means for AI to be 'transformative' is needed. In this paper, we
propose three different levels on which AI might be said to be transformative,
associated with different levels of societal change. We suggest that these
distinctions would improve conversations between policy makers and decision
makers concerning the mid- to long-term impacts of advances in AI. Further, we
feel this would have a positive effect on strategic foresight efforts involving
advanced AI, which we expect to illuminate paths to alternative futures. We
conclude with a discussion of the benefits of our new framework and by
highlighting directions for future work in this area.

Recent concern about harms of information technologies motivate consideration
of regulatory action to forestall or constrain certain developments in the
field of artificial intelligence (AI). However, definitional ambiguity hampers
the possibility of conversation about this urgent topic of public concern.
Legal and regulatory interventions require agreed-upon definitions, but
consensus around a definition of AI has been elusive, especially in policy
conversations. With an eye towards practical working definitions and a broader
understanding of positions on these issues, we survey experts and review
published policy documents to examine researcher and policy-maker conceptions
of AI. We find that while AI researchers favor definitions of AI that emphasize
technical functionality, policy-makers instead use definitions that compare
systems to human thinking and behavior. We point out that definitions adhering
closely to the functionality of AI systems are more inclusive of technologies
in use today, whereas definitions that emphasize human-like capabilities are
most applicable to hypothetical future technologies. As a result of this gap,
ethical and regulatory efforts may overemphasize concern about future
technologies at the expense of pressing issues with existing deployed
technologies.

As the influence and use of artificial intelligence (AI) have grown and its
transformative potential has become more apparent, many questions have been
raised regarding the economic, political, social, and ethical implications of
its use. Public opinion plays an important role in these discussions,
influencing product adoption, commercial development, research funding, and
regulation. In this paper we present results of an in-depth survey of public
opinion of artificial intelligence conducted with 10,005 respondents spanning
eight countries and six continents. We report widespread perception that AI
will have significant impact on society, accompanied by strong support for the
responsible development and use of AI, and also characterize the public's
sentiment towards AI with four key themes (exciting, useful, worrying, and
futuristic) whose prevalence distinguishes response to AI in different
countries.

With the maturing of AI and multiagent systems research, we have a tremendous
opportunity to direct these advances towards addressing complex societal
problems. In pursuit of this goal of AI for Social Impact, we as AI researchers
must go beyond improvements in computational methodology; it is important to
step out in the field to demonstrate social impact. To this end, we focus on
the problems of public safety and security, wildlife conservation, and public
health in low-resource communities, and present research advances in multiagent
systems to address one key cross-cutting challenge: how to effectively deploy
our limited intervention resources in these problem domains. We present case
studies from our deployments around the world as well as lessons learned that
we hope are of use to researchers who are interested in AI for Social Impact.
In pushing this research agenda, we believe AI can indeed play an important
role in fighting social injustice and improving society.

The artificial intelligence community (AI) has recently engaged in activism
in relation to their employers, other members of the community, and their
governments in order to shape the societal and ethical implications of AI. It
has achieved some notable successes, but prospects for further political
organising and activism are uncertain. We survey activism by the AI community
over the last six years; apply two analytical frameworks drawing upon the
literature on epistemic communities, and worker organising and bargaining; and
explore what they imply for the future prospects of the AI community. Success
thus far has hinged on a coherent shared culture, and high bargaining power due
to the high demand for a limited supply of AI talent. Both are crucial to the
future of AI activism and worthy of sustained attention.

Given AI's growing role in modeling and improving decision-making, how and
when to present users with feedback is an urgent topic to address. We
empirically examined the effect of feedback from false AI on moral
decision-making about donor kidney allocation. We found some evidence that
judgments about whether a patient should receive a kidney can be influenced by
feedback about participants' own decision-making perceived to be given by AI,
even if the feedback is entirely random. We also discovered different effects
between assessments presented as being from human experts and assessments
presented as being from AI.

The automatic identification system (AIS) reports vessels' static and dynamic
information, which are essential for maritime traffic situation awareness.
However, AIS transponders can be switched off to hide suspicious activities,
such as illegal fishing, or piracy. Therefore, this paper uses real world AIS
data to analyze the possibility of successful detection of various anomalies in
the maritime domain. We propose a multi-class artificial neural network
(ANN)-based anomaly detection framework to classify intentional and
non-intentional AIS on-off switching anomalies. The multi-class anomaly
framework captures AIS message dropouts due to various reasons, e.g., channel
effects or intentional one for carrying illegal activities. We extract
position, speed, course and timing information from real world AIS data, and
use them to train a 2-class (normal and anomaly) and a 3-class (normal, power
outage and anomaly) anomaly detection models. Our results show that the models
achieve around 99.9% overall accuracy, and are able to classify a test sample
in the order of microseconds.

Organizations are rapidly deploying artificial intelligence (AI) systems to
manage their workers. However, AI has been found at times to be unfair to
workers. Unfairness toward workers has been associated with decreased worker
effort and increased worker turnover. To avoid such problems, AI systems must
be designed to support fairness and redress instances of unfairness. Despite
the attention related to AI unfairness, there has not been a theoretical and
systematic approach to developing a design agenda. This paper addresses the
issue in three ways. First, we introduce the organizational justice theory,
three different fairness types (distributive, procedural, interactional), and
the frameworks for redressing instances of unfairness (retributive justice,
restorative justice). Second, we review the design literature that specifically
focuses on issues of AI fairness in organizations. Third, we propose a design
agenda for AI fairness in organizations that applies each of the fairness types
to organizational scenarios. Then, the paper concludes with implications for
future research.

High availability of data is responsible for the current trends in Artificial
Intelligence (AI) and Machine Learning (ML). However, high-grade datasets are
reluctantly shared between actors because of lacking trust and fear of losing
control. Provenance tracing systems are a possible measure to build trust by
improving transparency. Especially the tracing of AI assets along complete AI
value chains bears various challenges such as trust, privacy, confidentiality,
traceability, and fair remuneration. In this paper we design a graph-based
provenance model for AI assets and their relations within an AI value chain.
Moreover, we propose a protocol to exchange AI assets securely to selected
parties. The provenance model and exchange protocol are then combined and
implemented as a smart contract on a permission-less blockchain. We show how
the smart contract enables the tracing of AI assets in an existing industry use
case while solving all challenges. Consequently, our smart contract helps to
increase traceability and transparency, encourages trust between actors and
thus fosters collaboration between them.

Interpretability of machine learning models has gained more and more
attention among researchers in the artificial intelligence (AI) and
human-computer interaction (HCI) communities. Most existing work focuses on
decision making, whereas we consider knowledge discovery. In particular, we
focus on evaluating AI-discovered knowledge/rules in the arts and humanities.
From a specific scenario, we present an experimental procedure to collect and
assess human-generated verbal interpretations of AI-generated music
theory/rules rendered as sophisticated symbolic/numeric objects. Our goal is to
reveal both the possibilities and the challenges in such a process of decoding
expressive messages from AI sources. We treat this as a first step towards 1)
better design of AI representations that are human interpretable and 2) a
general methodology to evaluate interpretability of AI-discovered knowledge
representations.

Like any technology, AI systems come with inherent risks and potential
benefits. It comes with potential disruption of established norms and methods
of work, societal impacts and externalities. One may think of the adoption of
technology as a form of social contract, which may evolve or fluctuate in time,
scale, and impact. It is important to keep in mind that for AI, meeting the
expectations of this social contract is critical, because recklessly driving
the adoption and implementation of unsafe, irresponsible, or unethical AI
systems may trigger serious backlash against industry and academia involved
which could take decades to resolve, if not actually seriously harm society.
For the purpose of this paper, we consider that a social contract arises when
there is sufficient consensus within society to adopt and implement this new
technology. As such, to enable a social contract to arise for the adoption and
implementation of AI, developing: 1) A socially accepted purpose, through 2) A
safe and responsible method, with 3) A socially aware level of risk involved,
for 4) A socially beneficial outcome, is key.

Reuse is a common system architecture approach that seeks to instantiate a
system architecture with existing components. However, reusing components with
AI capabilities might introduce new risks as there is currently no framework
that guides the selection of necessary information to assess their portability
to operate in a system different than the one for which the component was
originally purposed. We know from SW-intensive systems that AI algorithms are
generally fragile and behave unexpectedly to changes in context and boundary
conditions. The question we address in this paper is, what type of information
should be captured in the Interface Control Document (ICD) of an AI-enabled
system or component to assess its compatibility with a system for which it was
not designed originally. We present ongoing work on establishing an interface
description template that captures the main information of an AI-enabled
component to facilitate its adequate reuse across different systems and
operational contexts. Our work is inspired by Google's Model Card concept,
which was developed with the same goal but focused on the reusability of AI
algorithms. We extend that concept to address system-level autonomy
capabilities of AI-enabled cyber-physical systems.

Artificial Intelligence (AI) is increasingly being applied to law and a
myriad of legal tasks amid attempts to bolster AI Legal Reasoning (AILR)
autonomous capabilities. A major question that has generally been unaddressed
involves how we will know when AILR has achieved autonomous capacities. The
field of AI has grappled with similar quandaries over how to assess the
attainment of Artificial General Intelligence (AGI), a persistently discussed
issue among scholars since the inception of AI, with the Turing Test communally
being considered as the bellwether for ascertaining such matters. This paper
proposes a variant of the Turing Test that is customized for specific use in
the AILR realm, including depicting how this famous gold standard of AI
fulfillment can be robustly applied across the autonomous levels of AI Legal
Reasoning.

Test, Evaluation, Verification, and Validation (TEVV) for Artificial
Intelligence (AI) is a challenge that threatens to limit the economic and
societal rewards that AI researchers have devoted themselves to producing. A
central task of TEVV for AI is estimating brittleness, where brittleness
implies that the system functions well within some bounds and poorly outside of
those bounds. This paper argues that neither of those criteria are certain of
Deep Neural Networks. First, highly touted AI successes (eg. image
classification and speech recognition) are orders of magnitude more
failure-prone than are typically certified in critical systems even within
design bounds (perfectly in-distribution sampling). Second, performance falls
off only gradually as inputs become further Out-Of-Distribution (OOD). Enhanced
emphasis is needed on designing systems that are resilient despite
failure-prone AI components as well as on evaluating and improving OOD
performance in order to get AI to where it can clear the challenging hurdles of
TEVV and certification.

Artificial Intelligence (AI) education is an increasingly popular topic area
for K-12 teachers. However, little research has investigated how AI education
can be designed to be more accessible to all learners. We organized co-design
workshops with 15 K-12 teachers to identify opportunities to integrate AI
education into core curriculum to leverage learners' interests. During the
co-design workshops, teachers and researchers co-created lesson plans where AI
concepts were embedded into various core subjects. We found that K-12 teachers
need additional scaffolding in the curriculum to facilitate ethics and data
discussions, and value supports for learner engagement, collaboration, and
reflection. We identify opportunities for researchers and teachers to
collaborate to make AI education more accessible, and present an exemplar
lesson plan that shows entry points for teaching AI in non-computing subjects.
We also reflect on co-designing with K-12 teachers in a remote setting.

Game AI competitions are important to foster research and development on Game
AI and AI in general. These competitions supply different challenging problems
that can be translated into other contexts, virtual or real. They provide
frameworks and tools to facilitate the research on their core topics and
provide means for comparing and sharing results. A competition is also a way to
motivate new researchers to study these challenges. In this document, we
present the Geometry Friends Game AI Competition. Geometry Friends is a
two-player cooperative physics-based puzzle platformer computer game. The
concept of the game is simple, though its solving has proven to be difficult.
While the main and apparent focus of the game is cooperation, it also relies on
other AI-related problems such as planning, plan execution, and motion control,
all connected to situational awareness. All of these must be solved in
real-time. In this paper, we discuss the competition and the challenges it
brings, and present an overview of the current solutions.

Artificial intelligent (AI) algorithms, such as deep learning and XGboost,
are used in numerous applications including computer vision, autonomous
driving, and medical diagnostics. The robustness of these AI algorithms is of
great interest as inaccurate prediction could result in safety concerns and
limit the adoption of AI systems. In this paper, we propose a framework based
on design of experiments to systematically investigate the robustness of AI
classification algorithms. A robust classification algorithm is expected to
have high accuracy and low variability under different application scenarios.
The robustness can be affected by a wide range of factors such as the imbalance
of class labels in the training dataset, the chosen prediction algorithm, the
chosen dataset of the application, and a change of distribution in the training
and test datasets. To investigate the robustness of AI classification
algorithms, we conduct a comprehensive set of mixture experiments to collect
prediction performance results. Then statistical analyses are conducted to
understand how various factors affect the robustness of AI classification
algorithms. We summarize our findings and provide suggestions to practitioners
in AI applications.

Current advances in Artificial Intelligence (AI) and Machine Learning (ML)
have achieved unprecedented impact across research communities and industry.
Nevertheless, concerns about trust, safety, interpretability and accountability
of AI were raised by influential thinkers. Many have identified the need for
well-founded knowledge representation and reasoning to be integrated with deep
learning and for sound explainability. Neural-symbolic computing has been an
active area of research for many years seeking to bring together robust
learning in neural networks with reasoning and explainability via symbolic
representations for network models. In this paper, we relate recent and early
research results in neurosymbolic AI with the objective of identifying the key
ingredients of the next wave of AI systems. We focus on research that
integrates in a principled way neural network-based learning with symbolic
knowledge representation and logical reasoning. The insights provided by 20
years of neural-symbolic computing are shown to shed new light onto the
increasingly prominent role of trust, safety, interpretability and
accountability of AI. We also identify promising directions and challenges for
the next decade of AI research from the perspective of neural-symbolic systems.

Conversational Artificial Intelligence (AI) systems have recently
sky-rocketed in popularity and are now used in many applications, from car
assistants to customer support. The development of conversational AI systems is
supported by a large variety of software platforms, all with similar goals, but
different focus points and functionalities. A systematic foundation for
classifying conversational AI platforms is currently lacking. We propose a
framework for assessing the maturity level of conversational AI development
platforms. Our framework is based on a systematic literature review, in which
we extracted common and distinguishing features of various open-source and
commercial (or in-house) platforms. Inspired by language reference frameworks,
we identify different maturity levels that a conversational AI development
platform may exhibit in understanding and responding to user inputs. Our
framework can guide organizations in selecting a conversational AI development
platform according to their needs, as well as helping researchers and platform
developers improving the maturity of their platforms.

Artificial intelligence (AI) has been used to advance different fields, such
as education, healthcare, and finance. However, the application of AI in the
field of project management (PM) has not progressed equally. This paper reports
on a systematic review of the published studies used to investigate the
application of AI in PM. This systematic review identified relevant papers
using Web of Science, Science Direct, and Google Scholar databases. Of the 652
articles found, 58 met the predefined criteria and were included in the review.
Included papers were classified per the following dimensions: PM knowledge
areas, PM processes, and AI techniques. The results indicated that the
application of AI in PM was in its early stages and AI models have not applied
for multiple PM processes especially in processes groups of project stakeholder
management, project procurements management, and project communication
management. However, the most popular PM processes among included papers were
project effort prediction and cost estimation, and the most popular AI
techniques were support vector machines, neural networks, and genetic
algorithms.

As AI-powered systems increasingly mediate consequential decision-making,
their explainability is critical for end-users to take informed and accountable
actions. Explanations in human-human interactions are socially-situated. AI
systems are often socio-organizationally embedded. However, Explainable AI
(XAI) approaches have been predominantly algorithm-centered. We take a
developmental step towards socially-situated XAI by introducing and exploring
Social Transparency (ST), a sociotechnically informed perspective that
incorporates the socio-organizational context into explaining AI-mediated
decision-making. To explore ST conceptually, we conducted interviews with 29 AI
users and practitioners grounded in a speculative design scenario. We suggested
constitutive design elements of ST and developed a conceptual framework to
unpack ST's effect and implications at the technical, decision-making, and
organizational level. The framework showcases how ST can potentially calibrate
trust in AI, improve decision-making, facilitate organizational collective
actions, and cultivate holistic explainability. Our work contributes to the
discourse of Human-Centered XAI by expanding the design space of XAI.

The field of Artificial Intelligence (AI) has undoubtedly received
significant attention in recent years. AI is being adopted to provide solutions
to problems in fields such as medicine, engineering, education, government and
several other domains. In order to analyze the state of the art of research in
the field of AI, we present a systematic literature review focusing on the
Evolution of AI programming languages. We followed the systematic literature
review method by searching relevant databases like SCOPUS, IEEE Xplore and
Google Scholar. EndNote reference manager was used to catalog the relevant
extracted papers. Our search returned a total of 6565 documents, whereof 69
studies were retained. Of the 69 retained studies, 15 documents discussed LISP
programming language, another 34 discussed PROLOG programming language, the
remaining 20 documents were spread between Logic and Object Oriented
Programming (LOOP), ARCHLOG, Epistemic Ontology Language with Constraints
(EOLC), Python, C++, ADA and JAVA programming languages. This review provides
information on the year of implementation, development team, capabilities,
limitations and applications of each of the AI programming languages discussed.
The information in this review could guide practitioners and researchers in AI
to make the right choice of languages to implement their novel AI methods.

Those best-positioned to profit from the proliferation of artificial
intelligence (AI) systems are those with the most economic power. Extant global
inequality has motivated Western institutions to involve more diverse groups in
the development and application of AI systems, including hiring foreign labour
and establishing extra-national data centers and laboratories. However, given
both the propensity of wealth to abet its own accumulation and the lack of
contextual knowledge in top-down AI solutions, we argue that more focus should
be placed on the redistribution of power, rather than just on including
underrepresented groups. Unless more is done to ensure that opportunities to
lead AI development are distributed justly, the future may hold only AI systems
which are unsuited to their conditions of application, and exacerbate
inequality.

Artificial intelligences (AI) are increasingly being embodied and embedded in
the world to carry out tasks and support decision-making with and for people.
Robots, recommender systems, voice assistants, virtual humans - do these
disparate types of embodied AI have something in common? Here we show how they
can manifest as "socially embodied AI." We define this as the state that
embodied AI "circumstantially" take on within interactive contexts when
perceived as both social and agentic by people. We offer a working ontology
that describes how embodied AI can dynamically transition into socially
embodied AI. We propose an ontological heuristic for describing the threshold:
the Tepper line. We reinforce our theoretical work with expert insights from a
card sort workshop. We end with two case studies to illustrate the dynamic and
contextual nature of this heuristic.

The growth in AI is rapidly transforming the structure of economic
production. However, very little is known about how within-AI specialization
may relate to broad-based economic diversification. This paper provides a
data-driven framework to integrate the interconnection between AI-based
specialization with goods and services export specialization to help design
future comparative advantage based on the inherent capabilities of nations.
Using detailed data on private investment in AI and export specialization for
more than 80 countries, we propose a systematic framework to help identify the
connection from AI to goods and service sector specialization. The results are
instructive for nations that aim to harness AI specialization to help guide
sources of future competitive advantage. The operational framework could help
inform the public and private sector to uncover connections with nearby areas
of specialization.

The development of educational AI (AIEd) systems has often been motivated by
their potential to promote educational equity and reduce achievement gaps
across different groups of learners -- for example, by scaling up the benefits
of one-on-one human tutoring to a broader audience, or by filling gaps in
existing educational services. Given these noble intentions, why might AIEd
systems have inequitable impacts in practice? In this chapter, we discuss four
lenses that can be used to examine how and why AIEd systems risk amplifying
existing inequities. Building from these lenses, we then outline possible paths
towards more equitable futures for AIEd, while highlighting debates surrounding
each proposal. In doing so, we hope to provoke new conversations around the
design of equitable AIEd, and to push ongoing conversations in the field
forward.

Artificial Intelligence is becoming part of any technology we use nowadays.
If the AI informs people's decisions, the explanation about AI's outcomes,
results, and behavior becomes a necessary capability. However, the discussion
of XAI features with various stakeholders is not a trivial task. Most of the
available frameworks and methods for XAI focus on data scientists and ML
developers as users. Our research is about XAI for end-users of AI systems. We
argue that we need to discuss XAI early in the AI-system design process and
with all stakeholders. In this work, we aimed at investigating how to
operationalize the discussion about XAI scenarios and opportunities among
designers and developers of AI and its end-users. We took the Signifying
Message as our conceptual tool to structure and discuss XAI scenarios. We
experiment with its use for the discussion of a healthcare AI-System.

Future advances in AI that automate away human labor may have stark
implications for labor markets and inequality. This paper proposes a framework
to analyze the effects of specific types of AI systems on the labor market,
based on how much labor demand they will create versus displace, while taking
into account that productivity gains also make society wealthier and thereby
contribute to additional labor demand. This analysis enables ethically-minded
companies creating or deploying AI systems as well as researchers and
policymakers to take into account the effects of their actions on labor markets
and inequality, and therefore to steer progress in AI in a direction that
advances shared prosperity and an inclusive economic future for all of
humanity.

The widespread utilization of AI systems has drawn attention to the potential
impacts of such systems on society. Of particular concern are the consequences
that prediction errors may have on real-world scenarios, and the trust humanity
places in AI systems. It is necessary to understand how we can evaluate
trustworthiness in AI and how individuals and entities alike can develop
trustworthy AI systems. In this paper, we analyze each element of
trustworthiness and provide a set of 20 guidelines that can be leveraged to
ensure optimal AI functionality while taking into account the greater ethical,
technical, and practical impacts to humanity. Moreover, the guidelines help
ensure that trustworthiness is provable and can be demonstrated, they are
implementation agnostic, and they can be applied to any AI system in any
sector.

The Department of Defense (DoD) has significantly increased its investment in
the design, evaluation, and deployment of Artificial Intelligence and Machine
Learning (AI/ML) capabilities to address national security needs. While there
are numerous AI/ML successes in the academic and commercial sectors, many of
these systems have also been shown to be brittle and nonrobust. In a complex
and ever-changing national security environment, it is vital that the DoD
establish a sound and methodical process to evaluate the performance and
robustness of AI/ML models before these new capabilities are deployed to the
field. This paper reviews the AI/ML development process, highlights common best
practices for AI/ML model evaluation, and makes recommendations to DoD
evaluators to ensure the deployment of robust AI/ML capabilities for national
security needs.

Current AI-driven research in radiology requires resources and expertise that
are often inaccessible to small and resource-limited labs. The clinicians who
are able to participate in AI research are frequently well-funded,
well-staffed, and either have significant experience with AI and computing, or
have access to colleagues or facilities that do. Current imaging data is
clinician-oriented and is not easily amenable to machine learning initiatives,
resulting in inefficient, time consuming, and costly efforts that rely upon a
crew of data engineers and machine learning scientists, and all too often
preclude radiologists from driving AI research and innovation. We present the
system and methodology we have developed to address infrastructure and platform
needs, while reducing the staffing and resource barriers to entry. We emphasize
a data-first and modular approach that streamlines the AI development and
deployment process while providing efficient and familiar interfaces for
radiologists, such that they can be the drivers of new AI innovations.

Artificial intelligence (AI) has significant potential to positively impact
and advance medical imaging, including positron emission tomography (PET)
imaging applications. AI has the ability to enhance and optimize all aspects of
the PET imaging chain from patient scheduling, patient setup, protocoling, data
acquisition, detector signal processing, reconstruction, image processing and
interpretation. AI poses industry-specific challenges which will need to be
addressed and overcome to maximize the future potentials of AI in PET. This
paper provides an overview of these industry-specific challenges for the
development, standardization, commercialization, and clinical adoption of AI,
and explores the potential enhancements to PET imaging brought on by AI in the
near future. In particular, the combination of on-demand image reconstruction,
AI, and custom designed data processing workflows may open new possibilities
for innovation which would positively impact the industry and ultimately
patients.

With the advances in 5G and IoT devices, the industries are vastly adopting
artificial intelligence (AI) techniques for improving classification and
prediction-based services. However, the use of AI also raises concerns
regarding privacy and security that can be misused or leaked. Private AI was
recently coined to address the data security issue by combining AI with
encryption techniques, but existing studies have shown that model inversion
attacks can be used to reverse engineer the images from model parameters. In
this regard, we propose a Federated Learning and Encryption-based Private
(FLEP) AI framework that provides two-tier security for data and model
parameters in an IIoT environment. We proposed a three-layer encryption method
for data security and provide a hypothetical method to secure the model
parameters. Experimental results show that the proposed method achieves better
encryption quality at the expense of slightly increased execution time. We also
highlight several open issues and challenges regarding the FLEP AI framework's
realization.

There is mounting public concern over the influence that AI based systems has
in our society. Coalitions in all sectors are acting worldwide to resist hamful
applications of AI. From indigenous people addressing the lack of reliable
data, to smart city stakeholders, to students protesting the academic
relationships with sex trafficker and MIT donor Jeffery Epstein, the
questionable ethics and values of those heavily investing in and profiting from
AI are under global scrutiny. There are biased, wrongful, and disturbing
assumptions embedded in AI algorithms that could get locked in without
intervention. Our best human judgment is needed to contain AI's harmful impact.
Perhaps one of the greatest contributions of AI will be to make us ultimately
understand how important human wisdom truly is in life on earth.

The objectives of this ongoing research are to build Real-Time AI-Powered
Educational Dashboard (RAED) as a decision support tool for instructors, and to
measure its impact on them while making decisions. Current developments in AI
can be combined with the educational dashboards to make them AI-Powered. Thus,
AI can help in providing recommendations based on the students' performances.
AI-Powered educational dashboards can also assist instructors in tracking
real-time student activities. In this ongoing research, our aim is to develop
the AI component as well as improve the existing design component of the RAED.
Further, we will conduct experiments to study its impact on instructors, and
understand how much they trust RAED to guide them while making decisions. This
paper elaborates on the ongoing research and future direction.

In this paper we outline a proposal for improving the governance of
artificial intelligence (AI) by investing in government capacity to
systematically measure and monitor the capabilities and impacts of AI systems.
If adopted, this would give governments greater information about the AI
ecosystem, equipping them to more effectively direct AI development and
deployment in the most societally and economically beneficial directions. It
would also create infrastructure that could rapidly identify potential threats
or harms that could occur as a consequence of changes in the AI ecosystem, such
as the emergence of strategically transformative capabilities, or the
deployment of harmful systems.
  We begin by outlining the problem which motivates this proposal: in brief,
traditional governance approaches struggle to keep pace with the speed of
progress in AI. We then present our proposal for addressing this problem:
governments must invest in measurement and monitoring infrastructure. We
discuss this proposal in detail, outlining what specific things governments
could focus on measuring and monitoring, and the kinds of benefits this would
generate for policymaking. Finally, we outline some potential pilot projects
and some considerations for implementing this in practice.

We use an autoethnographic case study of a Latinx high school student from an
agricultural community in California to highlight how AI is learned outside
classrooms and how her personal background influenced her social-justice
oriented applications of AI technologies. Applying the concept of learning
pathways from the learning sciences, we argue that redesigning AI education to
be more inclusive with respect to socioeconomic status, ethnoracial identity,
and gender is important in the development of computational projects that
address social-injustice. We also learn about the role of institutions, power
structures, and community as they relate to her journey of learning and
applying AI. The future of AI, its potential to address issues of social
injustice and limiting the negative consequences of its use, will depend on the
participation and voice of students from the most vulnerable communities.

As a promising tool to navigate in the vast chemical space, artificial
intelligence (AI) is leveraged for drug design. From the year 2017 to 2021, the
number of applications of several recent AI models (i.e. graph neural network
(GNN), recurrent neural network (RNN), variation autoencoder (VAE), generative
adversarial network (GAN), flow and reinforcement learning (RL)) in drug design
increases significantly. Many relevant literature reviews exist. However, none
of them provides an in-depth summary of many applications of the recent AI
models in drug design. To complement the existing literature, this survey
includes the theoretical development of the previously mentioned AI models and
detailed summaries of 42 recent applications of AI in drug design. Concretely,
13 of them leverage GNN for molecular property prediction and 29 of them use RL
and/or deep generative models for molecule generation and optimization. In most
cases, the focus of the summary is the models, their variants, and
modifications for specific tasks in drug design. Moreover, 60 additional
applications of AI in molecule generation and optimization are briefly
summarized in a table. Finally, this survey provides a holistic discussion of
the abundant applications so that the tasks, potential solutions, and
challenges in AI-based drug design become evident.

Generative Artificial Intelligence (AI) models are a compelling way to
introduce K-12 students to AI education using an artistic medium, and hence
have drawn attention from K-12 AI educators. Previous Creative AI curricula
mainly focus on Generative Adversarial Networks (GANs) while paying less
attention to Autoregressive Models, Variational Autoencoders (VAEs), or other
generative models, which have since become common in the field of generative
AI. VAEs' latent-space structure and interpolation ability could effectively
ground the interdisciplinary learning of AI, creative arts, and philosophy.
Thus, we designed a lesson to teach high school students about VAEs. We
developed a web-based game and used Plato's cave, a philosophical metaphor, to
introduce how VAEs work. We used a Google Colab notebook for students to
re-train VAEs with their hand-written digits to consolidate their
understandings. Finally, we guided the exploration of creative VAE tools such
as SketchRNN and MusicVAE to draw the connection between what they learned and
real-world applications. This paper describes the lesson design and shares
insights from the pilot studies with 22 students. We found that our approach
was effective in teaching students about a novel AI concept.

Artificial Intelligence (AI) algorithms are increasingly providing decision
making and operational support across multiple domains. AI includes a wide
library of algorithms for different problems. One important notion for the
adoption of AI algorithms into operational decision process is the concept of
assurance. The literature on assurance, unfortunately, conceals its outcomes
within a tangled landscape of conflicting approaches, driven by contradicting
motivations, assumptions, and intuitions. Accordingly, albeit a rising and
novel area, this manuscript provides a systematic review of research works that
are relevant to AI assurance, between years 1985 - 2021, and aims to provide a
structured alternative to the landscape. A new AI assurance definition is
adopted and presented and assurance methods are contrasted and tabulated.
Additionally, a ten-metric scoring system is developed and introduced to
evaluate and compare existing methods. Lastly, in this manuscript, we provide
foundational insights, discussions, future directions, a roadmap, and
applicable recommendations for the development and deployment of AI assurance.

Based on the collective input of Dagstuhl Seminar (21342), this paper
presents a comprehensive discussion on AI methods and capabilities in the
context of edge computing, referred as Edge AI. In a nutshell, we envision Edge
AI to provide adaptation for data-driven applications, enhance network and
radio access, and allow the creation, optimization, and deployment of
distributed AI/ML pipelines with given quality of experience, trust, security
and privacy targets. The Edge AI community investigates novel ML methods for
the edge computing environment, spanning multiple sub-fields of computer
science, engineering and ICT. The goal is to share an envisioned roadmap that
can bring together key actors and enablers to further advance the domain of
Edge AI.

AI and humans bring complementary skills to group deliberations. Modeling
this group decision making is especially challenging when the deliberations
include an element of risk and an exploration-exploitation process of
appraising the capabilities of the human and AI agents. To investigate this
question, we presented a sequence of intellective issues to a set of human
groups aided by imperfect AI agents. A group's goal was to appraise the
relative expertise of the group's members and its available AI agents, evaluate
the risks associated with different actions, and maximize the overall reward by
reaching consensus. We propose and empirically validate models of human-AI team
decision making under such uncertain circumstances, and show the value of
socio-cognitive constructs of prospect theory, influence dynamics, and Bayesian
learning in predicting the behavior of human-AI groups.

Responsible Artificial Intelligence (AI) - the practice of developing,
evaluating, and maintaining accurate AI systems that also exhibit essential
properties such as robustness and explainability - represents a multifaceted
challenge that often stretches standard machine learning tooling, frameworks,
and testing methods beyond their limits. In this paper, we present two new
software libraries - hydra-zen and the rAI-toolbox - that address critical
needs for responsible AI engineering. hydra-zen dramatically simplifies the
process of making complex AI applications configurable, and their behaviors
reproducible. The rAI-toolbox is designed to enable methods for evaluating and
enhancing the robustness of AI-models in a way that is scalable and that
composes naturally with other popular ML frameworks. We describe the design
principles and methodologies that make these tools effective, including the use
of property-based testing to bolster the reliability of the tools themselves.
Finally, we demonstrate the composability and flexibility of the tools by
showing how various use cases from adversarial robustness and explainable AI
can be concisely implemented with familiar APIs.

Control Systems, particularly closed-loop control systems (CLCS), are
frequently used in production machines, vehicles, and robots nowadays. CLCS are
needed to actively align actual values of a process to a given reference or set
values in real-time with a very high precession. Yet, artificial intelligence
(AI) is not used to model, design, optimize, and tune CLCS. This paper will
highlight potential AI-empowered and -based control system designs and
designing procedures, gathering new opportunities and research direction in the
field of control system engineering. Therefore, this paper illustrates which
building blocks within the standard block diagram of CLCS can be replaced by
AI, i.e., artificial neuronal networks (ANN). Having processes with real-time
contains and functional safety in mind, it is discussed if AI-based controller
blocks can cope with these demands. By concluding the paper, the pros and cons
of AI-empowered as well as -based CLCS designs are discussed, and possible
research directions for introducing AI in the domain of control system
engineering are given.

Artificial intelligence (AI), especially deep learning, requires vast amounts
of data for training, testing, and validation. Collecting these data and the
corresponding annotations requires the implementation of imaging biobanks that
provide access to these data in a standardized way. This requires careful
design and implementation based on the current standards and guidelines and
complying with the current legal restrictions. However, the realization of
proper imaging data collections is not sufficient to train, validate and deploy
AI as resource demands are high and require a careful hybrid implementation of
AI pipelines both on-premise and in the cloud. This chapter aims to help the
reader when technical considerations have to be made about the AI environment
by providing a technical background of different concepts and implementation
aspects involved in data storage, cloud usage, and AI pipelines.

Developing smart software services requires both Software Engineering and
Artificial Intelligence (AI) skills. AI practitioners, such as data scientists
often focus on the AI side, for example, creating and training Machine Learning
(ML) models given a specific use case and data. They are typically not
concerned with the entire software development life-cycle, architectural
decisions for the system and performance issues beyond the predictive ML models
(e.g., regarding the security, privacy, throughput, scalability, availability,
as well as ethical, legal and regulatory compliance). In this manuscript, we
propose a novel approach to enable Model-Driven Software Engineering and
Model-Driven AI Engineering. In particular, we support Automated ML, thus
assisting software engineers without deep AI knowledge in developing
AI-intensive systems by choosing the most appropriate ML model, algorithm and
techniques with suitable hyper-parameters for the task at hand. To validate our
work, we carry out a case study in the smart energy domain.

Throughout their history, homo sapiens have used technologies to better
satisfy their needs. The relation between needs and technology is so
fundamental that the US National Research Council defined the distinguishing
characteristic of technology as its goal "to make modifications in the world to
meet human needs". Artificial intelligence (AI) is one of the most promising
emerging technologies of our time. Similar to other technologies, AI is
expected "to meet [human] needs". In this article, we reflect on the
relationship between needs and AI, and call for the realisation of needs-aware
AI systems. We argue that re-thinking needs for, through, and by AI can be a
very useful means towards the development of realistic approaches for
Sustainable, Human-centric, Accountable, Lawful, and Ethical (HALE) AI systems.
We discuss some of the most critical gaps, barriers, enablers, and drivers of
co-creating future AI-based socio-technical systems in which [human] needs are
well considered and met. Finally, we provide an overview of potential threats
and HALE considerations that should be carefully taken into account, and call
for joint, immediate, and interdisciplinary efforts and collaborations.

Engendering trust in technically acceptable and psychologically embraceable
systems requires domain-specific research to capture unique characteristics of
the field of application. The architecture, engineering, and construction (AEC)
research community has been recently harnessing advanced solutions offered by
artificial intelligence (AI) to improve project workflows. Despite the unique
characteristics of work, workers, and workplaces in the AEC industry, the
concept of trust in AI has received very little attention in the literature.
This paper presents a comprehensive analysis of the academic literature in two
main areas of trust in AI and AI in the AEC, to explore the interplay between
AEC projects unique aspects and the sociotechnical concepts that lead to trust
in AI. A total of 490 peer-reviewed scholarly articles are analyzed in this
study. The main constituents of human trust in AI are identified from the
literature and are characterized within the AEC project types, processes, and
technologies.

It is often overseen that AI-enabled systems are also software systems and
therefore rely on software quality assurance (SQA). Thus, the goal of this
study is to investigate the software quality assurance strategies adopted
during the development, integration, and maintenance of AI/ML components and
code. We conducted semi-structured interviews with representatives of ten
Austrian SMEs that develop AI-enabled systems. A qualitative analysis of the
interview data identified 12 issues in the development of AI/ML components.
Furthermore, we identified when quality issues arise in AI/ML components and
how they are detected. The results of this study should guide future work on
software quality assurance processes and techniques for AI/ML components.

Embedding artificial intelligence into systems introduces significant
challenges to modern engineering practices. Hazard analysis tools and processes
have not yet been adequately adapted to the new paradigm. This paper describes
initial research and findings regarding current practices in AI-related hazard
analysis and on the tools used to conduct this work. Our goal with this initial
research is to better understand the needs of practitioners and the emerging
challenges of considering hazards and risks for AI-enabled products and
services. Our primary research question is: Can we develop new structured
thinking methods and systems engineering tools to support effective and
engaging ways for preemptively considering failure modes in AI systems? The
preliminary findings from our review of the literature and interviews with
practitioners highlight various challenges around integrating hazard analysis
into modern AI development processes and suggest opportunities for exploration
of usable, human-centered hazard analysis tools.

An important goal in the field of human-AI interaction is to help users more
appropriately trust AI systems' decisions. A situation in which the user may
particularly benefit from more appropriate trust is when the AI receives
anomalous input or provides anomalous output. To the best of our knowledge,
this is the first work towards understanding how anomaly alerts may contribute
to appropriate trust of AI. In a formative mixed-methods study with 4
radiologists and 4 other physicians, we explore how AI alerts for anomalous
input, very high and low confidence, and anomalous saliency-map explanations
affect users' experience with mockups of an AI clinical decision support system
(CDSS) for evaluating chest x-rays for pneumonia. We find evidence suggesting
that the four anomaly alerts are desired by non-radiologists, and the
high-confidence alerts are desired by both radiologists and non-radiologists.
In a follow-up user study, we investigate how high- and low-confidence alerts
affect the accuracy and thus appropriate trust of 33 radiologists working with
AI CDSS mockups. We observe that these alerts do not improve users' accuracy or
experience and discuss potential reasons why.

Recent progress in large-scale language models has enabled breakthroughs in
previously intractable computer programming tasks. Prior work in meta-learning
and neural architecture search has led to substantial successes across various
task domains, spawning myriad approaches for algorithmically optimizing the
design and learning dynamics of deep learning models. At the intersection of
these research areas, we implement a code-generating language model with the
ability to modify its own source code. Self-programming AI algorithms have been
of interest since the dawn of AI itself. Although various theoretical
formulations of generalized self-programming AI have been posed, no such system
has been successfully implemented to date under real-world computational
constraints. Applying AI-based code generation to AI itself, we develop and
experimentally validate the first practical implementation of a
self-programming AI system. We empirically show that a self-programming AI
implemented using a code generation model can successfully modify its own
source code to improve performance and program sub-models to perform auxiliary
tasks. Our model can self-modify various properties including model
architecture, computational capacity, and learning dynamics.

Welcome to the fifth edition of the AI Index Report! The latest edition
includes data from a broad set of academic, private, and nonprofit
organizations as well as more self-collected data and original analysis than
any previous editions, including an expanded technical performance chapter, a
new survey of robotics researchers around the world, data on global AI
legislation records in 25 countries, and a new chapter with an in-depth
analysis of technical AI ethics metrics.
  The AI Index Report tracks, collates, distills, and visualizes data related
to artificial intelligence. Its mission is to provide unbiased, rigorously
vetted, and globally sourced data for policymakers, researchers, executives,
journalists, and the general public to develop a more thorough and nuanced
understanding of the complex field of AI. The report aims to be the world's
most credible and authoritative source for data and insights about AI.

As the deployment of artificial intelligence (AI) is changing many fields and
industries, there are concerns about AI systems making decisions and
recommendations without adequately considering various ethical aspects, such as
accountability, reliability, transparency, explainability, contestability,
privacy, and fairness. While many sets of AI ethics principles have been
recently proposed that acknowledge these concerns, such principles are
high-level and do not provide tangible advice on how to develop ethical and
responsible AI systems. To gain insight on the possible implementation of the
principles, we conducted an empirical investigation involving semi-structured
interviews with a cohort of AI practitioners. The salient findings cover four
aspects of AI system design and development, adapting processes used in
software engineering: (i) high-level view, (ii) requirements engineering, (iii)
design and implementation, (iv) deployment and operation.

With Artificial intelligence (AI) to aid or automate decision-making
advancing rapidly, a particular concern is its fairness. In order to create
reliable, safe and trustworthy systems through human-centred artificial
intelligence (HCAI) design, recent efforts have produced user interfaces (UIs)
for AI experts to investigate the fairness of AI models. In this work, we
provide a design space exploration that supports not only data scientists but
also domain experts to investigate AI fairness. Using loan applications as an
example, we held a series of workshops with loan officers and data scientists
to elicit their requirements. We instantiated these requirements into FairHIL,
a UI to support human-in-the-loop fairness investigations, and describe how
this UI could be generalized to other use cases. We evaluated FairHIL through a
think-aloud user study. Our work contributes better designs to investigate an
AI model's fairness-and move closer towards responsible AI.

We consider a series of legal provocations emerging from the proposed
European Union AI Act 2021 (AIA) and how they open up new possibilities for HCI
in the design and development of trustworthy autonomous systems. The AIA
continues the by design trend seen in recent EU regulation of emerging
technologies. The AIA targets AI developments that pose risks to society and
citizens fundamental rights, introducing mandatory design and development
requirements for high-risk AI systems (HRAIS). These requirements regulate
different stages of the AI development cycle including ensuring data quality
and governance strategies, mandating testing of systems, ensuring appropriate
risk management, designing for human oversight, and creating technical
documentation. These requirements open up new opportunities for HCI that reach
beyond established concerns with the ethics and explainability of AI and
situate AI development in human-centered processes and methods of design to
enable compliance with regulation and foster societal trust in AI.

In conventional software development, user experience (UX) designers and
engineers collaborate through separation of concerns (SoC): designers create
human interface specifications, and engineers build to those specifications.
However, we argue that Human-AI systems thwart SoC because human needs must
shape the design of the AI interface, the underlying AI sub-components, and
training data. How do designers and engineers currently collaborate on AI and
UX design? To find out, we interviewed 21 industry professionals (UX
researchers, AI engineers, data scientists, and managers) across 14
organizations about their collaborative work practices and associated
challenges. We find that hidden information encapsulated by SoC challenges
collaboration across design and engineering concerns. Practitioners describe
inventing ad-hoc representations exposing low-level design and implementation
details (which we characterize as leaky abstractions) to "puncture" SoC and
share information across expertise boundaries. We identify how leaky
abstractions are employed to collaborate at the AI-UX boundary and formalize a
process of creating and using leaky abstractions.

Generative, ML-driven interactive systems have the potential to change how
people interact with computers in creative processes - turning tools into
co-creators. However, it is still unclear how we might achieve effective
human-AI collaboration in open-ended task domains. There are several known
challenges around communication in the interaction with ML-driven systems. An
overlooked aspect in the design of co-creative systems is how users can be
better supported in learning to collaborate with such systems. Here we reframe
human-AI collaboration as a learning problem: Inspired by research on team
learning, we hypothesize that similar learning strategies that apply to
human-human teams might also increase the collaboration effectiveness and
quality of humans working with co-creative generative systems. In this position
paper, we aim to promote team learning as a lens for designing more effective
co-creative human-AI collaboration and emphasize collaboration process quality
as a goal for co-creative systems. Furthermore, we outline a preliminary
schematic framework for embedding team learning support in co-creative AI
systems. We conclude by proposing a research agenda and posing open questions
for further study on supporting people in learning to collaborate with
generative AI systems.

This paper summarizes and evaluates various approaches, methods, and
techniques for pursuing fairness in artificial intelligence (AI) systems. It
examines the merits and shortcomings of these measures and proposes practical
guidelines for defining, measuring, and preventing bias in AI. In particular,
it cautions against some of the simplistic, yet common, methods for evaluating
bias in AI systems, and offers more sophisticated and effective alternatives.
The paper also addresses widespread controversies and confusions in the field
by providing a common language among different stakeholders of high-impact AI
systems. It describes various trade-offs involving AI fairness, and provides
practical recommendations for balancing them. It offers techniques for
evaluating the costs and benefits of fairness targets, and defines the role of
human judgment in setting these targets. This paper provides discussions and
guidelines for AI practitioners, organization leaders, and policymakers, as
well as various links to additional materials for a more technical audience.
Numerous real-world examples are provided to clarify the concepts, challenges,
and recommendations from a practical perspective.

With the rapid development of artificial intelligence (AI) community,
education in AI is receiving more and more attentions. There have been many AI
related courses in the respects of algorithms and applications, while not many
courses in system level are seriously taken into considerations. In order to
bridge the gap between AI and computing systems, we are trying to explore how
to conduct AI education from the perspective of computing systems. In this
paper, a course practice in intelligent computing architectures are provided to
demonstrate the system education in AI era. The motivation for this course
practice is first introduced as well as the learning orientations. The main
goal of this course aims to teach students for designing AI accelerators on
FPGA platforms. The elaborated course contents include lecture notes and
related technical materials. Especially several practical labs and projects are
detailed illustrated. Finally, some teaching experiences and effects are
discussed as well as some potential improvements in the future.

AI is being increasingly used to aid response efforts to humanitarian
emergencies at multiple levels of decision-making. Such AI systems are
generally understood to be stand-alone tools for decision support, with ethical
assessments, guidelines and frameworks applied to them through this lens.
However, as the prevalence of AI increases in this domain, such systems will
begin to encounter each other through information flow networks created by
interacting decision-making entities, leading to multi-AI complex systems which
are often ill understood. In this paper we describe how these multi-AI systems
can arise, even in relatively simple real-world humanitarian response
scenarios, and lead to potentially emergent and erratic erroneous behavior. We
discuss how we can better work towards more trustworthy multi-AI systems by
exploring some of the associated challenges and opportunities, and how we can
design better mechanisms to understand and assess such systems. This paper is
designed to be a first exposition on this topic in the field of humanitarian
response, raising awareness, exploring the possible landscape of this domain,
and providing a starting point for future work within the wider community.

Increasingly, scientific discovery requires sophisticated and scalable
workflows. Workflows have become the ``new applications,'' wherein multi-scale
computing campaigns comprise multiple and heterogeneous executable tasks. In
particular, the introduction of AI/ML models into the traditional HPC workflows
has been an enabler of highly accurate modeling, typically reducing
computational needs compared to traditional methods. This chapter discusses
various modes of integrating AI/ML models to HPC computations, resulting in
diverse types of AI-coupled HPC workflows. The increasing need of coupling
AI/ML and HPC across scientific domains is motivated, and then exemplified by a
number of production-grade use cases for each mode. We additionally discuss the
primary challenges of extreme-scale AI-coupled HPC campaigns -- task
heterogeneity, adaptivity, performance -- and several framework and middleware
solutions which aim to address them. While both HPC workflow and AI/ML
computing paradigms are independently effective, we highlight how their
integration, and ultimate convergence, is leading to significant improvements
in scientific performance across a range of domains, ultimately resulting in
scientific explorations otherwise unattainable.

Artificial intelligence (AI) has emerged as a powerful technology that
improves system performance and enables new features in 5G and beyond.
Standardization, defining functionality and interfaces, is essential for
driving the industry alignment required to deliver the mass adoption of AI in
5G-Advanced and 6G. However, fragmented efforts in different standards bodies,
such as the third generation partnership project (3GPP) and the open radio
access network (O-RAN) Alliance, can lead to confusion and uncertainty about
which standards to follow and which aspects of the standards to embrace. This
article provides a joint 3GPP and O-RAN perspective on the state of the art in
AI adoption in mobile communication systems, including the fundamentals of 5G
architecture and its evolution towards openness and intelligence, AI for
5G-Advanced evolution, and a case study on AI-enabled traffic steering. We also
identify several areas for future exploration to accelerate AI adoption on the
path towards 6G.

Superhuman artificial general intelligence could be created this century and
would likely be a significant source of existential risk. Delaying the creation
of superintelligent AI (ASI) could decrease total existential risk by
increasing the amount of time humanity has to work on the AI alignment problem.
  However, since ASI could reduce most risks, delaying the creation of ASI
could also increase other existential risks, especially from advanced future
technologies such as synthetic biology and molecular nanotechnology.
  If AI existential risk is high relative to the sum of other existential risk,
delaying the creation of ASI will tend to decrease total existential risk and
vice-versa.
  Other factors such as war and a hardware overhang could increase AI risk and
cognitive enhancement could decrease AI risk. To reduce total existential risk,
humanity should take robustly positive actions such as working on existential
risk analysis, AI governance and safety, and reducing all sources of
existential risk by promoting differential technological development.

Although AI-based systems are increasingly being leveraged to provide value
to organizations, individuals, and society, significant attendant risks have
been identified. These risks have led to proposed regulations, litigation, and
general societal concerns.
  As with any promising technology, organizations want to benefit from the
positive capabilities of AI technology while reducing the risks. The best way
to reduce risks is to implement comprehensive AI lifecycle governance where
policies and procedures are described and enforced during the design,
development, deployment, and monitoring of an AI system. While support for
comprehensive governance is beginning to emerge, organizations often need to
identify the risks of deploying an already-built model without knowledge of how
it was constructed or access to its original developers.
  Such an assessment will quantitatively assess the risks of an existing model
in a manner analogous to how a home inspector might assess the energy
efficiency of an already-built home or a physician might assess overall patient
health based on a battery of tests. This paper explores the concept of a
quantitative AI Risk Assessment, exploring the opportunities, challenges, and
potential impacts of such an approach, and discussing how it might improve AI
regulations.

Pseudo Artificial Intelligence bias (PAIB) is broadly disseminated in the
literature, which can result in unnecessary AI fear in society, exacerbate the
enduring inequities and disparities in access to and sharing the benefits of AI
applications, and waste social capital invested in AI research. This study
systematically reviews publications in the literature to present three types of
PAIBs identified due to: a) misunderstandings, b) pseudo mechanical bias, and
c) over-expectations. We discussed the consequences of and solutions to PAIBs,
including certifying users for AI applications to mitigate AI fears, providing
customized user guidance for AI applications, and developing systematic
approaches to monitor bias. We concluded that PAIB due to misunderstandings,
pseudo mechanical bias, and over-expectations of algorithmic predictions is
socially harmful.

We conduct the first large-scale user study examining how users interact with
an AI Code assistant to solve a variety of security related tasks across
different programming languages. Overall, we find that participants who had
access to an AI assistant based on OpenAI's codex-davinci-002 model wrote
significantly less secure code than those without access. Additionally,
participants with access to an AI assistant were more likely to believe they
wrote secure code than those without access to the AI assistant. Furthermore,
we find that participants who trusted the AI less and engaged more with the
language and format of their prompts (e.g. re-phrasing, adjusting temperature)
provided code with fewer security vulnerabilities. Finally, in order to better
inform the design of future AI-based Code assistants, we provide an in-depth
analysis of participants' language and interaction behavior, as well as release
our user interface as an instrument to conduct similar studies in the future.

With the rapid development and integration of artificial intelligence (AI)
methods in next-generation networks (NextG), AI algorithms have provided
significant advantages for NextG in terms of frequency spectrum usage,
bandwidth, latency, and security. A key feature of NextG is the integration of
AI, i.e., self-learning architecture based on self-supervised algorithms, to
improve the performance of the network. A secure AI-powered structure is also
expected to protect NextG networks against cyber-attacks. However, AI itself
may be attacked, i.e., model poisoning targeted by attackers, and it results in
cybersecurity violations. This paper proposes an AI trust platform using
Streamlit for NextG networks that allows researchers to evaluate, defend,
certify, and verify their AI models and applications against adversarial
threats of evasion, poisoning, extraction, and interference.

Under the Autonomous Mobile Clinics (AMCs) initiative, we are developing,
open sourcing, and standardizing health AI technologies to enable healthcare
access in least developed countries (LDCs). We deem AMCs as the next generation
of health care delivery platforms, whereas health AI engines are applications
on these platforms, similar to how various applications expand the usage
scenarios of smart phones. Facing the recent global monkeypox outbreak, in this
article, we introduce AICOM-MP, an AI-based monkeypox detector specially aiming
for handling images taken from resource-constrained devices. Compared to
existing AI-based monkeypox detectors, AICOM-MP has achieved state-of-the-art
(SOTA) performance. We have hosted AICOM-MP as a web service to allow universal
access to monkeypox screening technology. We have also open sourced both the
source code and the dataset of AICOM-MP to allow health AI professionals to
integrate AICOM-MP into their services. Also, through the AICOM-MP project, we
have generalized a methodology of developing health AI technologies for AMCs to
allow universal access even in resource-constrained environments.

In recent years, digital object management practices to support findability,
accessibility, interoperability, and reusability (FAIR) have begun to be
adopted across a number of data-intensive scientific disciplines. These digital
objects include datasets, AI models, software, notebooks, workflows,
documentation, etc. With the collective dataset at the Large Hadron Collider
scheduled to reach the zettabyte scale by the end of 2032, the experimental
particle physics community is looking at unprecedented data management
challenges. It is expected that these grand challenges may be addressed by
creating end-to-end AI frameworks that combine FAIR and AI-ready datasets,
advances in AI, modern computing environments, and scientific data
infrastructure. In this work, the FAIR4HEP collaboration explores the
interpretation of FAIR principles in the context of data and AI models for
experimental high energy physics research. We investigate metrics to quantify
the FAIRness of experimental datasets and AI models, and provide open source
notebooks to guide new users on the use of FAIR principles in practice.

Artificial Intelligence (AI) systems have been increasingly used to make
decision-making processes faster, more accurate, and more efficient. However,
such systems are also at constant risk of being attacked. While the majority of
attacks targeting AI-based applications aim to manipulate classifiers or
training data and alter the output of an AI model, recently proposed Sponge
Attacks against AI models aim to impede the classifier's execution by consuming
substantial resources. In this work, we propose \textit{Dual Denial of Decision
(DDoD) attacks against collaborative Human-AI teams}. We discuss how such
attacks aim to deplete \textit{both computational and human} resources, and
significantly impair decision-making capabilities. We describe DDoD on human
and computational resources and present potential risk scenarios in a series of
exemplary domains.

The latest AI language modules can produce original, high quality full
short-form ($300$-word) Physics essays within seconds. These technologies such
as ChatGPT and davinci-003 are freely available to anyone with an internet
connection. In this work, we present evidence of AI generated short-form essays
achieving first-class grades on an essay writing assessment from an accredited,
current university Physics module. The assessment requires students answer five
open-ended questions with a short, $300$-word essay each. Fifty AI answers were
generated to create ten submissions that were independently marked by five
separate markers. The AI generated submissions achieved an average mark of $71
\pm 2 \%$, in strong agreement with the current module average of $71 \pm 5 %$.
A typical AI submission would therefore most-likely be awarded a First Class,
the highest classification available at UK universities. Plagiarism detection
software returned a plagiarism score between $2 \pm 1$% (Grammarly) and $7 \pm
2$% (TurnitIn). We argue that these results indicate that current AI MLPs
represent a significant threat to the fidelity of short-form essays as an
assessment method in Physics courses.

A major shift from skilled to unskilled workers was one of the many changes
caused by the Industrial Revolution, when the switch to machines contributed to
decline in the social and economic status of artisans, whose skills were
dismembered into discrete actions by factory-line workers. We consider what may
be an analogous computing technology: the recent introduction of AI-generated
art software. AI art generators such as Dall-E and Midjourney can create fully
rendered images based solely on a user's prompt, just at the click of a button.
Some artists fear if the cheaper price and conveyor-belt speed that comes with
AI-produced images is seen as an improvement to the current system, it may
permanently change the way society values/views art and artists. In this
article, we consider the implications that AI art generation introduces through
a post-industrial revolution historical lens. We then reflect on the analogous
issues that appear to arise as a result of the AI art revolution, and we
conclude that the problems raised mirror those of industrialization, giving a
vital glimpse into what may lie ahead.

Open Artificial Intelligence (Open source AI) collaboratives offer
alternative pathways for how AI can be developed beyond well-resourced
technology companies and who can be a part of the process. To understand how
and why they work and what additionality they bring to the landscape, we focus
on three such communities, each focused on a different kind of activity around
AI: building models (BigScience workshop), tools and ways of working (The
Turing Way), and ecosystems (Mozilla Festival's Building Trustworthy AI Working
Group). First, we document the community structures that facilitate these
distributed, volunteer-led teams, comparing the collaboration styles that drive
each group towards their specific goals. Through interviews with community
leaders, we map user journeys for how members discover, join, contribute, and
participate. Ultimately, this paper aims to highlight the diversity of AI work
and workers that have come forth through these collaborations and how they
offer a broader practice of openness to the AI space.

Artificial intelligence (AI) is everywhere, with many children having
increased exposure to AI technologies in daily life. We aimed to understand
middle school girls' (a group often excluded group in tech) perceptions and
knowledge gaps about AI. We created and explored the feasibility of a
lightweight (less than 3 hours) educational workshop in which learners
considered challenges in their lives and communities and critically considered
how existing and future AI could have an impact. After the workshop, learners
had nuanced perceptions of AI, understanding AI can both help and harm. We
discuss design implications for creating educational experiences in AI and
fairness that embolden learners.

As the use of interactive AI systems becomes increasingly prevalent in our
daily lives, it is crucial to understand how individuals feel when interacting
with such systems. In this work, we investigate the comfort level of
individuals when interacting with intent-predicting AI systems and identify the
factors of influence. We introduce a study protocol to analyze human
comfortability when interacting with intent-predicting AI systems and execute
the study with over a dozen participants. The study findings suggest that users
are comfortable with AI systems if they have control and their privacy is not
affected. Additionally, the study found that users could differentiate between
AI and human responses, but this did not significantly affect their comfort
levels. This research paper's significance lies in its contribution to the
growing body of literature on interactive AI systems, and it emphasizes the
need to consider user perceptions in the development and deployment.

Artificial intelligence (AI) has a history of nearly a century from its
inception to the present day. We have summarized the development trends and
discovered universal rules, including both success and failure. We have
analyzed the reasons from both technical and philosophical perspectives to help
understand the reasons behind the past failures and current successes of AI,
and to provide a basis for thinking and exploring future development.
Specifically, we have found that the development of AI in different fields,
including computer vision, natural language processing, and machine learning,
follows a pattern from rules to statistics to data-driven methods. In the face
of past failures and current successes, we need to think systematically about
the reasons behind them. Given the unity of AI between natural and social
sciences, it is necessary to incorporate philosophical thinking to understand
and solve AI problems, and we believe that starting from the dialectical method
of Marx is a feasible path. We have concluded that the sustainable development
direction of AI should be human-machine collaboration and a technology path
centered on computing power. Finally, we have summarized the impact of AI on
society from this trend.

As AI systems proliferate, their greenhouse gas emissions are an increasingly
important concern for human societies. We analyze the emissions of several AI
systems (ChatGPT, BLOOM, DALL-E2, Midjourney) relative to those of humans
completing the same tasks. We find that an AI writing a page of text emits 130
to 1500 times less CO2e than a human doing so. Similarly, an AI creating an
image emits 310 to 2900 times less. Emissions analysis do not account for
social impacts such as professional displacement, legality, and rebound
effects. In addition, AI is not a substitute for all human tasks. Nevertheless,
at present, the use of AI holds the potential to carry out several major
activities at much lower emission levels than can humans.

Generative AI, i.e., the group of technologies that automatically generate
visual or written content based on text prompts, has undergone a leap in
complexity and become widely available within just a few years. Such
technologies potentially introduce a massive disruption to creative fields.
This paper presents the results of a qualitative survey ($N$ = 23)
investigating how creative professionals think about generative AI. The results
show that the advancement of these AI models prompts important reflections on
what defines creativity and how creatives imagine using AI to support their
workflows. Based on these reflections, we discuss how we might design
\textit{participatory AI} in the domain of creative expertise with the goal of
empowering creative professionals in their present and future coexistence with
AI.

Recent years have seen growing adoption of AI-based decision-support systems
(ADS) in homeless services, yet we know little about stakeholder desires and
concerns surrounding their use. In this work, we aim to understand impacted
stakeholders' perspectives on a deployed ADS that prioritizes scarce housing
resources. We employed AI lifecycle comicboarding, an adapted version of the
comicboarding method, to elicit stakeholder feedback and design ideas across
various components of an AI system's design. We elicited feedback from county
workers who operate the ADS daily, service providers whose work is directly
impacted by the ADS, and unhoused individuals in the region. Our participants
shared concerns and design suggestions around the AI system's overall
objective, specific model design choices, dataset selection, and use in
deployment. Our findings demonstrate that stakeholders, even without AI
knowledge, can provide specific and critical feedback on an AI system's design
and deployment, if empowered to do so.

AI documentation is a rapidly-growing channel for coordinating the design of
AI technologies with policies for transparency and accessibility. Calls to
standardize and enact documentation of algorithmic harms and impacts are now
commonplace. However, documentation standards for AI remain inchoate, and fail
to match the capabilities and social effects of increasingly impactful
architectures such as Large Language Models (LLMs). In this paper, we show the
limits of present documentation protocols, and argue for dynamic documentation
as a new paradigm for understanding and evaluating AI systems. We first review
canonical approaches to system documentation outside the context of AI,
focusing on the complex history of Environmental Impact Statements (EISs). We
next compare critical elements of the EIS framework to present challenges with
algorithmic documentation, which have inherited the limitations of EISs without
incorporating their strengths. These challenges are specifically illustrated
through the growing popularity of Model Cards and two case studies of
algorithmic impact assessment in China and Canada. Finally, we evaluate more
recent proposals, including Reward Reports, as potential components of fully
dynamic AI documentation protocols.

Generative AI models, including large language models and multimodal models
that include text and other media, are on the cusp of transforming many aspects
of modern life, including entertainment, education, civic life, the arts, and a
range of professions. There is potential for Generative AI to have a
substantive impact on the methods and pace of discovery for a range of
scientific disciplines. We interviewed twenty scientists from a range of fields
(including the physical, life, and social sciences) to gain insight into
whether or how Generative AI technologies might add value to the practice of
their respective disciplines, including not only ways in which AI might
accelerate scientific discovery (i.e., research), but also other aspects of
their profession, including the education of future scholars and the
communication of scientific findings. In addition to identifying opportunities
for Generative AI to augment scientists' current practices, we also asked
participants to reflect on concerns about AI. These findings can help guide the
responsible development of models and interfaces for scientific education,
inquiry, and communication.

While AI-assisted individual qualitative analysis has been substantially
studied, AI-assisted collaborative qualitative analysis (CQA)-a process that
involves multiple researchers working together to interpret data-remains
relatively unexplored. After identifying CQA practices and design opportunities
through formative interviews, we designed and implemented CoAIcoder, a tool
leveraging AI to enhance human-to-human collaboration within CQA through four
distinct collaboration methods. With a between-subject design, we evaluated
CoAIcoder with 32 pairs of CQA-trained participants across common CQA phases
under each collaboration method. Our findings suggest that while using a shared
AI model as a mediator among coders could improve CQA efficiency and foster
agreement more quickly in the early coding stage, it might affect the final
code diversity. We also emphasize the need to consider the independence level
when using AI to assist human-to-human collaboration in various CQA scenarios.
Lastly, we suggest design implications for future AI-assisted CQA systems.

Children increasingly use applications utilizing Artificial Intelligence /
Machine Learning (AI/ML). Given the propensity of such applications to
propagate existing social, gender, and racial biases, it becomes imperative to
consider designing and developing child-centered AI applications for children.
Furthermore, children should have opportunities and skills to critically
reflect on current applications and envision and design better AI/ML
applications that are ethical, specifically, those that are inclusive and fair.
In our work, we focus on child-centered AI and inclusion. Using a two-fanged
approach to inclusion and employing design futuring in our research with
schools in India and Finland, children critically considered future technology
design for all. In this paper, we present three cases of this work: a study
with students at a school in New Delhi and two studies with students at schools
in Oulu. Our work showcases how to design for inclusion - by designing for all,
and how to design inclusively - by inviting children to envision the future,
through design futuring approaches.

The rapid advancement of artificial intelligence (AI) such as the emergence
of large language models including ChatGPT and DALLE 2 has brought both
opportunities for improving productivity and raised ethical concerns. This
paper investigates the ethics of using artificial intelligence (AI) in
cartography, with a particular focus on the generation of maps using DALLE 2.
To accomplish this, we first create an open-sourced dataset that includes
synthetic (AI-generated) and real-world (human-designed) maps at multiple
scales with a variety settings. We subsequently examine four potential ethical
concerns that may arise from the characteristics of DALLE 2 generated maps,
namely inaccuracies, misleading information, unanticipated features, and
reproducibility. We then develop a deep learning-based ethical examination
system that identifies those AI-generated maps. Our research emphasizes the
importance of ethical considerations in the development and use of AI
techniques in cartography, contributing to the growing body of work on
trustworthy maps. We aim to raise public awareness of the potential risks
associated with AI-generated maps and support the development of ethical
guidelines for their future use.

The release of ChatGPT has drawn huge interests on foundations models. There
is a broad consensus that foundations models will be the fundamental building
blocks for future AI systems. However, there is a lack of systematic guidance
on the architecture design. Particularly, the the rapidly growing capabilities
of foundations models can eventually absorb other components of AI systems,
posing challenges of moving boundary and interface evolution in architecture
design. Furthermore, incorporating foundations models into AI systems raises
significant concerns about responsible AI due to their opaque nature and
rapidly advancing intelligence. To address these challenges, the paper first
presents an architecture evolution of AI systems in the era of foundation
models, transitioning from "foundation-model-as-a-connector" to
"foundation-model-as-a-monolithic architecture". The paper then identifies key
design decisions and proposes a pattern-oriented reference architecture for
designing responsible foundation-model-based systems. The patterns can enable
the potential of foundation models while minimising associated risks.

We investigated the potential and limitations of generative artificial
intelligence (AI) in reflecting the authors' cognitive processes through
creative expression. The focus is on the AI-generated artwork's ability to
understand human intent (alignment) and visually represent emotions based on
criteria such as creativity, aesthetic, novelty, amusement, and depth. Results
show a preference for images based on the descriptions of the authors' emotions
over the main events. We also found that images that overrepresent specific
elements or stereotypes negatively impact AI alignment. Our findings suggest
that AI could facilitate creativity and the self-expression of emotions. Our
research framework with generative AIs can help design AI-based interventions
in related fields (e.g., mental health education, therapy, and counseling).

In this 4-page manuscript we discuss the problem of long-term AI Safety from
a Software Engineering (SE) research viewpoint. We briefly summarize long-term
AI Safety, and the challenge of avoiding harms from AI as systems meet or
exceed human capabilities, including software engineering capabilities (and
approach AGI / "HLMI"). We perform a quantified literature review suggesting
that AI Safety discussions are not common at SE venues. We make conjectures
about how software might change with rising capabilities, and categorize
"subproblems" which fit into traditional SE areas, proposing how work on
similar problems might improve the future of AI and SE.

This study aims to develop an AI education policy for higher education by
examining the perceptions and implications of text generative AI technologies.
Data was collected from 457 students and 180 teachers and staff across various
disciplines in Hong Kong universities, using both quantitative and qualitative
research methods. Based on the findings, the study proposes an AI Ecological
Education Policy Framework to address the multifaceted implications of AI
integration in university teaching and learning. This framework is organized
into three dimensions: Pedagogical, Governance, and Operational. The
Pedagogical dimension concentrates on using AI to improve teaching and learning
outcomes, while the Governance dimension tackles issues related to privacy,
security, and accountability. The Operational dimension addresses matters
concerning infrastructure and training. The framework fosters a nuanced
understanding of the implications of AI integration in academic settings,
ensuring that stakeholders are aware of their responsibilities and can take
appropriate actions accordingly.

Recent advances in artificial intelligence (AI) have raised questions about
whether the use of AI is appropriate and legal in various professional
contexts. Here, we present a perspective on how scholars may approach writing
in conjunction with AI, and offer approaches to evaluating whether or not such
AI-writing violates copyright or falls within the safe harbor of fair use. We
present a set of best practices for standard of care with regard to plagiarism,
copyright, and fair use. As AI is likely to grow more capable in the coming
years, it is appropriate to begin integrating AI into scholarly writing
activities. We offer a framework for establishing sound legal and scholarly
foundations.

As voice-based Conversational Assistants (CAs), including Alexa, Siri, Google
Home, have become commonly embedded in households, many children now routinely
interact with Artificial Intelligence (AI) systems. It is important to research
children's experiences with consumer devices which use AI techniques because
these shape their understanding of AI and its capabilities. We conducted a
mixed-methods study (questionnaires and interviews) with primary-school
children aged 6-11 in Scotland to establish children's understanding of how
voice-based CAs work, how they perceive their cognitive abilities, agency and
other human-like qualities, their awareness and trust of privacy aspects when
using CAs and what they perceive as appropriate verbal interactions with CAs.
Most children overestimated the CAs' intelligence and were uncertain about the
systems' feelings or agency. They also lacked accurate understanding of data
privacy and security aspects, and believed it was wrong to be rude to
conversational assistants. Exploring children's current understanding of
AI-supported technology has educational implications; such findings will enable
educators to develop appropriate materials to address the pressing need for AI
literacy.

The current literature on AI-advised decision making -- involving explainable
AI systems advising human decision makers -- presents a series of inconclusive
and confounding results. To synthesize these findings, we propose a simple
theory that elucidates the frequent failure of AI explanations to engender
appropriate reliance and complementary decision making performance. We argue
explanations are only useful to the extent that they allow a human decision
maker to verify the correctness of an AI's prediction, in contrast to other
desiderata, e.g., interpretability or spelling out the AI's reasoning process.
Prior studies find in many decision making contexts AI explanations do not
facilitate such verification. Moreover, most tasks fundamentally do not allow
easy verification, regardless of explanation method, limiting the potential
benefit of any type of explanation. We also compare the objective of
complementary performance with that of appropriate reliance, decomposing the
latter into the notions of outcome-graded and strategy-graded reliance.

While several previous studies have analyzed gender bias in research, we are
still missing a comprehensive analysis of gender differences in the AI
community, covering diverse topics and different development trends. Using the
AI Scholar dataset of 78K researchers in the field of AI, we identify several
gender differences: (1) Although female researchers tend to have fewer overall
citations than males, this citation difference does not hold for all
academic-age groups; (2) There exist large gender homophily in co-authorship on
AI papers; (3) Female first-authored papers show distinct linguistic styles,
such as longer text, more positive emotion words, and more catchy titles than
male first-authored papers. Our analysis provides a window into the current
demographic trends in our AI community, and encourages more gender equality and
diversity in the future. Our code and data are at
https://github.com/causalNLP/ai-scholar-gender.

Quality control in the manufacturing industry has improved with the use of
artificial intelligence (AI). However, the manual inspection of trimming die
designs, which is time-consuming and prone to errors, is still done by
engineers. This study introduces an automatic design inspection system for
automobile trimming dies by integrating AI modules and computer-aided design
(CAD) software. The AI modules replace engineers' judgment, and the CAD
software carries out operations requested by the AI modules. The inspection
process involves a zigzag interaction between the AI modules and CAD software,
enabling one-click operation without expert intervention. The AI modules are
CAD-independent and data-efficient, making them adaptable to other CAD
software. They achieve high performance even with limited training data, with
an average length measurement error of only 2.4%. The inspection time is
reduced to approximately one-fifth of the time required for manual inspection
by experts.

The emergence of large-language models (LLMs) that excel at code generation
and commercial products such as GitHub's Copilot has sparked interest in
human-AI pair programming (referred to as "pAIr programming") where an AI
system collaborates with a human programmer. While traditional pair programming
between humans has been extensively studied, it remains uncertain whether its
findings can be applied to human-AI pair programming. We compare human-human
and human-AI pair programming, exploring their similarities and differences in
interaction, measures, benefits, and challenges. We find that the effectiveness
of both approaches is mixed in the literature (though the measures used for
pAIr programming are not as comprehensive). We summarize moderating factors on
the success of human-human pair programming, which provides opportunities for
pAIr programming research. For example, mismatched expertise makes pair
programming less productive, therefore well-designed AI programming assistants
may adapt to differences in expertise levels.

This study examines the impact of GitHub Copilot on a large sample of Copilot
users (n=934,533). The analysis shows that users on average accept nearly 30%
of the suggested code, leading to increased productivity. Furthermore, our
research demonstrates that the acceptance rate rises over time and is
particularly high among less experienced developers, providing them with
substantial benefits. Additionally, our estimations indicate that the adoption
of generative AI productivity tools could potentially contribute to a $1.5
trillion increase in global GDP by 2030. Moreover, our investigation sheds
light on the diverse contributors in the generative AI landscape, including
major technology companies, startups, academia, and individual developers. The
findings suggest that the driving force behind generative AI software
innovation lies within the open-source ecosystem, particularly in the United
States. Remarkably, a majority of repositories on GitHub are led by individual
developers. As more developers embrace these tools and acquire proficiency in
the art of prompting with generative AI, it becomes evident that this novel
approach to software development has forged a unique inextricable link between
humans and artificial intelligence. This symbiotic relationship has the
potential to shape the construction of the world's software for future
generations.

The development of Natural Language Generation models has led to the creation
of powerful Artificial Intelligence-assisted writing tools. These tools are
capable of predicting users' needs and actively providing suggestions as they
write. In this work, we conduct a comparative user-study between such tools
from an information retrieval lens: pull and push. Specifically, we investigate
the user demand of AI-assisted writing, the impact of the two paradigms on
quality, ownership of the writing product, and efficiency and enjoyment of the
writing process. We also seek to understand the impact of bias of AI-assisted
writing. Our findings show that users welcome seamless assistance of AI in
their writing. Furthermore, AI helped users to diversify the ideas in their
writing while keeping it clear and concise more quickly. Users also enjoyed the
collaboration with AI-assisted writing tools and did not feel a lack of
ownership. Finally, although participants did not experience bias in our
experiments, they still expressed explicit and clear concerns that should be
addressed in future AI-assisted writing tools.

In the context of designing and implementing ethical Artificial Intelligence
(AI), varying perspectives exist regarding developing trustworthy AI for
autonomous cars. This study sheds light on the differences in perspectives and
provides recommendations to minimize such divergences. By exploring the diverse
viewpoints, we identify key factors contributing to the differences and propose
strategies to bridge the gaps. This study goes beyond the trolley problem to
visualize the complex challenges of trustworthy and ethical AI. Three pillars
of trustworthy AI have been defined: transparency, reliability, and safety.
This research contributes to the field of trustworthy AI for autonomous cars,
providing practical recommendations to enhance the development of AI systems
that prioritize both technological advancement and ethical principles.

In this paper, the adoption patterns of Generative Artificial Intelligence
(AI) tools within software engineering are investigated. Influencing factors at
the individual, technological, and societal levels are analyzed using a
mixed-methods approach for an extensive comprehension of AI adoption. An
initial structured interview was conducted with 100 software engineers,
employing the Technology Acceptance Model (TAM), the Diffusion of Innovations
theory (DOI), and the Social Cognitive Theory (SCT) as guiding theories. A
theoretical model named the Human-AI Collaboration and Adaptation Framework
(HACAF) was deduced using the Gioia Methodology, characterizing AI adoption in
software engineering. This model's validity was subsequently tested through
Partial Least Squares - Structural Equation Modeling (PLS-SEM), using data
collected from 183 software professionals. The results indicate that the
adoption of AI tools in these early integration stages is primarily driven by
their compatibility with existing development workflows. This finding counters
the traditional theories of technology acceptance. Contrary to expectations,
the influence of perceived usefulness, social aspects, and personal
innovativeness on adoption appeared to be less significant. This paper yields
significant insights for the design of future AI tools and supplies a structure
for devising effective strategies for organizational implementation.

As the use of artificial intelligent (AI) models becomes more prevalent in
industries such as engineering and manufacturing, it is essential that these
models provide transparent reasoning behind their predictions. This paper
proposes the AI-Reasoner, which extracts the morphological characteristics of
defects (DefChars) from images and utilises decision trees to reason with the
DefChar values. Thereafter, the AI-Reasoner exports visualisations (i.e.
charts) and textual explanations to provide insights into outputs made by
masked-based defect detection and classification models. It also provides
effective mitigation strategies to enhance data pre-processing and overall
model performance. The AI-Reasoner was tested on explaining the outputs of an
IE Mask R-CNN model using a set of 366 images containing defects. The results
demonstrated its effectiveness in explaining the IE Mask R-CNN model's
predictions. Overall, the proposed AI-Reasoner provides a solution for
improving the performance of AI models in industrial applications that require
defect analysis.

Due to the cultural and governance differences of countries around the world,
there currently exists a wide spectrum of AI regulation policy proposals that
have created a chaos in the global AI regulatory space. Properly regulating AI
technologies is extremely challenging, as it requires a delicate balance
between legal restrictions and technological developments. In this article, we
first present a comprehensive review of AI regulation proposals from different
geographical locations and cultural backgrounds. Then, drawing from historical
lessons, we develop a framework to facilitate a thorough analysis of AI
regulation proposals. Finally, we perform a systematic analysis of these AI
regulation proposals to understand how each proposal may fail. This study,
containing historical lessons and analysis methods, aims to help governing
bodies untangling the AI regulatory chaos through a divide-and-conquer manner.

Artificial intelligence (AI) reasoning and explainable AI (XAI) tasks have
gained popularity recently, enabling users to explain the predictions or
decision processes of AI models. This paper introduces Forest Monkey (FM), a
toolkit designed to reason the outputs of any AI-based defect detection and/or
classification model with data explainability. Implemented as a Python package,
FM takes input in the form of dataset folder paths (including original images,
ground truth labels, and predicted labels) and provides a set of charts and a
text file to illustrate the reasoning results and suggest possible
improvements. The FM toolkit consists of processes such as feature extraction
from predictions to reasoning targets, feature extraction from images to defect
characteristics, and a decision tree-based AI-Reasoner. Additionally, this
paper investigates the time performance of the FM toolkit when applied to four
AI models with different datasets. Lastly, a tutorial is provided to guide
users in performing reasoning tasks using the FM toolkit.

Artificial Intelligence (AI) refers to the intelligence demonstrated by
machines, and within the realm of AI, Machine Learning (ML) stands as a notable
subset. ML employs algorithms that undergo training on data sets, enabling them
to carry out specific tasks autonomously. Notably, AI holds immense potential
in the field of software engineering, particularly in project management and
planning. In this literature survey, we explore the use of AI in Software
Engineering and summarize previous works in this area. We first review eleven
different publications related to this subject, then compare the surveyed
works. We then comment on the possible challenges present in the utilization of
AI in software engineering and suggest possible further research avenues and
the ways in which AI could evolve with software engineering in the future.

The prevailing discourse around AI ethics lacks the language and formalism
necessary to capture the diverse ethical concerns that emerge when AI systems
interact with individuals. Drawing on Sen and Nussbaum's capability approach,
we present a framework formalizing a network of ethical concepts and
entitlements necessary for AI systems to confer meaningful benefit or
assistance to stakeholders. Such systems enhance stakeholders' ability to
advance their life plans and well-being while upholding their fundamental
rights. We characterize two necessary conditions for morally permissible
interactions between AI systems and those impacted by their functioning, and
two sufficient conditions for realizing the ideal of meaningful benefit. We
then contrast this ideal with several salient failure modes, namely, forms of
social interactions that constitute unjustified paternalism, coercion,
deception, exploitation and domination. The proliferation of incidents
involving AI in high-stakes domains underscores the gravity of these issues and
the imperative to take an ethics-led approach to AI systems from their
inception.

The Fourth Industrial Revolution, particularly Artificial Intelligence (AI),
has had a profound impact on society, raising concerns about its implications
and ethical considerations. The emergence of text generative AI tools like
ChatGPT has further intensified concerns regarding ethics, security, privacy,
and copyright. This study aims to examine the perceptions of individuals in
different information flow categorizations toward AI. The results reveal key
themes in participant-supplied definitions of AI and the fourth industrial
revolution, emphasizing the replication of human intelligence, machine
learning, automation, and the integration of digital technologies. Participants
expressed concerns about job replacement, privacy invasion, and inaccurate
information provided by AI. However, they also recognized the benefits of AI,
such as solving complex problems and increasing convenience. Views on
government involvement in shaping the fourth industrial revolution varied, with
some advocating for strict regulations and others favoring support and
development. The anticipated changes brought by the fourth industrial
revolution include automation, potential job impacts, increased social
disconnect, and reliance on technology. Understanding these perceptions is
crucial for effectively managing the challenges and opportunities associated
with AI in the evolving digital landscape.

The future of the arts and artificial intelligence (AI) is promising as
technology advances. As the use of AI in design becomes more widespread, art
practice may not be a human-only art form and could instead become a digitally
integrated experience. With enhanced creativity and collaboration, arts and AI
could work together towards creating artistic outputs that are visually
appealing and meet the needs of the artist and viewer. While it is uncertain
how far the integration will go, arts and AI will likely influence one another.
This workshop pictorial puts forward first-person research that shares
interactions between an HCI researcher and AI as they try to escape the
creative block. The pictorial paper explores two questions: How can AI support
artists' creativity, and what does it mean to be explainable in this context?
HIs, ChatGPT and Midjourney were engaged; the result was a series of
reflections that require further discussion and explorations in the XAIxArts
community: Transparency of attribution, the creation process, ethics of asking,
and inspiration vs copying.

This paper presents a novel approach to scientific discovery using an
artificial intelligence (AI) environment known as ChatGPT, developed by OpenAI.
This is the first paper entirely generated with outputs from ChatGPT. We
demonstrate how ChatGPT can be instructed through a gamification environment to
define and benchmark hypothetical physical theories. Through this environment,
ChatGPT successfully simulates the creation of a new improved model, called
GPT$^4$, which combines the concepts of GPT in AI (generative pretrained
transformer) and GPT in physics (generalized probabilistic theory). We show
that GPT$^4$ can use its built-in mathematical and statistical capabilities to
simulate and analyze physical laws and phenomena. As a demonstration of its
language capabilities, GPT$^4$ also generates a limerick about itself. Overall,
our results demonstrate the promising potential for human-AI collaboration in
scientific discovery, as well as the importance of designing systems that
effectively integrate AI's capabilities with human intelligence.

The rise of artificial intelligence (AI) has led to various means of
integration of AI aimed to provide efficiency in tasks, one of which is career
counseling. A key part of getting a job is having a solid resume that passes
through the first round of programs and recruiters. It is difficult to find
good resources or schedule an appointment with a career counselor to help with
editing a resume for a specific role. With the rise of ChatGPT, Bard, and
several other AI chat programs it is possible to provide specific, automated
feedback on various concerns to suggest places for improvement within the
context of career counseling. This paper begins with a quick literature review
on the ethical considerations and limitations of AI in career counseling. The
authors also have created their own website service, called ResumAI, to test
and review the functionality of an AI career counselor. The findings of this
study will contribute to the understanding of chat AI ResumAI reviewer programs
and sites. The implications of the findings for the field of career counseling,
AI development, and ethical practice will be discussed.

In the past decade, the deployment of deep learning (Artificial Intelligence
(AI)) methods has become pervasive across a spectrum of real-world
applications, often in safety-critical contexts. This comprehensive research
article rigorously investigates the ethical dimensions intricately linked to
the rapid evolution of AI technologies, with a particular focus on the
healthcare domain. Delving deeply, it explores a multitude of facets including
transparency, adept data management, human oversight, educational imperatives,
and international collaboration within the realm of AI advancement. Central to
this article is the proposition of a conscientious AI framework, meticulously
crafted to accentuate values of transparency, equity, answerability, and a
human-centric orientation. The second contribution of the article is the
in-depth and thorough discussion of the limitations inherent to AI systems. It
astutely identifies potential biases and the intricate challenges of navigating
multifaceted contexts. Lastly, the article unequivocally accentuates the
pressing need for globally standardized AI ethics principles and frameworks.
Simultaneously, it aptly illustrates the adaptability of the ethical framework
proposed herein, positioned skillfully to surmount emergent challenges.

Historical emphasis on writing mastery has shifted with advances in
generative AI, especially in scientific writing. This study analysed six AI
chatbots for scholarly writing in humanities and archaeology. Using methods
that assessed factual correctness and scientific contribution, ChatGPT-4 showed
the highest quantitative accuracy, closely followed by ChatGPT-3.5, Bing, and
Bard. However, Claude 2 and Aria scored considerably lower. Qualitatively, all
AIs exhibited proficiency in merging existing knowledge, but none produced
original scientific content. Inter-estingly, our findings suggest ChatGPT-4
might represent a plateau in large language model size. This research
emphasizes the unique, intricate nature of human research, suggesting that AI's
emulation of human originality in scientific writing is challenging. As of
2023, while AI has transformed content generation, it struggles with original
contributions in humanities. This may change as AI chatbots continue to evolve
into LLM-powered software.

With ubiquitous exposure of AI systems today, we believe AI development
requires crucial considerations to be deemed trustworthy. While the potential
of AI systems is bountiful, though, is still unknown-as are their risks. In
this work, we offer a brief, high-level overview of societal impacts of AI
systems. To do so, we highlight the requirement of multi-disciplinary
governance and convergence throughout its lifecycle via critical systemic
examinations (e.g., energy consumption), and later discuss induced effects on
the environment (i.e., carbon footprint) and its users (i.e., social
development). In particular, we consider these impacts from a
multi-disciplinary perspective: computer science, sociology, environmental
science, and so on to discuss its inter-connected societal risks and inability
to simultaneously satisfy aspects of well-being. Therefore, we accentuate the
necessity of holistically addressing pressing concerns of AI systems from a
socioethical impact assessment perspective to explicate its harmful societal
effects to truly enable humanity-centered Trustworthy AI.

With recent advances in generative AI, conversational models like ChatGPT
have become feasible candidates for TAs. We investigate the practicality of
using generative AI as TAs in introductory programming education by examining
novice learners' interaction with TAs in a subgoal learning environment. To
compare the learners' interaction and perception of the AI and human TAs, we
conducted a between-subject study with 20 novice programming learners. Learners
solve programming tasks by producing subgoals and subsolutions with the
guidance of a TA. Our study shows that learners can solve tasks faster with
comparable scores with AI TAs. Learners' perception of the AI TA is on par with
that of human TAs in terms of speed and comprehensiveness of the replies and
helpfulness, difficulty, and satisfaction of the conversation. Finally, we
suggest guidelines to better design and utilize generative AI as TAs in
programming education from the result of our chat log analysis.

AI Uncertainty Quantification (UQ) has the potential to improve human
decision-making beyond AI predictions alone by providing additional
probabilistic information to users. The majority of past research on AI and
human decision-making has concentrated on model explainability and
interpretability, with little focus on understanding the potential impact of UQ
on human decision-making. We evaluated the impact on human decision-making for
instance-level UQ, calibrated using a strict scoring rule, in two online
behavioral experiments. In the first experiment, our results showed that UQ was
beneficial for decision-making performance compared to only AI predictions. In
the second experiment, we found UQ had generalizable benefits for
decision-making across a variety of representations for probabilistic
information. These results indicate that implementing high quality,
instance-level UQ for AI may improve decision-making with real systems compared
to AI predictions alone.

We examine whether substantial AI automation could accelerate global economic
growth by about an order of magnitude, akin to the economic growth effects of
the Industrial Revolution. We identify three primary drivers for such growth:
1) the scalability of an AI ``labor force" restoring a regime of increasing
returns to scale, 2) the rapid expansion of an AI labor force, and 3) a massive
increase in output from rapid automation occurring over a brief period of time.
Against this backdrop, we evaluate nine counterarguments, including regulatory
hurdles, production bottlenecks, alignment issues, and the pace of automation.
We tentatively assess these arguments, finding most are unlikely deciders. We
conclude that explosive growth seems plausible with AI capable of broadly
substituting for human labor, but high confidence in this claim seems currently
unwarranted. Key questions remain about the intensity of regulatory responses
to AI, physical bottlenecks in production, the economic value of superhuman
abilities, and the rate at which AI automation could occur.

Despite the growing consensus that stakeholders affected by AI systems should
participate in their design, enormous variation and implicit disagreements
exist among current approaches. For researchers and practitioners who are
interested in taking a participatory approach to AI design and development, it
remains challenging to assess the extent to which any participatory approach
grants substantive agency to stakeholders. This article thus aims to ground
what we dub the "participatory turn" in AI design by synthesizing existing
theoretical literature on participation and through empirical investigation and
critique of its current practices. Specifically, we derive a conceptual
framework through synthesis of literature across technology design, political
theory, and the social sciences that researchers and practitioners can leverage
to evaluate approaches to participation in AI design. Additionally, we
articulate empirical findings concerning the current state of participatory
practice in AI design based on an analysis of recently published research and
semi-structured interviews with 12 AI researchers and practitioners. We use
these empirical findings to understand the current state of participatory
practice and subsequently provide guidance to better align participatory goals
and methods in a way that accounts for practical constraints.

Artificial intelligence (AI) and machine learning (ML) are increasingly
integrated into the functioning of physical and digital products, creating
unprecedented opportunities for interaction and functionality. However, there
is a challenge for designers to ideate within this creative landscape,
balancing the possibilities of technology with human interactional concerns. We
investigate techniques for exploring and reflecting on the interactional
affordances, the unique relational possibilities, and the wider social
implications of AI systems. We introduced into an interaction design course
(n=100) nine 'AI exercises' that draw on more than human design, responsible
AI, and speculative enactment to create experiential engagements around AI
interaction design. We find that exercises around metaphors and enactments make
questions of training and learning, privacy and consent, autonomy and agency
more tangible, and thereby help students be more reflective and responsible on
how to design with AI and its complex properties in both their design process
and outcomes.

Artificial intelligence has made significant progress in the last decade,
leading to a rise in the popularity of model sharing. The model zoo ecosystem,
a repository of pre-trained AI models, has advanced the AI open-source
community and opened new avenues for cyber risks. Malicious attackers can
exploit shared models to launch cyber-attacks. This work focuses on the
steganalysis of injected malicious Least Significant Bit (LSB) steganography
into AI models, and it is the first work focusing on AI model attacks. In
response to this threat, this paper presents a steganalysis method specifically
tailored to detect and mitigate malicious LSB steganography attacks based on
supervised and unsupervised AI detection steganalysis methods. Our proposed
technique aims to preserve the integrity of shared models, protect user trust,
and maintain the momentum of open collaboration within the AI community. In
this work, we propose 3 steganalysis methods and open source our code. We found
that the success of the steganalysis depends on the LSB attack location. If the
attacker decides to exploit the least significant bits in the LSB, the ability
to detect the attacks is low. However, if the attack is in the most significant
LSB bits, the attack can be detected with almost perfect accuracy.

Welcome to the sixth edition of the AI Index Report. This year, the report
introduces more original data than any previous edition, including a new
chapter on AI public opinion, a more thorough technical performance chapter,
original analysis about large language and multimodal models, detailed trends
in global AI legislation records, a study of the environmental impact of AI
systems, and more. The AI Index Report tracks, collates, distills, and
visualizes data related to artificial intelligence. Our mission is to provide
unbiased, rigorously vetted, broadly sourced data in order for policymakers,
researchers, executives, journalists, and the general public to develop a more
thorough and nuanced understanding of the complex field of AI. The report aims
to be the world's most credible and authoritative source for data and insights
about AI.

This paper presents Social data and knowledge collective intelligence
platform for TRaining Ethical AI Models (STREAM) to address the challenge of
aligning AI models with human moral values, and to provide ethics datasets and
knowledge bases to help promote AI models "follow good advice as naturally as a
stream follows its course". By creating a comprehensive and representative
platform that accurately mirrors the moral judgments of diverse groups
including humans and AIs, we hope to effectively portray cultural and group
variations, and capture the dynamic evolution of moral judgments over time,
which in turn will facilitate the Establishment, Evaluation, Embedding,
Embodiment, Ensemble, and Evolvement (6Es) of the moral capabilities of AI
models. Currently, STREAM has already furnished a comprehensive collection of
ethical scenarios, and amassed substantial moral judgment data annotated by
volunteers and various popular Large Language Models (LLMs), collectively
portraying the moral preferences and performances of both humans and AIs across
a range of moral contexts. This paper will outline the current structure and
construction of STREAM, explore its potential applications, and discuss its
future prospects.

Concerns around future dangers from advanced AI often centre on systems
hypothesised to have intrinsic characteristics such as agent-like behaviour,
strategic awareness, and long-range planning. We label this cluster of
characteristics as "Property X". Most present AI systems are low in "Property
X"; however, in the absence of deliberate steering, current research directions
may rapidly lead to the emergence of highly capable AI systems that are also
high in "Property X". We argue that "Property X" characteristics are
intrinsically dangerous, and when combined with greater capabilities will
result in AI systems for which safety and control is difficult to guarantee.
Drawing on several scholars' alternative frameworks for possible AI research
trajectories, we argue that most of the proposed benefits of advanced AI can be
obtained by systems designed to minimise this property. We then propose
indicators and governance interventions to identify and limit the development
of systems with risky "Property X" characteristics.

Generative AI (GenAI) has shown remarkable capabilities in generating diverse
and realistic content across different formats like images, videos, and text.
In Generative AI, human involvement is essential, thus HCI literature has
investigated how to effectively create collaborations between humans and GenAI
systems. However, the current literature lacks a comprehensive framework to
better understand Human-GenAI Interactions, as the holistic aspects of
human-centered GenAI systems are rarely analyzed systematically. In this paper,
we present a survey of 291 papers, providing a novel taxonomy and analysis of
Human-GenAI Interactions from both human and Gen-AI perspectives. The
dimensions of design space include 1) Purposes of Using Generative AI, 2)
Feedback from Models to Users, 3) Control from Users to Models, 4) Levels of
Engagement, 5) Application Domains, and 6) Evaluation Strategies. Our work is
also timely at the current development stage of GenAI, where the Human-GenAI
interaction design is of paramount importance. We also highlight challenges and
opportunities to guide the design of Gen-AI systems and interactions towards
the future design of human-centered Generative AI applications.

In a human-AI collaboration, users build a mental model of the AI system
based on its reliability and how it presents its decision, e.g. its
presentation of system confidence and an explanation of the output. Modern NLP
systems are often uncalibrated, resulting in confidently incorrect predictions
that undermine user trust. In order to build trustworthy AI, we must understand
how user trust is developed and how it can be regained after potential
trust-eroding events. We study the evolution of user trust in response to these
trust-eroding events using a betting game. We find that even a few incorrect
instances with inaccurate confidence estimates damage user trust and
performance, with very slow recovery. We also show that this degradation in
trust reduces the success of human-AI collaboration and that different types of
miscalibration -- unconfidently correct and confidently incorrect -- have
different negative effects on user trust. Our findings highlight the importance
of calibration in user-facing AI applications and shed light on what aspects
help users decide whether to trust the AI system.

This study explores the impact of AI-generated digital self-clones on
improving online presentation skills. We carried out a mixed-design experiment
involving 44 international students, comparing self-recorded videos (control)
with self-clone videos (AI group) for English presentation practice. The AI
videos utilized voice cloning, face swapping, lip-sync, and body-language
simulation to refine participants' original presentations in terms of
repetition, filler words, and pronunciation. Machine-rated scores indicated
enhancements in speech performance for both groups. Though the groups didn't
significantly differ, the AI group exhibited a heightened depth of reflection,
self-compassion, and a meaningful transition from a corrective to an enhancive
approach to self-critique. Within the AI group, congruence between
self-perception and AI self-clones resulted in diminished speech anxiety and
increased enjoyment. Our findings recommend the ethical employment of digital
self-clones to enhance the emotional and cognitive facets of skill development.

In recent years, the combination of artificial intelligence (AI) and unmanned
aerial vehicles (UAVs) has brought about advancements in various areas. This
comprehensive analysis explores the changing landscape of AI-powered UAVs and
friendly computing in their applications. It covers emerging trends, futuristic
visions, and the inherent challenges that come with this relationship. The
study examines how AI plays a role in enabling navigation, detecting and
tracking objects, monitoring wildlife, enhancing precision agriculture,
facilitating rescue operations, conducting surveillance activities, and
establishing communication among UAVs using environmentally conscious computing
techniques. By delving into the interaction between AI and UAVs, this analysis
highlights the potential for these technologies to revolutionise industries
such as agriculture, surveillance practices, disaster management strategies,
and more. While envisioning possibilities, it also takes a look at ethical
considerations, safety concerns, regulatory frameworks to be established, and
the responsible deployment of AI-enhanced UAV systems. By consolidating
insights from research endeavours in this field, this review provides an
understanding of the evolving landscape of AI-powered UAVs while setting the
stage for further exploration in this transformative domain.

We explore the value of generative AI tools, such as ChatGPT, in helping
investors uncover dimensions of corporate risk. We develop and validate
firm-level measures of risk exposure to political, climate, and AI-related
risks. Using the GPT 3.5 model to generate risk summaries and assessments from
the context provided by earnings call transcripts, we show that GPT-based
measures possess significant information content and outperform the existing
risk measures in predicting (abnormal) firm-level volatility and firms' choices
such as investment and innovation. Importantly, information in risk assessments
dominates that in risk summaries, establishing the value of general AI
knowledge. We also find that generative AI is effective at detecting emerging
risks, such as AI risk, which has soared in recent quarters. Our measures
perform well both within and outside the GPT's training window and are priced
in equity markets. Taken together, an AI-based approach to risk measurement
provides useful insights to users of corporate disclosures at a low cost.

In recent years, research involving human participants has been critical to
advances in artificial intelligence (AI) and machine learning (ML),
particularly in the areas of conversational, human-compatible, and cooperative
AI. For example, around 12% and 6% of publications at recent AAAI and NeurIPS
conferences indicate the collection of original human data, respectively. Yet
AI and ML researchers lack guidelines for ethical, transparent research
practices with human participants. Fewer than one out of every four of these
AAAI and NeurIPS papers provide details of ethical review, the collection of
informed consent, or participant compensation. This paper aims to bridge this
gap by exploring normative similarities and differences between AI research and
related fields that involve human participants. Though psychology,
human-computer interaction, and other adjacent fields offer historic lessons
and helpful insights, AI research raises several specific
concerns$\unicode{x2014}$namely, participatory design, crowdsourced dataset
development, and an expansive role of corporations$\unicode{x2014}$that
necessitate a contextual ethics framework. To address these concerns, this
paper outlines a set of guidelines for ethical and transparent practice with
human participants in AI and ML research. These guidelines can be found in
Section 4 on pp. 4$\unicode{x2013}$7.

Artificial intelligence experts often question whether AI is fair. They view
fairness as a property of AI systems rather than of sociopolitical and economic
systems. This paper emphasizes the need to be fair in the social, political,
and economic contexts within which an educational system operates and uses AI.
Taking Swedish decentralized compulsory education as the context, this paper
examines whether and how the use of AI envisaged by national authorities and
edtech companies exacerbates unfairness. A qualitative content analysis of
selected Swedish policy documents and edtech reports was conducted using the
concept of relevant social groups to understand how different groups view the
risks and benefits of AI for fairness. Three groups that view efficiency as a
key value of AI are identified, and interpreted as economical, pedagogical and
accessibility-related. By separating fairness from social justice, this paper
challenges the notion of fairness as the formal equality of opportunities.

Exciting advances in generative artificial intelligence (AI) have sparked
concern for jobs, education, productivity, and the future of work. As with past
technologies, generative AI may not lead to mass unemployment. But, unlike past
technologies, generative AI is creative, cognitive, and potentially ubiquitous
which makes the usual assumptions of automation predictions ill-suited for
today. Existing projections suggest that generative AI will impact workers in
occupations that were previously considered immune to automation. As AI's full
set of capabilities and applications emerge, policy makers should promote
workers' career adaptability. This goal requires improved data on job
separations and unemployment by locality and job titles in order to identify
early-indicators for the workers facing labor disruption. Further, prudent
policy should incentivize education programs to accommodate learning with AI as
a tool while preparing students for the demands of the future of work.

In this study, I explored the impact of Generative AI on learning efficacy in
academic reading materials using experimental methods. College-educated
participants engaged in three cycles of reading and writing tasks. After each
cycle, they responded to comprehension questions related to the material. After
adjusting for background knowledge and demographic factors, complete reliance
on AI for writing tasks led to a 25.1% reduction in accuracy. In contrast,
AI-assisted reading resulted in a 12% decline. Interestingly, using AI for
summarization significantly improved both quality and output. Accuracy
exhibited notable variance in the AI-assisted section. Further analysis
revealed that individuals with a robust background in the reading topic and
superior reading/writing skills benefitted the most. I conclude the research by
discussing educational policy implications, emphasizing the need for educators
to warn students about the dangers of over-dependence on AI and provide
guidance on its optimal use in educational settings.

As AI models evolve, understanding the influence of underlying models on user
experience and performance in AI-infused systems becomes critical, particularly
while transitioning between different model versions. We studied the influence
of model change by conducting two complementary studies in the context of
AI-based facial recognition for historical person identification tasks. First,
we ran an online experiment where crowd workers interacted with two different
facial recognition models: an older version and a recently updated,
developer-certified more accurate model. Second, we studied a real-world
deployment of these models on a popular historical photo platform through a
diary study with 10 users. Our findings sheds light on models affecting
human-AI team performance, users' abilities to differentiate between different
models, the folk theories they develop, and how these theories influence their
preferences. Drawing from these insights, we discuss design implications for
updating models in AI-infused systems.

This article analyzes AI ethics from a distinct business ethics perspective,
i.e., 'ordoliberalism 2.0.' It argues that the ongoing discourse on
(generative) AI relies too much on corporate self-regulation and voluntary
codes of conduct and thus lacks adequate governance mechanisms. To address
these issues, the paper suggests not only introducing hard-law legislation with
a more effective oversight structure but also merging already existing AI
guidelines with an ordoliberal-inspired regulatory and competition policy.
However, this link between AI ethics, regulation, and antitrust is not yet
adequately discussed in the academic literature and beyond. The paper thus
closes a significant gap in the academic literature and adds to the
predominantly legal-political and philosophical discourse on AI governance. The
paper's research questions and goals are twofold: First, it identifies
ordoliberal-inspired AI ethics principles that could serve as the foundation
for a 'digital bill of rights.' Second, it shows how those principles could be
implemented at the macro level with the help of ordoliberal competition and
regulatory policy.

This paper explores the space of optimizing feedback mechanisms in complex
domains, such as data science, by combining two prevailing approaches:
Artificial Intelligence (AI) and learnersourcing. Towards addressing the
challenges posed by each approach, this work compares traditional
learnersourcing with an AI-supported approach. We report on the results of a
randomized controlled experiment conducted with 72 Master's level students in a
data visualization course, comparing two conditions: students writing hints
independently versus revising hints generated by GPT-4. The study aimed to
evaluate the quality of learnersourced hints, examine the impact of student
performance on hint quality, gauge learner preference for writing hints with or
without AI support, and explore the potential of the student-AI collaborative
exercise in fostering critical thinking about LLMs. Based on our findings, we
provide insights for designing learnersourcing activities leveraging AI support
and optimizing students' learning as they interact with LLMs.

Artificial Intelligence (AI), particularly through the advent of large-scale
generative AI (GenAI) models such as Large Language Models (LLMs), has become a
transformative element in contemporary technology. While these models have
unlocked new possibilities, they simultaneously present significant challenges,
such as concerns over data privacy and the propensity to generate misleading or
fabricated content. Current frameworks for Responsible AI (RAI) often fall
short in providing the granular guidance necessary for tangible application,
especially for Accountability-a principle that is pivotal for ensuring
transparent and auditable decision-making, bolstering public trust, and meeting
increasing regulatory expectations. This study bridges the accountability gap
by introducing our effort towards a comprehensive metrics catalogue, formulated
through a systematic multivocal literature review (MLR) that integrates
findings from both academic and grey literature. Our catalogue delineates
process metrics that underpin procedural integrity, resource metrics that
provide necessary tools and frameworks, and product metrics that reflect the
outputs of AI systems. This tripartite framework is designed to operationalize
Accountability in AI, with a special emphasis on addressing the intricacies of
GenAI.

This study investigated the performance, explainability, and robustness of
deployed artificial intelligence (AI) models in predicting mortality during the
COVID-19 pandemic and beyond. The first study of its kind, we found that
Bayesian Neural Networks (BNNs) and intelligent training techniques allowed our
models to maintain performance amidst significant data shifts. Our results
emphasize the importance of developing robust AI models capable of matching or
surpassing clinician predictions, even under challenging conditions. Our
exploration of model explainability revealed that stochastic models generate
more diverse and personalized explanations thereby highlighting the need for AI
models that provide detailed and individualized insights in real-world clinical
settings. Furthermore, we underscored the importance of quantifying uncertainty
in AI models which enables clinicians to make better-informed decisions based
on reliable predictions. Our study advocates for prioritizing implementation
science in AI research for healthcare and ensuring that AI solutions are
practical, beneficial, and sustainable in real-world clinical environments. By
addressing unique challenges and complexities in healthcare settings,
researchers can develop AI models that effectively improve clinical practice
and patient outcomes.

In the rapidly advancing field of artificial intelligence, software
development has emerged as a key area of innovation. Despite the plethora of
general-purpose AI assistants available, their effectiveness diminishes in
complex, domain-specific scenarios. Noting this limitation, both the academic
community and industry players are relying on contextualized coding AI
assistants. These assistants surpass general-purpose AI tools by integrating
proprietary, domain-specific knowledge, offering precise and relevant
solutions. Our study focuses on the initial experiences of 62 participants who
used a contextualized coding AI assistant -- named StackSpot AI -- in a
controlled setting. According to the participants, the assistants' use resulted
in significant time savings, easier access to documentation, and the generation
of accurate codes for internal APIs. However, challenges associated with the
knowledge sources necessary to make the coding assistant access more contextual
information as well as variable responses and limitations in handling complex
codes were observed. The study's findings, detailing both the benefits and
challenges of contextualized AI assistants, underscore their potential to
revolutionize software development practices, while also highlighting areas for
further refinement.

A current concern in the field of Artificial Intelligence (AI) is to ensure
the trustworthiness of AI systems. The development of explainability methods is
one prominent way to address this, which has often resulted in the assumption
that the use of explainability will lead to an increase in the trust of users
and wider society. However, the dynamics between explainability and trust are
not well established and empirical investigations of their relation remain
mixed or inconclusive. In this paper we provide a detailed description of the
concepts of user trust and distrust in AI and their relation to appropriate
reliance. For that we draw from the fields of machine learning, human-computer
interaction, and the social sciences. Furthermore, we have created a survey of
existing empirical studies that investigate the effects of AI systems and XAI
methods on user (dis)trust. With clarifying the concepts and summarizing the
empirical investigations, we aim to provide researchers, who examine user trust
in AI, with an improved starting point for developing user studies to measure
and evaluate the user's attitude towards and reliance on AI systems.

This research paper delves into the evolving landscape of fine-tuning large
language models (LLMs) to align with human users, extending beyond basic
alignment to propose "personality alignment" for language models in
organizational settings. Acknowledging the impact of training methods on the
formation of undefined personality traits in AI models, the study draws
parallels with human fitting processes using personality tests. Through an
original case study, we demonstrate the necessity of personality fine-tuning
for AIs and raise intriguing questions about applying human-designed tests to
AIs, engineering specialized AI personality tests, and shaping AI personalities
to suit organizational roles. The paper serves as a starting point for
discussions and developments in the burgeoning field of AI personality
alignment, offering a foundational anchor for future exploration in
human-machine teaming and co-existence.

Recent breakthroughs in artificial intelligence (AI) algorithms have
highlighted the need for novel computing hardware in order to truly unlock the
potential for AI. Physics-based hardware, such as thermodynamic computing, has
the potential to provide a fast, low-power means to accelerate AI primitives,
especially generative AI and probabilistic AI. In this work, we present the
first continuous-variable thermodynamic computer, which we call the stochastic
processing unit (SPU). Our SPU is composed of RLC circuits, as unit cells, on a
printed circuit board, with 8 unit cells that are all-to-all coupled via
switched capacitances. It can be used for either sampling or linear algebra
primitives, and we demonstrate Gaussian sampling and matrix inversion on our
hardware. The latter represents the first thermodynamic linear algebra
experiment. We also illustrate the applicability of the SPU to uncertainty
quantification for neural network classification. We envision that this
hardware, when scaled up in size, will have significant impact on accelerating
various probabilistic AI applications.

This paper presents a critical analysis of generative Artificial Intelligence
(AI) detection tools in higher education assessments. The rapid advancement and
widespread adoption of generative AI, particularly in education, necessitates a
reevaluation of traditional academic integrity mechanisms. We explore the
effectiveness, vulnerabilities, and ethical implications of AI detection tools
in the context of preserving academic integrity. Our study synthesises insights
from various case studies, newspaper articles, and student testimonies to
scrutinise the practical and philosophical challenges associated with AI
detection. We argue that the reliance on detection mechanisms is misaligned
with the educational landscape, where AI plays an increasingly widespread role.
This paper advocates for a strategic shift towards robust assessment methods
and educational policies that embrace generative AI usage while ensuring
academic integrity and authenticity in assessments.

Fairness in AI is a growing concern for high-stakes decision making. Engaging
stakeholders, especially lay users, in fair AI development is promising yet
overlooked. Recent efforts explore enabling lay users to provide AI
fairness-related feedback, but there is still a lack of understanding of how to
integrate users' feedback into an AI model and the impacts of doing so. To
bridge this gap, we collected feedback from 58 lay users on the fairness of a
XGBoost model trained on the Home Credit dataset, and conducted offline
experiments to investigate the effects of retraining models on accuracy, and
individual and group fairness. Our work contributes baseline results of
integrating user fairness feedback in XGBoost, and a dataset and code framework
to bootstrap research in engaging stakeholders in AI fairness. Our discussion
highlights the challenges of employing user feedback in AI fairness and points
the way to a future application area of interactive machine learning.

Intelligent transportation systems play a crucial role in modern traffic
management and optimization, greatly improving traffic efficiency and safety.
With the rapid development of generative artificial intelligence (Generative
AI) technologies in the fields of image generation and natural language
processing, generative AI has also played a crucial role in addressing key
issues in intelligent transportation systems, such as data sparsity, difficulty
in observing abnormal scenarios, and in modeling data uncertainty. In this
review, we systematically investigate the relevant literature on generative AI
techniques in addressing key issues in different types of tasks in intelligent
transportation systems. First, we introduce the principles of different
generative AI techniques, and their potential applications. Then, we classify
tasks in intelligent transportation systems into four types: traffic
perception, traffic prediction, traffic simulation, and traffic
decision-making. We systematically illustrate how generative AI techniques
addresses key issues in these four different types of tasks. Finally, we
summarize the challenges faced in applying generative AI to intelligent
transportation systems, and discuss future research directions based on
different application scenarios.

Recent publications explore AI biases in detecting objects and people in the
environment. However, there is no research tackling how AI examines nature.
This case study presents a pioneering exploration into the AI attitudes
(ecocentric, anthropocentric and antipathetic) toward nature. Experiments with
a Large Language Model (LLM) and an image captioning algorithm demonstrate the
presence of anthropocentric biases in AI. Moreover, to delve deeper into these
biases and Human-Nature-AI interaction, we conducted a real-life experiment in
which participants underwent an immersive de-anthropocentric experience in a
forest and subsequently engaged with ChatGPT to co-create narratives. By
creating fictional AI chatbot characters with ecocentric attributes, emotions
and views, we successfully amplified ecocentric exchanges. We encountered some
difficulties, mainly that participants deviated from narrative co-creation to
short dialogues and questions and answers, possibly due to the novelty of
interacting with LLMs. To solve this problem, we recommend providing
preliminary guidelines on interacting with LLMs and allowing participants to
get familiar with the technology. We plan to repeat this experiment in various
countries and forests to expand our corpus of ecocentric materials.

AI has been proposed as an important tool to support several efforts related
to nature-based climate solutions such as the detection of wildfires that
affect forests and vegetation-based offsets. While this and other use-cases
provide important demonstrative value of the power of AI in climate change
mitigation, such efforts have typically been undertaken in silos, without
awareness of the integrative nature of real-world climate policy-making. In
this paper, we propose a novel overarching framework for AI-aided integrated
and comprehensive decision support for various aspects of nature-based climate
decision-making. Focusing on vegetation-based solutions such as forests, we
demonstrate how different AI-aided decision support models such as AI-aided
wildfire detection, AI-aided vegetation carbon stock assessment, reversal risk
mitigation, and disaster response planning can be integrated into a
comprehensive framework. Rather than being disparate elements, we posit that
the exchange of data and analytical results across elements of the framework,
and careful mitigation of uncertainty propagation will provide tremendous value
relative to the status-quo for real-world climate policy-making.

$ $The usage of generative artificial intelligence (AI) tools based on large
language models, including ChatGPT, Bard, and Claude, for text generation has
many exciting applications with the potential for phenomenal productivity
gains. One issue is authorship attribution when using AI tools. This is
especially important in an academic setting where the inappropriate use of
generative AI tools may hinder student learning or stifle research by creating
a large amount of automatically generated derivative work. Existing plagiarism
detection systems can trace the source of submitted text but are not yet
equipped with methods to accurately detect AI-generated text. This paper
introduces the idea of direct origin detection and evaluates whether generative
AI systems can recognize their output and distinguish it from human-written
texts. We argue why current transformer-based models may be able to self-detect
their own generated text and perform a small empirical study using zero-shot
learning to investigate if that is the case. Results reveal varying
capabilities of AI systems to identify their generated text. Google's Bard
model exhibits the largest capability of self-detection with an accuracy of
94\%, followed by OpenAI's ChatGPT with 83\%. On the other hand, Anthropic's
Claude model seems to be not able to self-detect.

The remarkable advancements in artificial intelligence (AI), primarily driven
by deep neural networks, have significantly impacted various aspects of our
lives. However, the current challenges surrounding unsustainable computational
trajectories, limited robustness, and a lack of explainability call for the
development of next-generation AI systems. Neuro-symbolic AI (NSAI) emerges as
a promising paradigm, fusing neural, symbolic, and probabilistic approaches to
enhance interpretability, robustness, and trustworthiness while facilitating
learning from much less data. Recent NSAI systems have demonstrated great
potential in collaborative human-AI scenarios with reasoning and cognitive
capabilities. In this paper, we provide a systematic review of recent progress
in NSAI and analyze the performance characteristics and computational operators
of NSAI models. Furthermore, we discuss the challenges and potential future
directions of NSAI from both system and architectural perspectives.

Appropriate reliance is critical to achieving synergistic human-AI
collaboration. For instance, when users over-rely on AI assistance, their
human-AI team performance is bounded by the model's capability. This work
studies how the presentation of model uncertainty may steer users'
decision-making toward fostering appropriate reliance. Our results demonstrate
that showing the calibrated model uncertainty alone is inadequate. Rather,
calibrating model uncertainty and presenting it in a frequency format allow
users to adjust their reliance accordingly and help reduce the effect of
confirmation bias on their decisions. Furthermore, the critical nature of our
skin cancer screening task skews participants' judgment, causing their reliance
to vary depending on their initial decision. Additionally, step-wise multiple
regression analyses revealed how user demographics such as age and familiarity
with probability and statistics influence human-AI collaborative
decision-making. We discuss the potential for model uncertainty presentation,
initial user decision, and user demographics to be incorporated in designing
personalized AI aids for appropriate reliance.

Machine learning (ML) and artificial intelligence (AI) techniques have now
become commonplace in software products and services. When threat modelling a
system, it is therefore important that we consider threats unique to ML and AI
techniques, in addition to threats to our software. In this paper, we present a
threat model that can be used to systematically uncover threats to AI based
software. The threat model consists of two main parts, a model of the software
development process for AI based software and an attack taxonomy that has been
developed using attacks found in adversarial AI research. We apply the threat
model to two real life AI based software and discuss the process and the
threats found.

In an era of AI's growing capabilities and influences, recent advancements
are reshaping HCI and CSCW's view of AI as mere tools. Playful interactions
with AI systems naturally emerged as a way for users to make sense of the
ever-changing technology. However, these emergent and playful interactions are
underexamined. We target this gap by investigating playful interactions
exhibited by users of a recently trending powerful AI technology, ChatGPT.
Through a thematic analysis of 372 user-generated posts on the ChatGPT
subreddit, we found that a substantial portion of user discourse revolves
around playful interactions. The analysis further allowed us to construct a
preliminary taxonomy to describe these interactions, categorizing them into six
types: reflecting, jesting, imitating, challenging, tricking, and contriving;
each included sub-categories. Overall, this study contributes to the field of
HCI and CSCW by illuminating the multifaceted nature of playful interactions
with AI, underlining their significance in shaping the human-AI relationship.

Powered by the increasing predictive capabilities of machine learning
algorithms, artificial intelligence (AI) systems have begun to be used to
overrule human mistakes in many settings. We provide the first field evidence
this AI oversight carries psychological costs that can impact human
decision-making. We investigate one of the highest visibility settings in which
AI oversight has occurred: the Hawk-Eye review of umpires in top tennis
tournaments. We find that umpires lowered their overall mistake rate after the
introduction of Hawk-Eye review, in line with rational inattention given
psychological costs of being overruled by AI. We also find that umpires
increased the rate at which they called balls in, which produced a shift from
making Type II errors (calling a ball out when in) to Type I errors (calling a
ball in when out). We structurally estimate the psychological costs of being
overruled by AI using a model of rational inattentive umpires, and our results
suggest that because of these costs, umpires cared twice as much about Type II
errors under AI oversight.

Responsible design of AI systems is a shared goal across HCI and AI
communities. Responsible AI (RAI) tools have been developed to support
practitioners to identify, assess, and mitigate ethical issues during AI
development. These tools take many forms (e.g., design playbooks, software
toolkits, documentation protocols). However, research suggests that use of RAI
tools is shaped by organizational contexts, raising questions about how
effective such tools are in practice. To better understand how RAI tools are --
and might be -- evaluated, we conducted a qualitative analysis of 37
publications that discuss evaluations of RAI tools. We find that most
evaluations focus on usability, while questions of tools' effectiveness in
changing AI development are sidelined. While usability evaluations are an
important approach to evaluate RAI tools, we draw on evaluation approaches from
other fields to highlight developer- and community-level steps to support
evaluations of RAI tools' effectiveness in shaping AI development practices and
outcomes.

The dominant paradigm in AI ethics and value alignment is highly
anthropocentric. The focus of these disciplines is strictly on human values
which limits the depth and breadth of their insights. Recently, attempts to
expand to a sentientist perspective have been initiated. We argue that neither
of these outlooks is sufficient to capture the actual complexity of the
biosphere and ensure that AI does not damage it. Thus, we propose a new
paradigm -- Biospheric AI that assumes an ecocentric perspective. We discuss
hypothetical ways in which such an AI might be designed. Moreover, we give
directions for research and application of the modern AI models that would be
consistent with the biospheric interests. All in all, this work attempts to
take first steps towards a comprehensive program of research that focuses on
the interactions between AI and the biosphere.

Collaboration is key to STEM, where multidisciplinary team research can solve
complex problems. However, inequality in STEM fields hinders their full
potential, due to persistent psychological barriers in underrepresented
students' experience. This paper documents teamwork in STEM and explores the
transformative potential of computational modeling and generative AI in
promoting STEM-team diversity and inclusion. Leveraging generative AI, this
paper outlines two primary areas for advancing diversity, equity, and
inclusion. First, formalizing collaboration assessment with inclusive analytics
can capture fine-grained learner behavior. Second, adaptive, personalized AI
systems can support diversity and inclusion in STEM teams. Four policy
recommendations highlight AI's capacity: formalized collaborative skill
assessment, inclusive analytics, funding for socio-cognitive research, human-AI
teaming for inclusion training. Researchers, educators, policymakers can build
an equitable STEM ecosystem. This roadmap advances AI-enhanced collaboration,
offering a vision for the future of STEM where diverse voices are actively
encouraged and heard within collaborative scientific endeavors.

An important aim of this paper is to convey some basics of mathematical logic
to the legal community working with Artificial Intelligence. After analysing
what AI is, we decide to delimit ourselves to rule-based AI leaving Neural
Networks and Machine Learning aside. Rule based AI allows for Formal methods
which are described in a rudimentary form. We will then see how mathematical
logic interacts with legal rule-based AI practice. We shall see how
mathematical logic imposes limitations and complications to AI applications. We
classify the limitations and interactions between mathematical logic and legal
AI in three categories: logical, computational and mathematical. The examples
to showcase the interactions will largely come from European traffic
regulations. The paper closes off with some reflections on how and where AI
could be used and on basic mechanisms that shape society.

This paper evaluates No-Code AutoML as a solution for challenges in AI
product prototyping, characterized by unpredictability and inaccessibility to
non-experts, and proposes a conceptual framework. This complexity of AI
products hinders seamless execution and interdisciplinary collaboration crucial
for human-centered AI products. Relevant to industry and innovation, it affects
strategic decision-making and investment risk mitigation. Current approaches
provide limited insights into the potential and feasibility of AI product
ideas. Employing Design Science Research, the study identifies challenges and
integrates no-code AutoML as a solution by presenting a framework for AI
product prototyping with No-code AutoML. A case study confirms its potential in
supporting non-experts, offering a structured approach to AI product
development. The framework facilitates accessible and interpretable
prototyping, benefiting academia, managers, and decision-makers. Strategic
integration of no-code AutoML enhances efficiency, empowers non-experts, and
informs early-stage decisions, albeit with acknowledged limitations.

In recent years, black-box machine learning approaches have become a dominant
modeling paradigm for knowledge extraction in Remote Sensing. Despite the
potential benefits of uncovering the inner workings of these models with
explainable AI, a comprehensive overview summarizing the used explainable AI
methods and their objectives, findings, and challenges in Remote Sensing
applications is still missing. In this paper, we address this issue by
performing a systematic review to identify the key trends of how explainable AI
is used in Remote Sensing and shed light on novel explainable AI approaches and
emerging directions that tackle specific Remote Sensing challenges. We also
reveal the common patterns of explanation interpretation, discuss the extracted
scientific insights in Remote Sensing, and reflect on the approaches used for
explainable AI methods evaluation. Our review provides a complete summary of
the state-of-the-art in the field. Further, we give a detailed outlook on the
challenges and promising research directions, representing a basis for novel
methodological development and a useful starting point for new researchers in
the field of explainable AI in Remote Sensing.

AI tools are increasingly deployed in community contexts. However, datasets
used to evaluate AI are typically created by developers and annotators outside
a given community, which can yield misleading conclusions about AI performance.
How might we empower communities to drive the intentional design and curation
of evaluation datasets for AI that impacts them? We investigate this question
on Wikipedia, an online community with multiple AI-based content moderation
tools deployed. We introduce Wikibench, a system that enables communities to
collaboratively curate AI evaluation datasets, while navigating ambiguities and
differences in perspective through discussion. A field study on Wikipedia shows
that datasets curated using Wikibench can effectively capture community
consensus, disagreement, and uncertainty. Furthermore, study participants used
Wikibench to shape the overall data curation process, including refining label
definitions, determining data inclusion criteria, and authoring data
statements. Based on our findings, we propose future directions for systems
that support community-driven data curation.

In the European context, both the EU AI Act proposal and the draft
Standardisation Request on safe and trustworthy AI link standardisation to
fundamental rights. However, these texts do not provide any guidelines that
specify and detail the relationship between AI standards and fundamental
rights, its meaning or implication. This chapter aims to clarify this critical
regulatory blind spot. The main issue tackled is whether the adoption of AI
harmonised standards, based on the future AI Act, should take into account
fundamental rights. In our view, the response is yes. The high risks posed by
certain AI systems relate in particular to infringements of fundamental rights.
Therefore, mitigating such risks involves fundamental rights considerations and
this is what future harmonised standards should reflect. At the same time,
valid criticisms of the European standardisation process have to be addressed.
Finally, the practical incorporation of fundamental rights considerations in
the ongoing European standardisation of AI systems is discussed.

Generative AI has the potential to create a new form of interactive media:
AI-bridged creative language arts (CLA), which bridge the author and audience
by personalizing the author's vision to the audience's context and taste at
scale. However, it is unclear what the authors' values and attitudes would be
regarding AI-bridged CLA. To identify these values and attitudes, we conducted
an interview study with 18 authors across eight genres (e.g., poetry, comics)
by presenting speculative but realistic AI-bridged CLA scenarios. We identified
three benefits derived from the dynamics between author, artifact, and
audience: those that 1) authors get from the process, 2) audiences get from the
artifact, and 3) authors get from the audience. We found how AI-bridged CLA
would either promote or reduce these benefits, along with authors' concerns. We
hope our investigation hints at how AI can provide intriguing experiences to
CLA audiences while promoting authors' values.

Content creators increasingly utilize generative artificial intelligence
(Gen-AI) on platforms such as YouTube, TikTok, Instagram, and various blogging
sites to produce imaginative images, AI-generated videos, and articles using
Large Language Models (LLMs). Despite its growing popularity, there remains an
underexplored area concerning the specific domains where AI-generated content
is being applied, and the methodologies content creators employ with Gen-AI
tools during the creation process. This study initially explores this emerging
area through a qualitative analysis of 68 YouTube videos demonstrating Gen-AI
usage. Our research focuses on identifying the content domains, the variety of
tools used, the activities performed, and the nature of the final products
generated by Gen-AI in the context of user-generated content.

Advances in Generative Artificial Intelligence (AI) are resulting in
AI-generated media output that is (nearly) indistinguishable from human-created
content. This can drastically impact users and the media sector, especially
given global risks of misinformation. While the currently discussed European AI
Act aims at addressing these risks through Article 52's AI transparency
obligations, its interpretation and implications remain unclear. In this early
work, we adopt a participatory AI approach to derive key questions based on
Article 52's disclosure obligations. We ran two workshops with researchers,
designers, and engineers across disciplines (N=16), where participants
deconstructed Article 52's relevant clauses using the 5W1H framework. We
contribute a set of 149 questions clustered into five themes and 18 sub-themes.
We believe these can not only help inform future legal developments and
interpretations of Article 52, but also provide a starting point for
Human-Computer Interaction research to (re-)examine disclosure transparency
from a human-centered AI lens.

The evolution of Artificial Intelligence Generated Contents (AIGCs) is
advancing towards higher quality. The growing interactions with AIGCs present a
new challenge to the data-driven AI community: While AI-generated contents have
played a crucial role in a wide range of AI models, the potential hidden risks
they introduce have not been thoroughly examined. Beyond human-oriented forgery
detection, AI-generated content poses potential issues for AI models originally
designed to process natural data. In this study, we underscore the exacerbated
hallucination phenomena in Large Vision-Language Models (LVLMs) caused by
AI-synthetic images. Remarkably, our findings shed light on a consistent AIGC
\textbf{hallucination bias}: the object hallucinations induced by synthetic
images are characterized by a greater quantity and a more uniform position
distribution, even these synthetic images do not manifest unrealistic or
additional relevant visual features compared to natural images. Moreover, our
investigations on Q-former and Linear projector reveal that synthetic images
may present token deviations after visual projection, thereby amplifying the
hallucination bias.

Identifying scientific publications that are within a dynamic field of
research often requires costly annotation by subject-matter experts. Resources
like widely-accepted classification criteria or field taxonomies are
unavailable for a domain like artificial intelligence (AI), which spans
emerging topics and technologies. We address these challenges by inferring a
functional definition of AI research from existing expert labels, and then
evaluating state-of-the-art chatbot models on the task of expert data
annotation. Using the arXiv publication database as ground-truth, we experiment
with prompt engineering for GPT chatbot models to identify an alternative,
automated expert annotation pipeline that assigns AI labels with 94% accuracy.
For comparison, we fine-tune SPECTER, a transformer language model pre-trained
on scientific publications, that achieves 96% accuracy (only 2% higher than
GPT) on classifying AI publications. Our results indicate that with effective
prompt engineering, chatbots can be used as reliable data annotators even where
subject-area expertise is required. To evaluate the utility of
chatbot-annotated datasets on downstream classification tasks, we train a new
classifier on GPT-labeled data and compare its performance to the arXiv-trained
model. The classifier trained on GPT-labeled data outperforms the arXiv-trained
model by nine percentage points, achieving 82% accuracy.

In AI-assisted decision-making, it is crucial but challenging for humans to
achieve appropriate reliance on AI. This paper approaches this problem from a
human-centered perspective, "human self-confidence calibration". We begin by
proposing an analytical framework to highlight the importance of calibrated
human self-confidence. In our first study, we explore the relationship between
human self-confidence appropriateness and reliance appropriateness. Then in our
second study, We propose three calibration mechanisms and compare their effects
on humans' self-confidence and user experience. Subsequently, our third study
investigates the effects of self-confidence calibration on AI-assisted
decision-making. Results show that calibrating human self-confidence enhances
human-AI team performance and encourages more rational reliance on AI (in some
aspects) compared to uncalibrated baselines. Finally, we discuss our main
findings and provide implications for designing future AI-assisted
decision-making interfaces.

Sustainable AI projects are continuously responsive to the transformative
effects as well as short-, medium-, and long-term impacts on individuals and
society that the design, development, and deployment of AI technologies may
have. Projects, which centre AI Sustainability, ensure that values-led,
collaborative, and anticipatory reflection both guides the assessment of
potential social and ethical impacts and steers responsible innovation
practices.
  This workbook is the first part of a pair that provides the concepts and
tools needed to put AI Sustainability into practice. It introduces the SUM
Values, which help AI project teams to assess the potential societal impacts
and ethical permissibility of their projects. It then presents a Stakeholder
Engagement Process (SEP), which provides tools to facilitate proportionate
engagement of and input from stakeholders with an emphasis on equitable and
meaningful participation and positionality awareness.

Artificial Intelligence (AI) requires new ways of evaluating national
technology use and strategy for African nations. We conduct a survey of
existing 'readiness' assessments both for general digital adoption and for AI
policy in particular. We conclude that existing global readiness assessments do
not fully capture African states' progress in AI readiness and lay the
groundwork for how assessments can be better used for the African context. We
consider the extent to which these indicators map to the African context and
what these indicators miss in capturing African states' on-the-ground work in
meeting AI capability. Through case studies of four African nations of diverse
geographic and economic dimensions, we identify nuances missed by global
assessments and offer high-level policy considerations for how states can best
improve their AI readiness standards and prepare their societies to capture the
benefits of AI.

Artificial Intelligence (AI) is becoming ubiquitous in domains such as
medicine and natural science research. However, when AI systems are implemented
in practice, domain experts often refuse them. Low acceptance hinders effective
human-AI collaboration, even when it is essential for progress. In natural
science research, scientists' ineffective use of AI-enabled systems can impede
them from analysing their data and advancing their research. We conducted an
ethnographically informed study of 10 in-depth interviews with AI practitioners
and natural scientists at the organisation facing low adoption of algorithmic
systems. Results were consolidated into recommendations for better AI adoption:
i) actively supporting experts during the initial stages of system use, ii)
communicating the capabilities of a system in a user-relevant way, and iii)
following predefined collaboration rules. We discuss the broader implications
of our findings and expand on how our proposed requirements could support
practitioners and experts across domains.

The topic of fairness in AI, as debated in the FATE (Fairness,
Accountability, Transparency, and Ethics in AI) communities, has sparked
meaningful discussions in the past years. However, from a legal perspective,
particularly from European Union law, many open questions remain. Whereas
algorithmic fairness aims to mitigate structural inequalities at the design
level, European non-discrimination law is tailored to individual cases of
discrimination after an AI model has been deployed. The AI Act might present a
tremendous step towards bridging these two concepts by shifting
non-discrimination responsibilities into the design stage of AI models. Based
on an integrative reading of the AI Act, we comment on legal as well as
technical enforcement problems and propose practical implications on bias
detection and bias correction in order to specify and comply with specific
technical requirements.

We explore ideas and inclusive practices for designing and testing
child-centered artificially intelligent technologies for neurodivergent
children. AI is promising for supporting social communication, self-regulation,
and sensory processing challenges common for neurodivergent children. The
authors, both neurodivergent individuals and related to neurodivergent people,
draw from their professional and personal experiences to offer insights on
creating AI technologies that are accessible and include input from
neurodivergent children. We offer ideas for designing AI technologies for
neurodivergent children and considerations for including them in the design
process while accounting for their sensory sensitivities. We conclude by
emphasizing the importance of adaptable and supportive AI technologies and
design processes and call for further conversation to refine child-centered AI
design and testing methods.

The field of eXplainable artificial intelligence (XAI) has produced a
plethora of methods (e.g., saliency-maps) to gain insight into artificial
intelligence (AI) models, and has exploded with the rise of deep learning (DL).
However, human-participant studies question the efficacy of these methods,
particularly when the AI output is wrong. In this study, we collected and
analyzed 156 human-generated text and saliency-based explanations collected in
a question-answering task (N=40) and compared them empirically to
state-of-the-art XAI explanations (integrated gradients, conservative LRP, and
ChatGPT) in a human-participant study (N=136). Our findings show that
participants found human saliency maps to be more helpful in explaining AI
answers than machine saliency maps, but performance negatively correlated with
trust in the AI model and explanations. This finding hints at the dilemma of AI
errors in explanation, where helpful explanations can lead to lower task
performance when they support wrong AI predictions.

Building socially-intelligent AI agents (Social-AI) is a multidisciplinary,
multimodal research goal that involves creating agents that can sense,
perceive, reason about, learn from, and respond to affect, behavior, and
cognition of other agents (human or artificial). Progress towards Social-AI has
accelerated in the past decade across several computing communities, including
natural language processing, machine learning, robotics, human-machine
interaction, computer vision, and speech. Natural language processing, in
particular, has been prominent in Social-AI research, as language plays a key
role in constructing the social world. In this position paper, we identify a
set of underlying technical challenges and open questions for researchers
across computing communities to advance Social-AI. We anchor our discussion in
the context of social intelligence concepts and prior progress in Social-AI
research.

This study explores a new method in food development by utilizing AI
including generative AI, aiming to craft products that delight the senses and
resonate with consumers' emotions. The food ingredient recommendation approach
used in this study can be considered as a form of multimodal generation in a
broad sense, as it takes text as input and outputs food ingredient candidates.
This Study focused on producing "Romance Bread," a collection of breads infused
with flavors that reflect the nuances of a romantic Japanese television
program. We analyzed conversations from TV programs and lyrics from songs
featuring fruits and sweets to recommend ingredients that express romantic
feelings. Based on these recommendations, the bread developers then considered
the flavoring of the bread and developed new bread varieties. The research
included a tasting evaluation involving 31 participants and interviews with the
product developers. Findings indicate a notable correlation between tastes
generated by AI and human preferences. This study validates the concept of
using AI in food innovation and highlights the broad potential for developing
unique consumer experiences that focus on emotional engagement through AI and
human collaboration.

Reinforcing or even exacerbating societal biases and inequalities will
increase significantly as generative AI increasingly produces useful artifacts,
from text to images and beyond, for the real world. We address these issues by
formally characterizing the notion of fairness for generative AI as a basis for
monitoring and enforcing fairness. We define two levels of fairness using the
notion of infinite sequences of abstractions of AI-generated artifacts such as
text or images. The first is the fairness demonstrated on the generated
sequences, which is evaluated only on the outputs while agnostic to the prompts
and models used. The second is the inherent fairness of the generative AI
model, which requires that fairness be manifested when input prompts are
neutral, that is, they do not explicitly instruct the generative AI to produce
a particular type of output. We also study relative intersectional fairness to
counteract the combinatorial explosion of fairness when considering multiple
categories together with lazy fairness enforcement. Finally, fairness
monitoring and enforcement are tested against some current generative AI
models.

The pervasive integration of Artificial Intelligence (AI) has introduced
complex challenges in the responsibility and accountability in the event of
incidents involving AI-enabled systems. The interconnectivity of these systems,
ethical concerns of AI-induced incidents, coupled with uncertainties in AI
technology and the absence of corresponding regulations, have made traditional
responsibility attribution challenging. To this end, this work proposes a
Computational Reflective Equilibrium (CRE) approach to establish a coherent and
ethically acceptable responsibility attribution framework for all stakeholders.
The computational approach provides a structured analysis that overcomes the
limitations of conceptual approaches in dealing with dynamic and multifaceted
scenarios, showcasing the framework's explainability, coherence, and adaptivity
properties in the responsibility attribution process. We examine the pivotal
role of the initial activation level associated with claims in equilibrium
computation. Using an AI-assisted medical decision-support system as a case
study, we illustrate how different initializations lead to diverse
responsibility distributions. The framework offers valuable insights into
accountability in AI-induced incidents, facilitating the development of a
sustainable and resilient system through continuous monitoring, revision, and
reflection.

The evolution of Artificial Intelligence (AI) stands as a pivotal force
shaping our society, finding applications across diverse domains such as
education, sustainability, and safety. Leveraging AI within mobile applications
makes it easily accessible to the public, catalyzing its transformative
potential. In this paper, we present a methodology for the rapid development of
AI agent applications using the development platform provided by MIT App
Inventor. To demonstrate its efficacy, we share the development journey of
three distinct mobile applications: SynchroNet for fostering sustainable
communities; ProductiviTeams for addressing procrastination; and iHELP for
enhancing community safety. All three applications seamlessly integrate a
spectrum of generative AI features, leveraging OpenAI APIs. Furthermore, we
offer insights gleaned from overcoming challenges in integrating diverse tools
and AI functionalities, aiming to inspire young developers to join our efforts
in building practical AI agent applications.

Generative Artificial Intelligence (AI) is integrated into everyday
technology, including news, education, and social media. AI has further
pervaded private conversations as conversational partners, auto-completion, and
response suggestions. As social media becomes young people's main method of
peer support exchange, we need to understand when and how AI can facilitate and
assist in such exchanges in a beneficial, safe, and socially appropriate way.
We asked 622 young people to complete an online survey and evaluate blinded
human- and AI-generated responses to help-seeking messages. We found that
participants preferred the AI-generated response to situations about
relationships, self-expression, and physical health. However, when addressing a
sensitive topic, like suicidal thoughts, young people preferred the human
response. We also discuss the role of training in online peer support exchange
and its implications for supporting young people's well-being. Disclaimer: This
paper includes sensitive topics, including suicide ideation. Reader discretion
is advised.

Generative AI technologies demand new practical and critical competencies,
which call on design to respond to and foster these. We present an exploratory
study guided by Research-through-Design, in which we partnered with a primary
school to develop a constructionist curriculum centered on students interacting
with a generative AI technology. We provide a detailed account of the design of
and outputs from the curriculum and learning materials, finding centrally that
the reflexive and prolonged `hands-on' approach led to a co-development of
students' practical and critical competencies. From the study, we contribute
guidance for designing constructionist approaches to generative AI technology
education; further arguing to do so with `critical responsivity.' We then
discuss how HCI researchers may leverage constructionist strategies in
designing interactions with generative AI technologies; and suggest that
Research-through-Design can play an important role as a `rapid response
methodology' capable of reacting to fast-evolving, disruptive technologies such
as generative AI.

Recent advancements in technology have led to the emergence of Cyber-Physical
Systems (CPS), which seamlessly integrate the cyber and physical domains in
various sectors such as agriculture, autonomous systems, and healthcare. This
integration presents opportunities for enhanced efficiency and automation
through the utilization of artificial intelligence (AI) and machine learning
(ML). However, the complexity of CPS brings forth challenges related to
transparency, bias, and trust in AI-enabled decision-making processes. This
research explores the significance of AI and ML in enabling CPS in these
domains and addresses the challenges associated with interpreting and trusting
AI systems within CPS. Specifically, the role of explainable AI (XAI) in
enhancing trustworthiness and reliability in AI-enabled decision-making
processes is discussed. Key challenges such as transparency, security, and
privacy are identified, along with the necessity of building trust through
transparency, accountability, and ethical considerations.

This study developed an explainable AI for ship collision avoidance.
Initially, a critic network composed of sub-task critic networks was proposed
to individually evaluate each sub-task in collision avoidance to clarify the AI
decision-making processes involved. Additionally, an attempt was made to
discern behavioral intentions through a Q-value analysis and an Attention
mechanism. The former focused on interpreting intentions by examining the
increment of the Q-value resulting from AI actions, while the latter
incorporated the significance of other ships in the decision-making process for
collision avoidance into the learning objective. AI's behavioral intentions in
collision avoidance were visualized by combining the perceived collision danger
with the degree of attention to other ships. The proposed method was evaluated
through a numerical experiment. The developed AI was confirmed to be able to
safely avoid collisions under various congestion levels, and AI's
decision-making process was rendered comprehensible to humans. The proposed
method not only facilitates the understanding of DRL-based controllers/systems
in the ship collision avoidance task but also extends to any task comprising
sub-tasks.

Driven by the algorithmic advancements in reinforcement learning and the
increasing number of implementations of human-AI collaboration, Collaborative
Reinforcement Learning (CRL) has been receiving growing attention. Despite this
recent upsurge, this area is still rarely systematically studied. In this
paper, we provide an extensive survey, investigating CRL methods based on both
interactive reinforcement learning algorithms and human-AI collaborative
frameworks that were proposed in the past decade. We elucidate and discuss via
synergistic analysis methods both the growth of the field and the
state-of-the-art; we conceptualise the existing frameworks from the
perspectives of design patterns, collaborative levels, parties and
capabilities, and review interactive methods and algorithmic models.
Specifically, we create a new Human-AI CRL Design Trajectory Map, as a
systematic modelling tool for the selection of existing CRL frameworks, as well
as a method of designing new CRL systems, and finally of improving future CRL
designs. Furthermore, we elaborate generic Human-AI CRL challenges, providing
the research community with a guide towards novel research directions. The aim
of this paper is to empower researchers with a systematic framework for the
design of efficient and 'natural' human-AI collaborative methods, making it
possible to work on maximised realisation of humans' and AI's potentials.

This book-length article combines several peer reviewed papers and new
material to analyze the issues of ethical artificial intelligence (AI). The
behavior of future AI systems can be described by mathematical equations, which
are adapted to analyze possible unintended AI behaviors and ways that AI
designs can avoid them. This article makes the case for utility-maximizing
agents and for avoiding infinite sets in agent definitions. It shows how to
avoid agent self-delusion using model-based utility functions and how to avoid
agents that corrupt their reward generators (sometimes called "perverse
instantiation") using utility functions that evaluate outcomes at one point in
time from the perspective of humans at a different point in time. It argues
that agents can avoid unintended instrumental actions (sometimes called "basic
AI drives" or "instrumental goals") by accurately learning human values. This
article defines a self-modeling agent framework and shows how it can avoid
problems of resource limits, being predicted by other agents, and inconsistency
between the agent's utility function and its definition (one version of this
problem is sometimes called "motivated value selection"). This article also
discusses how future AI will differ from current AI, the politics of AI, and
the ultimate use of AI to help understand the nature of the universe and our
place in it.

Adaptive interventions (AIs) are increasingly becoming popular in medical and
behavioral sciences. An AI is a sequence of individualized intervention options
that specify for whom and under what conditions different intervention options
should be offered, in order to address the changing needs of individuals as
they progress over time. The sequential, multiple assignment, randomized trial
(SMART) is a novel trial design that was developed to aid in empirically
constructing effective AIs. The sequential randomizations in a SMART often
yield multiple AIs that are embedded in the trial by design. Many SMARTs are
motivated by scientific questions pertaining to the comparison of such embedded
AIs. Existing data analytic methods and sample size planning resources for
SMARTs are suitable for superiority testing, namely for testing whether one
embedded AI yields better primary outcomes on average than another. This
represents a major scientific gap since AIs are often motivated by the need to
deliver support/care in a less costly or less burdensome manner, while still
yielding benefits that are equivalent or non-inferior to those produced by a
more costly/burdensome standard of care. Here, we develop data analytic methods
and sample size formulas for SMART studies aiming to test the non-inferiority
or equivalence of one AI over another. Sample size and power considerations are
discussed with supporting simulations, and online sample size planning
resources are provided. For illustration, we use an example from a SMART study
aiming to develop an AI for promoting weight loss among overweight/obese
adults.

The ubiquity of systems using artificial intelligence or "AI" has brought
increasing attention to how those systems should be regulated. The choice of
how to regulate AI systems will require care. AI systems have the potential to
synthesize large amounts of data, allowing for greater levels of
personalization and precision than ever before---applications range from
clinical decision support to autonomous driving and predictive policing. That
said, there exist legitimate concerns about the intentional and unintentional
negative consequences of AI systems. There are many ways to hold AI systems
accountable. In this work, we focus on one: explanation. Questions about a
legal right to explanation from AI systems was recently debated in the EU
General Data Protection Regulation, and thus thinking carefully about when and
how explanation from AI systems might improve accountability is timely. In this
work, we review contexts in which explanation is currently required under the
law, and then list the technical considerations that must be considered if we
desired AI systems that could provide kinds of explanations that are currently
required of humans.

Artificial Intelligence (AI) - the phenomenon of machines being able to solve
problems that require human intelligence - has in the past decade seen an
enormous rise of interest due to significant advances in effectiveness and use.
The health sector, one of the most important sectors for societies and
economies worldwide, is particularly interesting for AI applications, given the
ongoing digitalisation of all types of health information. The potential for AI
assistance in the health domain is immense, because AI can support medical
decision making at reduced costs, everywhere. However, due to the complexity of
AI algorithms, it is difficult to distinguish good from bad AI-based solutions
and to understand their strengths and weaknesses, which is crucial for
clarifying responsibilities and for building trust. For this reason, the
International Telecommunication Union (ITU) has established a new Focus Group
on "Artificial Intelligence for Health" (FG-AI4H) in partnership with the World
Health Organization (WHO). Health and care services are usually the
responsibility of a government - even when provided through private insurance
systems - and thus under the responsibility of WHO/ITU member states. FG-AI4H
will identify opportunities for international standardization, which will
foster the application of AI to health issues on a global scale. In particular,
it will establish a standardized assessment framework with open benchmarks for
the evaluation of AI-based methods for health, such as AI-based diagnosis,
triage or treatment decisions.

Recently, many AI researchers and practitioners have embarked on research
visions that involve doing AI for "Good". This is part of a general drive
towards infusing AI research and practice with ethical thinking. One frequent
theme in current ethical guidelines is the requirement that AI be good for all,
or: contribute to the Common Good. But what is the Common Good, and is it
enough to want to be good? Via four lead questions, I will illustrate
challenges and pitfalls when determining, from an AI point of view, what the
Common Good is and how it can be enhanced by AI. The questions are: What is the
problem / What is a problem?, Who defines the problem?, What is the role of
knowledge?, and What are important side effects and dynamics? The illustration
will use an example from the domain of "AI for Social Good", more specifically
"Data Science for Social Good". Even if the importance of these questions may
be known at an abstract level, they do not get asked sufficiently in practice,
as shown by an exploratory study of 99 contributions to recent conferences in
the field. Turning these challenges and pitfalls into a positive
recommendation, as a conclusion I will draw on another characteristic of
computer-science thinking and practice to make these impediments visible and
attenuate them: "attacks" as a method for improving design. This results in the
proposal of ethics pen-testing as a method for helping AI designs to better
contribute to the Common Good.

Item response theory (IRT) can be applied to the analysis of the evaluation
of results from AI benchmarks. The two-parameter IRT model provides two
indicators (difficulty and discrimination) on the side of the item (or AI
problem) while only one indicator (ability) on the side of the respondent (or
AI agent). In this paper we analyse how to make this set of indicators dual, by
adding a fourth indicator, generality, on the side of the respondent.
Generality is meant to be dual to discrimination, and it is based on
difficulty. Namely, generality is defined as a new metric that evaluates
whether an agent is consistently good at easy problems and bad at difficult
ones. With the addition of generality, we see that this set of four key
indicators can give us more insight on the results of AI benchmarks. In
particular, we explore two popular benchmarks in AI, the Arcade Learning
Environment (Atari 2600 games) and the General Video Game AI competition. We
provide some guidelines to estimate and interpret these indicators for other AI
benchmarks and competitions.

Transformative AI technologies have the potential to reshape critical aspects
of society in the near future. However, in order to properly prepare policy
initiatives for the arrival of such technologies accurate forecasts and
timelines are necessary. A survey was administered to attendees of three AI
conferences during the summer of 2018 (ICML, IJCAI and the HLAI conference).
The survey included questions for estimating AI capabilities over the next
decade, questions for forecasting five scenarios of transformative AI and
questions concerning the impact of computational resources in AI research.
Respondents indicated a median of 21.5% of human tasks (i.e., all tasks that
humans are currently paid to do) can be feasibly automated now, and that this
figure would rise to 40% in 5 years and 60% in 10 years. Median forecasts
indicated a 50% probability of AI systems being capable of automating 90% of
current human tasks in 25 years and 99% of current human tasks in 50 years. The
conference of attendance was found to have a statistically significant impact
on all forecasts, with attendees of HLAI providing more optimistic timelines
with less uncertainty. These findings suggest that AI experts expect major
advances in AI technology to continue over the next decade to a degree that
will likely have profound transformative impacts on society.

The ability to create artificial intelligence (AI) capable of performing
complex tasks is rapidly outpacing our ability to ensure the safe and assured
operation of AI-enabled systems. Fortunately, a landscape of AI safety research
is emerging in response to this asymmetry and yet there is a long way to go. In
particular, recent simulation environments created to illustrate AI safety
risks are relatively simple or narrowly-focused on a particular issue. Hence,
we see a critical need for AI safety research environments that abstract
essential aspects of complex real-world applications. In this work, we
introduce the AI safety TanksWorld as an environment for AI safety research
with three essential aspects: competing performance objectives, human-machine
teaming, and multi-agent competition. The AI safety TanksWorld aims to
accelerate the advancement of safe multi-agent decision-making algorithms by
providing a software framework to support competitions with both system
performance and safety objectives. As a work in progress, this paper introduces
our research objectives and learning environment with reference code and
baseline performance metrics to follow in a future work.

Predicting and modeling human behavior and finding trends within human
decision-making processes is a major problem of social science. Rock Paper
Scissors (RPS) is the fundamental strategic question in many game theory
problems and real-world competitions. Finding the right approach to beat a
particular human opponent is challenging. Here we use an AI (artificial
intelligence) algorithm based on Markov Models of one fixed memory length
(abbreviated as "single AI") to compete against humans in an iterated RPS game.
We model and predict human competition behavior by combining many Markov Models
with different fixed memory lengths (abbreviated as "multi-AI"), and develop an
architecture of multi-AI with changeable parameters to adapt to different
competition strategies. We introduce a parameter called "focus length" (a
positive number such as 5 or 10) to control the speed and sensitivity for our
multi-AI to adapt to the opponent's strategy change. The focus length is the
number of previous rounds that the multi-AI should look at when determining
which Single-AI has the best performance and should choose to play for the next
game. We experimented with 52 different people, each playing 300 rounds
continuously against one specific multi-AI model, and demonstrated that our
strategy could win against more than 95% of human opponents.

With 5G cellular systems being actively deployed worldwide, the research
community has started to explore novel technological advances for the
subsequent generation, i.e., 6G. It is commonly believed that 6G will be built
on a new vision of ubiquitous AI, an hyper-flexible architecture that brings
human-like intelligence into every aspect of networking systems. Despite its
great promise, there are several novel challenges expected to arise in
ubiquitous AI-based 6G. Although numerous attempts have been made to apply AI
to wireless networks, these attempts have not yet seen any large-scale
implementation in practical systems. One of the key challenges is the
difficulty to implement distributed AI across a massive number of heterogeneous
devices. Federated learning (FL) is an emerging distributed AI solution that
enables data-driven AI solutions in heterogeneous and potentially massive-scale
networks. Although it still in an early stage of development, FL-inspired
architecture has been recognized as one of the most promising solutions to
fulfill ubiquitous AI in 6G. In this article, we identify the requirements that
will drive convergence between 6G and AI. We propose an FL-based network
architecture and discuss its potential for addressing some of the novel
challenges expected in 6G. Future trends and key research problems for
FL-enabled 6G are also discussed.

The COVID-19 pandemic caused by the SARS-CoV-2 virus has spread rapidly
worldwide, leading to a global outbreak. Most governments, enterprises, and
scientific research institutions are participating in the COVID-19 struggle to
curb the spread of the pandemic. As a powerful tool against COVID-19,
artificial intelligence (AI) technologies are widely used in combating this
pandemic. In this survey, we investigate the main scope and contributions of AI
in combating COVID-19 from the aspects of disease detection and diagnosis,
virology and pathogenesis, drug and vaccine development, and epidemic and
transmission prediction. In addition, we summarize the available data and
resources that can be used for AI-based COVID-19 research. Finally, the main
challenges and potential directions of AI in fighting against COVID-19 are
discussed. Currently, AI mainly focuses on medical image inspection, genomics,
drug development, and transmission prediction, and thus AI still has great
potential in this field. This survey presents medical and AI researchers with a
comprehensive view of the existing and potential applications of AI technology
in combating COVID-19 with the goal of inspiring researchers to continue to
maximize the advantages of AI and big data to fight COVID-19.

Artificial intelligence (AI) has huge potential to improve the health and
well-being of people, but adoption in clinical practice is still limited. Lack
of transparency is identified as one of the main barriers to implementation, as
clinicians should be confident the AI system can be trusted. Explainable AI has
the potential to overcome this issue and can be a step towards trustworthy AI.
In this paper we review the recent literature to provide guidance to
researchers and practitioners on the design of explainable AI systems for the
health-care domain and contribute to formalization of the field of explainable
AI. We argue the reason to demand explainability determines what should be
explained as this determines the relative importance of the properties of
explainability (i.e. interpretability and fidelity). Based on this, we propose
a framework to guide the choice between classes of explainable AI methods
(explainable modelling versus post-hoc explanation; model-based,
attribution-based, or example-based explanations; global and local
explanations). Furthermore, we find that quantitative evaluation metrics, which
are important for objective standardized evaluation, are still lacking for some
properties (e.g. clarity) and types of explanations (e.g. example-based
methods). We conclude that explainable modelling can contribute to trustworthy
AI, but the benefits of explainability still need to be proven in practice and
complementary measures might be needed to create trustworthy AI in health care
(e.g. reporting data quality, performing extensive (external) validation, and
regulation).

The plethora of complex artificial intelligence (AI) algorithms and available
high performance computing (HPC) power stimulates the expeditious development
of AI components with heterogeneous designs. Consequently, the need for
cross-stack performance benchmarking of AI-HPC systems emerges rapidly. The de
facto HPC benchmark LINPACK can not reflect AI computing power and I/O
performance without representative workload. The current popular AI benchmarks
like MLPerf have fixed problem size therefore limited scalability. To address
these issues, we propose an end-to-end benchmark suite utilizing automated
machine learning (AutoML), which not only represents real AI scenarios, but
also is auto-adaptively scalable to various scales of machines. We implement
the algorithms in a highly parallel and flexible way to ensure the efficiency
and optimization potential on diverse systems with customizable configurations.
We utilize operations per second (OPS), which is measured in an analytical and
systematic approach, as the major metric to quantify the AI performance. We
perform evaluations on various systems to ensure the benchmark's stability and
scalability, from 4 nodes with 32 NVIDIA Tesla T4 (56.1 Tera-OPS measured), up
to 512 nodes with 4096 Huawei Ascend 910 (194.53 Peta-OPS measured), and the
results show near-linear weak scalability. With flexible workload and single
metric, our benchmark can scale and rank AI-HPC easily.

This paper describes problems in AI research and how the SP System (described
in an appendix) may help to solve them. Most of the problems are described by
leading researchers in AI in interviews with science writer Martin Ford, and
reported by him in his book {\em Architects of Intelligence}. These problems
are: the need to bridge the divide between symbolic and non-symbolic kinds of
knowledge and processing; the tendency of deep neural networks (DNNs) to make
large and unexpected errors in recognition; the need to strengthen the
representation and processing of natural languages; the challenges of
unsupervised learning; the need for a coherent account of generalisation; how
to learn usable knowledge from a single exposure; how to achieve transfer
learning; how to increase the efficiency of AI processing; the need for
transparency in AI structures and processes; how to achieve varieties of
probabilistic reasoning; the need for more emphasis on top-down strategies; how
to minimise the risk of accidents with self-driving vehicles; the need for
strong compositionality in AI knowledge; the challenges of commonsense
reasoning and commonsense knowledge; establishing the importance of information
compression in AI research; establishing the importance of a biological
perspective in AI research; establishing whether knowledge in the brain is
represented in `distributed' or `localist' form; how to bypassing the limited
scope for adaptation in deep neural networks; the need to develop `broad AI';
and how to eliminate the problem of catastrophic forgetting.

The Artificial Intelligence (AI) for Human-Robot Interaction (HRI) Symposium
has been a successful venue of discussion and collaboration since 2014. In that
time, the related topic of trust in robotics has been rapidly growing, with
major research efforts at universities and laboratories across the world.
Indeed, many of the past participants in AI-HRI have been or are now involved
with research into trust in HRI. While trust has no consensus definition, it is
regularly associated with predictability, reliability, inciting confidence, and
meeting expectations. Furthermore, it is generally believed that trust is
crucial for adoption of both AI and robotics, particularly when transitioning
technologies from the lab to industrial, social, and consumer applications.
However, how does trust apply to the specific situations we encounter in the
AI-HRI sphere? Is the notion of trust in AI the same as that in HRI? We see a
growing need for research that lives directly at the intersection of AI and HRI
that is serviced by this symposium. Over the course of the two-day meeting, we
propose to create a collaborative forum for discussion of current efforts in
trust for AI-HRI, with a sub-session focused on the related topic of
explainable AI (XAI) for HRI.

The knowledge contained in academic literature is interesting to mine.
Inspired by the idea of molecular markers tracing in the field of biochemistry,
three named entities, namely, methods, datasets and metrics are used as AI
markers for AI literature. These entities can be used to trace the research
process described in the bodies of papers, which opens up new perspectives for
seeking and mining more valuable academic information. Firstly, the entity
extraction model is used in this study to extract AI markers from large-scale
AI literature. Secondly, original papers are traced for AI markers. Statistical
and propagation analysis are performed based on tracing results. Finally, the
co-occurrences of AI markers are used to achieve clustering. The evolution
within method clusters and the influencing relationships amongst different
research scene clusters are explored. The above-mentioned mining based on AI
markers yields many meaningful discoveries. For example, the propagation of
effective methods on the datasets is rapidly increasing with the development of
time; effective methods proposed by China in recent years have increasing
influence on other countries, whilst France is the opposite. Saliency
detection, a classic computer vision research scene, is the least likely to be
affected by other research scenes.

Although AI holds promise for improving human decision making in societally
critical domains, it remains an open question how human-AI teams can reliably
outperform AI alone and human alone in challenging prediction tasks (also known
as complementary performance). We explore two directions to understand the gaps
in achieving complementary performance. First, we argue that the typical
experimental setup limits the potential of human-AI teams. To account for lower
AI performance out-of-distribution than in-distribution because of distribution
shift, we design experiments with different distribution types and investigate
human performance for both in-distribution and out-of-distribution examples.
Second, we develop novel interfaces to support interactive explanations so that
humans can actively engage with AI assistance. Using virtual pilot studies and
large-scale randomized experiments across three tasks, we demonstrate a clear
difference between in-distribution and out-of-distribution, and observe mixed
results for interactive explanations: while interactive explanations improve
human perception of AI assistance's usefulness, they may reinforce human biases
and lead to limited performance improvement. Overall, our work points out
critical challenges and future directions towards enhancing human performance
with AI assistance.

This paper draws upon the evolutionary concepts of technological relatedness
and knowledge complexity to enhance our understanding of the long-term
evolution of Artificial Intelligence (AI). We reveal corresponding patterns in
the emergence of AI - globally and in the context of specific geographies of
the US, Japan, South Korea, and China. We argue that AI emergence is associated
with increasing related variety due to knowledge commonalities as well as
increasing complexity. We use patent-based indicators for the period between
1974-2018 to analyse the evolution of AI's global technological space, to
identify its technological core as well as changes to its overall relatedness
and knowledge complexity. At the national level, we also measure countries'
overall specialisations against AI-specific ones. At the global level, we find
increasing overall relatedness and complexity of AI. However, for the
technological core of AI, which has been stable over time, we find decreasing
related variety and increasing complexity. This evidence points out that AI
innovations related to core technologies are becoming increasingly distinct
from each other. At the country level, we find that the US and Japan have been
increasing the overall relatedness of their innovations. The opposite is the
case for China and South Korea, which we associate with the fact that these
countries are overall less technologically developed than the US and Japan.
Finally, we observe a stable increasing overall complexity for all countries
apart from China, which we explain by the focus of this country in technologies
not strongly linked to AI.

Recent research shows -- somewhat astonishingly -- that people are willing to
ascribe moral blame to AI-driven systems when they cause harm [1]-[4]. In this
paper, we explore the moral-psychological underpinnings of these findings. Our
hypothesis was that the reason why people ascribe moral blame to AI systems is
that they consider them capable of entertaining inculpating mental states (what
is called mens rea in the law). To explore this hypothesis, we created a
scenario in which an AI system runs a risk of poisoning people by using a novel
type of fertilizer. Manipulating the computational (or quasi-cognitive)
abilities of the AI system in a between-subjects design, we tested whether
people's willingness to ascribe knowledge of a substantial risk of harm (i.e.,
recklessness) and blame to the AI system. Furthermore, we investigated whether
the ascription of recklessness and blame to the AI system would influence the
perceived blameworthiness of the system's user (or owner). In an experiment
with 347 participants, we found (i) that people are willing to ascribe blame to
AI systems in contexts of recklessness, (ii) that blame ascriptions depend
strongly on the willingness to attribute recklessness and (iii) that the
latter, in turn, depends on the perceived "cognitive" capacities of the system.
Furthermore, our results suggest (iv) that the higher the computational
sophistication of the AI system, the more blame is shifted from the human user
to the AI system.

Artificial Intelligence (AI) is increasingly used in critical applications.
Thus, the need for dependable AI systems is rapidly growing. In 2018, the
European Commission appointed experts to a High-Level Expert Group on AI
(AI-HLEG). AI-HLEG defined Trustworthy AI as 1) lawful, 2) ethical, and 3)
robust and specified seven corresponding key requirements. To help development
organizations, AI-HLEG recently published the Assessment List for Trustworthy
AI (ALTAI). We present an illustrative case study from applying ALTAI to an
ongoing development project of an Advanced Driver-Assistance System (ADAS) that
relies on Machine Learning (ML). Our experience shows that ALTAI is largely
applicable to ADAS development, but specific parts related to human agency and
transparency can be disregarded. Moreover, bigger questions related to societal
and environmental impact cannot be tackled by an ADAS supplier in isolation. We
present how we plan to develop the ADAS to ensure ALTAI-compliance. Finally, we
provide three recommendations for the next revision of ALTAI, i.e., life-cycle
variants, domain-specific adaptations, and removed redundancy.

The potential presented by Artificial Intelligence (AI) for healthcare has
long been recognised by the technical community. More recently, this potential
has been recognised by policymakers, resulting in considerable public and
private investment in the development of AI for healthcare across the globe.
Despite this, excepting limited success stories, real-world implementation of
AI systems into front-line healthcare has been limited. There are numerous
reasons for this, but a main contributory factor is the lack of internationally
accepted, or formalised, regulatory standards to assess AI safety and impact
and effectiveness. This is a well-recognised problem with numerous ongoing
research and policy projects to overcome it. Our intention here is to
contribute to this problem-solving effort by seeking to set out a minimally
viable framework for evaluating the safety, acceptability and efficacy of AI
systems for healthcare. We do this by conducting a systematic search across
Scopus, PubMed and Google Scholar to identify all the relevant literature
published between January 1970 and November 2020 related to the evaluation of:
output performance; efficacy; and real-world use of AI systems, and
synthesising the key themes according to the stages of evaluation: pre-clinical
(theoretical phase); exploratory phase; definitive phase; and post-market
surveillance phase (monitoring). The result is a framework to guide AI system
developers, policymakers, and regulators through a sufficient evaluation of an
AI system designed for use in healthcare.

Artificial Intelligence (AI) has made leapfrogs in development across all the
industrial sectors especially when deep learning has been introduced. Deep
learning helps to learn the behaviour of an entity through methods of
recognising and interpreting patterns. Despite its limitless potential, the
mystery is how deep learning algorithms make a decision in the first place.
Explainable AI (XAI) is the key to unlocking AI and the black-box for deep
learning. XAI is an AI model that is programmed to explain its goals, logic,
and decision making so that the end users can understand. The end users can be
domain experts, regulatory agencies, managers and executive board members, data
scientists, users that use AI, with or without awareness, or someone who is
affected by the decisions of an AI model. Chest CT has emerged as a valuable
tool for the clinical diagnostic and treatment management of the lung diseases
associated with COVID-19. AI can support rapid evaluation of CT scans to
differentiate COVID-19 findings from other lung diseases. However, how these AI
tools or deep learning algorithms reach such a decision and which are the most
influential features derived from these neural networks with typically deep
layers are not clear. The aim of this study is to propose and develop XAI
strategies for COVID-19 classification models with an investigation of
comparison. The results demonstrate promising quantification and qualitative
visualisations that can further enhance the clinician's understanding and
decision making with more granular information from the results given by the
learned XAI models.

Research in artificial intelligence (AI) for social good presupposes some
definition of social good, but potential definitions have been seldom suggested
and never agreed upon. The normative question of what AI for social good
research should be "for" is not thoughtfully elaborated, or is frequently
addressed with a utilitarian outlook that prioritizes the needs of the majority
over those who have been historically marginalized, brushing aside realities of
injustice and inequity. We argue that AI for social good ought to be assessed
by the communities that the AI system will impact, using as a guide the
capabilities approach, a framework to measure the ability of different policies
to improve human welfare equity. Furthermore, we lay out how AI research has
the potential to catalyze social progress by expanding and equalizing
capabilities. We show how the capabilities approach aligns with a participatory
approach for the design and implementation of AI for social good research in a
framework we introduce called PACT, in which community members affected should
be brought in as partners and their input prioritized throughout the project.
We conclude by providing an incomplete set of guiding questions for carrying
out such participatory AI research in a way that elicits and respects a
community's own definition of social good.

Advances in AI technologies have resulted in superior levels of AI-based
model performance. However, this has also led to a greater degree of model
complexity, resulting in 'black box' models. In response to the AI black box
problem, the field of explainable AI (xAI) has emerged with the aim of
providing explanations catered to human understanding, trust, and transparency.
Yet, we still have a limited understanding of how xAI addresses the need for
explainable AI in the context of healthcare. Our research explores the
differing explanation needs amongst stakeholders during the development of an
AI-system for classifying COVID-19 patients for the ICU. We demonstrate that
there is a constellation of stakeholders who have different explanation needs,
not just the 'user'. Further, the findings demonstrate how the need for xAI
emerges through concerns associated with specific stakeholder groups i.e., the
development team, subject matter experts, decision makers, and the audience.
Our findings contribute to the expansion of xAI by highlighting that different
stakeholders have different explanation needs. From a practical perspective,
the study provides insights on how AI systems can be adjusted to support
different stakeholders needs, ensuring better implementation and operation in a
healthcare context.

Many real-world applications are widely adopting the edge computing paradigm
due to its low latency and better privacy protection. With notable success in
AI and deep learning (DL), edge devices and AI accelerators play a crucial role
in deploying DL inference services at the edge of the Internet. While prior
works quantified various edge devices' efficiency, most studies focused on the
performance of edge devices with single DL tasks. Therefore, there is an urgent
need to investigate AI multi-tenancy on edge devices, required by many advanced
DL applications for edge computing.
  This work investigates two techniques - concurrent model executions and
dynamic model placements - for AI multi-tenancy on edge devices. With image
classification as an example scenario, we empirically evaluate AI multi-tenancy
on various edge devices, AI accelerators, and DL frameworks to identify its
benefits and limitations. Our results show that multi-tenancy significantly
improves DL inference throughput by up to 3.3x -- 3.8x on Jetson TX2. These AI
multi-tenancy techniques also open up new opportunities for flexible deployment
of multiple DL services on edge devices and AI accelerators.

Most Fairness in AI research focuses on exposing biases in AI systems. A
broader lens on fairness reveals that AI can serve a greater aspiration:
rooting out societal inequities from their source. Specifically, we focus on
inequities in health information, and aim to reduce bias in that domain using
AI. The AI algorithms under the hood of search engines and social media, many
of which are based on recommender systems, have an outsized impact on the
quality of medical and health information online. Therefore, embedding bias
detection and reduction into these recommender systems serving up medical and
health content online could have an outsized positive impact on patient
outcomes and wellbeing.
  In this position paper, we offer the following contributions: (1) we propose
a novel framework of Fairness via AI, inspired by insights from medical
education, sociology and antiracism; (2) we define a new term, bisinformation,
which is related to, but distinct from, misinformation, and encourage
researchers to study it; (3) we propose using AI to study, detect and mitigate
biased, harmful, and/or false health information that disproportionately hurts
minority groups in society; and (4) we suggest several pillars and pose several
open problems in order to seed inquiry in this new space. While part (3) of
this work specifically focuses on the health domain, the fundamental computer
science advances and contributions stemming from research efforts in bias
reduction and Fairness via AI have broad implications in all areas of society.

International collaboration has become imperative in the field of AI.
However, few studies exist concerning how distance factors have affected the
international collaboration in AI research. In this study, we investigate this
problem by using 1,294,644 AI related collaborative papers harvested from the
Microsoft Academic Graph (MAG) dataset. A framework including 13 indicators to
quantify the distance factors between countries from 5 perspectives (i.e.,
geographic distance, economic distance, cultural distance, academic distance,
and industrial distance) is proposed. The relationships were conducted by the
methods of descriptive analysis and regression analysis. The results show that
international collaboration in the field of AI today is not prevalent (only
15.7%). All the separations in international collaborations have increased over
years, except for the cultural distance in masculinity/felinity dimension and
the industrial distance. The geographic distance, economic distance and
academic distances have shown significantly negative relationships with the
degree of international collaborations in the field of AI. The industrial
distance has a significant positive relationship with the degree of
international collaboration in the field of AI. Also, the results demonstrate
that the participation of the United States and China have promoted the
international collaboration in the field of AI. This study provides a
comprehensive understanding of internationalizing AI research in geographic,
economic, cultural, academic, and industrial aspects.

The impact of Artificial Intelligence does not depend only on fundamental
research and technological developments, but for a large part on how these
systems are introduced into society and used in everyday situations. Even
though AI is traditionally associated with rational decision making,
understanding and shaping the societal impact of AI in all its facets requires
a relational perspective. A rational approach to AI, where computational
algorithms drive decision making independent of human intervention, insights
and emotions, has shown to result in bias and exclusion, laying bare societal
vulnerabilities and insecurities. A relational approach, that focus on the
relational nature of things, is needed to deal with the ethical, legal,
societal, cultural, and environmental implications of AI. A relational approach
to AI recognises that objective and rational reasoning cannot does not always
result in the 'right' way to proceed because what is 'right' depends on the
dynamics of the situation in which the decision is taken, and that rather than
solving ethical problems the focus of design and use of AI must be on asking
the ethical question. In this position paper, I start with a general discussion
of current conceptualisations of AI followed by an overview of existing
approaches to governance and responsible development and use of AI. Then, I
reflect over what should be the bases of a social paradigm for AI and how this
should be embedded in relational, feminist and non-Western philosophies, in
particular the Ubuntu philosophy.

Explainability, interpretability and how much they affect human trust in AI
systems are ultimately problems of human cognition as much as machine learning,
yet the effectiveness of AI recommendations and the trust afforded by end-users
are typically not evaluated quantitatively. We developed and validated a
general purpose Human-AI interaction paradigm which quantifies the impact of AI
recommendations on human decisions. In our paradigm we confronted human users
with quantitative prediction tasks: asking them for a first response, before
confronting them with an AI's recommendations (and explanation), and then
asking the human user to provide an updated final response. The difference
between final and first responses constitutes the shift or sway in the human
decision which we use as metric of the AI's recommendation impact on the human,
representing the trust they place on the AI. We evaluated this paradigm on
hundreds of users through Amazon Mechanical Turk using a multi-branched
experiment confronting users with good/poor AI systems that had good, poor or
no explainability. Our proof-of-principle paradigm allows one to quantitatively
compare the rapidly growing set of XAI/IAI approaches in terms of their effect
on the end-user and opens up the possibility of (machine) learning trust.

Benchmarks are seen as the cornerstone for measuring technical progress in
Artificial Intelligence (AI) research and have been developed for a variety of
tasks ranging from question answering to facial recognition. An increasingly
prominent research area in AI is ethics, which currently has no set of
benchmarks nor commonly accepted way for measuring the 'ethicality' of an AI
system. In this paper, drawing upon research in moral philosophy and
metaethics, we argue that it is impossible to develop such a benchmark. As
such, alternative mechanisms are necessary for evaluating whether an AI system
is 'ethical'. This is especially pressing in light of the prevalence of
applied, industrial AI research. We argue that it makes more sense to talk
about 'values' (and 'value alignment') rather than 'ethics' when considering
the possible actions of present and future AI systems. We further highlight
that, because values are unambiguously relative, focusing on values forces us
to consider explicitly what the values are and whose values they are. Shifting
the emphasis from ethics to values therefore gives rise to several new ways of
understanding how researchers might advance research programmes for robustly
safe or beneficial AI. We conclude by highlighting a number of possible ways
forward for the field as a whole, and we advocate for different approaches
towards more value-aligned AI research.

AI technologies are being increasingly tested and applied in critical
environments including healthcare. Without an effective way to detect and
mitigate AI induced inequalities, AI might do more harm than good, potentially
leading to the widening of underlying inequalities. This paper proposes a
generic allocation-deterioration framework for detecting and quantifying AI
induced inequality. Specifically, AI induced inequalities are quantified as the
area between two allocation-deterioration curves. To assess the framework's
performance, experiments were conducted on ten synthetic datasets (N>33,000)
generated from HiRID - a real-world Intensive Care Unit (ICU) dataset, showing
its ability to accurately detect and quantify inequality proportionally to
controlled inequalities. Extensive analyses were carried out to quantify health
inequalities (a) embedded in two real-world ICU datasets; (b) induced by AI
models trained for two resource allocation scenarios. Results showed that
compared to men, women had up to 33% poorer deterioration in markers of
prognosis when admitted to HiRID ICUs. All four AI models assessed were shown
to induce significant inequalities (2.45% to 43.2%) for non-White compared to
White patients. The models exacerbated data embedded inequalities significantly
in 3 out of 8 assessments, one of which was >9 times worse.
  The codebase is at https://github.com/knowlab/DAindex-Framework.

Private companies, public sector organizations, and academic groups have
outlined ethical values they consider important for responsible artificial
intelligence technologies. While their recommendations converge on a set of
central values, little is known about the values a more representative public
would find important for the AI technologies they interact with and might be
affected by. We conducted a survey examining how individuals perceive and
prioritize responsible AI values across three groups: a representative sample
of the US population (N=743), a sample of crowdworkers (N=755), and a sample of
AI practitioners (N=175). Our results empirically confirm a common concern: AI
practitioners' value priorities differ from those of the general public.
Compared to the US-representative sample, AI practitioners appear to consider
responsible AI values as less important and emphasize a different set of
values. In contrast, self-identified women and black respondents found
responsible AI values more important than other groups. Surprisingly, more
liberal-leaning participants, rather than participants reporting experiences
with discrimination, were more likely to prioritize fairness than other groups.
Our findings highlight the importance of paying attention to who gets to define
responsible AI.

With the rise of AI systems in real-world applications comes the need for
reliable and trustworthy AI. An essential aspect of this are explainable AI
systems. However, there is no agreed standard on how explainable AI systems
should be assessed. Inspired by the Turing test, we introduce a human-centric
assessment framework where a leading domain expert accepts or rejects the
solutions of an AI system and another domain expert. By comparing the
acceptance rates of provided solutions, we can assess how the AI system
performs compared to the domain expert, and whether the AI system's
explanations (if provided) are human-understandable. This setup -- comparable
to the Turing test -- can serve as a framework for a wide range of
human-centric AI system assessments. We demonstrate this by presenting two
instantiations: (1) an assessment that measures the classification accuracy of
a system with the option to incorporate label uncertainties; (2) an assessment
where the usefulness of provided explanations is determined in a human-centric
manner.

Artificial intelligence (AI) has the potential to greatly improve society,
but as with any powerful technology, it comes with heightened risks and
responsibilities. Current AI research lacks a systematic discussion of how to
manage long-tail risks from AI systems, including speculative long-term risks.
Keeping in mind the potential benefits of AI, there is some concern that
building ever more intelligent and powerful AI systems could eventually result
in systems that are more powerful than us; some say this is like playing with
fire and speculate that this could create existential risks (x-risks). To add
precision and ground these discussions, we provide a guide for how to analyze
AI x-risk, which consists of three parts: First, we review how systems can be
made safer today, drawing on time-tested concepts from hazard analysis and
systems safety that have been designed to steer large processes in safer
directions. Next, we discuss strategies for having long-term impacts on the
safety of future systems. Finally, we discuss a crucial concept in making AI
systems safer by improving the balance between safety and general capabilities.
We hope this document and the presented concepts and tools serve as a useful
guide for understanding how to analyze AI x-risk.

Artificial intelligence is already being applied in and impacting many
important sectors in society, including healthcare, finance, and policing.
These applications will increase as AI capabilities continue to progress, which
has the potential to be highly beneficial for society, or to cause serious
harm. The role of AI governance is ultimately to take practical steps to
mitigate this risk of harm while enabling the benefits of innovation in AI.
This requires answering challenging empirical questions about current and
potential risks and benefits of AI: assessing impacts that are often widely
distributed and indirect, and making predictions about a highly uncertain
future. It also requires thinking through the normative question of what
beneficial use of AI in society looks like, which is equally challenging.
Though different groups may agree on high-level principles that uses of AI
should respect (e.g., privacy, fairness, and autonomy), challenges arise when
putting these principles into practice. For example, it is straightforward to
say that AI systems must protect individual privacy, but there is presumably
some amount or type of privacy that most people would be willing to give up to
develop life-saving medical treatments. Despite these challenges, research can
and has made progress on these questions. The aim of this chapter will be to
give readers an understanding of this progress, and of the challenges that
remain.

Distributed artificial intelligence (AI) has recently accomplished tremendous
breakthroughs in various communication services, ranging from fault-tolerant
factory automation to smart cities. When distributed learning is run over a set
of wireless connected devices, random channel fluctuations, and the incumbent
services simultaneously running on the same network affect the performance of
distributed learning. In this paper, we investigate the interplay between
distributed AI workflow and ultra-reliable low latency communication (URLLC)
services running concurrently over a network. Using 3GPP compliant simulations
in a factory automation use case, we show the impact of various distributed AI
settings (e.g., model size and the number of participating devices) on the
convergence time of distributed AI and the application layer performance of
URLLC. Unless we leverage the existing 5G-NR quality of service handling
mechanisms to separate the traffic from the two services, our simulation
results show that the impact of distributed AI on the availability of the URLLC
devices is significant. Moreover, with proper setting of distributed AI (e.g.,
proper user selection), we can substantially reduce network resource
utilization, leading to lower latency for distributed AI and higher
availability for the URLLC users. Our results provide important insights for
future 6G and AI standardization.

The AI Act has been recently proposed by the European Commission to regulate
the use of AI in the EU, especially on high-risk applications, i.e. systems
intended to be used as safety components in the management and operation of
road traffic and the supply of water, gas, heating and electricity. On the
other hand, IEC 61508, one of the most adopted international standards for
safety-critical electronic components, seem to mostly forbid the use of AI in
such systems. Given this conflict between IEC 61508 and the proposed AI Act,
also stressed by the fact that IEC 61508 is not an harmonised European
standard, with the present paper we study and analyse what is going to happen
to industry after the entry into force of the AI Act. In particular, we focus
on how the proposed AI Act might positively impact on the sustainability of
critical infrastructures by allowing the use of AI on an industry where it was
previously forbidden. To do so, we provide several examples of AI-based
solutions falling under the umbrella of IEC 61508 that might have a positive
impact on sustainability in alignment with the current long-term goals of the
EU and the Sustainable Development Goals of the United Nations, i.e.,
affordable and clean energy, sustainable cities and communities.

There is a substantial and ever-growing corpus of evidence and literature
exploring the impacts of Artificial intelligence (AI) technologies on society,
politics, and humanity as a whole. A separate, parallel body of work has
explored existential risks to humanity, including but not limited to that
stemming from unaligned Artificial General Intelligence (AGI). In this paper,
we problematise the notion that current and near-term artificial intelligence
technologies have the potential to contribute to existential risk by acting as
intermediate risk factors, and that this potential is not limited to the
unaligned AGI scenario. We propose the hypothesis that certain
already-documented effects of AI can act as existential risk factors,
magnifying the likelihood of previously identified sources of existential risk.
Moreover, future developments in the coming decade hold the potential to
significantly exacerbate these risk factors, even in the absence of artificial
general intelligence. Our main contribution is a (non-exhaustive) exposition of
potential AI risk factors and the causal relationships between them, focusing
on how AI can affect power dynamics and information security. This exposition
demonstrates that there exist causal pathways from AI systems to existential
risks that do not presuppose hypothetical future AI capabilities.

The Artificial Intelligence (AI) for Human-Robot Interaction (HRI) Symposium
has been a successful venue of discussion and collaboration on AI theory and
methods aimed at HRI since 2014. This year, after a review of the achievements
of the AI-HRI community over the last decade in 2021, we are focusing on a
visionary theme: exploring the future of AI-HRI. Accordingly, we added a Blue
Sky Ideas track to foster a forward-thinking discussion on future research at
the intersection of AI and HRI. As always, we appreciate all contributions
related to any topic on AI/HRI and welcome new researchers who wish to take
part in this growing community.
  With the success of past symposia, AI-HRI impacts a variety of communities
and problems, and has pioneered the discussions in recent trends and interests.
This year's AI-HRI Fall Symposium aims to bring together researchers and
practitioners from around the globe, representing a number of university,
government, and industry laboratories. In doing so, we hope to accelerate
research in the field, support technology transition and user adoption, and
determine future directions for our group and our research.

Developed and used responsibly Artificial Intelligence (AI) is a force for
global sustainable development. Given this opportunity, we expect that the many
of the existing guidelines and recommendations for trustworthy or responsible
AI will provide explicit guidance on how AI can contribute to the achievement
of United Nations' Sustainable Development Goals (SDGs). This would in
particular be the case for the AI strategies of the Nordic countries, at least
given their high ranking and overall political focus when it comes to the
achievement of the SDGs. In this paper, we present an analysis of existing AI
recommendations from 10 different countries or organisations based on topic
modelling techniques to identify how much these strategy documents refer to the
SDGs. The analysis shows no significant difference on how much these documents
refer to SDGs. Moreover, the Nordic countries are not different from the others
albeit their long-term commitment to SDGs. More importantly, references to
\textit{gender equality} (SDG 5) and \textit{inequality} (SDG 10), as well as
references to environmental impact of AI development and use, and in particular
the consequences for life on earth, are notably missing from the guidelines.

Artificial intelligence (AI) has enormous potential to improve Air Force
pilot training by providing actionable feedback to pilot trainees on the
quality of their maneuvers and enabling instructor-less flying familiarization
for early-stage trainees in low-cost simulators. Historically, AI challenges
consisting of data, problem descriptions, and example code have been critical
to fueling AI breakthroughs. The Department of the Air Force-Massachusetts
Institute of Technology AI Accelerator (DAF-MIT AI Accelerator) developed such
an AI challenge using real-world Air Force flight simulator data. The Maneuver
ID challenge assembled thousands of virtual reality simulator flight recordings
collected by actual Air Force student pilots at Pilot Training Next (PTN). This
dataset has been publicly released at Maneuver-ID.mit.edu and represents the
first of its kind public release of USAF flight training data. Using this
dataset, we have applied a variety of AI methods to separate "good" vs "bad"
simulator data and categorize and characterize maneuvers. These data,
algorithms, and software are being released as baselines of model performance
for others to build upon to enable the AI ecosystem for flight simulator
training.

Several policy options exist, or have been proposed, to further responsible
artificial intelligence (AI) development and deployment. Institutions,
including U.S. government agencies, states, professional societies, and private
and public sector businesses, are well positioned to implement these policies.
However, given limited resources, not all policies can or should be equally
prioritized. We define and review nine suggested policies for furthering
responsible AI, rank each policy on potential use and impact, and recommend
prioritization relative to each institution type. We find that pre-deployment
audits and assessments and post-deployment accountability are likely to have
the highest impact but also the highest barriers to adoption. We recommend that
U.S. government agencies and companies highly prioritize development of
pre-deployment audits and assessments, while the U.S. national legislature
should highly prioritize post-deployment accountability. We suggest that U.S.
government agencies and professional societies should highly prioritize
policies that support responsible AI research and that states should highly
prioritize support of responsible AI education. We propose that companies can
highly prioritize involving community stakeholders in development efforts and
supporting diversity in AI development. We advise lower levels of
prioritization across institutions for AI ethics statements and databases of AI
technologies or incidents. We recognize that no one policy will lead to
responsible AI and instead advocate for strategic policy implementation across
institutions.

It is a common sense that datasets with high-quality data samples play an
important role in artificial intelligence (AI), machine learning (ML) and
related studies. However, although AI/ML has been introduced in wireless
researches long time ago, few datasets are commonly used in the research
community. Without a common dataset, AI-based methods proposed for wireless
systems are hard to compare with both the traditional baselines and even each
other. The existing wireless AI researches usually rely on datasets generated
based on statistical models or ray-tracing simulations with limited
environments. The statistical data hinder the trained AI models from further
fine-tuning for a specific scenario, and ray-tracing data with limited
environments lower down the generalization capability of the trained AI models.
In this paper, we present the Wireless AI Research Dataset (WAIR-D)1, which
consists of two scenarios. Scenario 1 contains 10,000 environments with
sparsely dropped user equipments (UEs), and Scenario 2 contains 100
environments with densely dropped UEs. The environments are randomly picked up
from more than 40 cities in the real world map. The large volume of the data
guarantees that the trained AI models enjoy good generalization capability,
while fine-tuning can be easily carried out on a specific chosen environment.
Moreover, both the wireless channels and the corresponding environmental
information are provided in WAIR-D, so that extra-information-aided
communication mechanism can be designed and evaluated. WAIR-D provides the
researchers benchmarks to compare their different designs or reproduce results
of others. In this paper, we show the detailed construction of this dataset and
examples of using it.

Prior work has identified a resilient phenomenon that threatens the
performance of human-AI decision-making teams: overreliance, when people agree
with an AI, even when it is incorrect. Surprisingly, overreliance does not
reduce when the AI produces explanations for its predictions, compared to only
providing predictions. Some have argued that overreliance results from
cognitive biases or uncalibrated trust, attributing overreliance to an
inevitability of human cognition. By contrast, our paper argues that people
strategically choose whether or not to engage with an AI explanation,
demonstrating empirically that there are scenarios where AI explanations reduce
overreliance. To achieve this, we formalize this strategic choice in a
cost-benefit framework, where the costs and benefits of engaging with the task
are weighed against the costs and benefits of relying on the AI. We manipulate
the costs and benefits in a maze task, where participants collaborate with a
simulated AI to find the exit of a maze. Through 5 studies (N = 731), we find
that costs such as task difficulty (Study 1), explanation difficulty (Study 2,
3), and benefits such as monetary compensation (Study 4) affect overreliance.
Finally, Study 5 adapts the Cognitive Effort Discounting paradigm to quantify
the utility of different explanations, providing further support for our
framework. Our results suggest that some of the null effects found in
literature could be due in part to the explanation not sufficiently reducing
the costs of verifying the AI's prediction.

Artificial intelligence (AI) and Machine learning (ML) are increasingly used
in energy and engineering systems, but these models must be fair, unbiased, and
explainable. It is critical to have confidence in AI's trustworthiness. ML
techniques have been useful in predicting important parameters and in improving
model performance. However, for these AI techniques to be useful for making
decisions, they need to be audited, accounted for, and easy to understand.
Therefore, the use of explainable AI (XAI) and interpretable machine learning
(IML) is crucial for the accurate prediction of prognostics, such as remaining
useful life (RUL), in a digital twin system, to make it intelligent while
ensuring that the AI model is transparent in its decision-making processes and
that the predictions it generates can be understood and trusted by users. By
using AI that is explainable, interpretable, and trustworthy, intelligent
digital twin systems can make more accurate predictions of RUL, leading to
better maintenance and repair planning, and ultimately, improved system
performance. The objective of this paper is to explain the ideas of XAI and IML
and to justify the important role of AI/ML in the digital twin framework and
components, which requires XAI to understand the prediction better. This paper
explains the importance of XAI and IML in both local and global aspects to
ensure the use of trustworthy AI/ML applications for RUL prediction. We used
the RUL prediction for the XAI and IML studies and leveraged the integrated
Python toolbox for interpretable machine learning~(PiML).

AI ethics is an emerging field with multiple, competing narratives about how
to best solve the problem of building human values into machines. Two major
approaches are focused on bias and compliance, respectively. But neither of
these ideas fully encompasses ethics: using moral principles to decide how to
act in a particular situation. Our method posits that the way data is labeled
plays an essential role in the way AI behaves, and therefore in the ethics of
machines themselves. The argument combines a fundamental insight from ethics
(i.e. that ethics is about values) with our practical experience building and
scaling machine learning systems. We want to build AI that is actually ethical
by first addressing foundational concerns: how to build good systems, how to
define what is good in relation to system architecture, and who should provide
that definition.
  Building ethical AI creates a foundation of trust between a company and the
users of that platform. But this trust is unjustified unless users experience
the direct value of ethical AI. Until users have real control over how
algorithms behave, something is missing in current AI solutions. This causes
massive distrust in AI, and apathy towards AI ethics solutions. The scope of
this paper is to propose an alternative path that allows for the plurality of
values and the freedom of individual expression. Both are essential for
realizing true moral character.

Artificial intelligence (AI) has enabled a new paradigm of smart applications
-- changing our way of living entirely. Many of these AI-enabled applications
have very stringent latency requirements, especially for applications on mobile
devices (e.g., smartphones, wearable devices, and vehicles). Hence, smaller and
quantized deep neural network (DNN) models are developed for mobile devices,
which provide faster and more energy-efficient computation for mobile AI
applications. However, how AI models consume energy in a mobile device is still
unexplored. Predicting the energy consumption of these models, along with their
different applications, such as vision and non-vision, requires a thorough
investigation of their behavior using various processing sources. In this
paper, we introduce a comprehensive study of mobile AI applications considering
different DNN models and processing sources, focusing on computational resource
utilization, delay, and energy consumption. We measure the latency, energy
consumption, and memory usage of all the models using four processing sources
through extensive experiments. We explain the challenges in such investigations
and how we propose to overcome them. Our study highlights important insights,
such as how mobile AI behaves in different applications (vision and non-vision)
using CPU, GPU, and NNAPI. Finally, we propose a novel Gaussian process
regression-based general predictive energy model based on DNN structures,
computation resources, and processors, which can predict the energy for each
complete application cycle irrespective of device configuration and
application. This study provides crucial facts and an energy prediction
mechanism to the AI research community to help bring energy efficiency to
mobile AI applications.

Recent advancements in pre-trained language models have enabled convenient
methods for generating human-like text at a large scale. Though these
generation capabilities hold great potential for breakthrough applications, it
can also be a tool for an adversary to generate misinformation. In particular,
social media platforms like Twitter are highly susceptible to AI-generated
misinformation. A potential threat scenario is when an adversary hijacks a
credible user account and incorporates a natural language generator to generate
misinformation. Such threats necessitate automated detectors for AI-generated
tweets in a given user's Twitter timeline. However, tweets are inherently
short, thus making it difficult for current state-of-the-art pre-trained
language model-based detectors to accurately detect at what point the AI starts
to generate tweets in a given Twitter timeline. In this paper, we present a
novel algorithm using stylometric signals to aid detecting AI-generated tweets.
We propose models corresponding to quantifying stylistic changes in human and
AI tweets in two related tasks: Task 1 - discriminate between human and
AI-generated tweets, and Task 2 - detect if and when an AI starts to generate
tweets in a given Twitter timeline. Our extensive experiments demonstrate that
the stylometric features are effective in augmenting the state-of-the-art
AI-generated text detectors.

Artificial Intelligence (AI) is making a profound impact in almost every
domain. A vital enabler of its great success is the availability of abundant
and high-quality data for building machine learning models. Recently, the role
of data in AI has been significantly magnified, giving rise to the emerging
concept of data-centric AI. The attention of researchers and practitioners has
gradually shifted from advancing model design to enhancing the quality and
quantity of the data. In this survey, we discuss the necessity of data-centric
AI, followed by a holistic view of three general data-centric goals (training
data development, inference data development, and data maintenance) and the
representative methods. We also organize the existing literature from
automation and collaboration perspectives, discuss the challenges, and tabulate
the benchmarks for various tasks. We believe this is the first comprehensive
survey that provides a global view of a spectrum of tasks across various stages
of the data lifecycle. We hope it can help the readers efficiently grasp a
broad picture of this field, and equip them with the techniques and further
research ideas to systematically engineer data for building AI systems. A
companion list of data-centric AI resources will be regularly updated on
https://github.com/daochenzha/data-centric-AI

Radiology AI models have made significant progress in near-human performance
or surpassing it. However, AI model's partnership with human radiologist
remains an unexplored challenge due to the lack of health information
standards, contextual and workflow differences, and data labeling variations.
To overcome these challenges, we integrated an AI model service that uses DICOM
standard SR annotations into the OHIF viewer in the open-source LibreHealth
Radiology Information Systems (RIS). In this paper, we describe the novel
Human-AI partnership capabilities of the platform, including few-shot learning
and swarm learning approaches to retrain the AI models continuously. Building
on the concept of machine teaching, we developed an active learning strategy
within the RIS, so that the human radiologist can enable/disable AI annotations
as well as "fix"/relabel the AI annotations. These annotations are then used to
retrain the models. This helps establish a partnership between the radiologist
user and a user-specific AI model. The weights of these user-specific models
are then finally shared between multiple models in a swarm learning approach.

Today, many systems use artificial intelligence (AI) to solve complex
problems. While this often increases system effectiveness, developing a
production-ready AI-based system is a difficult task. Thus, solid AI
engineering practices are required to ensure the quality of the resulting
system and to improve the development process. While several practices have
already been proposed for the development of AI-based systems, detailed
practical experiences of applying these practices are rare.
  In this paper, we aim to address this gap by collecting such experiences
during a case study, namely the development of an autonomous stock trading
system that uses machine learning functionality to invest in stocks. We
selected 10 AI engineering practices from the literature and systematically
applied them during development, with the goal to collect evidence about their
applicability and effectiveness. Using structured field notes, we documented
our experiences. Furthermore, we also used field notes to document challenges
that occurred during the development, and the solutions we applied to overcome
them. Afterwards, we analyzed the collected field notes, and evaluated how each
practice improved the development. Lastly, we compared our evidence with
existing literature.
  Most applied practices improved our system, albeit to varying extent, and we
were able to overcome all major challenges. The qualitative results provide
detailed accounts about 10 AI engineering practices, as well as challenges and
solutions associated with such a project. Our experiences therefore enrich the
emerging body of evidence in this field, which may be especially helpful for
practitioner teams new to AI engineering.

Recommender systems typically retrieve items from an item corpus for
personalized recommendations. However, such a retrieval-based recommender
paradigm faces two limitations: 1) the human-generated items in the corpus
might fail to satisfy the users' diverse information needs, and 2) users
usually adjust the recommendations via inefficient passive feedback, e.g.,
clicks. Nowadays, AI-Generated Content (AIGC) has revealed significant success,
offering the potential to overcome these limitations: 1) generative AI can
produce personalized items to satisfy users' information needs, and 2) the
newly emerged large language models significantly reduce the efforts of users
to precisely express information needs via natural language instructions. In
this light, the boom of AIGC points the way towards the next-generation
recommender paradigm with two new objectives: 1) generating personalized
content through generative AI, and 2) integrating user instructions to guide
content generation.
  To this end, we propose a novel Generative Recommender paradigm named
GeneRec, which adopts an AI generator to personalize content generation and
leverages user instructions. Specifically, we pre-process users' instructions
and traditional feedback via an instructor to output the generation guidance.
Given the guidance, we instantiate the AI generator through an AI editor and an
AI creator to repurpose existing items and create new items. Eventually,
GeneRec can perform content retrieval, repurposing, and creation to satisfy
users' information needs. Besides, to ensure the trustworthiness of the
generated items, we emphasize various fidelity checks. Moreover, we provide a
roadmap to envision future developments of GeneRec and several domain-specific
applications of GeneRec with potential research tasks. Lastly, we study the
feasibility of implementing AI editor and AI creator on micro-video generation.

Recent advances in generative artificial intelligence (AI) have captured
worldwide attention. Tools such as Dalle-2 and ChatGPT suggest that tasks
previously thought to be beyond the capabilities of AI may now augment the
productivity of creative media in various new ways, including through the
generation of synthetic video. This research paper explores the utility of
using AI-generated synthetic video to create viable educational content for
online educational settings. To date, there is limited research investigating
the real-world educational value of AI-generated synthetic media. To address
this gap, we examined the impact of using AI-generated synthetic video in an
online learning platform on both learners content acquisition and learning
experience. We took a mixed-method approach, randomly assigning adult learners
(n=83) into one of two micro-learning conditions, collecting pre- and
post-learning assessments, and surveying participants on their learning
experience. The control condition included a traditionally produced instructor
video, while the experimental condition included a synthetic video with a
realistic AI-generated character. The results show that learners in both
conditions demonstrated significant improvement from pre- to post-learning
(p<.001), with no significant differences in gains between the two conditions
(p=.80). In addition, no differences were observed in how learners perceived
the traditional and synthetic videos. These findings suggest that AI-generated
synthetic learning videos have the potential to be a viable substitute for
videos produced via traditional methods in online educational settings, making
high quality educational content more accessible across the globe.

Data storytelling plays an important role in data workers' daily jobs since
it boosts team collaboration and public communication. However, to make an
appealing data story, data workers spend tremendous efforts on various tasks,
including outlining and styling the story. Recently, a growing research trend
has been exploring how to assist data storytelling with advanced artificial
intelligence (AI). However, existing studies may focus on individual tasks in
the workflow of data storytelling and do not reveal a complete picture of
humans' preference for collaborating with AI. To better understand real-world
needs, we interviewed eighteen data workers from both industry and academia to
learn where and how they would like to collaborate with AI. Surprisingly,
though the participants showed excitement about collaborating with AI, many of
them also expressed reluctance and pointed out nuanced reasons. Based on their
responses, we first characterize stages and tasks in the practical data
storytelling workflows and the desired roles of AI. Then the preferred
collaboration patterns in different tasks are identified. Next, we summarize
the interviewees' reasons why and why not they would like to collaborate with
AI. Finally, we provide suggestions for human-AI collaborative data
storytelling to hopefully shed light on future related research.

The close coupling of artificial intelligence (AI) and electroencephalography
(EEG) has substantially advanced human-computer interaction (HCI) technologies
in the AI era. Different from traditional EEG systems, the interpretability and
robustness of AI-based EEG systems are becoming particularly crucial. The
interpretability clarifies the inner working mechanisms of AI models and thus
can gain the trust of users. The robustness reflects the AI's reliability
against attacks and perturbations, which is essential for sensitive and fragile
EEG signals. Thus the interpretability and robustness of AI in EEG systems have
attracted increasing attention, and their research has achieved great progress
recently. However, there is still no survey covering recent advances in this
field. In this paper, we present the first comprehensive survey and summarize
the interpretable and robust AI techniques for EEG systems. Specifically, we
first propose a taxonomy of interpretability by characterizing it into three
types: backpropagation, perturbation, and inherently interpretable methods.
Then we classify the robustness mechanisms into four classes: noise and
artifacts, human variability, data acquisition instability, and adversarial
attacks. Finally, we identify several critical and unresolved challenges for
interpretable and robust AI in EEG systems and further discuss their future
directions.

A generative AI model can generate extremely realistic-looking content,
posing growing challenges to the authenticity of information. To address the
challenges, watermark has been leveraged to detect AI-generated content.
Specifically, a watermark is embedded into an AI-generated content before it is
released. A content is detected as AI-generated if a similar watermark can be
decoded from it. In this work, we perform a systematic study on the robustness
of such watermark-based AI-generated content detection. We focus on
AI-generated images. Our work shows that an attacker can post-process a
watermarked image via adding a small, human-imperceptible perturbation to it,
such that the post-processed image evades detection while maintaining its
visual quality. We show the effectiveness of our attack both theoretically and
empirically. Moreover, to evade detection, our adversarial post-processing
method adds much smaller perturbations to AI-generated images and thus better
maintain their visual quality than existing popular post-processing methods
such as JPEG compression, Gaussian blur, and Brightness/Contrast. Our work
shows the insufficiency of existing watermark-based detection of AI-generated
content, highlighting the urgent needs of new methods. Our code is publicly
available: https://github.com/zhengyuan-jiang/WEvade.

The memorialization of mass atrocities such as war crimes and genocides
facilitates the remembrance of past suffering, honors those who resisted the
perpetrators, and helps prevent the distortion of historical facts. Digital
technologies have transformed memorialization practices by enabling less
top-down and more creative approaches to remember mass atrocities. At the same
time, they may also facilitate the spread of denialism and distortion, attempt
to justify past crimes and attack the dignity of victims. The emergence of
generative forms of artificial intelligence (AI), which produce textual and
visual content, has the potential to revolutionize the field of memorialization
even further. AI can identify patterns in training data to create new
narratives for representing and interpreting mass atrocities - and do so in a
fraction of the time it takes for humans. The use of generative AI in this
context raises numerous questions: For example, can the paucity of training
data on mass atrocities distort how AI interprets some atrocity-related
inquiries? How important is the ability to differentiate between human- and
AI-made content concerning mass atrocities? Can AI-made content be used to
promote false information concerning atrocities? This article addresses these
and other questions by examining the opportunities and risks associated with
using generative AIs for memorializing mass atrocities. It also discusses
recommendations for AIs integration in memorialization practices to steer the
use of these technologies toward a more ethical and sustainable direction.

This study applies Activity Theory to investigate how English as a foreign
language (EFL) students prompt generative artificial intelligence (AI) tools
during short story writing. Sixty-seven Hong Kong secondary school students
created generative-AI tools using open-source language models and wrote short
stories with them. The study collected and analyzed the students' generative-AI
tools, short stories, and written reflections on their conditions or purposes
for prompting. The research identified three main themes regarding the purposes
for which students prompt generative-AI tools during short story writing: a
lack of awareness of purposes, overcoming writer's block, and developing,
expanding, and improving the story. The study also identified common
characteristics of students' activity systems, including the sophistication of
their generative-AI tools, the quality of their stories, and their school's
overall academic achievement level, for their prompting of generative-AI tools
for the three purposes during short story writing. The study's findings suggest
that teachers should be aware of students' purposes for prompting generative-AI
tools to provide tailored instructions and scaffolded guidance. The findings
may also help designers provide differentiated instructions for users at
various levels of story development when using a generative-AI tool.

The rapid adoption of generative Artificial Intelligence (AI) tools that can
generate realistic images or text, such as DALL-E, MidJourney, or ChatGPT, have
put the societal impacts of these technologies at the center of public debate.
These tools are possible due to the massive amount of data (text and images)
that is publicly available through the Internet. At the same time, these
generative AI tools become content creators that are already contributing to
the data that is available to train future models. Therefore, future versions
of generative AI tools will be trained with a mix of human-created and
AI-generated content, causing a potential feedback loop between generative AI
and public data repositories. This interaction raises many questions: how will
future versions of generative AI tools behave when trained on a mixture of real
and AI generated data? Will they evolve and improve with the new data sets or
on the contrary will they degrade? Will evolution introduce biases or reduce
diversity in subsequent generations of generative AI tools? What are the
societal implications of the possible degradation of these models? Can we
mitigate the effects of this feedback loop? In this document, we explore the
effect of this interaction and report some initial results using simple
diffusion models trained with various image datasets. Our results show that the
quality and diversity of the generated images can degrade over time suggesting
that incorporating AI-created data can have undesired effects on future
versions of generative models.

Artificial intelligence (AI) has been widely used in bioimage image analysis
nowadays, but the efficiency of AI models, like the energy consumption and
latency is not ignorable due to the growing model size and complexity, as well
as the fast-growing analysis needs in modern biomedical studies. Like we can
compress large images for efficient storage and sharing, we can also compress
the AI models for efficient applications and deployment. In this work, we
present EfficientBioAI, a plug-and-play toolbox that can compress given
bioimaging AI models for them to run with significantly reduced energy cost and
inference time on both CPU and GPU, without compromise on accuracy. In some
cases, the prediction accuracy could even increase after compression, since the
compression procedure could remove redundant information in the model
representation and therefore reduce over-fitting. From four different bioimage
analysis applications, we observed around 2-5 times speed-up during inference
and 30-80$\%$ saving in energy. Cutting the runtime of large scale bioimage
analysis from days to hours or getting a two-minutes bioimaging AI model
inference done in near real-time will open new doors for method development and
biomedical discoveries. We hope our toolbox will facilitate
resource-constrained bioimaging AI and accelerate large-scale AI-based
quantitative biological studies in an eco-friendly way, as well as stimulate
further research on the efficiency of bioimaging AI.

Although the prevention of AI vulnerabilities is critical to preserve the
safety and privacy of users and businesses, educational tools for robust AI are
still underdeveloped worldwide. We present the design, implementation, and
assessment of Maestro. Maestro is an effective open-source game-based platform
that contributes to the advancement of robust AI education. Maestro provides
goal-based scenarios where college students are exposed to challenging
life-inspired assignments in a competitive programming environment. We assessed
Maestro's influence on students' engagement, motivation, and learning success
in robust AI. This work also provides insights into the design features of
online learning tools that promote active learning opportunities in the robust
AI domain. We analyzed the reflection responses (measured with Likert scales)
of 147 undergraduate students using Maestro in two quarterly college courses in
AI. According to the results, students who felt the acquisition of new skills
in robust AI tended to appreciate highly Maestro and scored highly on material
consolidation, curiosity, and mastery in robust AI. Moreover, the leaderboard,
our key gamification element in Maestro, has effectively contributed to
students' engagement and learning. Results also indicate that Maestro can be
effectively adapted to any course length and depth without losing its
educational quality.

As artificial intelligence (AI) systems are increasingly embedded in our
lives, their presence leads to interactions that shape our behaviour,
decision-making, and social interactions. Existing theoretical research has
primarily focused on human-to-human interactions, overlooking the unique
dynamics triggered by the presence of AI. In this paper, resorting to methods
from evolutionary game theory, we study how different forms of AI influence the
evolution of cooperation in a human population playing the one-shot Prisoner's
Dilemma game in both well-mixed and structured populations. We found that
Samaritan AI agents that help everyone unconditionally, including defectors,
can promote higher levels of cooperation in humans than Discriminatory AI that
only help those considered worthy/cooperative, especially in slow-moving
societies where change is viewed with caution or resistance (small intensities
of selection). Intuitively, in fast-moving societies (high intensities of
selection), Discriminatory AIs promote higher levels of cooperation than
Samaritan AIs.

As AI systems become increasingly prevalent in various aspects of daily life,
gaining a comprehensive understanding of public perception towards these AI
systems has become increasingly essential for several reasons such as ethical
considerations, user experience, fear, disinformation, regulation,
collaboration, and co-creation. In this study, we investigate how mass social
media users perceive the recent rise of AI frameworks such as ChatGPT. We
collect a total of 33,912 comments in 388 unique subreddits spanning from
November 30, 2022 to June 8, 2023 using a list of AI-related keywords. We
employ BERTopic to uncover the major themes regarding AI on Reddit.
Additionally, we seek to gain deeper insights into public opinion by examining
the distribution of topics across different subreddits. We observe that
technology-related subreddits predominantly focus on the technical aspects of
AI models. On the other hand, non-tech subreddits show greater interest in
social issues such as concerns about job replacement or furlough. We leverage
zero-shot prompting to analyze the sentiment and perception of AI among
individual users. Through a comprehensive sentiment and emotion analysis, we
discover that tech-centric communities exhibit greater polarization compared to
non-tech communities when discussing AI topics. This research contributes to
our broader understanding of public opinion surrounding artificial
intelligence.

New systems employ Machine Learning to sift through large knowledge sources,
creating flexible Large Language Models. These models discern context and
predict sequential information in various communication forms. Generative AI,
leveraging Transformers, generates textual or visual outputs mimicking human
responses. It proposes one or multiple contextually feasible solutions for a
user to contemplate. However, generative AI does not currently support
traceability of ideas, a useful feature provided by search engines indicating
origin of information. The narrative style of generative AI has gained positive
reception. People learn from stories. Yet, early ChatGPT efforts had difficulty
with truth, reference, calculations, and aspects like accurate maps. Current
capabilities of referencing locations and linking to apps seem to be better
catered by the link-centric search methods we've used for two decades.
Deploying truly believable solutions extends beyond simulating contextual
relevance as done by generative AI. Combining the creativity of generative AI
with the provenance of internet sources in hybrid scenarios could enhance
internet usage. Generative AI, viewed as drafts, stimulates thinking, offering
alternative ideas for final versions or actions. Scenarios for information
requests are considered. We discuss how generative AI can boost idea generation
by eliminating human bias. We also describe how search can verify facts, logic,
and context. The user evaluates these generated ideas for selection and usage.
This paper introduces a system for knowledge workers, Generate And Search Test,
enabling individuals to efficiently create solutions previously requiring top
collaborations of experts.

Language tests measure a person's ability to use a language in terms of
listening, speaking, reading, or writing. Such tests play an integral role in
academic, professional, and immigration domains, with entities such as
educational institutions, professional accreditation bodies, and governments
using them to assess candidate language proficiency. Recent advances in
Artificial Intelligence (AI) and the discipline of Natural Language Processing
have prompted language test providers to explore AI's potential applicability
within language testing, leading to transformative activity patterns
surrounding language instruction and learning. However, with concerns over AI's
trustworthiness, it is imperative to understand the implications of integrating
AI into language testing. This knowledge will enable stakeholders to make
well-informed decisions, thus safeguarding community well-being and testing
integrity. To understand the concerns and effects of AI usage in language
tests, we conducted interviews and surveys with English test-takers. To the
best of our knowledge, this is the first empirical study aimed at identifying
the implications of AI adoption in language tests from a test-taker
perspective. Our study reveals test-taker perceptions and behavioral patterns.
Specifically, we identify that AI integration may enhance perceptions of
fairness, consistency, and availability. Conversely, it might incite mistrust
regarding reliability and interactivity aspects, subsequently influencing the
behaviors and well-being of test-takers. These insights provide a better
understanding of potential societal implications and assist stakeholders in
making informed decisions concerning AI usage in language testing.

Despite large progress in Explainable and Safe AI, practitioners suffer from
a lack of regulation and standards for AI safety. In this work we merge recent
regulation efforts by the European Union and first proposals for AI guidelines
with recent trends in research: data and model cards. We propose the use of
standardized cards to document AI applications throughout the development
process. Our main contribution is the introduction of use-case and operation
cards, along with updates for data and model cards to cope with regulatory
requirements. We reference both recent research as well as the source of the
regulation in our cards and provide references to additional support material
and toolboxes whenever possible. The goal is to design cards that help
practitioners develop safe AI systems throughout the development process, while
enabling efficient third-party auditing of AI applications, being easy to
understand, and building trust in the system. Our work incorporates insights
from interviews with certification experts as well as developers and
individuals working with the developed AI applications.

Large AI models (e.g., Dall-E, GPT4) have electrified the scientific,
technological and societal landscape through their superhuman capabilities.
These services are offered largely in a traditional web2.0 format (e.g.,
OpenAI's GPT4 service). As more large AI models proliferate (personalizing and
specializing to a variety of domains), there is a tremendous need to have a
neutral trust-free platform that allows the hosting of AI models, clients
receiving AI services efficiently, yet in a trust-free, incentive compatible,
Byzantine behavior resistant manner. In this paper we propose SAKSHI, a
trust-free decentralized platform specifically suited for AI services. The key
design principles of SAKSHI are the separation of the data path (where AI query
and service is managed) and the control path (where routers and compute and
storage hosts are managed) from the transaction path (where the metering and
billing of services are managed over a blockchain). This separation is enabled
by a "proof of inference" layer which provides cryptographic resistance against
a variety of misbehaviors, including poor AI service, nonpayment for service,
copying of AI models. This is joint work between multiple universities
(Princeton University, University of Illinois at Urbana-Champaign, Tsinghua
University, HKUST) and two startup companies (Witness Chain and Eigen Layer).

As a representative cyber-physical system (CPS), robotic manipulator has been
widely adopted in various academic research and industrial processes,
indicating its potential to act as a universal interface between the cyber and
the physical worlds. Recent studies in robotics manipulation have started
employing artificial intelligence (AI) approaches as controllers to achieve
better adaptability and performance. However, the inherent challenge of
explaining AI components introduces uncertainty and unreliability to these
AI-enabled robotics systems, necessitating a reliable development platform for
system design and performance assessment. As a foundational step towards
building reliable AI-enabled robotics systems, we propose a public industrial
benchmark for robotics manipulation in this paper. It leverages NVIDIA
Omniverse Isaac Sim as the simulation platform, encompassing eight
representative manipulation tasks and multiple AI software controllers. An
extensive evaluation is conducted to analyze the performance of AI controllers
in solving robotics manipulation tasks, enabling a thorough understanding of
their effectiveness. To further demonstrate the applicability of our benchmark,
we develop a falsification framework that is compatible with physical
simulators and OpenAI Gym environments. This framework bridges the gap between
traditional testing methods and modern physics engine-based simulations. The
effectiveness of different optimization methods in falsifying AI-enabled
robotics manipulation with physical simulators is examined via a falsification
test. Our work not only establishes a foundation for the design and development
of AI-enabled robotics systems but also provides practical experience and
guidance to practitioners in this field, promoting further research in this
critical academic and industrial domain.

The present study identifies and assesses the bibliographic trend in
Artificial Intelligence (AI) research for the years 2015-2020 using the science
mapping method of bibliometric study. The required data has been collected from
the Scopus database. To make the collected data analysis-ready, essential data
transformation was performed manually and with the help of a tool viz.
OpenRefine. For determining the trend and performing the mapping techniques,
top five open access and commercial journals of AI have been chosen based on
their citescore driven ranking. The work includes 6880 articles published in
the specified period for analysis. The trend is based on Country-wise
publications, year-wise publications, topical terms in AI, top-cited articles,
prominent authors, major institutions, involvement of industries in AI and
Indian appearance. The results show that compared to open access journals;
commercial journals have a higher citescore and number of articles published
over the years. Additionally, IEEE is the prominent publisher which publishes
84% of the top-cited publications. Further, China and the United States are the
major contributors to literature in the AI domain. The study reveals that
neural networks and deep learning are the major topics included in top AI
research publications. Recently, not only public institutions but also private
bodies are investing their resources in AI research. The study also
investigates the relative position of Indian researchers in terms of AI
research. Present work helps in understanding the initial development, current
stand and future direction of AI.

Artificial Intelligence (AI) presents prodigious technological prospects for
development, however, all that glitters is not gold! The cyber-world faces the
worst nightmare with the advent of AI and quantum computers. Together with
Quantum Artificial Intelligence (QAI), they pose a catastrophic threat to
modern cryptography. It would also increase the capability of cryptanalysts
manifold, with its built-in persistent and extensive predictive intelligence.
This prediction ability incapacitates the constrained message space in device
cryptography. With the comparison of these assumptions and the intercepted
ciphertext, the code-cracking process will considerably accelerate. Before the
vigorous and robust developments in AI, we have never faced and never had to
prepare for such a plaintext-originating attack. The supremacy of AI can be
challenged by creating ciphertexts that would give the AI attacker erroneous
responses stymied by randomness and misdirect them. AI threat is deterred by
deviating from the conventional use of small, known-size keys and
pattern-loaded ciphers. The strategy is vested in implementing larger secret
size keys, supplemented by ad-hoc unilateral randomness of unbound limitations
and a pattern-devoid technique. The very large key size can be handled with low
processing and computational burden to achieve desired unicity distances. The
strategy against AI odds is feasible by implementing non-algorithmic
randomness, large and inexpensive memory chips, and wide-area communication
networks. The strength of AI, i.e., randomness and pattern detection can be
used to generate highly optimized ciphers and algorithms. These pattern-devoid,
randomness-rich ciphers also provide a timely and plausible solution for NIST's
proactive approach toward the quantum challenge.

Effective learning strategies based on principles like personalization,
retrieval practice, and spaced repetition are often challenging to implement
due to practical constraints. Here we explore the integration of AI tutors to
complement learning programs in accordance with learning sciences. A
semester-long study was conducted at UniDistance Suisse, where an AI tutor app
was provided to psychology students taking a neuroscience course (N=51). After
automatically generating microlearning questions from existing course materials
using GPT-3, the AI tutor developed a dynamic neural-network model of each
student's grasp of key concepts. This enabled the implementation of distributed
retrieval practice, personalized to each student's individual level and
abilities. The results indicate that students who actively engaged with the AI
tutor achieved significantly higher grades. Moreover, active engagement led to
an average improvement of up to 15 percentile points compared to a parallel
course without AI tutor. Additionally, the grasp strongly correlated with the
exam grade, thus validating the relevance of neural-network predictions. This
research demonstrates the ability of personal AI tutors to model human learning
processes and effectively enhance academic performance. By integrating AI
tutors into their programs, educators can offer students personalized learning
experiences grounded in the principles of learning sciences, thereby addressing
the challenges associated with implementing effective learning strategies.
These findings contribute to the growing body of knowledge on the
transformative potential of AI in education.

A comprehensive approach to addressing catastrophic risks from AI models
should cover the full model lifecycle. This paper explores contingency plans
for cases where pre-deployment risk management falls short: where either very
dangerous models are deployed, or deployed models become very dangerous.
  Informed by incident response practices from industries including
cybersecurity, we describe a toolkit of deployment corrections that AI
developers can use to respond to dangerous capabilities, behaviors, or use
cases of AI models that develop or are detected after deployment. We also
provide a framework for AI developers to prepare and implement this toolkit.
  We conclude by recommending that frontier AI developers should (1) maintain
control over model access, (2) establish or grow dedicated teams to design and
maintain processes for deployment corrections, including incident response
plans, and (3) establish these deployment corrections as allowable actions with
downstream users. We also recommend frontier AI developers, standard-setting
organizations, and regulators should collaborate to define a standardized
industry-wide approach to the use of deployment corrections in incident
response.
  Caveat: This work applies to frontier AI models that are made available
through interfaces (e.g., API) that provide the AI developer or another
upstream party means of maintaining control over access (e.g., GPT-4 or
Claude). It does not apply to management of catastrophic risk from open-source
models (e.g., BLOOM or Llama-2), for which the restrictions we discuss are
largely unenforceable.

A purported `AI Singularity' has been in the public eye recently. Mass media
and US national political attention focused on `AI Doom' narratives hawked by
social media influencers. The European Commission is announcing initiatives to
forestall `AI Extinction'. In my opinion, `AI Singularity' is the wrong
narrative for what's happening now; recent happenings signal something else
entirely. Something fundamental to computation-based research really changed in
the last ten years. In certain fields, progress is dramatically more rapid than
previously, as the fields undergo a transition to frictionless reproducibility
(FR). This transition markedly changes the rate of spread of ideas and
practices, affects mindsets, and erases memories of much that came before.
  The emergence of frictionless reproducibility follows from the maturation of
3 data science principles in the last decade. Those principles involve data
sharing, code sharing, and competitive challenges, however implemented in the
particularly strong form of frictionless open services. Empirical Machine
Learning (EML) is todays leading adherent field, and its consequent rapid
changes are responsible for the AI progress we see. Still, other fields can and
do benefit when they adhere to the same principles.
  Many rapid changes from this maturation are misidentified. The advent of FR
in EML generates a steady flow of innovations; this flow stimulates outsider
intuitions that there's an emergent superpower somewhere in AI. This opens the
way for PR to push worrying narratives: not only `AI Extinction', but also the
supposed monopoly of big tech on AI research. The helpful narrative observes
that the superpower of EML is adherence to frictionless reproducibility
practices; these practices are responsible for the striking progress in AI that
we see everywhere.

In recent years, Artificial Intelligence (AI) shows a spectacular ability of
insertion inside a variety of disciplines which use it for scientific
advancements and which sometimes improve it for their conceptual and
methodological needs. According to the transverse science framework originally
conceived by Shinn and Joerges, AI can be seen as an instrument which is
progressively acquiring a universal character through its diffusion across
science. In this paper we address empirically one aspect of this diffusion,
namely the penetration of AI into a specific field of research. Taking
neuroscience as a case study, we conduct a scientometric analysis of the
development of AI in this field. We especially study the temporal egocentric
citation network around the articles included in this literature, their
represented journals and their authors linked together by a temporal
collaboration network. We find that AI is driving the constitution of a
particular disciplinary ecosystem in neuroscience which is distinct from other
subfields, and which is gathering atypical scientific profiles who are coming
from neuroscience or outside it. Moreover we observe that this AI community in
neuroscience is socially confined in a specific subspace of the neuroscience
collaboration network, which also publishes in a small set of dedicated
journals that are mostly active in AI research. According to these results, the
diffusion of AI in a discipline such as neuroscience didn't really challenge
its disciplinary orientations but rather induced the constitution of a
dedicated socio-cognitive environment inside this field.

The Image Data Commons (IDC) contains publicly available cancer radiology
datasets that could be pertinent to the research and development of advanced
imaging tools and algorithms. However, the full extent of its research
capabilities is limited by the fact that these datasets have few if any,
annotations associated with them. Through this study with the Artificial
Intelligence (AI) AI in Medical Imaging (AIMI) initiative, we produced
high-quality, AI-generated imaging annotations of tissues, organs, and/or
cancers for 11 distinct medical imaging collections from the IDC. These
collections have a variety of image modalities, computed tomography (CT),
magnetic resonance imaging (MRI), and positron emission tomography (PET)
imaging modalities. Furthermore, the imaging collections cover various body
parities, such as the chest, breast, kidneys, prostate, and liver. Both
publicly available and novel AI algorithms were adopted and further developed
using open-sourced data coupled with expert annotations to create the
AI-generated annotations. A portion of the AI annotations were reviewed and
corrected by a radiologist to assess the AI model's performance. Both the AI's
and the radiologist's annotations conformed to DICOM standards for seamless
integration into the IDC collections as third-party analyses. All the models
and datasets (images and annotations) are publicly accessible.

AI faces a trifecta of grand challenges the Energy Wall, the Alignment
Problem and the Leap from Narrow AI to AGI. Contemporary AI solutions consume
unsustainable amounts of energy during model training and daily operations.
Making things worse, the amount of computation required to train each new AI
model has been doubling every 2 months since 2020, directly translating to
increases in energy consumption. The leap from AI to AGI requires multiple
functional subsystems operating in a balanced manner, which requires a system
architecture. However, the current approach to artificial intelligence lacks
system design; even though system characteristics play a key role in the human
brain from the way it processes information to how it makes decisions.
Similarly, current alignment and AI ethics approaches largely ignore system
design, yet studies show that the brains system architecture plays a critical
role in healthy moral decisions. In this paper, we argue that system design is
critically important in overcoming all three grand challenges. We posit that
system design is the missing piece in overcoming the grand challenges. We
present a Systematic AI Approach for AGI that utilizes system design principles
for AGI, while providing ways to overcome the energy wall and the alignment
challenges.

Knowledge Graphs (KGs) have emerged as fundamental platforms for powering
intelligent decision-making and a wide range of Artificial Intelligence (AI)
services across major corporations such as Google, Walmart, and AirBnb. KGs
complement Machine Learning (ML) algorithms by providing data context and
semantics, thereby enabling further inference and question-answering
capabilities. The integration of KGs with neuronal learning (e.g., Large
Language Models (LLMs)) is currently a topic of active research, commonly named
neuro-symbolic AI. Despite the numerous benefits that can be accomplished with
KG-based AI, its growing ubiquity within online services may result in the loss
of self-determination for citizens as a fundamental societal issue. The more we
rely on these technologies, which are often centralised, the less citizens will
be able to determine their own destinies. To counter this threat, AI
regulation, such as the European Union (EU) AI Act, is being proposed in
certain regions. The regulation sets what technologists need to do, leading to
questions concerning: How can the output of AI systems be trusted? What is
needed to ensure that the data fuelling and the inner workings of these
artefacts are transparent? How can AI be made accountable for its
decision-making? This paper conceptualises the foundational topics and research
pillars to support KG-based AI for self-determination. Drawing upon this
conceptual framework, challenges and opportunities for citizen
self-determination are illustrated and analysed in a real-world scenario. As a
result, we propose a research agenda aimed at accomplishing the recommended
objectives.

In the current study,we propose that, in the era of generative AI, there is
now a new form of literacy called "prompt literacy," which refers to the
ability to generate precise prompts as input for AI systems, interpret the
outputs, and iteratively refine prompts to achieve desired results. To explore
the emergence and development of this literacy skill, the current study
examined 30 EFL students' engagement in an AI-powered image creation project,
through which they created artworks representing the socio-cultural meanings of
English words by iteratively drafting and refining prompts in generative AI
tools. By examining AI-generated images and the participants' drafting and
revision of their prompts, this study demonstrated the emergence of learners'
prompt literacy skills. The survey data further showed the participants'
perceived improvement in their vocabulary learning strategies as a result of
engaging in the target AI-powered project. In addition, the participants'
post-project reflection revealed three benefits of developing prompt literacy:
enjoyment from manifesting imagined outcomes; recognition of its importance for
communication, problem-solving and career development; and the enhanced
understanding of the collaborative nature of human-AI interaction. These
findings suggest that prompt literacy is an increasingly crucial literacy for
the AI era.

Generative AI models for music and the arts in general are increasingly
complex and hard to understand. The field of eXplainable AI (XAI) seeks to make
complex and opaque AI models such as neural networks more understandable to
people. One approach to making generative AI models more understandable is to
impose a small number of semantically meaningful attributes on generative AI
models. This paper contributes a systematic examination of the impact that
different combinations of Variational Auto-Encoder models (MeasureVAE and
AdversarialVAE), configurations of latent space in the AI model (from 4 to 256
latent dimensions), and training datasets (Irish folk, Turkish folk, Classical,
and pop) have on music generation performance when 2 or 4 meaningful musical
attributes are imposed on the generative model. To date there have been no
systematic comparisons of such models at this level of combinatorial detail.
Our findings show that MeasureVAE has better reconstruction performance than
AdversarialVAE which has better musical attribute independence. Results
demonstrate that MeasureVAE was able to generate music across music genres with
interpretable musical dimensions of control, and performs best with low
complexity music such a pop and rock. We recommend that a 32 or 64 latent
dimensional space is optimal for 4 regularised dimensions when using MeasureVAE
to generate music across genres. Our results are the first detailed comparisons
of configurations of state-of-the-art generative AI models for music and can be
used to help select and configure AI models, musical features, and datasets for
more understandable generation of music.

The utilization of artificial intelligence (AI) in card games has been a
well-explored subject within AI research for an extensive period. Recent
advancements have propelled AI programs to showcase expertise in intricate card
games such as Mahjong, DouDizhu, and Texas Hold'em. In this work, we aim to
develop an AI program for an exceptionally complex and popular card game called
GuanDan. This game involves four players engaging in both competitive and
cooperative play throughout a long process to upgrade their level, posing great
challenges for AI due to its expansive state and action space, long episode
length, and complex rules. Employing reinforcement learning techniques,
specifically Deep Monte Carlo (DMC), and a distributed training framework, we
first put forward an AI program named DanZero for this game. Evaluation against
baseline AI programs based on heuristic rules highlights the outstanding
performance of our bot. Besides, in order to further enhance the AI's
capabilities, we apply policy-based reinforcement learning algorithm to
GuanDan. To address the challenges arising from the huge action space, which
will significantly impact the performance of policy-based algorithms, we adopt
the pre-trained model to facilitate the training process and the achieved AI
program manages to achieve a superior performance.

Automated coaching messages for weight control can save time and costs, but
their repetitive, generic nature may limit their effectiveness compared to
human coaching. Large language model (LLM) based artificial intelligence (AI)
chatbots, like ChatGPT, could offer more personalized and novel messages to
address repetition with their data-processing abilities. While LLM AI
demonstrates promise to encourage healthier lifestyles, studies have yet to
examine the feasibility and acceptability of LLM-based BWL coaching. 87 adults
in a weight-loss trial rated ten coaching messages' helpfulness (five
human-written, five ChatGPT-generated) using a 5-point Likert scale, providing
additional open-ended feedback to justify their ratings. Participants also
identified which messages they believed were AI-generated. The evaluation
occurred in two phases: messages in Phase 1 were perceived as impersonal and
negative, prompting revisions for Phase 2 messages. In Phase 1, AI-generated
messages were rated less helpful than human-written ones, with 66 percent
receiving a helpfulness rating of 3 or higher. However, in Phase 2, the AI
messages matched the human-written ones regarding helpfulness, with 82% scoring
three or above. Additionally, 50% were misidentified as human-written,
suggesting AI's sophistication in mimicking human-generated content. A thematic
analysis of open-ended feedback revealed that participants appreciated AI's
empathy and personalized suggestions but found them more formulaic, less
authentic, and too data-focused. This study reveals the preliminary feasibility
and acceptability of LLM AIs, like ChatGPT, in crafting potentially effective
weight control coaching messages. Our findings also underscore areas for future
enhancement.

The use of artificial intelligence (AI) in research across all disciplines is
becoming ubiquitous. However, this ubiquity is largely driven by hyperspecific
AI models developed during scientific studies for accomplishing a well-defined,
data-dense task. These AI models introduce apparent, human-recognizable biases
because they are trained with finite, specific data sets and parameters.
However, the efficacy of using large language models (LLMs) -- and LLM-powered
generative AI tools, such as ChatGPT -- to assist the research process is
currently indeterminate. These generative AI tools, trained on general and
imperceptibly large datasets along with human feedback, present challenges in
identifying and addressing biases. Furthermore, these models are susceptible to
goal misgeneralization, hallucinations, and adversarial attacks such as red
teaming prompts -- which can be unintentionally performed by human researchers,
resulting in harmful outputs. These outputs are reinforced in research -- where
an increasing number of individuals have begun to use generative AI to compose
manuscripts. Efforts into AI interpretability lag behind development, and the
implicit variations that occur when prompting and providing context to a
chatbot introduce uncertainty and irreproducibility. We thereby find that
incorporating generative AI in the process of writing research manuscripts
introduces a new type of context-induced algorithmic bias and has unintended
side effects that are largely detrimental to academia, knowledge production,
and communicating research.

This comprehensive survey explored the evolving landscape of generative
Artificial Intelligence (AI), with a specific focus on the transformative
impacts of Mixture of Experts (MoE), multimodal learning, and the speculated
advancements towards Artificial General Intelligence (AGI). It critically
examined the current state and future trajectory of generative Artificial
Intelligence (AI), exploring how innovations like Google's Gemini and the
anticipated OpenAI Q* project are reshaping research priorities and
applications across various domains, including an impact analysis on the
generative AI research taxonomy. It assessed the computational challenges,
scalability, and real-world implications of these technologies while
highlighting their potential in driving significant progress in fields like
healthcare, finance, and education. It also addressed the emerging academic
challenges posed by the proliferation of both AI-themed and AI-generated
preprints, examining their impact on the peer-review process and scholarly
communication. The study highlighted the importance of incorporating ethical
and human-centric methods in AI development, ensuring alignment with societal
norms and welfare, and outlined a strategy for future AI research that focuses
on a balanced and conscientious use of MoE, multimodality, and AGI in
generative AI.

While generative AI excels in content generation, it does not always increase
user engagement. This can be attributed to two main factors. First, generative
AI generates content without incorporating explicit or implicit feedback about
user interactions. Even if the generated content seems to be more informative
or well-written, it does not necessarily lead to an increase in user
activities, such as clicks. Second, there is a concern with the quality of the
content generative AI produces, which often lacks the distinctiveness and
authenticity that human-created content possesses. These two factors can lead
to content that fails to meet specific needs and preferences of users,
ultimately reducing its potential to be engaging.
  This paper presents a generic framework of how to improve user engagement
with generative AI by leveraging user feedback. Our solutions employ rejection
sampling, a technique used in reinforcement learning, to boost engagement
metrics. We leveraged the framework in the context of email notification
subject lines generation for an online social network, and achieved significant
engagement metric lift including +1% Session and +0.4% Weekly Active Users. We
believe our work offers a universal framework that enhances user engagement
with generative AI, particularly when standard generative AI reaches its limits
in terms of enhancing content to be more captivating. To the best of our
knowledge, this represents an early milestone in the industry's successful use
of generative AI to enhance user engagement.

Constructing a universal moral code for artificial intelligence (AI) is
difficult or even impossible, given that different human cultures have
different definitions of morality and different societal norms. We therefore
argue that the value system of an AI should be culturally attuned: just as a
child raised in a particular culture learns the specific values and norms of
that culture, we propose that an AI agent operating in a particular human
community should acquire that community's moral, ethical, and cultural codes.
How AI systems might acquire such codes from human observation and interaction
has remained an open question. Here, we propose using inverse reinforcement
learning (IRL) as a method for AI agents to acquire a culturally-attuned value
system implicitly. We test our approach using an experimental paradigm in which
AI agents use IRL to learn different reward functions, which govern the agents'
moral values, by observing the behavior of different cultural groups in an
online virtual world requiring real-time decision making. We show that an AI
agent learning from the average behavior of a particular cultural group can
acquire altruistic characteristics reflective of that group's behavior, and
this learned value system can generalize to new scenarios requiring altruistic
judgments. Our results provide, to our knowledge, the first demonstration that
AI agents could potentially be endowed with the ability to continually learn
their values and norms from observing and interacting with humans, thereby
becoming attuned to the culture they are operating in.

The rapid evolution of artificial intelligence (AI), especially in the domain
of Large Language Models (LLMs) and generative AI, has opened new avenues for
application across various fields, yet its role in business education remains
underexplored. This study introduces the first benchmark to assess the
performance of seven major LLMs, OpenAI's models (GPT-3.5 Turbo, GPT-4, and
GPT-4 Turbo), Google's models (PaLM 2, Gemini 1.0 Pro), and Anthropic's models
(Claude 2 and Claude 2.1), on the GMAT, which is a key exam in the admission
process for graduate business programs. Our analysis shows that most LLMs
outperform human candidates, with GPT-4 Turbo not only outperforming the other
models but also surpassing the average scores of graduate students at top
business schools. Through a case study, this research examines GPT-4 Turbo's
ability to explain answers, evaluate responses, identify errors, tailor
instructions, and generate alternative scenarios. The latest LLM versions,
GPT-4 Turbo, Claude 2.1, and Gemini 1.0 Pro, show marked improvements in
reasoning tasks compared to their predecessors, underscoring their potential
for complex problem-solving. While AI's promise in education, assessment, and
tutoring is clear, challenges remain. Our study not only sheds light on LLMs'
academic potential but also emphasizes the need for careful development and
application of AI in education. As AI technology advances, it is imperative to
establish frameworks and protocols for AI interaction, verify the accuracy of
AI-generated content, ensure worldwide access for diverse learners, and create
an educational environment where AI supports human expertise. This research
sets the stage for further exploration into the responsible use of AI to enrich
educational experiences and improve exam preparation and assessment methods.

This study explores how discussing metaphors for AI can help build awareness
of the frames that shape our understanding of AI systems, particularly large
language models (LLMs) like ChatGPT. Given the pressing need to teach "critical
AI literacy", discussion of metaphor provides an opportunity for inquiry and
dialogue with space for nuance, playfulness, and critique. Using a
collaborative autoethnographic methodology, we analyzed metaphors from a range
of sources, and reflected on them individually according to seven questions,
then met and discussed our interpretations. We then analyzed how our
reflections contributed to the three kinds of literacies delineated in Selber's
multiliteracies framework: functional, critical, and rhetorical. These allowed
us to analyze questions of ethics, equity, and accessibility in relation to AI.
We explored each metaphor along the dimension of whether or not it was
promoting anthropomorphizing, and to what extent such metaphors imply that AI
is sentient. Our findings highlight the role of metaphor reflection in
fostering a nuanced understanding of AI, suggesting that our collaborative
autoethnographic approach as well as the heuristic model of plotting AI
metaphors on dimensions of anthropomorphism and multiliteracies, might be
useful for educators and researchers in the pursuit of advancing critical AI
literacy.

Artificial intelligence (AI) is rapidly transforming the global economy, and
Latin America is no exception. In recent years, there has been a growing
interest in AI development and implementation in the region. This paper
presents a ranking of Latin American (LATAM) countries based on their potential
to become emerging powers in AI. The ranking is based on three pillars:
infrastructure, education, and finance. Infrastructure is measured by the
availability of electricity, high-speed internet, the quality of
telecommunications networks, and the availability of supercomputers. Education
is measured by the quality of education and the research status. Finance is
measured by the cost of investments, history of investments, economic metrics,
and current implementation of AI.
  While Brazil, Chile, and Mexico have established themselves as major players
in the AI industry in Latin America, our ranking demonstrates the new emerging
powers in the region. According to the results, Argentina, Colombia, Uruguay,
Costa Rica, and Ecuador are leading as new emerging powers in AI in Latin
America. These countries have strong education systems, well-developed
infrastructure, and growing financial resources. The ranking provides a useful
tool for policymakers, investors, and businesses interested in AI development
in Latin America. It can help to identify emerging LATAM countries with the
greatest potential for AI growth and success.

What counts as legitimate AI ethics labor, and consequently, what are the
epistemic terms on which AI ethics claims are rendered legitimate? Based on 75
interviews with technologists including researchers, developers, open source
contributors, and activists, this paper explores the various epistemic bases
from which AI ethics is discussed and practiced. In the context of outside
attacks on AI ethics as an impediment to "progress," I show how some AI ethics
practices have reached toward authority from automation and quantification, and
achieved some legitimacy as a result, while those based on richly embodied and
situated lived experience have not. This paper draws together the work of
feminist Anthropology and Science and Technology Studies scholars Diana
Forsythe and Lucy Suchman with the works of postcolonial feminist theorist Sara
Ahmed and Black feminist theorist Kristie Dotson to examine the implications of
dominant AI ethics practices.
  By entrenching the epistemic power of quantification, dominant AI ethics
practices -- employing Model Cards and similar interventions -- risk
legitimizing AI ethics as a project in equal and opposite measure to which they
marginalize embodied lived experience as a legitimate part of the same project.
In response, I propose humble technical practices: quantified or technical
practices which specifically seek to make their epistemic limits clear in order
to flatten hierarchies of epistemic power.

Designing seamless, frictionless user experiences has long been a dominant
trend in both applied behavioral science and artificial intelligence (AI), in
which the goal of making desirable actions easy and efficient informs efforts
to minimize friction in user experiences. However, in some settings, friction
can be genuinely beneficial, such as the insertion of deliberate delays to
increase reflection, preventing individuals from resorting to automatic or
biased behaviors, and enhancing opportunities for unexpected discoveries. More
recently, the popularization and availability of AI on a widespread scale has
only increased the need to examine how friction can help or hinder users of AI;
it also suggests a need to consider how positive friction can benefit AI
practitioners, both during development processes (e.g., working with diverse
teams) and to inform how AI is designed into offerings. This paper first
proposes a "positive friction" model that can help characterize how friction is
currently beneficial in user and developer experiences with AI, diagnose the
potential need for friction where it may not yet exist in these contexts, and
inform how positive friction can be used to generate solutions, especially as
advances in AI continue to be progress and new opportunities emerge. It then
explores this model in the context of AI users and developers by proposing the
value of taking a hybrid "AI+human" lens, and concludes by suggesting questions
for further exploration.

This study analyzed images generated by three popular generative artificial
intelligence (AI) tools - Midjourney, Stable Diffusion, and DALLE 2 -
representing various occupations to investigate potential bias in AI
generators. Our analysis revealed two overarching areas of concern in these AI
generators, including (1) systematic gender and racial biases, and (2) subtle
biases in facial expressions and appearances. Firstly, we found that all three
AI generators exhibited bias against women and African Americans. Moreover, we
found that the evident gender and racial biases uncovered in our analysis were
even more pronounced than the status quo when compared to labor force
statistics or Google images, intensifying the harmful biases we are actively
striving to rectify in our society. Secondly, our study uncovered more nuanced
prejudices in the portrayal of emotions and appearances. For example, women
were depicted as younger with more smiles and happiness, while men were
depicted as older with more neutral expressions and anger, posing a risk that
generative AI models may unintentionally depict women as more submissive and
less competent than men. Such nuanced biases, by their less overt nature, might
be more problematic as they can permeate perceptions unconsciously and may be
more difficult to rectify. Although the extent of bias varied depending on the
model, the direction of bias remained consistent in both commercial and
open-source AI generators. As these tools become commonplace, our study
highlights the urgency to identify and mitigate various biases in generative
AI, reinforcing the commitment to ensuring that AI technologies benefit all of
humanity for a more inclusive future.

The adoption of Artificial Intelligence in Education (AIED) holds the promise
of revolutionizing educational practices by offering personalized learning
experiences, automating administrative and pedagogical tasks, and reducing the
cost of content creation. However, the lack of standardized practices in the
development and deployment of AIED solutions has led to fragmented ecosystems,
which presents challenges in interoperability, scalability, and ethical
governance. This article aims to address the critical need to develop and
implement industry standards in AIED, offering a comprehensive analysis of the
current landscape, challenges, and strategic approaches to overcome these
obstacles. We begin by examining the various applications of AIED in various
educational settings and identify key areas lacking in standardization,
including system interoperability, ontology mapping, data integration,
evaluation, and ethical governance. Then, we propose a multi-tiered framework
for establishing robust industry standards for AIED. In addition, we discuss
methodologies for the iterative development and deployment of standards,
incorporating feedback loops from real-world applications to refine and adapt
standards over time. The paper also highlights the role of emerging
technologies and pedagogical theories in shaping future standards for AIED.
Finally, we outline a strategic roadmap for stakeholders to implement these
standards, fostering a cohesive and ethical AIED ecosystem. By establishing
comprehensive industry standards, such as those by IEEE Artificial Intelligence
Standards Committee (AISC) and International Organization for Standardization
(ISO), we can accelerate and scale AIED solutions to improve educational
outcomes, ensuring that technological advances align with the principles of
inclusivity, fairness, and educational excellence.

Generative AI offers significant opportunities for language learning. Tools
like ChatGPT can provide informal second language practice through chats in
written or voice forms, with the learner specifying through prompts
conversational parameters such as proficiency level, language register, and
discussion topics. AI can be instructed to give corrective feedback, create
practice exercises, or develop an extended study plan. Instructors can use AI
to build learning and assessment materials in a variety of media. AI is likely
to make immersive technologies more powerful and versatile, moving away from
scripted interactions. For both learners and teachers, it is important to
understand the limitations of AI systems that arise from their purely
statistical model of human language, which limits their ability to deal with
nuanced social and cultural aspects of language use. Additionally, there are
ethical concerns over how AI systems are created as well as practical
constraints in their use, especially for less privileged populations. The power
and versatility of AI tools are likely to turn them into valuable and constant
companions in many peoples lives (akin to smartphones), creating a close
connection that goes beyond simple tool use. Ecological theories such as
sociomaterialism are helpful in examining the shared agency that develops
through close user-AI interactions, as are the perspectives on human-object
relations from Indigenous cultures.

Recent advances in artificial intelligence (AI) capabilities have increased
the eagerness of companies to integrate AI into software systems. While AI can
be used to have a positive impact on several dimensions of sustainability, this
is often overshadowed by its potential negative influence. While many studies
have explored sustainability factors in isolation, there is insufficient
holistic coverage of potential sustainability benefits or costs that
practitioners need to consider during decision-making for AI adoption. We
therefore aim to synthesize trade-offs related to sustainability in the context
of integrating AI into software systems. We want to make the sustainability
benefits and costs of integrating AI more transparent and accessible for
practitioners.
  The study was conducted in collaboration with a Dutch financial organization.
We first performed a rapid review that led to the inclusion of 151 research
papers. Afterward, we conducted six semi-structured interviews to enrich the
data with industry perspectives. The combined results showcase the potential
sustainability benefits and costs of integrating AI. The labels synthesized
from the review regarding potential sustainability benefits were clustered into
16 themes, with "energy management" being the most frequently mentioned one. 11
themes were identified in the interviews, with the top mentioned theme being
"employee wellbeing". Regarding sustainability costs, the review discovered
seven themes, with "deployment issues" being the most popular one, followed by
"ethics & society". "Environmental issues" was the top theme from the
interviews. Our results provide valuable insights to organizations and
practitioners for understanding the potential sustainability implications of
adopting AI.

Large Language Models (LLMs) have become instrumental in advancing software
engineering (SE) tasks, showcasing their efficacy in code understanding and
beyond. Like traditional SE tools, open-source collaboration is key in
realising the excellent products. However, with AI models, the essential need
is in data. The collaboration of these AI-based SE models hinges on maximising
the sources of high-quality data. However, data especially of high quality,
often holds commercial or sensitive value, making it less accessible for
open-source AI-based SE projects. This reality presents a significant barrier
to the development and enhancement of AI-based SE tools within the software
engineering community. Therefore, researchers need to find solutions for
enabling open-source AI-based SE models to tap into resources by different
organisations. Addressing this challenge, our position paper investigates one
solution to facilitate access to diverse organizational resources for
open-source AI models, ensuring privacy and commercial sensitivities are
respected. We introduce a governance framework centered on federated learning
(FL), designed to foster the joint development and maintenance of open-source
AI code models while safeguarding data privacy and security. Additionally, we
present guidelines for developers on AI-based SE tool collaboration, covering
data requirements, model architecture, updating strategies, and version
control. Given the significant influence of data characteristics on FL, our
research examines the effect of code data heterogeneity on FL performance.

Offline licensing is a technical mechanism for compute governance that could
be used to prevent unregulated training of potentially dangerous frontier AI
models. The mechanism works by disabling AI chips unless they have an
up-to-date license from a regulator. In this report, we present a technical
design for a minimal version of offline licensing that could be delivered via a
firmware update. Existing AI chips could potentially support offline licensing
within a year if they have the following (relatively common) hardware security
features: firmware verification, firmware rollback protection, and secure
non-volatile memory. Public documentation suggests that NVIDIA's H100 AI chip
already has these security features. Without additional hardware modifications,
the system is susceptible to physical hardware attacks. However, these attacks
might require expensive equipment and could be difficult to reliably apply to
thousands of AI chips. A firmware-based offline licensing design shares the
same legal requirements and license approval mechanism as a hardware-based
solution. Implementing a firmware-based solution now could accelerate the
eventual deployment of a more secure hardware-based solution in the future. For
AI chip manufacturers, implementing this security mechanism might allow chips
to be sold to customers that would otherwise be prohibited by export
restrictions. For governments, it may be important to be able to prevent unsafe
or malicious actors from training frontier AI models in the next few years.
Based on this initial analysis, firmware-based offline licensing could
partially solve urgent security and trade problems and is technically feasible
for AI chips that have common hardware security features.

Reaching a consensus on the team plans is vital to human-AI coordination.
Although previous studies provide approaches through communications in various
ways, it could still be hard to coordinate when the AI has no explainable plan
to communicate. To cover this gap, we suggest incorporating external models to
assist humans in understanding the intentions of AI agents. In this paper, we
propose a two-stage paradigm that first trains a Theory of Mind (ToM) model
from collected offline trajectories of the target agent, and utilizes the model
in the process of human-AI collaboration by real-timely displaying the future
action predictions of the target agent. Such a paradigm leaves the AI agent as
a black box and thus is available for improving any agents. To test our
paradigm, we further implement a transformer-based predictor as the ToM model
and develop an extended online human-AI collaboration platform for experiments.
The comprehensive experimental results verify that human-AI teams can achieve
better performance with the help of our model. A user assessment attached to
the experiment further demonstrates that our paradigm can significantly enhance
the situational awareness of humans. Our study presents the potential to
augment the ability of humans via external assistance in human-AI
collaboration, which may further inspire future research.

Recent advances in multimodal large language models (LLMs) have lowered the
barriers to rapidly prototyping AI-powered features via prompting, especially
for mobile-intended use cases. Despite the value of situated user feedback, the
process of soliciting early, mobile-situated user feedback on AI prototypes
remains challenging. The broad scope and flexibility of LLMs means that, for a
given use-case-specific prototype, there is a crucial need to understand the
wide range of in-the-wild input likely to be provided by the user, as well as
their in-context expectations of the AI's behavior. To explore the concept of
in situ AI prototyping and testing, we created MobileMaker: an AI prototyping
tool that enables designers to rapidly create mobile AI prototypes that can be
tested on-device, and enables testers to make on-device, in-the-field revisions
of the prototype through natural language. In an exploratory study with 16
users, we explored how user feedback on prototypes created with MobileMaker
compares to that of existing prototyping tools (e.g., Figma, prompt editors).
We found that MobileMaker prototypes enabled more serendipitous discovery of:
model input edge cases, discrepancies between AI's and user's in-context
interpretation of the task, and contextual signals missed by the AI.
Furthermore, we learned that while the ability to make in-the-wild revisions
led users to feel more fulfilled as active participants in the design process,
it might also constrain their feedback to the subset of changes perceived as
more actionable or implementable by the prototyping tool.

Applications of Generative AI (Gen AI) are expected to revolutionize a number
of different areas, ranging from science & medicine to education. The potential
for these seismic changes has triggered a lively debate about the potential
risks of the technology, and resulted in calls for tighter regulation, in
particular from some of the major tech companies who are leading in AI
development. This regulation is likely to put at risk the budding field of
open-source generative AI. Using a three-stage framework for Gen AI development
(near, mid and long-term), we analyze the risks and opportunities of
open-source generative AI models with similar capabilities to the ones
currently available (near to mid-term) and with greater capabilities
(long-term). We argue that, overall, the benefits of open-source Gen AI
outweigh its risks. As such, we encourage the open sourcing of models, training
and evaluation data, and provide a set of recommendations and best practices
for managing risks associated with open-source generative AI.

An immune system inspired Artificial Immune System (AIS) algorithm is
presented, and is used for the purposes of automated program verification.
Relevant immunological concepts are discussed and the field of AIS is briefly
reviewed. It is proposed to use this AIS algorithm for a specific automated
program verification task: that of predicting shape of program invariants. It
is shown that the algorithm correctly predicts program invariant shape for a
variety of benchmarked programs.

In this paper, we study a matrix equation $X^n = aI$. We factorize $X^n - aI$
based upon the factorization of $x^n - a$ and then give a necessary and
sufficient condition for one of the factors to be the zero matrix.

Every AI system is deployed by a human organization. In high risk
applications, the combined human plus AI system must function as a
high-reliability organization in order to avoid catastrophic errors. This short
note reviews the properties of high-reliability organizations and draws
implications for the development of AI technology and the safe application of
that technology.

This article is submitted and accepted as ACM UIST 2019 Visions. UIST Visions
is a venue for forward thinking ideas to inspire the community. The goal is not
to report research but to project and propose new research directions. This
article, entitled "Homo Cyberneticus: The Era of Human-AI Integration",
proposes HCI research directions, namely human-augmentation and
human-AI-integration.

We propose a new model for regulation to achieve AI safety: global regulatory
markets. We first sketch the model in general terms and provide an overview of
the costs and benefits of this approach. We then demonstrate how the model
might work in practice: responding to the risk of adversarial attacks on AI
models employed in commercial drones.

This article surveys engineering and neuroscientific models of planning as a
cognitive function, which is regarded as a typical function of fluid
intelligence in the discussion of general intelligence. It aims to present
existing planning models as references for realizing the planning function in
brain-inspired AI or artificial general intelligence (AGI). It also proposes
themes for the research and development of brain-inspired AI from the viewpoint
of tasks and architecture.

Use of artificial intelligence is growing and expanding into applications
that impact people's lives. People trust their technology without really
understanding it or its limitations. There is the potential for harm and we are
already seeing examples of that in the world. AI researchers have an obligation
to consider the impact of intelligent applications they work on. While the
ethics of AI is not clear-cut, there are guidelines we can consider to minimize
the harm we might introduce.

In response to a national and international awakening on the issues of
anti-Blackness and systemic discrimination, we have penned this piece to serve
as a resource for allies in the AI community who are wondering how they can
more effectively engage with dismantling racist systems. This work aims to help
elucidate areas where the AI community actively and passively contributes to
anti-Blackness and offers actionable items on ways to reduce harm.

A diverse team of engineers, artists, and algorithms, collaborated to create
songs for SophiaPop, via various neural networks, robotics technologies, and
artistic tools, and animated the results on Sophia the Robot, a robotic
celebrity and animated character. Sophia is a platform for arts, research, and
other uses. To advance the art and technology of Sophia, we combine various AI
with a fictional narrative of her burgeoning career as a popstar. Her actual
AI-generated pop lyrics, music, and paintings, and animated conversations
wherein she interacts with humans real-time in narratives that discuss her
experiences. To compose the music, SophiaPop team built corpora from human and
AI-generated Sophia character personality content, along with pop music song
forms, to train and provide seeds for a number of AI algorithms including
expert models, and custom-trained transformer neural networks, which then
generated original pop-song lyrics and melodies. Our musicians including
Frankie Storm, Adam Pickrell, and Tiger Darrow, then performed interpretations
of the AI-generated musical content, including singing and instrumentation. The
human-performed singing data then was processed by a neural-network-based
Sophia voice, which was custom-trained from human performances by Cereproc.
This AI then generated the unique Sophia voice singing of the songs. Then we
animated Sophia to sing the songs in music videos, using a variety of animation
generators and human-generated animations. Being algorithms and humans, working
together, SophiaPop represents a human-AI collaboration, aspiring toward human
AI symbiosis. We believe that such a creative convergence of multiple
disciplines with humans and AI working together, can make AI relevant to human
culture in new and exciting ways, and lead to a hopeful vision for the future
of human-AI relations.

Compared to other global powers, the European Union (EU) is rarely considered
a leading player in the development of artificial intelligence (AI). Why is
this, and does this in fact accurately reflect the activities of the EU? What
would it take for the EU to take a more leading role in AI? This report surveys
core components of the current AI ecosystem of the EU, providing the crucial
background context for answering these questions.

We provide an abstract characterization for the Cuntz semigroup of unital
commutative AI-algebras, as well as a characterization for abstract Cuntz
semigroups of the form $\text{Lsc} (X,\overline{\mathbb{N}})$ for some
$T_1$-space $X$. In our investigations, we also uncover new properties that the
Cuntz semigroup of all AI-algebras satisfies.

The 3rd edition of the Montreal AI Ethics Institute's The State of AI Ethics
captures the most relevant developments in AI Ethics since October 2020. It
aims to help anyone, from machine learning experts to human rights activists
and policymakers, quickly digest and understand the field's ever-changing
developments. Through research and article summaries, as well as expert
commentary, this report distills the research and reporting surrounding various
domains related to the ethics of AI, including: algorithmic injustice,
discrimination, ethical AI, labor impacts, misinformation, privacy, risk and
security, social media, and more.
  In addition, The State of AI Ethics includes exclusive content written by
world-class AI Ethics experts from universities, research institutes,
consulting firms, and governments. Unique to this report is "The Abuse and
Misogynoir Playbook," written by Dr. Katlyn Tuner (Research Scientist, Space
Enabled Research Group, MIT), Dr. Danielle Wood (Assistant Professor, Program
in Media Arts and Sciences; Assistant Professor, Aeronautics and Astronautics;
Lead, Space Enabled Research Group, MIT) and Dr. Catherine D'Ignazio (Assistant
Professor, Urban Science and Planning; Director, Data + Feminism Lab, MIT). The
piece (and accompanying infographic), is a deep-dive into the historical and
systematic silencing, erasure, and revision of Black women's contributions to
knowledge and scholarship in the United Stations, and globally. Exposing and
countering this Playbook has become increasingly important following the firing
of AI Ethics expert Dr. Timnit Gebru (and several of her supporters) at Google.
  This report should be used not only as a point of reference and insight on
the latest thinking in the field of AI Ethics, but should also be used as a
tool for introspection as we aim to foster a more nuanced conversation
regarding the impacts of AI on the world.

This perspective illustrates some of the AI applications that can accelerate
the achievement of SDGs and also highlights some of the considerations that
could hinder the efforts towards them. This emphasizes the importance of
establishing standard AI guidelines and regulations for the beneficial
applications of AI.

We discuss our insights into interpretable artificial-intelligence (AI)
models, and how they are essential in the context of developing ethical AI
systems, as well as data-driven solutions compliant with the Sustainable
Development Goals (SDGs). We highlight the potential of extracting
truly-interpretable models from deep-learning methods, for instance via
symbolic models obtained through inductive biases, to ensure a sustainable
development of AI.

We propose a novel approach to explainable AI (XAI) based on the concept of
"instruction" from neural networks. In this case study, we demonstrate how a
superhuman neural network might instruct human trainees as an alternative to
traditional approaches to XAI. Specifically, an AI examines human actions and
calculates variations on the human strategy that lead to better performance.
Experiments with a JHU/APL-developed AI player for the cooperative card game
Hanabi suggest this technique makes unique contributions to explainability
while improving human performance. One area of focus for Instructive AI is in
the significant discrepancies that can arise between a human's actual strategy
and the strategy they profess to use. This inaccurate self-assessment presents
a barrier for XAI, since explanations of an AI's strategy may not be properly
understood or implemented by human recipients. We have developed and are
testing a novel, Instructive AI approach that estimates human strategy by
observing human actions. With neural networks, this allows a direct calculation
of the changes in weights needed to improve the human strategy to better
emulate a more successful AI. Subjected to constraints (e.g. sparsity) these
weight changes can be interpreted as recommended changes to human strategy
(e.g. "value A more, and value B less"). Instruction from AI such as this
functions both to help humans perform better at tasks, but also to better
understand, anticipate, and correct the actions of an AI. Results will be
presented on AI instruction's ability to improve human decision-making and
human-AI teaming in Hanabi.

As AI systems demonstrate increasingly strong predictive performance, their
adoption has grown in numerous domains. However, in high-stakes domains such as
criminal justice and healthcare, full automation is often not desirable due to
safety, ethical, and legal concerns, yet fully manual approaches can be
inaccurate and time consuming. As a result, there is growing interest in the
research community to augment human decision making with AI assistance. Besides
developing AI technologies for this purpose, the emerging field of human-AI
decision making must embrace empirical approaches to form a foundational
understanding of how humans interact and work with AI to make decisions. To
invite and help structure research efforts towards a science of understanding
and improving human-AI decision making, we survey recent literature of
empirical human-subject studies on this topic. We summarize the study design
choices made in over 100 papers in three important aspects: (1) decision tasks,
(2) AI models and AI assistance elements, and (3) evaluation metrics. For each
aspect, we summarize current trends, discuss gaps in current practices of the
field, and make a list of recommendations for future research. Our survey
highlights the need to develop common frameworks to account for the design and
research spaces of human-AI decision making, so that researchers can make
rigorous choices in study design, and the research community can build on each
other's work and produce generalizable scientific knowledge. We also hope this
survey will serve as a bridge for HCI and AI communities to work together to
mutually shape the empirical science and computational technologies for
human-AI decision making.

AI Blitz XIII Faces challenge hosted on www.aicrowd.com platform consisted of
five problems: Sentiment Classification, Age Prediction, Mask Prediction, Face
Recognition, and Face De-Blurring. Our team GLaDOS took second place. Here we
present our solutions and results. Code implementation:
https://github.com/ndrwmlnk/ai-blitz-xiii

Artificial intelligence (AI) assisting with antimicrobial prescribing raises
significant moral questions. Utilising ethical frameworks alongside AI-driven
systems, while considering infection specific complexities, can support moral
decision making to tackle antimicrobial resistance.

Recent advances in artificial intelligence (AI), especially in generative
language modelling, hold the promise of transforming government. Given the
advanced capabilities of new AI systems, it is critical that these are embedded
using standard operational procedures, clear epistemic criteria, and behave in
alignment with the normative expectations of society. Scholars in multiple
domains have subsequently begun to conceptualize the different forms that AI
applications may take, highlighting both their potential benefits and pitfalls.
However, the literature remains fragmented, with researchers in social science
disciplines like public administration and political science, and the
fast-moving fields of AI, ML, and robotics, all developing concepts in relative
isolation. Although there are calls to formalize the emerging study of AI in
government, a balanced account that captures the full depth of theoretical
perspectives needed to understand the consequences of embedding AI into a
public sector context is lacking. Here, we unify efforts across social and
technical disciplines by first conducting an integrative literature review to
identify and cluster 69 key terms that frequently co-occur in the
multidisciplinary study of AI. We then build on the results of this
bibliometric analysis to propose three new multifaceted concepts for
understanding and analysing AI-based systems for government (AI-GOV) in a more
unified way: (1) operational fitness, (2) epistemic alignment, and (3)
normative divergence. Finally, we put these concepts to work by using them as
dimensions in a conceptual typology of AI-GOV and connecting each with emerging
AI technical measurement standards to encourage operationalization, foster
cross-disciplinary dialogue, and stimulate debate among those aiming to rethink
government with AI.

Human-centered AI workflows involve stakeholders with multiple roles
interacting with each other and automated agents to accomplish diverse tasks.
In this paper, we call for a holistic view when designing support mechanisms,
such as interaction paradigms, interfaces, and systems, for these multifaceted
workflows.

This paper motivates and develops a framework for understanding how the
socio-technical systems surrounding AI development interact with social
welfare. It introduces the concept of ``signaling'' from evolutionary game
theory and demonstrates how it can enhance existing theory and practice
surrounding the evaluation and governance of AI systems.

Current proposals for AI regulation, in the EU and beyond, aim to spur AI
that is trustworthy (e.g., AI Act) and accountable (e.g., AI Liability) What is
missing, however, is a robust regulatory discourse and roadmap to make AI, and
technology more broadly, environmentally sustainable. This paper aims to take
first steps to fill this gap. The ICT sector contributes up to 3.9 percent of
global greenhouse gas (GHG) emissions-more than global air travel at 2.5
percent. The carbon footprint and water consumption of AI, especially
large-scale generative models like GPT-4, raise significant sustainability
concerns. The paper is the first to assess how current and proposed technology
regulations, including EU environmental law, the General Data Protection
Regulation (GDPR), and the AI Act, could be adjusted to better account for
environmental sustainability. The GDPR, for instance, could be interpreted to
limit certain individual rights like the right to erasure if these rights
significantly conflict with broader sustainability goals. In a second step, the
paper suggests a multi-faceted approach to achieve sustainable AI regulation.
It advocates for transparency mechanisms, such as disclosing the GHG footprint
of AI systems, as laid out in the proposed EU AI Act. However, sustainable AI
regulation must go beyond mere transparency. The paper proposes a regulatory
toolkit comprising co-regulation, sustainability-by-design principles,
restrictions on training data, and consumption caps, including integration into
the EU Emissions Trading Scheme. Finally, the paper argues that this regulatory
toolkit could serve as a blueprint for regulating other high-emission
technologies and infrastructures like blockchain, Metaverse applications, and
data centers. The framework aims to cohesively address the crucial dual
challenges of our era: digital transformation and climate change mitigation.

This white paper is a response to the "AI Accountability Policy Request for
Comments" by the National Telecommunications and Information Administration of
the United States. The question numbers for which comments were requested are
provided in superscripts at the end of key sentences answering the respective
questions. The white paper offers a set of interconnected recommendations for
an AI accountability policy.

By employing Gauss decomposition, we establish a direct and explicit
isomorphism between the twisted $q$-Yangians (in R-matrix presentation) and
affine $\imath$quantum groups (in current presentation) associated to symmetric
pair of type AI introduced by Molev-Ragoucy-Sorba and Lu-Wang, respectively. As
a corollary, we obtain a PBW type basis for affine $\imath$quantum groups of
type AI.

These informal notes are based on the author's lecture at the National
Academies of Science, Engineering, and Mathematics workshop on "AI to Assist
Mathematical Reasoning" in June 2023. The goal is to think through a path by
which we might arrive at AI that is useful for the research mathematician.

Artificial intelligence (AI) technologies have emerged as pivotal enablers
across a multitude of industries, including consumer electronics, healthcare,
and manufacturing, largely due to their resurgence over the past decade. The
transformative power of AI is primarily derived from the utilization of deep
neural networks (DNNs), which require extensive data for training and
substantial computational resources for processing. Consequently, DNN models
are typically trained and deployed on resource-rich cloud servers. However, due
to potential latency issues associated with cloud communications, deep learning
(DL) workflows are increasingly being transitioned to wireless edge networks
near end-user devices (EUDs). This shift is designed to support
latency-sensitive applications and has given rise to a new paradigm of edge AI,
which will play a critical role in upcoming 6G networks to support ubiquitous
AI applications. Despite its potential, edge AI faces substantial challenges,
mostly due to the dichotomy between the resource limitations of wireless edge
networks and the resource-intensive nature of DL. Specifically, the acquisition
of large-scale data, as well as the training and inference processes of DNNs,
can rapidly deplete the battery energy of EUDs. This necessitates an
energy-conscious approach to edge AI to ensure both optimal and sustainable
performance. In this paper, we present a contemporary survey on green edge AI.
We commence by analyzing the principal energy consumption components of edge AI
systems to identify the fundamental design principles of green edge AI. Guided
by these principles, we then explore energy-efficient design methodologies for
the three critical tasks in edge AI systems, including training data
acquisition, edge training, and edge inference. Finally, we underscore
potential future research directions to further enhance the energy efficiency
of edge AI.

In the largest survey of its kind, 2,778 researchers who had published in
top-tier artificial intelligence (AI) venues gave predictions on the pace of AI
progress and the nature and impacts of advanced AI systems The aggregate
forecasts give at least a 50% chance of AI systems achieving several milestones
by 2028, including autonomously constructing a payment processing site from
scratch, creating a song indistinguishable from a new song by a popular
musician, and autonomously downloading and fine-tuning a large language model.
If science continues undisrupted, the chance of unaided machines outperforming
humans in every possible task was estimated at 10% by 2027, and 50% by 2047.
The latter estimate is 13 years earlier than that reached in a similar survey
we conducted only one year earlier [Grace et al., 2022]. However, the chance of
all human occupations becoming fully automatable was forecast to reach 10% by
2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).
  Most respondents expressed substantial uncertainty about the long-term value
of AI progress: While 68.3% thought good outcomes from superhuman AI are more
likely than bad, of these net optimists 48% gave at least a 5% chance of
extremely bad outcomes such as human extinction, and 59% of net pessimists gave
5% or more to extremely good outcomes. Between 38% and 51% of respondents gave
at least a 10% chance to advanced AI leading to outcomes as bad as human
extinction. More than half suggested that "substantial" or "extreme" concern is
warranted about six different AI-related scenarios, including misinformation,
authoritarian control, and inequality. There was disagreement about whether
faster or slower AI progress would be better for the future of humanity.
However, there was broad agreement that research aimed at minimizing potential
risks from AI systems ought to be prioritized more.

Recent advances of artificial intelligence (AI) code generators are opening
new opportunities in software security research, including misuse by malicious
actors. We review use cases for AI code generators for security and introduce
an evaluation benchmark.

Generative AI's expanding footprint across numerous industries has led to
both excitement and increased scrutiny. This paper delves into the unique
security challenges posed by Generative AI, and outlines potential research
directions for managing these risks.

A central issue in copyright lawsuits against generative-AI companies is the
degree to which a generative-AI model does or does not "memorize" the data it
was trained on. Unfortunately, the debate has been clouded by ambiguity over
what "memorization" is, leading to legal debates in which participants often
talk past one another. In this essay, we attempt to bring clarity to the
conversation over memorization.

Reliability of AI systems is a fundamental concern for the successful
deployment and widespread adoption of AI technologies. Unfortunately, the
escalating complexity and heterogeneity of AI hardware systems make them
increasingly susceptible to hardware faults (e.g., bit flips) that can
potentially corrupt model parameters. When this occurs during AI
inference/servicing, it can potentially lead to incorrect or degraded model
output for users, ultimately affecting the quality and reliability of AI
services. In light of the escalating threat, it is crucial to address key
questions: How vulnerable are AI models to parameter corruptions, and how do
different components (such as modules, layers) of the models exhibit varying
vulnerabilities to parameter corruptions? To systematically address this
question, we propose a novel quantitative metric, Parameter Vulnerability
Factor (PVF), inspired by architectural vulnerability factor (AVF) in computer
architecture community, aiming to standardize the quantification of AI model
vulnerability against parameter corruptions. We define a model parameter's PVF
as the probability that a corruption in that particular model parameter will
result in an incorrect output. In this paper, we present several use cases on
applying PVF to three types of tasks/models during inference -- recommendation
(DLRM), vision classification (CNN), and text classification (BERT), while
presenting an in-depth vulnerability analysis on DLRM. PVF can provide pivotal
insights to AI hardware designers in balancing the tradeoff between fault
protection and performance/efficiency such as mapping vulnerable AI parameter
components to well-protected hardware modules. PVF metric is applicable to any
AI model and has a potential to help unify and standardize AI
vulnerability/resilience evaluation practice.

The trustworthiness of AI applications has been the subject of recent
research and is also addressed in the EU's recently adopted AI Regulation. The
currently emerging foundation models in the field of text, speech and image
processing offer completely new possibilities for developing AI applications.
This whitepaper shows how the trustworthiness of an AI application developed
with foundation models can be evaluated and ensured. For this purpose, the
application-specific, risk-based approach for testing and ensuring the
trustworthiness of AI applications, as developed in the 'AI Assessment Catalog
- Guideline for Trustworthy Artificial Intelligence' by Fraunhofer IAIS, is
transferred to the context of foundation models. Special consideration is given
to the fact that specific risks of foundation models can have an impact on the
AI application and must also be taken into account when checking
trustworthiness. Chapter 1 of the white paper explains the fundamental
relationship between foundation models and AI applications based on them in
terms of trustworthiness. Chapter 2 provides an introduction to the technical
construction of foundation models and Chapter 3 shows how AI applications can
be developed based on them. Chapter 4 provides an overview of the resulting
risks regarding trustworthiness. Chapter 5 shows which requirements for AI
applications and foundation models are to be expected according to the draft of
the European Union's AI Regulation and Chapter 6 finally shows the system and
procedure for meeting trustworthiness requirements.

Following the recent release of AI assistants, such as OpenAI's ChatGPT and
GitHub Copilot, the software industry quickly utilized these tools for software
development tasks, e.g., generating code or consulting AI for advice. While
recent research has demonstrated that AI-generated code can contain security
issues, how software professionals balance AI assistant usage and security
remains unclear. This paper investigates how software professionals use AI
assistants in secure software development, what security implications and
considerations arise, and what impact they foresee on secure software
development. We conducted 27 semi-structured interviews with software
professionals, including software engineers, team leads, and security testers.
We also reviewed 190 relevant Reddit posts and comments to gain insights into
the current discourse surrounding AI assistants for software development. Our
analysis of the interviews and Reddit posts finds that despite many security
and quality concerns, participants widely use AI assistants for
security-critical tasks, e.g., code generation, threat modeling, and
vulnerability detection. Their overall mistrust leads to checking AI
suggestions in similar ways to human code, although they expect improvements
and, therefore, a heavier use for security tasks in the future. We conclude
with recommendations for software professionals to critically check AI
suggestions, AI creators to improve suggestion security and capabilities for
ethical security tasks, and academic researchers to consider general-purpose AI
in software development.

Let (T,M) be a complete local domain containing the integers. Let p1
\subseteq p2 \subseteq ... \subseteq pn be a chain of nonmaximal prime ideals T
such that T_pn is a regular local ring. We construct a chain of excellent local
domains An \subseteq A1 such that for each i, the completion of Ai is T, the
generic formal fiber of Ai is local with maximal ideal pi, and if I is a
nonzero ideal of Ai then Ai/I is complete. Consequently, if in addition T is a
UFD, then we can construct a chain of excellent local UFDs satisfying the same
conditions.

Fixing a closed hyperbolic surface S, we define a moduli space AI(S) of
unmarked hyperbolic 3-manifolds homotopy equivalent to S. This 3-dimensional
analogue of the moduli space M(S) of unmarked hyperbolic surfaces homeomorphic
to S has bizarre local topology, possessing many points that are not closed.
There is, however, a natural embedding of M(S) into AI(S) and a
compactification of AI(S) such that this embedding extends to an embedding of
the Deligne-Mumford compactification of M(S) into the compactification of
AI(S).

For a cyclic $\AI$-algebra, a potential recording the structure constants can
be defined. We define an analogous potential for a homotopy cyclic
$\AI$-algebra and prove its properties. On the other hand, we find another
different potential for a homotopy cyclic $\AI$-algebra, which is related to
the algebraic analogue of generalized holonomy map of Abbaspour, Tradler and
Zeinalian.

In this short paper I briefly discuss 3D war Game based on artificial
intelligence concepts called AI WAR. Going in to the details, I present the
importance of CAICL language and how this language is used in AI WAR. Moreover
I also present a designed and implemented 3D War Cybug for AI WAR using CAICL
and discus the implemented strategy to defeat its enemies during the game life.

The Airy function Ai(z) and its derivative Ai'(z) occur in a large number of
applications in Chemistry and Physics. As a result, there is a continuing
interest in the properties of these functions. Recently, there has been
interest in obtaining general expressions for the higher derivatives of these
functions. In this work, general expressions for the polynomials which are
contained in these derivatives are given in terms of the partial Bell
polynomials.

Artificial Intelligence - what is this? That is the question! In earlier
papers we already gave a formal definition for AI, but if one desires to build
an actual AI implementation, the following issues require attention and are
treated here: the data format to be used, the idea of Undef and Nothing
symbols, various ways for defining the "meaning of life", and finally, a new
notion of "incorrect move". These questions are of minor importance in the
theoretical discussion, but we already know the answer of the question "Does AI
exist?" Now we want to make the next step and to create this program.

We present initial ideas for a programming paradigm based on simulation that
is targeted towards applications of artificial intelligence (AI). The approach
aims at integrating techniques from different areas of AI and is based on the
idea that simulated entities may freely exchange data and behavioural patterns.
We define basic notions of a simulation-based programming paradigm and show how
it can be used for implementing AI applications.

Success in the quest for artificial intelligence has the potential to bring
unprecedented benefits to humanity, and it is therefore worthwhile to
investigate how to maximize these benefits while avoiding potential pitfalls.
This article gives numerous examples (which should by no means be construed as
an exhaustive list) of such worthwhile research aimed at ensuring that AI
remains robust and beneficial.

Does the energy requirements for the human brain give energy constraints that
give reason to doubt the feasibility of artificial intelligence? This report
will review some relevant estimates of brain bioenergetics and analyze some of
the methods of estimating brain emulation energy requirements. Turning to AI,
there are reasons to believe the energy requirements for de novo AI to have
little correlation with brain (emulation) energy requirements since cost could
depend merely of the cost of processing higher-level representations rather
than billions of neural firings. Unless one thinks the human way of thinking is
the most optimal or most easily implementable way of achieving software
intelligence, we should expect de novo AI to make use of different, potentially
very compressed and fast, processes.

Given recent successes in AI (e.g., AlphaGo's victory against Lee Sedol in
the game of GO), it's become increasingly important to assess: how close are AI
systems to human-level intelligence? This paper describes the Allen AI Science
Challenge---an approach towards that goal which led to a unique Kaggle
Competition, its results, the lessons learned, and our next steps.

Verified artificial intelligence (AI) is the goal of designing AI-based
systems that that have strong, ideally provable, assurances of correctness with
respect to mathematically-specified requirements. This paper considers Verified
AI from a formal methods perspective. We describe five challenges for achieving
Verified AI, and five corresponding principles for addressing these challenges.

Artificial intelligence (AI) research enjoyed an initial period of enthusiasm
in the 1970s and 80s. But this enthusiasm was tempered by a long interlude of
frustration when genuinely useful AI applications failed to be forthcoming.
Today, we are experiencing once again a period of enthusiasm, fired above all
by the successes of the technology of deep neural networks or deep machine
learning. In this paper we draw attention to what we take to be serious
problems underlying current views of artificial intelligence encouraged by
these successes, especially in the domain of language processing. We then show
an alternative approach to language-centric AI, in which we identify a role for
philosophy.

We present a simple hypothesis about a compression property of artificial
intelligence (AI) classifiers and present theoretical arguments to show that
this hypothesis successfully accounts for the observed fragility of AI
classifiers to small adversarial perturbations. We also propose a new method
for detecting when small input perturbations cause classifier errors, and show
theoretical guarantees for the performance of this detection method. We present
experimental results with a voice recognition system to demonstrate this
method. The ideas in this paper are motivated by a simple analogy between AI
classifiers and the standard Shannon model of a communication system.

In this paper, we argue that competitive pressures could incentivize AI
companies to underinvest in ensuring their systems are safe, secure, and have a
positive social impact. Ensuring that AI systems are developed responsibly may
therefore require preventing and solving collective action problems between
companies. We note that there are several key factors that improve the
prospects for cooperation in collective action problems. We use this to
identify strategies to improve the prospects for industry cooperation on the
responsible development of AI.

Artificial Intelligence (AI) has received tremendous attention from academia,
industry, and the general public in recent years. The integration of geography
and AI, or GeoAI, provides novel approaches for addressing a variety of
problems in the natural environment and our human society. This entry briefly
reviews the recent development of AI with a focus on machine learning and deep
learning approaches. We discuss the integration of AI with geography and
particularly geographic information science, and present a number of GeoAI
applications and possible future directions.

This paper applies theories about the Human Visual System to make Adversarial
AI more effective. To date, Adversarial AI has modeled perceptual distances
between clean and adversarial examples of images using Lp norms. These norms
have the benefit of simple mathematical description and reasonable
effectiveness in approximating perceptual distance. However, in prior decades,
other areas of image processing have moved beyond simpler models like Mean
Squared Error (MSE) towards more complex models that better approximate the
Human Visual System (HVS). We demonstrate a proof of concept of incorporating
HVS models into Adversarial AI.

Traditionally, researchers in automatic face recognition and biometric
technologies have focused on developing accurate algorithms. With this
technology being integrated into operational systems, engineers and scientists
are being asked, do these systems meet societal norms? The origin of this line
of inquiry is `trust' of artificial intelligence (AI) systems. In this paper,
we concentrate on adapting explainable AI to face recognition and biometrics,
and we present four principles of explainable AI to face recognition and
biometrics. The principles are illustrated by $\it{four}$ case studies, which
show the challenges and issues in developing algorithms that can produce
explanations.

Today's AI deployments often require significant human involvement and skill
in the operational stages of the model lifecycle, including pre-release
testing, monitoring, problem diagnosis and model improvements. We present a set
of enabling technologies that can be used to increase the level of automation
in AI operations, thus lowering the human effort required. Since a common
source of human involvement is the need to assess the performance of deployed
models, we focus on technologies for performance prediction and KPI analysis
and show how they can be used to improve automation in the key stages of a
typical AI operations pipeline.

In this paper we discuss approaches to evaluating and validating the ethical
claims of a Conversational AI system. We outline considerations around both a
top-down regulatory approach and bottom-up processes. We describe the ethical
basis for each approach and propose a hybrid which we demonstrate by taking the
case of a customer service chatbot as an example. We speculate on the kinds of
top-down and bottom-up processes that would need to exist for a hybrid
framework to successfully function as both an enabler as well as a shepherd
among multiple use-cases and multiple competing AI solutions.

AI solutions seem to appear in any and all application domains. As AI becomes
more pervasive, the importance of quality assurance increases. Unfortunately,
there is no consensus on what artificial intelligence means and interpretations
range from simple statistical analysis to sentient humanoid robots. On top of
that, quality is a notoriously hard concept to pinpoint. What does this mean
for AI quality? In this paper, we share our working definition and a pragmatic
approach to address the corresponding quality assurance with a focus on
testing. Finally, we present our ongoing work on establishing the AIQ
Meta-Testbed.

Recent progress in Game AI has demonstrated that given enough data from human
gameplay, or experience gained via simulations, machines can rival or surpass
the most skilled human players in classic games such as Go, or commercial
computer games such as Starcraft. We review the current state-of-the-art
through the lens of wargaming, and ask firstly what features of wargames
distinguish them from the usual AI testbeds, and secondly which recent AI
advances are best suited to address these wargame-specific features.

We propose a new class of "grand challenge" AI problems that we call creative
captioning---generating clever, interesting, or abstract captions for images,
as well as understanding such captions. Creative captioning draws on core AI
research areas of vision, natural language processing, narrative reasoning, and
social reasoning, and across all these areas, it requires sophisticated uses of
common sense and cultural knowledge. In this paper, we analyze several specific
research problems that fall under creative captioning, using the popular board
game Dixit as both inspiration and proposed testing ground. We expect that
Dixit could serve as an engaging and motivating benchmark for creative
captioning across numerous AI research communities for the coming 1-2 decades.

We examine the problem of explainable AI (xAI) and explore what delivering
xAI means in practice, particularly in contexts that involve formal or informal
and ad-hoc collaboration where agency and accountability in decision-making are
achieved and sustained interactionally. We use an example from an earlier study
of collaborative decision-making in screening mammography and the difficulties
users faced when trying to interpret the behavior of an AI tool to illustrate
the challenges of delivering usable and effective xAI. We conclude by setting
out a study programme for future research to help advance our understanding of
xAI requirements for safe and ethical AI.

To reduce the danger of powerful super-intelligent AIs, we might make the
first such AIs oracles that can only send and receive messages. This paper
proposes a possibly practical means of using machine learning to create two
classes of narrow AI oracles that would provide chess advice: those aligned
with the player's interest, and those that want the player to lose and give
deceptively bad advice. The player would be uncertain which type of oracle it
was interacting with. As the oracles would be vastly more intelligent than the
player in the domain of chess, experience with these oracles might help us
prepare for future artificial general intelligence oracles.

The purpose of this paper is to discuss the possibilities for computing
machinery, or AI agents, to know and to possess knowledge. This is done mainly
from a virtue epistemology perspective and definition of knowledge. However,
this inquiry also shed light on the human condition, what it means for a human
to know, and to possess knowledge. The paper argues that it is possible for an
AI agent to know and examines this from both current state-of-the-art in
artificial intelligence as well as from the perspective of what the future AI
development might bring in terms of superintelligent AI agents.

There is a growing need for data-driven research efforts on how the public
perceives the ethical, moral, and legal issues of autonomous AI systems. The
current debate on the responsibility gap posed by these systems is one such
example. This work proposes a mixed AI ethics model that allows normative and
descriptive research to complement each other, by aiding scholarly discussion
with data gathered from the public. We discuss its implications on bridging the
gap between optimistic and pessimistic views towards AI systems' deployment.

With widespread adoption of AI models for important decision making, ensuring
reliability of such models remains an important challenge. In this paper, we
present an end-to-end generic framework for testing AI Models which performs
automated test generation for different modalities such as text, tabular, and
time-series data and across various properties such as accuracy, fairness, and
robustness. Our tool has been used for testing industrial AI models and was
very effective to uncover issues present in those models. Demo video link:
https://youtu.be/984UCU17YZI

As the use of artificial intelligence (AI) in high-stakes decision-making
increases, the ability to contest such decisions is being recognised in AI
ethics guidelines as an important safeguard for individuals. Yet, there is
little guidance on how AI systems can be designed to support contestation. In
this paper we explain that the design of a contestation process is important
due to its impact on perceptions of fairness and satisfaction. We also consider
design challenges, including a lack of transparency as well as the numerous
design options that decision-making entities will be faced with. We argue for a
human-centred approach to designing for contestability to ensure that the needs
of decision subjects, and the community, are met.

In this paper, we aim at providing a comprehensive outline of the different
threads of work in human-AI collaboration. By highlighting various aspects of
works on the human-AI team such as the flow of complementing, task horizon,
model representation, knowledge level, and teaming goal, we make a taxonomy of
recent works according to these dimensions. We hope that the survey will
provide a more clear connection between the works in the human-AI team and
guidance to new researchers in this area.

The evolution of AI is advancing rapidly, creating both challenges and
opportunities for industry-community collaboration. In this work, we present a
novel methodology aiming to facilitate this collaboration through crowdsourcing
of AI models. Concretely, we have implemented a system and a process that any
organization can easily adopt to host AI competitions. The system allows them
to automatically harvest and evaluate the submitted models against in-house
proprietary data and also to incorporate them as reusable services in a
product.

Artificial intelligence (AI) is supposed to help us make better choices. Some
of these choices are small, like what route to take to work, or what music to
listen to. Others are big, like what treatment to administer for a disease or
how long to sentence someone for a crime. If AI can assist with these big
decisions, we might think it can also help with hard choices, cases where
alternatives are neither better, worse nor equal but on a par. The aim of this
paper, however, is to show that this view is mistaken: the fact of parity shows
that there are hard limits on AI in decision making and choices that AI cannot,
and should not, resolve.

Artificial Intelligence will significantly impact the work environment of
radiologists. I suggest that up to 50% of a radiologists work in 2021 will be
performed by AI-models in 2025. However, it won't increase beyond that 50%
level, as radiologists remain key for human-centered aspects of their job. I
project that few to no radiologists will be laid off in China due to the
existing supply shortage of radiology services in 2021. The application of AI
in radiology could contribute 1.7 billion USD to China's GDP in 2025. It will
further allow radiologists to start productive work up to four years earlier.
AI in radiology will positively impact the health of patients and radiologists
themselves.

We review key considerations, practices, and areas for future work aimed at
the responsible development and fielding of AI technologies. We describe
critical challenges and make recommendations on topics that should be given
priority consideration, practices that should be implemented, and policies that
should be defined or updated to reflect developments with capabilities and uses
of AI technologies. The Key Considerations were developed with a lens for
adoption by U.S. government departments and agencies critical to national
security. However, they are relevant more generally for the design,
construction, and use of AI systems.

Emergency personnel respond to various situations ranging from fire, medical,
hazardous materials, industrial accidents, to natural disasters. Situations
such as natural disasters or terrorist acts require a multifaceted response of
firefighters, paramedics, hazmat teams, and other agencies. Engineering AI
systems that aid emergency personnel proves to be a difficult system
engineering problem. Mission-critical "edge AI" situations require low-latency,
reliable analytics. To further add complexity, a high degree of model accuracy
is required when lives are at stake, creating a need for the deployment of
highly accurate, however computationally intensive models to
resource-constrained devices. To address all these issues, we propose an
agent-based architecture for deployment of AI agents via 5G service-based
architecture.

In recent years, techniques developed in artificial intelligence (AI),
especially those in machine learning (ML), have been successfully applied in
various areas, leading to a widespread belief that AI will collectively play an
important role in future wireless communications. To accomplish the aspiration,
we present nine challenges to be addressed by the interdisciplinary areas of
AI/ML and wireless communications, with particular focus towards the sixth
generation (6G) wireless networks. Specifically, this article classifies the
nine challenges into computation in AI, distributed neural networks and
learning, and ML enabled semantic communications.

The last decade has seen tremendous progress in AI technology and
applications. With such widespread adoption, ensuring the reliability of the AI
models is crucial. In past, we took the first step of creating a testing
framework called AITEST for metamorphic properties such as fairness, robustness
properties for tabular, time-series, and text classification models. In this
paper, we extend the capability of the AITEST tool to include the testing
techniques for Image and Speech-to-text models along with interpretability
testing for tabular models. These novel extensions make AITEST a comprehensive
framework for testing AI models.

This paper presents JEDAI, an AI system designed for outreach and educational
efforts aimed at non-AI experts. JEDAI features a novel synthesis of research
ideas from integrated task and motion planning and explainable AI. JEDAI helps
users create high-level, intuitive plans while ensuring that they will be
executable by the robot. It also provides users customized explanations about
errors and helps improve their understanding of AI planning as well as the
limits and capabilities of the underlying robot system.

This paper reviews and proposes concerns in adopting, fielding, and
maintaining artificial intelligence (AI) systems. While the AI community has
made rapid progress, there are challenges in certifying AI systems. Using
procedures from design and operational test and evaluation, there are
opportunities towards determining performance bounds to manage expectations of
intended use. A notional use case is presented with image data fusion to
support AI object recognition certifiability considering precision versus
distance.

The concept of art has transposed meaning and medium across time, with its
context being a deciding factor for its evolution. However, human beings'
innermost functionality remains the same, and art, to this day, serves as an
expression of the subconscious. Accelerated by the conception of GANs in 2014,
automation has become a central medium in Artificial Intelligence (AI) Art.
However, this raises concern over AI's influence on artistic autonomy within
the process of creativity. This paper introduces the ethical responsibility of
AI towards maintaining the artist's volition in exercising autonomy and
utilizes principles of self-determination theory alongside fundamental limits
of creativity to do so.

Document AI, or Document Intelligence, is a relatively new research topic
that refers to the techniques for automatically reading, understanding, and
analyzing business documents. It is an important research direction for natural
language processing and computer vision. In recent years, the popularity of
deep learning technology has greatly advanced the development of Document AI,
such as document layout analysis, visual information extraction, document
visual question answering, document image classification, etc. This paper
briefly reviews some of the representative models, tasks, and benchmark
datasets. Furthermore, we also introduce early-stage heuristic rule-based
document analysis, statistical machine learning algorithms, and deep learning
approaches especially pre-training methods. Finally, we look into future
directions for Document AI research.

Neural Painters is a class of models that follows a GAN framework to generate
brushstrokes, which are then composed to create paintings. GANs are great
generative models for AI Art but they are known to be notoriously difficult to
train. To overcome GAN's limitations and to speed up the Neural Painter
training, we applied Transfer Learning to the process reducing it from days to
only hours, while achieving the same level of visual aesthetics in the final
paintings generated. We report our approach and results in this work.

The field of computer vision is rapidly evolving, particularly in the context
of new methods of neural architecture design. These models contribute to (1)
the Climate Crisis - increased CO2 emissions and (2) the Privacy Crisis - data
leakage concerns. To address the often overlooked impact the Computer Vision
(CV) community has on these crises, we outline a novel ethical framework,
\textit{P4AI}: Principlism for AI, an augmented principlistic view of ethical
dilemmas within AI. We then suggest using P4AI to make concrete recommendations
to the community to mitigate the climate and privacy crises.

We propose novel methods to develop action controllable agent that behaves
like a human and has the ability to align with human players in Multiplayer
Online Battle Arena (MOBA) games. By modeling the control problem as an action
generation process, we devise a deep latent alignment neural network model for
training agent, and a corresponding sampling algorithm for controlling an
agent's action. Particularly, we propose deterministic and stochastic attention
implementations of the core latent alignment model. Both simulated and online
experiments in the game Honor of Kings demonstrate the efficacy of the proposed
methods.

Ttraditional safety engineering is coming to a turning point moving from
deterministic, non-evolving systems operating in well-defined contexts to
increasingly autonomous and learning-enabled AI systems which are acting in
largely unpredictable operating contexts. We outline some of underlying
challenges of safe AI and suggest a rigorous engineering framework for
minimizing uncertainty, thereby increasing confidence, up to tolerable levels,
in the safe behavior of AI systems.

The Robust Artificial Intelligence System Assurance (RAISA) workshop will
focus on research, development and application of robust artificial
intelligence (AI) and machine learning (ML) systems. Rather than studying
robustness with respect to particular ML algorithms, our approach will be to
explore robustness assurance at the system architecture level, during both
development and deployment, and within the human-machine teaming context. While
the research community is converging on robust solutions for individual AI
models in specific scenarios, the problem of evaluating and assuring the
robustness of an AI system across its entire life cycle is much more complex.
Moreover, the operational context in which AI systems are deployed necessitates
consideration of robustness and its relation to principles of fairness,
privacy, and explainability.

This review seeks to present a comprehensive picture of recent discussions in
the social sciences of the anticipated impact of AI on the world of work.
Issues covered include technological unemployment, algorithmic management,
platform work an the politics of AI work. The review identifies the major
disciplinary and methodological perspectives on AI's impact on work, and the
obstacles they face in making predictions. Two parameters influencing the
development and deployment of AI in the economy are highlighted, the capitalist
imperative and nationalistic pressures.

Current test and evaluation (T&E) methods for assessing machine learning (ML)
system performance often rely on incomplete metrics. Testing is additionally
often siloed from the other phases of the ML system lifecycle. Research
investigating cross-domain approaches to ML T&E is needed to drive the state of
the art forward and to build an Artificial Intelligence (AI) engineering
discipline. This paper advocates for a robust, integrated approach to testing
by outlining six key questions for guiding a holistic T&E strategy.

In this position paper, we propose building a broader and deeper
understanding around Explainability in AI by 'grounding' it in social contexts,
the socio-technical systems operate in. We situate our understanding of
grounded explainability in the 'Global South' in general and India in
particular and express the need for more research within the global south
context when it comes to explainability and AI.

Interest in Artificial Intelligence (AI) continues to grow rapidly, hence it
is crucial to support researchers and organisations in understanding where AI
research is heading. In this study, we conducted a bibliometric analysis on
257K articles in AI, retrieved from OpenAlex. We identified the main conceptual
themes by performing clustering analysis on the co-occurrence network of
topics. Finally, we observed how such themes evolved over time. The results
highlight the growing academic interest in research themes like deep learning,
machine learning, and internet of things.

AI has the potential to improve approaches to talent management enabling
dynamic provisions through implementing advanced automation. This study aims to
identify the new requirements for developing AI-oriented artifacts to address
talent management issues. Focusing on enhancing interactions between
professional assessment and planning attributes, the design artifact is an
intelligent employment automation solution for career guidance that is largely
dependent on a talent intelligent module and an individuals growth needs. A
design science method is adopted for conducting the experimental study with
structured machine learning techniques which is the primary element of a
comprehensive AI solution framework informed through a proposed moderation of
the technology-organization-environment theory.

Inspired by recent and revolutionary developments in AI, particularly in
language understanding and generation, we set about designing AI systems that
are able to address complex scientific tasks that challenge human capabilities
to make new discoveries. Central to our approach is the notion of natural
language as core representation, reasoning, and exchange format between
scientific AI and human scientists. In this paper, we identify and discuss some
of the main research challenges to accomplish such vision.

In fighting games, individual players of the same skill level often exhibit
distinct strategies from one another through their gameplay. Despite this, the
majority of AI agents for fighting games have only a single strategy for each
"level" of difficulty. To make AI opponents more human-like, we'd ideally like
to see multiple different strategies at each level of difficulty, a concept we
refer to as "multidimensional" difficulty. In this paper, we introduce a
diversity-based deep reinforcement learning approach for generating a set of
agents of similar difficulty that utilize diverse strategies. We find this
approach outperforms a baseline trained with specialized, human-authored reward
functions in both diversity and performance.

This article reviews the landscape of ethical challenges of integrating
artificial intelligence (AI) into smart healthcare products, including medical
electronic devices. Differences between traditional ethics in the medical
domain and emerging ethical challenges with AI-driven healthcare are presented,
particularly as they relate to transparency, bias, privacy, safety,
responsibility, justice, and autonomy. Open challenges and recommendations are
outlined to enable the integration of ethical principles into the design,
validation, clinical trials, deployment, monitoring, repair, and retirement of
AI-based smart healthcare products.

The paper discusses the potential of large vision-language models as objects
of interest for empirical cultural studies. Focusing on the comparative
analysis of outputs from two popular text-to-image synthesis models, DALL-E 2
and Stable Diffusion, the paper tries to tackle the pros and cons of striving
towards culturally agnostic vs. culturally specific AI models. The paper
discusses several examples of memorization and bias in generated outputs which
showcase the trade-off between risk mitigation and cultural specificity, as
well as the overall impossibility of developing culturally agnostic models.

Our main contribution is that we are using AI to discern the key drivers of
variation of ESG mentions in the corporate filings. With AI, we are able to
separate "dimensions" along which the corporate management presents their ESG
policies to the world. These dimensions are 1) diversity, 2) hazardous
materials, and 3) greenhouse gasses. We are also able to identify separate
"background" dimensions of unofficial ESG activity in the firms, which provide
more color into the firms and their shareholders' thinking about their ESG
processes. We then measure investors' response to the ESG activity "factors".
The AI techniques presented can assist in building better, more reliable and
useful ESG ratings systems.

Ever since the creation of the first artificial intelligence (AI) machinery
built on machine learning (ML), public society has entertained the idea that
eventually computers could become sentient and develop a consciousness of their
own. As these models now get increasingly better and convincingly more
anthropomorphic, even some engineers have started to believe that AI might
become conscious, which would result in serious social consequences. The
present paper argues against the plausibility of sentient AI primarily based on
the theory of neurogenetic structuralism, which claims that the physiology of
biological neurons and their structural organization into complex brains are
necessary prerequisites for true consciousness to emerge.

Artificial Intelligence (AI) and Machine-Learning (ML) models have been
increasingly used in medical products, such as medical device software. General
considerations on the statistical aspects for the evaluation of AI/ML-enabled
medical diagnostic devices are discussed in this paper. We also provide
relevant academic references and note good practices in addressing various
statistical challenges in the clinical validation of AI/ML-enabled medical
devices in the context of their intended use.

The CoCoMo model proposes a computational solution to the challenge of
incorporating ethical and emotional intelligence considerations into AI
systems, with the aim of creating AI agents that combine knowledge with
compassion. To achieve this goal, CoCoMo prioritizes fairness, beneficence,
non-maleficence, empathy, adaptability, transparency, and critical and
exploratory thinking abilities. The model employs consciousness modeling,
reinforcement learning, and prompt template formulation to support these
desired traits. By incorporating ethical and emotional intelligence
considerations, a generative AI model can potentially lead to improved
fairness, reduced toxicity, and increased reliability.

Although large conversational AI models such as OpenAI's ChatGPT have
demonstrated great potential, we question whether such models can guarantee
factual accuracy. Recently, technology companies such as Microsoft and Google
have announced new services which aim to combine search engines with
conversational AI. However, we have found numerous mistakes in the public
demonstrations that suggest we should not easily trust the factual claims of
the AI models. Rather than criticizing specific models or companies, we hope to
call on researchers and developers to improve AI models' transparency and
factual correctness.

We study the staggered introduction of a generative AI-based conversational
assistant using data from 5,000 customer support agents. Access to the tool
increases productivity, as measured by issues resolved per hour, by 14 percent
on average, with the greatest impact on novice and low-skilled workers, and
minimal impact on experienced and highly skilled workers. We provide suggestive
evidence that the AI model disseminates the potentially tacit knowledge of more
able workers and helps newer workers move down the experience curve. In
addition, we show that AI assistance improves customer sentiment, reduces
requests for managerial intervention, and improves employee retention.

This paper follows calls for critical approaches to computing and
conceptualisations of intersectional, feminist, decolonial HCI and AI design
and asks what a feminist intersectional perspective in HCXAI research and
design might look like. Sketching out initial research directions and
implications for explainable AI design, it suggests that explainability from a
feminist perspective would include the fostering of response-ability - the
capacity to critically evaluate and respond to AI systems - and would centre
marginalised perspectives.

This paper explores the potential for utilizing generative AI models in
group-focused co-creative frameworks to enhance problem solving and ideation in
business innovation and co-creation contexts, and proposes a novel prompting
technique for conversational generative AI agents which employ methods inspired
by traditional 'human-to-human' facilitation and instruction to enable active
contribution to Design Thinking, a co-creative framework. Through experiments
using this prompting technique, we gather evidence that conversational
generative transformers (i.e. ChatGPT) have the capability to contribute
context-specific, useful, and creative input into Design Thinking activities.
We also discuss the potential benefits, limitations, and risks associated with
using generative AI models in co-creative ideation and provide recommendations
for future research.

As artificial intelligence and machine learning continue to advance, we must
understand their strategic importance in national security. This paper focuses
on unique AI applications in the military, emphasizes strategic imperatives for
success, and aims to rekindle excitement about AI's role in national security.
We will examine the United States progress in AI and ML from a military
standpoint, discuss the importance of securing these technologies from
adversaries, and explore the challenges and risks associated with their
integration. Finally, we will highlight the strategic significance of AI to
national security and a set of strategic imperatives for military leaders and
policymakers

With the explosive advancement of AI technologies in recent years, the scene
of the disinformation research is also expected to rapidly change. In this
viewpoint article, in particular, we first present the notion of
"disinformation 2.0" in the age of AI where disinformation would become more
targeted and personalized, its content becomes very difficult to distinguish
from real news, and its creation and dissemination become more accelerated by
AI. Then, we discuss how disinformation 2.0 and cybersecurity fit and a
possible layered countermeasure to address the threat in disinformation 2.0 in
a holistic manner.

Is the output of generative AI entitled to First Amendment protection? We're
inclined to say yes. Even though current AI programs are of course not people
and do not themselves have constitutional rights, their speech may potentially
be protected because of the rights of the programs' creators. But beyond that,
and likely more significantly, AI programs' speech should be protected because
of the rights of their users-both the users' rights to listen and their rights
to speak. In this short Article, we sketch the outlines of this analysis.

Generative AI tools, such as ChatGPT and Midjourney, are transforming
artistic creation as AI-art integration advances. However, Artificial
Intelligence Generated Content (AIGC) tools face user experience challenges,
necessitating a human-centric design approach. This paper offers a brief
overview of research on explainable AI (XAI) and user experience, examining
factors leading to suboptimal experiences with AIGC tools. Our proposed
solution integrates interpretable AI methodologies into the input and
adjustment feedback stages of AIGC products. We underscore XAI's potential to
enhance the user experience for ordinary users and present a conceptual
framework for improving AIGC user experience.

Artificial intelligence (AI) has been advancing at a fast pace and it is now
poised for deployment in a wide range of applications, such as autonomous
systems, medical diagnosis and natural language processing. Early adoption of
AI technology for real-world applications has not been without problems,
particularly for neural networks, which may be unstable and susceptible to
adversarial examples. In the longer term, appropriate safety assurance
techniques need to be developed to reduce potential harm due to avoidable
system failures and ensure trustworthiness. Focusing on certification and
explainability, this paper provides an overview of techniques that have been
developed to ensure safety of AI decisions and discusses future challenges.

In the 2023-2024 academic year, the widespread availability of generative
artificial intelligence, exemplified by ChatGPT's 1.6 billion monthly visits,
is set to impact academic integrity. With 77% of high school students
previously reporting engagement in dishonest behaviour, the rise of AI-driven
writing assistance, dubbed 'AI-giarism' by Chan (arXiv:2306.03358v2), will make
plagiarism more accessible and less detectable. While these concerns are
urgent, they also raise broader questions about the revolutionary nature of
this technology, including autonomy, data privacy, copyright, and equity. This
paper aims to explore generative AI from a social justice perspective,
examining the training of these models, the inherent biases, and the potential
injustices in detecting AI-generated writing.

This study investigates and suggests typologies for examining Artificial
Intelligence (AI) within the domains of journalism and mass communication
research. We aim to elucidate the seven distinct subfields of AI, which
encompass machine learning, natural language processing (NLP), speech
recognition, expert systems, planning, scheduling, optimization, robotics, and
computer vision, through the provision of concrete examples and practical
applications. The primary objective is to devise a structured framework that
can help AI researchers in the field of journalism. By comprehending the
operational principles of each subfield, scholars can enhance their ability to
focus on a specific facet when analyzing a particular research topic.

This first international workshop on explainable AI for the Arts (XAIxArts)
brought together a community of researchers in HCI, Interaction Design, AI,
explainable AI (XAI), and digital arts to explore the role of XAI for the Arts.
  Workshop held at the 15th ACM Conference on Creativity and Cognition (C&C
2023).

The term co-creativity has been used to describe a wide variety of human-AI
assemblages in which human and AI are both involved in a creative endeavor. In
order to assist with disambiguating research efforts, we present an ontology of
co-creative systems, focusing on how responsibilities are divided between human
and AI system and the information exchanged between them. We extend Lubart's
original ontology of creativity support tools with three new categories
emphasizing artificial intelligence: computer-as-subcontractor,
computer-as-critic, and computer-as-teammate, some of which have
sub-categorizations.

In human activity recognition (HAR), the limited availability of annotated
data presents a significant challenge. Drawing inspiration from the latest
advancements in generative AI, including Large Language Models (LLMs) and
motion synthesis models, we believe that generative AI can address this data
scarcity by autonomously generating virtual IMU data from text descriptions.
Beyond this, we spotlight several promising research pathways that could
benefit from generative AI for the community, including the generating
benchmark datasets, the development of foundational models specific to HAR, the
exploration of hierarchical structures within HAR, breaking down complex
activities, and applications in health sensing and activity summarization.

This paper discusses the different roles that explicit knowledge, in
particular ontologies, can play in Explainable AI and in the development of
human-centric explainable systems and intelligible explanations. We consider
three main perspectives in which ontologies can contribute significantly,
namely reference modelling, common-sense reasoning, and knowledge refinement
and complexity management. We overview some of the existing approaches in the
literature, and we position them according to these three proposed
perspectives. The paper concludes by discussing what challenges still need to
be addressed to enable ontology-based approaches to explanation and to evaluate
their human-understandability and effectiveness.

The COVID-19 pandemic has forced many people to limit their social
activities, which has resulted in a rise in mental illnesses, particularly
depression. To diagnose these illnesses with accuracy and speed, and prevent
severe outcomes such as suicide, the use of machine learning has become
increasingly important. Additionally, to provide precise and understandable
diagnoses for better treatment, AI scientists and researchers must develop
interpretable AI-based solutions. This article provides an overview of relevant
articles in the field of machine learning and interpretable AI, which helps to
understand the advantages and disadvantages of using AI in psychiatry disorder
detection applications.

What will likely be the effect of the emergence of ChatGPT and other forms of
artificial intelligence (AI) on the skill premium? To address this question, we
develop a nested constant elasticity of substitution production function that
distinguishes between industrial robots and AI. Industrial robots predominantly
substitute for low-skill workers, whereas AI mainly helps to perform the tasks
of high-skill workers. We show that AI reduces the skill premium as long as it
is more substitutable for high-skill workers than low-skill workers are for
high-skill workers.

The Generative Education (GenEd) Framework explores the transition from Large
Language Models (LLMs) to Large Multimodal Models (LMMs) in education,
envisioning a harmonious relationship between AI and educators to enhance
learning experiences. This paper delves into the potential of LMMs to create
personalized, interactive, and emotionally-aware learning environments. Through
addressing the Two-Sigma problem and the introduction of a conceptual product
named Harmony, the narrative emphasizes educator development, adapting policy
frameworks, and fostering cross-sector collaboration to realize the envisioned
AI-enhanced education landscape. The discussion underscores the urgency for
proactive adaptation amidst AI's evolution, offering a pragmatic roadmap to
navigate the technical, ethical, and policy intricacies of integrating AI in
education.

In this paper, I examine questions surrounding AI neutrality through the
prism of existing literature and scholarship about mediation and media
pluralism. Such traditions, I argue, provide a valuable theoretical framework
for how we should approach the (likely) impending era of AI mediation. In
particular, I suggest examining further the notion of algorithmic pluralism.
Contrasting this notion to the dominant idea of algorithmic transparency, I
seek to describe what algorithmic pluralism may be, and present both its
opportunities and challenges. Implemented thoughtfully and responsibly, I
argue, Algorithmic or AI pluralism has the potential to sustain the diversity,
multiplicity, and inclusiveness that are so vital to democracy.

The Universal Basic Computing Power (UBCP) initiative ensures global, free
access to a set amount of computing power specifically for AI research and
development (R&D). This initiative comprises three key elements. First, UBCP
must be cost free, with its usage limited to AI R&D and minimal additional
conditions. Second, UBCP should continually incorporate the state of the art AI
advancements, including efficiently distilled, compressed, and deployed
training data, foundational models, benchmarks, and governance tools. Lastly,
it's essential for UBCP to be universally accessible, ensuring convenience for
all users. We urge major stakeholders in AI development large platforms, open
source contributors, and policymakers to prioritize the UBCP initiative.

My research investigates the use of cutting-edge hybrid deep learning models
to accurately differentiate between AI-generated text and human writing. I
applied a robust methodology, utilising a carefully selected dataset comprising
AI and human texts from various sources, each tagged with instructions.
Advanced natural language processing techniques facilitated the analysis of
textual features. Combining sophisticated neural networks, the custom model
enabled it to detect nuanced differences between AI and human content.

The rise of new modes of interaction with AI skyrocketed the popularity,
applicability, and amount of use cases. Despite this evolution, conceptual
integration is falling behind. Studies suggest that there is hardly a
systematization in using AI in organizations. Thus, by taking a
service-dominant logic perspective, specifically, the concept of resource
integration patterns, the most potent application of AI for organizational use
- namely information retrieval - is analyzed. In doing so, we propose a
systematization that can be applied to deepen understanding of core technical
concepts, further investigate AI in contexts, and help explore research
directions guided by SDL.

Artificial intelligence (AI) recently had its 'iPhone moment' and adoption
has drastically accelerated. Quantum computing appears poised to follow suit
over the next years. However, while there has been discourse about how to use
AI responsibly, there is still little appreciation and awareness among
executives, managers, and practitioners about the broader ethical questions and
implications raised by the intersection of these emerging technologies. In this
article, it is highlighted why quantum computing and AI ethics must be taken
seriously by businesspersons and how these technologies affect strategic
decisions; moreover, recommendations and action areas are formulated.

In discussions about the development and governance of AI, a false binary is
often drawn between two groups: those most concerned about the existing, social
impacts of AI, and those most concerned about possible future risks of powerful
AI systems taking actions that don't align with human interests. In this piece,
we (i) describe the emergence of this false binary, (ii) explain why the
seemingly clean distinctions drawn between these two groups don't hold up under
scrutiny and (iii) highlight efforts to bridge this divide.

GPU remoting is a promising technique for supporting AI applications.
Networking plays a key role in enabling remoting. However, for efficient
remoting, the network requirements in terms of latency and bandwidth are
unknown. In this paper, we take a GPU-centric approach to derive the minimum
latency and bandwidth requirements for GPU remoting, while ensuring no (or
little) performance degradation for AI applications. Our study including
theoretical model demonstrates that, with careful remoting design, unmodified
AI applications can run on the remoting setup using commodity networking
hardware without any overhead or even with better performance, with low network
demands.

Artificial Intelligence (AI) in recent years has shown an unprecedentedly
impressive development, tending to play a catalytic role in all aspects of
life. The interest of the academic community, but also of governments, is huge
in the dynamics of AI and is reflected by the truly explosive amount of
investment and research that is underway. Enthusiastic opinions and statements
about AI are made every day, but at the same time they also bring to the fore
alarming predictions about its effects. This paper aims to describe the
opportunities emerging from the use of artificial intelligence and ChatGPT to
improve education, but also to identify the challenges and ethical issues that
arise.

In this research study, we propose a modern artificial intelligence (AI)
approach to recognize deepfake voice, also known as generative AI cloned
synthetic voice. Our proposed AI technology, called AntiDeepFake, consists of
all main pipelines from data to evaluation in the whole picture. We provide
experimental results and scores for all our proposed methods. The main source
code for our approach is available in the provided link:
https://github.com/enkhtogtokh/antideepfake repository.

The robustness of AI-content detection models against cultivated attacks
(e.g., paraphrasing or word switching) remains a significant concern. This
study proposes a novel token-ensemble generation strategy to challenge the
robustness of current AI-content detection approaches. We explore the ensemble
attack strategy by completing the prompt with the next token generated from
random candidate LLMs. We find the token-ensemble approach significantly drops
the performance of AI-content detection models (The code and test sets will be
released). Our findings reveal that token-ensemble generation poses a vital
challenge to current detection models and underlines the need for advancing
detection technologies to counter sophisticated adversarial strategies.

This paper introduces the Multi-Step Action Model (MSAM), a closed-source AI
model designed by Empsing to address challenges hindering AI adoption in
enterprises. Through a holistic examination, this paper explores MSAM's
foundational principles, design architecture, and future trajectory. It
evaluates MSAM's performance via rigorous testing methodologies and envisions
its potential impact on advancing AI adoption within organizations.

AI systems may have transformative and long-term effects on individuals and
society. To manage these impacts responsibly and direct the development of AI
systems toward optimal public benefit, considerations of AI ethics and
governance must be a first priority.
  In this workbook, we introduce and describe our PBG Framework, a multi-tiered
governance model that enables project teams to integrate ethical values and
practical principles into their innovation practices and to have clear
mechanisms for demonstrating and documenting this.

The miniaturization of AI accelerators is paving the way for next-generation
wearable applications within wearable technologies. We introduce Mojito, an
AI-native runtime with advanced MLOps designed to facilitate the development
and deployment of these applications on wearable devices. It emphasizes the
necessity of dynamic orchestration of distributed resources equipped with
ultra-low-power AI accelerators to overcome challenges associated with
unpredictable runtime environments. Through its innovative approaches, Mojito
demonstrates how future wearable technologies can evolve to be more autonomous.

Recently, the development of large-scale models has paved the way for various
interdisciplinary research, including architecture. By using generative AI, we
present a novel workflow that utilizes AI models to generate conceptual
floorplans and 3D models from simple sketches, enabling rapid ideation and
controlled generation of architectural renderings based on textual
descriptions. Our work demonstrates the potential of generative AI in the
architectural design process, pointing towards a new direction of
computer-aided architectural design. Our project website is available at:
https://zrealli.github.io/sketch2arc

We diagnose and briefly discuss the dearth of the author: a condition that
arises when AI-based creativity support tools for writing allow users to
produce large amounts of text without making a commensurate number of creative
decisions, resulting in output that is sparse in expressive intent. We argue
that the dearth of the author helps to explain a number of recurring
difficulties and anxieties around AI-based writing support tools, but that it
also suggests an ambitious new goal for AI-based CSTs.

As the manufacturing industry advances with sensor integration and
automation, the opaque nature of deep learning models in machine learning poses
a significant challenge for fault detection and diagnosis. And despite the
related predictive insights Artificial Intelligence (AI) can deliver, advanced
machine learning engines often remain a black box. This paper reviews the
eXplainable AI (XAI) tools and techniques in this context. We explore various
XAI methodologies, focusing on their role in making AI decision-making
transparent, particularly in critical scenarios where humans are involved. We
also discuss current limitations and potential future research that aims to
balance explainability with model performance while improving trustworthiness
in the context of AI applications for critical industrial use cases.

This study evaluates the performance of general-purpose AI, like ChatGPT, in
legal question-answering tasks, highlighting significant risks to legal
professionals and clients. It suggests leveraging foundational models enhanced
by domain-specific knowledge to overcome these issues. The paper advocates for
creating open-source legal AI systems to improve accuracy, transparency, and
narrative diversity, addressing general AI's shortcomings in legal contexts.

This study investigates the optimization of Generative AI (GenAI) systems
through human feedback, focusing on how varying feedback mechanisms influence
the quality of GenAI outputs. We devised a Human-AI training loop where 32
students, divided into two groups, evaluated AI-generated responses based on a
single prompt. One group assessed a single output, while the other compared two
outputs. Preliminary results from this small-scale experiment suggest that
comparative feedback might encourage more nuanced evaluations, highlighting the
potential for improved human-AI collaboration in prompt optimization. Future
research with larger samples is recommended to validate these findings and
further explore effective feedback strategies for GenAI systems.

This paper summarizes the current copyright related risks that Machine
Learning (ML) and Artificial Intelligence (AI) systems (including Large
Language Models --LLMs) incur. These risks affect different stakeholders:
owners of the copyright of the training data, the users of ML/AI systems, the
creators of trained models, and the operators of AI systems. This paper also
provides an overview of ongoing legal cases in the United States related to
these risks.

Thanks to the Big Data revolution and increasing computing capacities,
Artificial Intelligence (AI) has made an impressive revival over the past few
years and is now omnipresent in both research and industry. The creative
sectors have always been early adopters of AI technologies and this continues
to be the case. As a matter of fact, recent technological developments keep
pushing the boundaries of intelligent systems in creative applications: the
critically acclaimed movie "Sunspring", released in 2016, was entirely written
by AI technology, and the first-ever Music Album, called "Hello World",
produced using AI has been released this year. Simultaneously, the exploratory
nature of the creative process is raising important technical challenges for AI
such as the ability for AI-powered techniques to be accurate under limited data
resources, as opposed to the conventional "Big Data" approach, or the ability
to process, analyse and match data from multiple modalities (text, sound,
images, etc.) at the same time. The purpose of this white paper is to
understand future technological advances in AI and their growing impact on
creative industries. This paper addresses the following questions: Where does
AI operate in creative Industries? What is its operative role? How will AI
transform creative industries in the next ten years? This white paper aims to
provide a realistic perspective of the scope of AI actions in creative
industries, proposes a vision of how this technology could contribute to
research and development works in such context, and identifies research and
development challenges.

Artificial intelligence (AI) products can be trained to recognize
tuberculosis (TB)-related abnormalities on chest radiographs. Various AI
products are available commercially, yet there is lack of evidence on how their
performance compared with each other and with radiologists. We evaluated five
AI software products for screening and triaging TB using a large dataset that
had not been used to train any commercial AI products. Individuals (>=15 years
old) presenting to three TB screening centers in Dhaka, Bangladesh, were
recruited consecutively. All CXR were read independently by a group of three
Bangladeshi registered radiologists and five commercial AI products: CAD4TB
(v7), InferReadDR (v2), Lunit INSIGHT CXR (v4.9.0), JF CXR-1 (v2), and qXR
(v3). All five AI products significantly outperformed the Bangladeshi
radiologists. The areas under the receiver operating characteristic curve are
qXR: 90.81% (95% CI:90.33-91.29%), CAD4TB: 90.34% (95% CI:89.81-90.87), Lunit
INSIGHT CXR: 88.61% (95% CI:88.03%-89.20%), InferReadDR: 84.90% (95% CI:
84.27-85.54%) and JF CXR-1: 84.89% (95% CI:84.26-85.53%). Only qXR met the TPP
with 74.3% specificity at 90% sensitivity. Five AI algorithms can reduce the
number of Xpert tests required by 50%, while maintaining a sensitivity above
90%. All AI algorithms performed worse among the older age and people with
prior TB history. AI products can be highly accurate and useful screening and
triage tools for TB detection in high burden regions and outperform human
readers.

Purpose: To develop an Artificial Intelligence (AI) agent for fully-automated
rapid head and neck (H&N) IMRT plan generation without time-consuming inverse
planning.$$$$
  Methods: This AI agent was trained using a conditional Generative Adversarial
Network architecture. The generator, PyraNet, is a novel Deep Learning network
that implements 28 classic ResNet blocks in pyramid-like concatenations. The
discriminator is a customized 4-layer DenseNet. The AI agent first generates
customized 2D projections at 9 template beam angles from 3D CT volume and
structures of a patient. These projections are then stacked as 4D inputs of
PyraNet, from which 9 radiation fluence maps are generated simultaneously.
Finally, the predicted fluence maps are imported into a commercial treatment
planning system (TPS) for plan integrity checks. The AI agent was built and
tested upon 231 oropharyngeal plans from a TPS plan library. Only the primary
plans in the sequential boost regime were studied. A customized Harr wavelet
loss was adopted for fluence map comparison. Isodose distributions in test AI
plans and TPS plans were qualitatively evaluated. Key dosimetric metrics were
statistically compared.$$$$
  Results: All test AI plans were successfully generated. Isodose gradients
outside of PTV in AI plans were comparable with TPS plans. After PTV coverage
normalization, $D_{mean}$ of parotids and oral cavity in AI plans and TPS plans
were comparable without statistical significance. AI plans achieved comparable
$D_{max}$ at 0.01cc of brainstem and cord+5mm without clinically relevant
differences, but body $D_{max}$ was higher than the TPS plan results. The AI
agent needs ~3s per case to predict fluence maps.$$$$
  Conclusions: The developed AI agent can generate H&N IMRT plans with
satisfying dosimetry quality. With rapid and fully automated implementation, it
holds great potential for clinical applications.

There has been an emerging paradigm shift from the era of "internet AI" to
"embodied AI", where AI algorithms and agents no longer learn from datasets of
images, videos or text curated primarily from the internet. Instead, they learn
through interactions with their environments from an egocentric perception
similar to humans. Consequently, there has been substantial growth in the
demand for embodied AI simulators to support various embodied AI research
tasks. This growing interest in embodied AI is beneficial to the greater
pursuit of Artificial General Intelligence (AGI), but there has not been a
contemporary and comprehensive survey of this field. This paper aims to provide
an encyclopedic survey for the field of embodied AI, from its simulators to its
research. By evaluating nine current embodied AI simulators with our proposed
seven features, this paper aims to understand the simulators in their provision
for use in embodied AI research and their limitations. Lastly, this paper
surveys the three main research tasks in embodied AI -- visual exploration,
visual navigation and embodied question answering (QA), covering the
state-of-the-art approaches, evaluation metrics and datasets. Finally, with the
new insights revealed through surveying the field, the paper will provide
suggestions for simulator-for-task selections and recommendations for the
future directions of the field.

We address the use of asymptotic incompatibility (AI) to assess the
quantumness of a multiparameter quantum statistical model. AI is a recently
introduced measure which quantifies the difference between the Holevo and the
SLD scalar bounds, and can be evaluated using only the symmetric logarithmic
derivative (SLD) operators of the model. At first, we evaluate analytically the
AI of the most general quantum statistical models involving two-level (qubit)
and single-mode Gaussian continuous-variable quantum systems, and prove that AI
is a simple monotonous function of the state purity. Then, we numerically
investigate the same problem for qudits ($d$-dimensional quantum systems, with
$2 < d \leq 4$), showing that, while in general AI is not in general a function
of purity, we have enough numerical evidence to conclude that the maximum
amount of AI is attainable only for quantum statistical models characterized by
a purity larger than $\mu_{\sf min} = 1/(d-1)$. In addition, by parametrizing
qudit states as thermal (Gibbs) states, numerical results suggest that, once
the spectrum of the Hamiltonian is fixed, the AI measure is in one-to-one
correspondence with the fictitious temperature parameter $\beta$ characterizing
the family of density operators. Finally, by studying in detail the definition
and properties of the AI measure we find that: i) given a quantum statistical
model, one can readily identify the maximum number of asymptotically
compatibile parameters; ii) the AI of a quantum statistical model bounds from
above the AI of any sub-model that can be defined by fixing one or more of the
original unknown parameters (or functions thereof), leading to possibly useful
bounds on the AI of models involving noisy quantum dynamics.

Artificial intelligence (AI) is paving the way towards the fourth industrial
revolution with the fire domain (Fire 4.0). As a matter of fact, the next few
years will be elemental to how this technology will shape our academia,
practice, and entrepreneurship. Despite the growing interest between fire
research groups, AI remains absent of our curriculum, and we continue to lack a
methodical framework to adopt, apply and create AI solutions suitable for our
problems. The above is also true for parallel engineering domains (i.e.,
civil/mechanical engineering), and in order to negate the notion of history
repeats itself (e.g., look at the continued debate with regard to modernizing
standardized fire testing, etc.), it is the motivation behind this letter to
the Editor to demystify some of the big ideas behind AI to jump-start prolific
and strategic discussions on the front of AI & Fire. In addition, this letter
intends to explain some of the most fundamental concepts and clear common
misconceptions specific to the adoption of AI in fire engineering. This short
letter is a companion to the Smart Systems in Fire Engineering special issue
sponsored by Fire Technology. An in-depth review of AI algorithms [1] and
success stories to the proper implementations of such algorithms can be found
in the aforenoted special issue and collection of papers. This letter comprises
two sections. The first section outlines big ideas pertaining to AI, and
answers some of the burning questions with regard to the merit of adopting AI
in our domain. The second section presents a set of rules or technical
recommendations an AI user may deem helpful to practice whenever AI is used as
an investigation methodology. The presented set of rules are complementary to
the big ideas.

How can humans remain in control of artificial intelligence (AI)-based
systems designed to perform tasks autonomously? Such systems are increasingly
ubiquitous, creating benefits - but also undesirable situations where moral
responsibility for their actions cannot be properly attributed to any
particular person or group. The concept of meaningful human control has been
proposed to address responsibility gaps and mitigate them by establishing
conditions that enable a proper attribution of responsibility for humans;
however, clear requirements for researchers, designers, and engineers are yet
inexistent, making the development of AI-based systems that remain under
meaningful human control challenging. In this paper, we address the gap between
philosophical theory and engineering practice by identifying, through an
iterative process of abductive thinking, four actionable properties for
AI-based systems under meaningful human control, which we discuss making use of
two applications scenarios: automated vehicles and AI-based hiring. First, a
system in which humans and AI algorithms interact should have an explicitly
defined domain of morally loaded situations within which the system ought to
operate. Second, humans and AI agents within the system should have appropriate
and mutually compatible representations. Third, responsibility attributed to a
human should be commensurate with that human's ability and authority to control
the system. Fourth, there should be explicit links between the actions of the
AI agents and actions of humans who are aware of their moral responsibility. We
argue that these four properties will support practically-minded professionals
to take concrete steps toward designing and engineering for AI systems that
facilitate meaningful human control.

Advances in artificial intelligence (AI) are enabling systems that augment
and collaborate with humans to perform simple, mechanistic tasks like
scheduling meetings and grammar-checking text. However, such Human-AI
collaboration poses challenges for more complex, creative tasks, such as
carrying out empathic conversations, due to difficulties of AI systems in
understanding complex human emotions and the open-ended nature of these tasks.
Here, we focus on peer-to-peer mental health support, a setting in which
empathy is critical for success, and examine how AI can collaborate with humans
to facilitate peer empathy during textual, online supportive conversations. We
develop Hailey, an AI-in-the-loop agent that provides just-in-time feedback to
help participants who provide support (peer supporters) respond more
empathically to those seeking help (support seekers). We evaluate Hailey in a
non-clinical randomized controlled trial with real-world peer supporters on
TalkLife (N=300), a large online peer-to-peer support platform. We show that
our Human-AI collaboration approach leads to a 19.60% increase in
conversational empathy between peers overall. Furthermore, we find a larger
38.88% increase in empathy within the subsample of peer supporters who
self-identify as experiencing difficulty providing support. We systematically
analyze the Human-AI collaboration patterns and find that peer supporters are
able to use the AI feedback both directly and indirectly without becoming
overly reliant on AI while reporting improved self-efficacy post-feedback. Our
findings demonstrate the potential of feedback-driven, AI-in-the-loop writing
systems to empower humans in open-ended, social, creative tasks such as
empathic conversations.

Incorporating interdisciplinary perspectives is seen as an essential step
towards enhancing artificial intelligence (AI) ethics. In this regard, the
field of arts is perceived to play a key role in elucidating diverse historical
and cultural narratives, serving as a bridge across research communities. Most
of the works that examine the interplay between the field of arts and AI ethics
concern digital artworks, largely exploring the potential of computational
tools in being able to surface biases in AI systems. In this paper, we
investigate a complementary direction--that of uncovering the unique
socio-cultural perspectives embedded in human-made art, which in turn, can be
valuable in expanding the horizon of AI ethics. Through semi-structured
interviews across sixteen artists, art scholars, and researchers of diverse
Indian art forms like music, sculpture, painting, floor drawings, dance, etc.,
we explore how {\it non-Western} ethical abstractions, methods of learning, and
participatory practices observed in Indian arts, one of the most ancient yet
perpetual and influential art traditions, can shed light on aspects related to
ethical AI systems. Through a case study concerning the Indian dance system
(i.e. the {\it `Natyashastra'}), we analyze potential pathways towards
enhancing ethics in AI systems. Insights from our study outline the need for
(1) incorporating empathy in ethical AI algorithms, (2) integrating multimodal
data formats for ethical AI system design and development, (3) viewing AI
ethics as a dynamic, diverse, cumulative, and shared process rather than as a
static, self-contained framework to facilitate adaptability without
annihilation of values (4) consistent life-long learning to enhance AI
accountability

Healthcare systems are increasingly incorporating Artificial Intelligence
into their systems, but it is not a solution for all difficulties. AI's
extraordinary potential is being held back by challenges such as a lack of
medical datasets for training AI models, adversarial attacks, and a lack of
trust due to its black box working style. We explored how blockchain technology
can improve the reliability and trustworthiness of AI-based healthcare. This
paper has conducted a Systematic Literature Review to explore the
state-of-the-art research studies conducted in healthcare applications
developed with different AI techniques and Blockchain Technology. This
systematic literature review proceeds with three different paths as natural
language processing-based healthcare systems, computer vision-based healthcare
systems and acoustic AI-based healthcare systems. We found that 1) Defence
techniques for adversarial attacks on AI are available for specific kind of
attacks and even adversarial training is AI based technique which in further
prone to different attacks. 2) Blockchain can address security and privacy
issues in healthcare fraternity. 3) Medical data verification and user
provenance can be enabled with Blockchain. 4) Blockchain can protect
distributed learning on heterogeneous medical data. 5) The issues like single
point of failure, non-transparency in healthcare systems can be resolved with
Blockchain. Nevertheless, it has been identified that research is at the
initial stage. As a result, we have synthesized a conceptual framework using
Blockchain Technology for AI-based healthcare applications that considers the
needs of each NLP, Computer Vision, and Acoustic AI application. A global
solution for all sort of adversarial attacks on AI based healthcare. However,
this technique has significant limits and challenges that need to be addressed
in future studies.

The increasing deployment of artificial intelligence (AI) tools to inform
decision making across diverse areas including healthcare, employment, social
benefits, and government policy, presents a serious risk for disabled people,
who have been shown to face bias in AI implementations. While there has been
significant work on analysing and mitigating algorithmic bias, the broader
mechanisms of how bias emerges in AI applications are not well understood,
hampering efforts to address bias where it begins. In this article, we
illustrate how bias in AI-assisted decision making can arise from a range of
specific design decisions, each of which may seem self-contained and
non-biasing when considered separately. These design decisions include basic
problem formulation, the data chosen for analysis, the use the AI technology is
put to, and operational design elements in addition to the core algorithmic
design. We draw on three historical models of disability common to different
decision-making settings to demonstrate how differences in the definition of
disability can lead to highly distinct decisions on each of these aspects of
design, leading in turn to AI technologies with a variety of biases and
downstream effects. We further show that the potential harms arising from
inappropriate definitions of disability in fundamental design stages are
further amplified by a lack of transparency and disabled participation
throughout the AI design process. Our analysis provides a framework for
critically examining AI technologies in decision-making contexts and guiding
the development of a design praxis for disability-related AI analytics. We put
forth this article to provide key questions to facilitate disability-led design
and participatory development to produce more fair and equitable AI
technologies in disability-related contexts.

Human-AI complementarity is important when neither the algorithm nor the
human yields dominant performance across all instances in a given context.
Recent work that explored human-AI collaboration has considered decisions that
correspond to classification tasks. However, in many important contexts where
humans can benefit from AI complementarity, humans undertake course of action.
In this paper, we propose a framework for a novel human-AI collaboration for
selecting advantageous course of action, which we refer to as Learning
Complementary Policy for Human-AI teams (\textsc{lcp-hai}). Our solution aims
to exploit the human-AI complementarity to maximize decision rewards by
learning both an algorithmic policy that aims to complement humans by a routing
model that defers decisions to either a human or the AI to leverage the
resulting complementarity. We then extend our approach to leverage
opportunities and mitigate risks that arise in important contexts in practice:
1) when a team is composed of multiple humans with differential and potentially
complementary abilities, 2) when the observational data includes consistent
deterministic actions, and 3) when the covariate distribution of future
decisions differ from that in the historical data. We demonstrate the
effectiveness of our proposed methods using data on real human responses and
semi-synthetic, and find that our methods offer reliable and advantageous
performance across setting, and that it is superior to when either the
algorithm or the AI make decisions on their own. We also find that the
extensions we propose effectively improve the robustness of the human-AI
collaboration performance in the presence of different challenging settings.

This paper examines the current landscape of AI regulations across various
jurisdictions, highlighting divergent approaches being taken, and proposes an
alternative contextual, coherent, and commensurable (3C) framework to bridge
the global divide. While the U.N. is developing an international AI governance
framework and the G7 has endorsed a risk-based approach, there is no consensus
on their details. The EU, Canada, and Brazil (and potentially South Korea)
follow a horizontal or lateral approach that postulates the homogeneity of AI,
seeks to identify common causes of harm, and demands uniform human
interventions. In contrast, the U.S., the U.K., Israel, and Switzerland (and
potentially China) have pursued a context-specific or modular approach,
tailoring regulations to the specific use cases of AI systems. Horizonal
approaches like the EU AI Act do not guarantee sufficient levels of
proportionality and foreseeability; rather, this approach imposes a
one-size-fits-all bundle of regulations on any high-risk AI, when feasible, to
differentiate between various AI models and legislate them individually. The
context-specific approach holds greater promise, but requires further
development regarding details, coherent regulatory objectives, and
commensurable standards. To strike a balance, this paper proposes a hybrid 3C
framework. To ensure contextuality, the framework bifurcates the AI life cycle
into two phases: learning and utilization for specific tasks; and categorizes
these tasks based on their application and interaction with humans as follows:
autonomous, discriminative (allocative, punitive, and cognitive), and
generative AI. To ensure coherency, each category is assigned regulatory
objectives. To ensure commensurability, the framework promotes the adoption of
international industry standards that convert principles into quantifiable
metrics to be readily integrated into AI systems.

The vision of AI collaborators is a staple of mythology and science fiction,
where artificial agents with special talents assist human partners and teams.
In this dream, sophisticated AIs understand nuances of collaboration and human
communication. The AI as collaborator dream is different from computer tools
that augment human intelligence (IA) or intermediate human collaboration. Those
tools have their roots in the 1960s and helped to drive an information
technology revolution. They can be useful but they are not intelligent and do
not collaborate as effectively as skilled people. With the increase of hybrid
and remote work since the COVID pandemic, the benefits and requirements for
better coordination, collaboration, and communication are becoming hot topics
in the workplace. Employers and workers face choices and trade-offs as they
negotiate the options for working from home versus working at the office. Many
factors such as the high costs of homes near employers are impeding a mass
return to the office. Government advisory groups and leaders in AI have
advocated for years that AIs should be transparent and effective collaborators.
Nonetheless, robust AIs that collaborate like talented people remain out of
reach. Are AI teammates part of a solution? How artificially intelligent (AI)
could and should they be? This position paper reviews the arc of technology and
public calls for human-machine teaming. It draws on earlier research in
psychology and the social sciences about what human-like collaboration
requires. This paper sets a context for a second science-driven paper that
advocates a radical shift in technology and methodology for creating resilient,
intelligent, and human-compatible AIs (Stefik & Price, 2023). The aspirational
goal is that such AIs would learn, share what they learn, and collaborate to
achieve high capabilities.

English as foreign language_EFL_students' use of text generated from
artificial intelligence_AI_natural language generation_NLG_tools may improve
their writing quality. However, it remains unclear to what extent AI-generated
text in these students' writing might lead to higher-quality writing. We
explored 23 Hong Kong secondary school students' attempts to write stories
comprising their own words and AI-generated text. Human experts scored the
stories for dimensions of content, language and organization. We analyzed the
basic organization and structure and syntactic complexity of the stories'
AI-generated text and performed multiple linear regression and cluster
analyses. The results show the number of human words and the number of
AI-generated words contribute significantly to scores. Besides, students can be
grouped into competent and less competent writers who use more AI-generated
text or less AI-generated text compared to their peers. Comparisons of clusters
reveal some benefit of AI-generated text in improving the quality of both
high-scoring students' and low-scoring students' writing. The findings can
inform pedagogical strategies to use AI-generated text for EFL students'
writing and to address digital divides. This study contributes designs of NLG
tools and writing activities to implement AI-generated text in schools.

When designing evidence-based policies and programs, decision-makers must
distill key information from a vast and rapidly growing literature base.
Identifying relevant literature from raw search results is time and resource
intensive, and is often done by manual screening. In this study, we develop an
AI agent based on a bidirectional encoder representations from transformers
(BERT) model and incorporate it into a human team designing an evidence
synthesis product for global development. We explore the effectiveness of the
human-AI hybrid team in accelerating the evidence synthesis process. To further
improve team efficiency, we enhance the human-AI hybrid team through active
learning (AL). Specifically, we explore different sampling strategies,
including random sampling, least confidence (LC) sampling, and highest priority
(HP) sampling, to study their influence on the collaborative screening process.
Results show that incorporating the BERT-based AI agent into the human team can
reduce the human screening effort by 68.5% compared to the case of no AI
assistance and by 16.8% compared to the case of using a support vector machine
(SVM)-based AI agent for identifying 80% of all relevant documents. When we
apply the HP sampling strategy for AL, the human screening effort can be
reduced even more: by 78.3% for identifying 80% of all relevant documents
compared to no AI assistance. We apply the AL-enhanced human-AI hybrid teaming
workflow in the design process of three evidence gap maps (EGMs) for USAID and
find it to be highly effective. These findings demonstrate how AI can
accelerate the development of evidence synthesis products and promote timely
evidence-based decision making in global development in a human-AI hybrid
teaming context.

To learn how to behave, the current revolutionary generation of AIs must be
trained on vast quantities of published images, written works, and sounds, many
of which fall within the core subject matter of copyright law. To some, the use
of copyrighted works as training sets for AI is merely a transitory and
non-consumptive use that does not materially interfere with owners' content or
copyrights protecting it. Companies that use such content to train their AI
engine often believe such usage should be considered "fair use" under United
States law (sometimes known as "fair dealing" in other countries). By contrast,
many copyright owners, as well as their supporters, consider the incorporation
of copyrighted works into training sets for AI to constitute misappropriation
of owners' intellectual property, and, thus, decidedly not fair use under the
law. This debate is vital to the future trajectory of AI and its applications.
  In this article, we analyze the arguments in favor of, and against, viewing
the use of copyrighted works in training sets for AI as fair use. We call this
form of fair use "fair training". We identify both strong and spurious
arguments on both sides of this debate. In addition, we attempt to take a
broader perspective, weighing the societal costs (e.g., replacement of certain
forms of human employment) and benefits (e.g., the possibility of novel
AI-based approaches to global issues such as environmental disruption) of
allowing AI to make easy use of copyrighted works as training sets to
facilitate the development, improvement, adoption, and diffusion of AI.
Finally, we suggest that the debate over AI and copyrighted works may be a
tempest in a teapot when placed in the wider context of massive societal
challenges such as poverty, equality, climate change, and loss of biodiversity,
to which AI may be part of the solution.

While demands for change and accountability for harmful AI consequences
mount, foreseeing the downstream effects of deploying AI systems remains a
challenging task. We developed AHA! (Anticipating Harms of AI), a generative
framework to assist AI practitioners and decision-makers in anticipating
potential harms and unintended consequences of AI systems prior to development
or deployment. Given an AI deployment scenario, AHA! generates descriptions of
possible harms for different stakeholders. To do so, AHA! systematically
considers the interplay between common problematic AI behaviors as well as
their potential impacts on different stakeholders, and narrates these
conditions through vignettes. These vignettes are then filled in with
descriptions of possible harms by prompting crowd workers and large language
models. By examining 4113 harms surfaced by AHA! for five different AI
deployment scenarios, we found that AHA! generates meaningful examples of
harms, with different problematic AI behaviors resulting in different types of
harms. Prompting both crowds and a large language model with the vignettes
resulted in more diverse examples of harms than those generated by either the
crowd or the model alone. To gauge AHA!'s potential practical utility, we also
conducted semi-structured interviews with responsible AI professionals (N=9).
Participants found AHA!'s systematic approach to surfacing harms important for
ethical reflection and discovered meaningful stakeholders and harms they
believed they would not have thought of otherwise. Participants, however,
differed in their opinions about whether AHA! should be used upfront or as a
secondary-check and noted that AHA! may shift harm anticipation from an
ideation problem to a potentially demanding review problem. Drawing on our
results, we discuss design implications of building tools to help practitioners
envision possible harms.

Recent advances in machine learning and AI, including Generative AI and LLMs,
are disrupting technological innovation, product development, and society as a
whole. AI's contribution to technology can come from multiple approaches that
require access to large training data sets and clear performance evaluation
criteria, ranging from pattern recognition and classification to generative
models. Yet, AI has contributed less to fundamental science in part because
large data sets of high-quality data for scientific practice and model
discovery are more difficult to access. Generative AI, in general, and Large
Language Models in particular, may represent an opportunity to augment and
accelerate the scientific discovery of fundamental deep science with
quantitative models. Here we explore and investigate aspects of an AI-driven,
automated, closed-loop approach to scientific discovery, including self-driven
hypothesis generation and open-ended autonomous exploration of the hypothesis
space. Integrating AI-driven automation into the practice of science would
mitigate current problems, including the replication of findings, systematic
production of data, and ultimately democratisation of the scientific process.
Realising these possibilities requires a vision for augmented AI coupled with a
diversity of AI approaches able to deal with fundamental aspects of causality
analysis and model discovery while enabling unbiased search across the space of
putative explanations. These advances hold the promise to unleash AI's
potential for searching and discovering the fundamental structure of our world
beyond what human scientists have been able to achieve. Such a vision would
push the boundaries of new fundamental science rather than automatize current
workflows and instead open doors for technological innovation to tackle some of
the greatest challenges facing humanity today.

Global food systems confront the urgent challenge of supplying sustainable,
nutritious diets in the face of escalating demands. The advent of Artificial
Intelligence (AI) is bringing in a personal choice revolution, wherein
AI-driven individual decisions transform food systems from dinner tables, to
the farms, and back to our plates. In this context, AI algorithms refine
personal dietary choices, subsequently shaping agricultural outputs, and
promoting an optimized feedback loop from consumption to cultivation.
Initially, we delve into AI tools and techniques spanning the food supply
chain, and subsequently assess how AI subfields$\unicode{x2013}$encompassing
machine learning, computer vision, and speech recognition$\unicode{x2013}$are
harnessed within the AI-enabled Food System (AIFS) framework, which
increasingly leverages Internet of Things, multimodal sensors and real-time
data exchange. We spotlight the AIFS framework, emphasizing its fusion of AI
with technologies such as digitalization, big data analytics, biotechnology,
and IoT extensively used in modern food systems in every component. This
paradigm shifts the conventional "farm to fork" narrative to a cyclical
"consumer-driven farm to fork" model for better achieving sustainable,
nutritious diets. This paper explores AI's promise and the intrinsic challenges
it poses within the food domain. By championing stringent AI governance,
uniform data architectures, and cross-disciplinary partnerships, we argue that
AI, when synergized with consumer-centric strategies, holds the potential to
steer food systems toward a sustainable trajectory. We furnish a comprehensive
survey for the state-of-the-art in diverse facets of food systems, subsequently
pinpointing gaps and advocating for the judicious and efficacious deployment of
emergent AI methodologies.

Advancements in artificial intelligence (AI) over the last decade demonstrate
that machines can exhibit communicative behavior and influence how humans
think, feel, and behave. In fact, the recent development of ChatGPT has shown
that large language models (LLMs) can be leveraged to generate high-quality
communication content at scale and across domains, suggesting that they will be
increasingly used in practice. However, many questions remain about how knowing
the source of the messages influences recipients' evaluation of and preference
for AI-generated messages compared to human-generated messages. This paper
investigated this topic in the context of vaping prevention messaging. In Study
1, which was pre-registered, we examined the influence of source disclosure on
people's evaluation of AI-generated health prevention messages compared to
human-generated messages. We found that source disclosure (i.e., labeling the
source of a message as AI vs. human) significantly impacted the evaluation of
the messages but did not significantly alter message rankings. In a follow-up
study (Study 2), we examined how the influence of source disclosure may vary by
the participants' negative attitudes towards AI. We found a significant
moderating effect of negative attitudes towards AI on message evaluation, but
not for message selection. However, for those with moderate levels of negative
attitudes towards AI, source disclosure decreased the preference for
AI-generated messages. Overall, the results of this series of studies showed a
slight bias against AI-generated messages once the source was disclosed, adding
to the emerging area of study that lies at the intersection of AI and
communication.

Generative AI has the potential to transform how public services are
delivered by enhancing productivity and reducing time spent on bureaucracy.
Furthermore, unlike other types of artificial intelligence, it is a technology
that has quickly become widely available for bottom-up adoption: essentially
anyone can decide to make use of it in their day to day work. But to what
extent is generative AI already in use in the public sector? Our survey of 938
public service professionals within the UK (covering education, health, social
work and emergency services) seeks to answer this question. We find that use of
generative AI systems is already widespread: 45% of respondents were aware of
generative AI usage within their area of work, while 22% actively use a
generative AI system. Public sector professionals were positive about both
current use of the technology and its potential to enhance their efficiency and
reduce bureaucratic workload in the future. For example, those working in the
NHS thought that time spent on bureaucracy could drop from 50% to 30% if
generative AI was properly exploited, an equivalent of one day per week (an
enormous potential impact). Our survey also found a high amount of trust (61%)
around generative AI outputs, and a low fear of replacement (16%). While
respondents were optimistic overall, areas of concern included feeling like the
UK is missing out on opportunities to use AI to improve public services (76%),
and only a minority of respondents (32%) felt like there was clear guidance on
generative AI usage in their workplaces. In other words, it is clear that
generative AI is already transforming the public sector, but uptake is
happening in a disorganised fashion without clear guidelines. The UK's public
sector urgently needs to develop more systematic methods for taking advantage
of the technology.

AI is increasingly being used in the public sector, including public
security. In this context, the use of AI-powered remote biometric
identification (RBI) systems is a much-discussed technology. RBI systems are
used to identify criminal activity in public spaces, but are criticised for
inheriting biases and violating fundamental human rights. It is therefore
important to ensure that such systems are developed in the public interest,
which means that any technology that is deployed for public use needs to be
scrutinised. While there is a consensus among business leaders, policymakers
and scientists that AI must be developed in an ethical and trustworthy manner,
scholars have argued that ethical guidelines do not guarantee ethical AI, but
rather prevent stronger regulation of AI. As a possible counterweight, public
opinion can have a decisive influence on policymakers to establish boundaries
and conditions under which AI systems should be used -- if at all. However, we
know little about the conditions that lead to regulatory demand for AI systems.
In this study, we focus on the role of trust in AI as well as trust in law
enforcement as potential factors that may lead to demands for regulation of AI
technology. In addition, we explore the mediating effects of discrimination
perceptions regarding RBI. We test the effects on four different use cases of
RBI varying the temporal aspect (real-time vs. post hoc analysis) and purpose
of use (persecution of criminals vs. safeguarding public events) in a survey
among German citizens. We found that German citizens do not differentiate
between the different modes of application in terms of their demand for RBI
regulation. Furthermore, we show that perceptions of discrimination lead to a
demand for stronger regulation, while trust in AI and trust in law enforcement
lead to opposite effects in terms of demand for a ban on RBI systems.

Recent generative AI systems have demonstrated more advanced persuasive
capabilities and are increasingly permeating areas of life where they can
influence decision-making. Generative AI presents a new risk profile of
persuasion due the opportunity for reciprocal exchange and prolonged
interactions. This has led to growing concerns about harms from AI persuasion
and how they can be mitigated, highlighting the need for a systematic study of
AI persuasion. The current definitions of AI persuasion are unclear and related
harms are insufficiently studied. Existing harm mitigation approaches
prioritise harms from the outcome of persuasion over harms from the process of
persuasion. In this paper, we lay the groundwork for the systematic study of AI
persuasion. We first put forward definitions of persuasive generative AI. We
distinguish between rationally persuasive generative AI, which relies on
providing relevant facts, sound reasoning, or other forms of trustworthy
evidence, and manipulative generative AI, which relies on taking advantage of
cognitive biases and heuristics or misrepresenting information. We also put
forward a map of harms from AI persuasion, including definitions and examples
of economic, physical, environmental, psychological, sociocultural, political,
privacy, and autonomy harm. We then introduce a map of mechanisms that
contribute to harmful persuasion. Lastly, we provide an overview of approaches
that can be used to mitigate against process harms of persuasion, including
prompt engineering for manipulation classification and red teaming. Future work
will operationalise these mitigations and study the interaction between
different types of mechanisms of persuasion.

This report introduces researchers in AI to some of the concepts in quantum
heurisitics and quantum AI.

In this article, I discuss how the AI community views concerns about the
emergence of superintelligent AI and related philosophical issues.

We consider the gauge transformations of a metric $G$-bundle over a compact
Riemannian surface with boundary. By employing the heat flow method, the local
existence and the long time existence of generalized solution are proved.

We prove the energy identity and the no neck property for a sequence of
smooth extrinsic polyharmonic maps with bounded total energy.

Smart optical networks are the next evolution of programmable networking and
programmable automation of optical networks, with human-in-the-loop network
control and management. The paper discusses this evolution and the role of
Artificial Intelligence (AI).

Integrals occurring in Thomas-Fermi theory which contains the logarithm of
the Airy function Ai'(x) have been obtained in terms of analytical expressions.

The complexity and diversity of big data and AI workloads make understanding
them difficult and challenging. This paper proposes a new approach to modelling
and characterizing big data and AI workloads. We consider each big data and AI
workload as a pipeline of one or more classes of units of computation performed
on different initial or intermediate data inputs. Each class of unit of
computation captures the common requirements while being reasonably divorced
from individual implementations, and hence we call it a data motif. For the
first time, among a wide variety of big data and AI workloads, we identify
eight data motifs that take up most of the run time of those workloads,
including Matrix, Sampling, Logic, Transform, Set, Graph, Sort and Statistic.
We implement the eight data motifs on different software stacks as the micro
benchmarks of an open-source big data and AI benchmark suite ---BigDataBench
4.0 (publicly available from http://prof.ict.ac.cn/BigDataBench), and perform
comprehensive characterization of those data motifs from perspective of data
sizes, types, sources, and patterns as a lens towards fully understanding big
data and AI workloads. We believe the eight data motifs are promising
abstractions and tools for not only big data and AI benchmarking, but also
domain-specific hardware and software co-design.

The rise of deep learning has brought artificial intelligence (AI) to the
forefront. The ultimate goal of AI is to realize machines with human mind and
consciousness, but existing achievements mainly simulate intelligent behavior
on computer platforms. These achievements all belong to weak AI rather than
strong AI. How to achieve strong AI is not known yet in the field of
intelligence science. Currently, this field is calling for a new paradigm,
especially Theory of Cognitive Relativity (TCR). The TCR aims to summarize a
simple and elegant set of first principles about the nature of intelligence, at
least including the Principle of World's Relativity and the Principle of
Symbol's Relativity. The Principle of World's Relativity states that the
subjective world an intelligent agent can observe is strongly constrained by
the way it perceives the objective world. The Principle of Symbol's Relativity
states that an intelligent agent can use any physical symbol system to express
what it observes in its subjective world. The two principles are derived from
scientific facts and life experience. Thought experiments show that they are
important to understand high-level intelligence and necessary to establish a
scientific theory of mind and consciousness. Rather than brain-like
intelligence, the TCR indeed advocates a promising change in direction to
realize true AI, i.e. artificial general intelligence or artificial
consciousness, particularly different from humans' and animals'. Furthermore, a
TCR creed has been presented and extended to reveal the secrets of
consciousness and to guide realization of conscious machines. In the sense that
true AI could be diversely implemented in a brain-different way, the TCR would
probably drive an intelligence revolution in combination with some additional
first principles.

The Artificial Intelligence (AI) revolution foretold of during the 1960s is
well underway in the second decade of the 21st century. Its period of
phenomenal growth likely lies ahead. Still, we believe, there are crucial
lessons that biology can offer that will enable a prosperous future for AI. For
machines in general, and for AI's especially, operating over extended periods
or in extreme environments will require energy usage orders of magnitudes more
efficient than exists today. In many operational environments, energy sources
will be constrained. Any plans for AI devices operating in a challenging
environment must begin with the question of how they are powered, where fuel is
located, how energy is stored and made available to the machine, and how long
the machine can operate on specific energy units. Hence, the materials and
technologies that provide the needed energy represent a critical challenge
towards future use-scenarios of AI and should be integrated into their design.
Here we make four recommendations for stakeholders and especially decision
makers to facilitate a successful trajectory for this technology. First, that
scientific societies and governments coordinate Biomimetic Research for
Energy-efficient, AI Designs (BREAD); a multinational initiative and a funding
strategy for investments in the future integrated design of energetics into AI.
Second, that biomimetic energetic solutions be central to design consideration
for future AI. Third, that a pre-competitive space be organized between
stakeholder partners and fourth, that a trainee pipeline be established to
ensure the human capital required for success in this area.

While artificial intelligence (AI) holds promise for addressing societal
challenges, issues of exactly which tasks to automate and to what extent to do
so remain understudied. We approach this problem of task delegability from a
human-centered perspective by developing a framework on human perception of
task delegation to AI. We consider four high-level factors that can contribute
to a delegation decision: motivation, difficulty, risk, and trust. To obtain an
empirical understanding of human preferences in different tasks, we build a
dataset of 100 tasks from academic papers, popular media portrayal of AI, and
everyday life, and administer a survey based on our proposed framework. We find
little preference for full AI control and a strong preference for
machine-in-the-loop designs, in which humans play the leading role. Among the
four factors, trust is the most correlated with human preferences of optimal
human-machine delegation. This framework represents a first step towards
characterizing human preferences of AI automation across tasks. We hope this
work encourages future efforts towards understanding such individual attitudes;
our goal is to inform the public and the AI research community rather than
dictating any direction in technology development.

Despite recent breakthroughs, the ability of deep learning and reinforcement
learning to outperform traditional approaches to control physically embodied
robotic agents remains largely unproven. To help bridge this gap, we created
the 'AI Driving Olympics' (AI-DO), a competition with the objective of
evaluating the state of the art in machine learning and artificial intelligence
for mobile robotics. Based on the simple and well specified autonomous driving
and navigation environment called 'Duckietown', AI-DO includes a series of
tasks of increasing complexity -- from simple lane-following to fleet
management. For each task, we provide tools for competitors to use in the form
of simulators, logs, code templates, baseline implementations and low-cost
access to robotic hardware. We evaluate submissions in simulation online, on
standardized hardware environments, and finally at the competition event. The
first AI-DO, AI-DO 1, occurred at the Neural Information Processing Systems
(NeurIPS) conference in December 2018. The results of AI-DO 1 highlight the
need for better benchmarks, which are lacking in robotics, as well as improved
mechanisms to bridge the gap between simulation and reality.

Artificial Intelligence (AI), defined in its most simple form, is a
technological tool that makes machines intelligent. Since learning is at the
core of intelligence, machine learning poses itself as a core sub-field of AI.
Then there comes a subclass of machine learning, known as deep learning, to
address the limitations of their predecessors. AI has generally acquired its
prominence over the past few years due to its considerable progress in various
fields. AI has vastly invaded the realm of research. This has led physicists to
attentively direct their research towards implementing AI tools. Their central
aim has been to gain better understanding and enrich their intuition. This
review article is meant to supplement the previously presented efforts to
bridge the gap between AI and physics, and take a serious step forward to
filter out the "Babelian" clashes brought about from such gabs. This
necessitates first to have fundamental knowledge about common AI tools. To this
end, the review's primary focus shall be on deep learning models called
artificial neural networks. They are deep learning models which train
themselves through different learning processes. It discusses also the concept
of Markov decision processes. Finally, shortcut to the main goal, the review
thoroughly examines how these neural networks are capable to construct a
physical theory describing some observations without applying any previous
physical knowledge.

This paper investigates a paradigm for offering artificial intelligence as a
service (AI-aaS) on software-defined infrastructures (SDIs). The increasing
complexity of networking and computing infrastructures is already driving the
introduction of automation in networking and cloud computing management
systems. Here we consider how these automation mechanisms can be leveraged to
offer AI-aaS. Use cases for AI-aaS are easily found in addressing smart
applications in sectors such as transportation, manufacturing, energy, water,
air quality, and emissions. We propose an architectural scheme based on SDIs
where each AI-aaS application is comprised of a monitoring, analysis, policy,
execution plus knowledge (MAPE-K) loop (MKL). Each application is composed as
one or more specific service chains embedded in SDI, some of which will include
a Machine Learning (ML) pipeline. Our model includes a new training plane and
an AI-aaS plane to deal with the model-development and operational phases of AI
applications. We also consider the role of an ML/MKL sandbox in ensuring
coherency and consistency in the operation of multiple parallel MKL loops. We
present experimental measurement results for three AI-aaS applications deployed
on the SAVI testbed: 1. Compressing monitored data in SDI using autoencoders;
2. Traffic monitoring to allocate CPUs resources to VNFs; and 3. Highway
segment classification in smart transportation.

Recently, there has been great interest in developing Artificial Intelligence
(AI) enabled computer-aided diagnostics solutions for the diagnosis of skin
cancer. With the increasing incidence of skin cancers, low awareness among a
growing population, and a lack of adequate clinical expertise and services,
there is an immediate need for AI systems to assist clinicians in this domain.
A large number of skin lesion datasets are available publicly, and researchers
have developed AI-based image classification solutions, particularly deep
learning algorithms, to distinguish malignant skin lesions from benign lesions
in different image modalities such as dermoscopic, clinical, and histopathology
images. Despite the various claims of AI systems achieving higher accuracy than
dermatologists in the classification of different skin lesions, these AI
systems are still in the very early stages of clinical application in terms of
being ready to aid clinicians in the diagnosis of skin cancers. In this review,
we discuss advancements in the digital image-based AI solutions for the
diagnosis of skin cancer, along with some challenges and future opportunities
to improve these AI systems to support dermatologists and enhance their ability
to diagnose skin cancer.

As the transformative potential of AI has become increasingly salient as a
matter of public and political interest, there has been growing discussion
about the need to ensure that AI broadly benefits humanity. This in turn has
spurred debate on the social responsibilities of large technology companies to
serve the interests of society at large. In response, ethical principles and
codes of conduct have been proposed to meet the escalating demand for this
responsibility to be taken seriously. As yet, however, few institutional
innovations have been suggested to translate this responsibility into legal
commitments which apply to companies positioned to reap large financial gains
from the development and use of AI. This paper offers one potentially
attractive tool for addressing such issues: the Windfall Clause, which is an ex
ante commitment by AI firms to donate a significant amount of any eventual
extremely large profits. By this we mean an early commitment that profits that
a firm could not earn without achieving fundamental, economically
transformative breakthroughs in AI capabilities will be donated to benefit
humanity broadly, with particular attention towards mitigating any downsides
from deployment of windfall-generating AI.

Last decade has seen major improvements in the performance of artificial
intelligence which has driven wide-spread applications. Unforeseen effects of
such mass-adoption has put the notion of AI safety into the public eye. AI
safety is a relatively new field of research focused on techniques for building
AI beneficial for humans. While there exist survey papers for the field of AI
safety, there is a lack of a quantitative look at the research being conducted.
The quantitative aspect gives a data-driven insight about the emerging trends,
knowledge gaps and potential areas for future research. In this paper,
bibliometric analysis of the literature finds significant increase in research
activity since 2015. Also, the field is so new that most of the technical
issues are open, including: explainability with its long-term utility, and
value alignment which we have identified as the most important long-term
research topic. Equally, there is a severe lack of research into concrete
policies regarding AI. As we expect AI to be the one of the main driving forces
of changes in society, AI safety is the field under which we need to decide the
direction of humanity's future.

The application of Artificial Intelligence (AI) tools in different domains
are becoming mandatory for all companies wishing to excel in their industries.
One major challenge for a successful application of AI is to combine the
machine learning (ML) expertise with the domain knowledge to have the best
results applying AI tools. Domain specialists have an understanding of the data
and how it can impact their decisions. ML experts have the ability to use
AI-based tools dealing with large amounts of data and generating insights for
domain experts. But without a deep understanding of the data, ML experts are
not able to tune their models to get optimal results for a specific domain.
Therefore, domain experts are key users for ML tools and the explainability of
those AI tools become an essential feature in that context. There are a lot of
efforts to research AI explainability for different contexts, users and goals.
In this position paper, we discuss interesting findings about how ML experts
can express concerns about AI explainability while defining features of an ML
tool to be developed for a specific domain. We analyze data from two brainstorm
sessions done to discuss the functionalities of an ML tool to support
geoscientists (domain experts) on analyzing seismic data (domain-specific data)
with ML resources.

Modern real-world application scenarios like Internet services consist of a
diversity of AI and non-AI modules with huge code sizes and long and
complicated execution paths, which raises serious benchmarking or evaluating
challenges. Using AI components or micro benchmarks alone can lead to
error-prone conclusions. This paper presents a methodology to attack the above
challenge. We formalize a real-world application scenario as a Directed Acyclic
Graph-based model and propose the rules to distill it into a permutation of
essential AI and non-AI tasks, which we call a scenario benchmark. Together
with seventeen industry partners, we extract nine typical scenario benchmarks.
We design and implement an extensible, configurable, and flexible benchmark
framework. We implement two Internet service AI scenario benchmarks based on
the framework as proxies to two real-world application scenarios. We consider
scenario, component, and micro benchmarks as three indispensable parts for
evaluating. Our evaluation shows the advantage of our methodology against using
component or micro AI benchmarks alone. The specifications, source code,
testbed, and results are publicly available from
\url{https://www.benchcouncil.org/aibench/scenario/}.

In recent years Artificial Intelligence (AI) has gained much popularity, with
the scientific community as well as with the public. AI is often ascribed many
positive impacts for different social domains such as medicine and the economy.
On the other side, there is also growing concern about its precarious impact on
society and individuals. Several opinion polls frequently query the public fear
of autonomous robots and artificial intelligence (FARAI), a phenomenon coming
also into scholarly focus. As potential threat perceptions arguably vary with
regard to the reach and consequences of AI functionalities and the domain of
application, research still lacks necessary precision of a respective
measurement that allows for wide-spread research applicability. We propose a
fine-grained scale to measure threat perceptions of AI that accounts for four
functional classes of AI systems and is applicable to various domains of AI
applications. Using a standardized questionnaire in a survey study (N=891), we
evaluate the scale over three distinct AI domains (loan origination, job
recruitment and medical treatment). The data support the dimensional structure
of the proposed Threats of AI (TAI) scale as well as the internal consistency
and factoral validity of the indicators. Implications of the results and the
empirical application of the scale are discussed in detail. Recommendations for
further empirical use of the TAI scale are provided.

Recent developments in AI have provided assisting tools to support
pathologists' diagnoses. However, it remains challenging to incorporate such
tools into pathologists' practice; one main concern is AI's insufficient
workflow integration with medical decisions. We observed pathologists'
examination and discovered that the main hindering factor to integrate AI is
its incompatibility with pathologists' workflow. To bridge the gap between
pathologists and AI, we developed a human-AI collaborative diagnosis tool --
xPath -- that shares a similar examination process to that of pathologists,
which can improve AI's integration into their routine examination. The
viability of xPath is confirmed by a technical evaluation and work sessions
with twelve medical professionals in pathology. This work identifies and
addresses the challenge of incorporating AI models into pathology, which can
offer first-hand knowledge about how HCI researchers can work with medical
professionals side-by-side to bring technological advances to medical tasks
towards practical applications.

This paper reviews the current state of the art in Artificial Intelligence
(AI) technologies and applications in the context of the creative industries. A
brief background of AI, and specifically Machine Learning (ML) algorithms, is
provided including Convolutional Neural Network (CNNs), Generative Adversarial
Networks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement
Learning (DRL). We categorise creative applications into five groups related to
how AI technologies are used: i) content creation, ii) information analysis,
iii) content enhancement and post production workflows, iv) information
extraction and enhancement, and v) data compression. We critically examine the
successes and limitations of this rapidly advancing technology in each of these
areas. We further differentiate between the use of AI as a creative tool and
its potential as a creator in its own right. We foresee that, in the near
future, machine learning-based AI will be adopted widely as a tool or
collaborative assistant for creativity. In contrast, we observe that the
successes of machine learning in domains with fewer constraints, where AI is
the `creator', remain modest. The potential of AI (or its developers) to win
awards for its original creations in competition with human creatives is also
limited, based on contemporary technologies. We therefore conclude that, in the
context of creative industries, maximum benefit from AI will be derived where
its focus is human centric -- where it is designed to augment, rather than
replace, human creativity.

The field of Artificial Intelligence (AI) and, in particular, the Machine
Learning area, counts on a wide range of performance metrics and benchmark data
sets to assess the problem-solving effectiveness of its solutions. However, the
appearance of research centres, projects or institutions addressing AI
solutions from a multidisciplinary and multi-stakeholder perspective suggests a
new approach to assessment comprising ethical guidelines, reports or tools and
frameworks to help both academia and business to move towards a responsible
conceptualisation of AI. They all highlight the relevance of three key aspects:
(i) enhancing cooperation among the different stakeholders involved in the
design, deployment and use of AI; (ii) promoting multidisciplinary dialogue,
including different domains of expertise in this process; and (iii) fostering
public engagement to maximise a trusted relation with new technologies and
practitioners. In this paper, we introduce the Observatory on Society and
Artificial Intelligence (OSAI), an initiative grew out of the project AI4EU
aimed at stimulating reflection on a broad spectrum of issues of AI (ethical,
legal, social, economic and cultural). In particular, we describe our work in
progress around OSAI and suggest how this and similar initiatives can promote a
wider appraisal of progress in AI. This will give us the opportunity to present
our vision and our modus operandi to enhance the implementation of these three
fundamental dimensions.

Explainability has been a challenge in AI for as long as AI has existed. With
the recently increased use of AI in society, it has become more important than
ever that AI systems would be able to explain the reasoning behind their
results also to end-users in situations such as being eliminated from a
recruitment process or having a bank loan application refused by an AI system.
Especially if the AI system has been trained using Machine Learning, it tends
to contain too many parameters for them to be analysed and understood, which
has caused them to be called `black-box' systems. Most Explainable AI (XAI)
methods are based on extracting an interpretable model that can be used for
producing explanations. However, the interpretable model does not necessarily
map accurately to the original black-box model. Furthermore, the
understandability of interpretable models for an end-user remains questionable.
The notions of Contextual Importance and Utility (CIU) presented in this paper
make it possible to produce human-like explanations of black-box outcomes
directly, without creating an interpretable model. Therefore, CIU explanations
map accurately to the black-box model itself. CIU is completely model-agnostic
and can be used with any black-box system. In addition to feature importance,
the utility concept that is well-known in Decision Theory provides a new
dimension to explanations compared to most existing XAI methods. Finally, CIU
can produce explanations at any level of abstraction and using different
vocabularies and other means of interaction, which makes it possible to adjust
explanations and interaction according to the context and to the target users.

AI systems have found a wide range of application areas in financial
services. Their involvement in broader and increasingly critical decisions has
escalated the need for compliance and effective model governance. Current
governance practices have evolved from more traditional financial applications
and modeling frameworks. They often struggle with the fundamental differences
in AI characteristics such as uncertainty in the assumptions, and the lack of
explicit programming. AI model governance frequently involves complex review
flows and relies heavily on manual steps. As a result, it faces serious
challenges in effectiveness, cost, complexity, and speed. Furthermore, the
unprecedented rate of growth in the AI model complexity raises questions on the
sustainability of the current practices. This paper focuses on the challenges
of AI model governance in the financial services industry. As a part of the
outlook, we present a system-level framework towards increased self-regulation
for robustness and compliance. This approach aims to enable potential solution
opportunities through increased automation and the integration of monitoring,
management, and mitigation capabilities. The proposed framework also provides
model governance and risk management improved capabilities to manage model risk
during deployment.

Increasingly, modern Artificial Intelligence (AI) research has become more
computationally intensive. However, a growing concern is that due to unequal
access to computing power, only certain firms and elite universities have
advantages in modern AI research. Using a novel dataset of 171394 papers from
57 prestigious computer science conferences, we document that firms, in
particular, large technology firms and elite universities have increased
participation in major AI conferences since deep learning's unanticipated rise
in 2012. The effect is concentrated among elite universities, which are ranked
1-50 in the QS World University Rankings. Further, we find two strategies
through which firms increased their presence in AI research: first, they have
increased firm-only publications; and second, firms are collaborating primarily
with elite universities. Consequently, this increased presence of firms and
elite universities in AI research has crowded out mid-tier (QS ranked 201-300)
and lower-tier (QS ranked 301-500) universities. To provide causal evidence
that deep learning's unanticipated rise resulted in this divergence, we
leverage the generalized synthetic control method, a data-driven counterfactual
estimator. Using machine learning based text analysis methods, we provide
additional evidence that the divergence between these two groups - large firms
and non-elite universities - is driven by access to computing power or compute,
which we term as the "compute divide". This compute divide between large firms
and non-elite universities increases concerns around bias and fairness within
AI technology, and presents an obstacle towards "democratizing" AI. These
results suggest that a lack of access to specialized equipment such as compute
can de-democratize knowledge production.

Future wireless communication networks are expected to fulfill the
unprecedented performance requirements to support our highly digitized and
globally data-driven society. Various technological challenges must be overcome
to achieve our goal. Among many potential technologies, reconfigurable
intelligent surface (RIS) and artificial intelligence (AI) have attracted
extensive attention, thereby leading to a proliferation of studies for
utilizing them in wireless communication systems. The RIS-based wireless
communication frameworks and AI-enabled technologies, two of the promising
technologies for the sixth-generation networks, interact and promote with each
other, striving to collaboratively create a controllable, intelligent,
reconfigurable, and programmable wireless propagation environment. This paper
explores the road to implementing the combination of RIS and AI; specifically,
integrating AI-enabled technologies into RIS-based frameworks for maximizing
the practicality of RIS to facilitate the realization of smart radio
propagation environments, elaborated from shallow to deep insights. We begin
with the basic concept and fundamental characteristics of RIS, followed by the
overview of the research status of RIS. Then, we analyze the inevitable trend
of RIS to be combined with AI. In particular, we focus on recent research about
RIS-based architectures embedded with AI, elucidating from the intelligent
structures and systems of metamaterials to the AI-embedded RIS-assisted
wireless communication systems. Finally, the challenges and potential of the
topic are discussed.

While we have witnessed a rapid growth of ethics documents meant to guide AI
development, the promotion of AI ethics has nonetheless proceeded with little
input from AI practitioners themselves. Given the proliferation of AI for
Social Good initiatives, this is an emerging gap that needs to be addressed in
order to develop more meaningful ethical approaches to AI use and development.
This paper offers a methodology, a shared fairness approach, aimed at
identifying the needs of AI practitioners when it comes to confronting and
resolving ethical challenges and to find a third space where their operational
language can be married with that of the more abstract principles that
presently remain at the periphery of their work experiences. We offer a
grassroots approach to operational ethics based on dialog and mutualised
responsibility. This methodology is centred around conversations intended to
elicit practitioners perceived ethical attribution and distribution over key
value laden operational decisions, to identify when these decisions arise and
what ethical challenges they confront, and to engage in a language of ethics
and responsibility which enables practitioners to internalise ethical
responsibility. The methodology bridges responsibility imbalances that rest in
structural decision making power and elite technical knowledge, by commencing
with personal, facilitated conversations, returning the ethical discourse to
those meant to give it meaning at the sharp end of the ecosystem. Our primary
contribution is to add to the recent literature seeking to bring AI
practitioners' experiences to the fore by offering a methodology for
understanding how ethics manifests as a relational and interdependent
sociotechnical practice in their work.

Artificial intelligence (AI) systems have become increasingly common and the
trend will continue. Examples of AI systems include autonomous vehicles (AV),
computer vision, natural language processing, and AI medical experts. To allow
for safe and effective deployment of AI systems, the reliability of such
systems needs to be assessed. Traditionally, reliability assessment is based on
reliability test data and the subsequent statistical modeling and analysis. The
availability of reliability data for AI systems, however, is limited because
such data are typically sensitive and proprietary. The California Department of
Motor Vehicles (DMV) oversees and regulates an AV testing program, in which
many AV manufacturers are conducting AV road tests. Manufacturers participating
in the program are required to report recurrent disengagement events to
California DMV. This information is being made available to the public. In this
paper, we use recurrent disengagement events as a representation of the
reliability of the AI system in AV, and propose a statistical framework for
modeling and analyzing the recurrent events data from AV driving tests. We use
traditional parametric models in software reliability and propose a new
nonparametric model based on monotonic splines to describe the event process.
We develop inference procedures for selecting the best models, quantifying
uncertainty, and testing heterogeneity in the event process. We then analyze
the recurrent events data from four AV manufacturers, and make inferences on
the reliability of the AI systems in AV. We also describe how the proposed
analysis can be applied to assess the reliability of other AI systems.

Background: With the rising popularity of Artificial Intelligence (AI), there
is a growing need to build large and complex AI-based systems in a
cost-effective and manageable way. Like with traditional software, Technical
Debt (TD) will emerge naturally over time in these systems, therefore leading
to challenges and risks if not managed appropriately. The influence of data
science and the stochastic nature of AI-based systems may also lead to new
types of TD or antipatterns, which are not yet fully understood by researchers
and practitioners. Objective: The goal of our study is to provide a clear
overview and characterization of the types of TD (both established and new
ones) that appear in AI-based systems, as well as the antipatterns and related
solutions that have been proposed. Method: Following the process of a
systematic mapping study, 21 primary studies are identified and analyzed.
Results: Our results show that (i) established TD types, variations of them,
and four new TD types (data, model, configuration, and ethics debt) are present
in AI-based systems, (ii) 72 antipatterns are discussed in the literature, the
majority related to data and model deficiencies, and (iii) 46 solutions have
been proposed, either to address specific TD types, antipatterns, or TD in
general. Conclusions: Our results can support AI professionals with reasoning
about and communicating aspects of TD present in their systems. Additionally,
they can serve as a foundation for future research to further our understanding
of TD in AI-based systems.

Artificial intelligence (AI) technologies have dramatically advanced in
recent years, resulting in revolutionary changes in people's lives. Empowered
by edge computing, AI workloads are migrating from centralized cloud
architectures to distributed edge systems, introducing a new paradigm called
edge AI. While edge AI has the promise of bringing significant increases in
autonomy and intelligence into everyday lives through common edge devices, it
also raises new challenges, especially for the development of its algorithms
and the deployment of its services, which call for novel design methodologies
catered to these unique challenges. In this paper, we provide a comprehensive
survey of the latest enabling design methodologies that span the entire edge AI
development stack. We suggest that the key methodologies for effective edge AI
development are single-layer specialization and cross-layer co-design. We
discuss representative methodologies in each category in detail, including
on-device training methods, specialized software design, dedicated hardware
design, benchmarking and design automation, software/hardware co-design,
software/compiler co-design, and compiler/hardware co-design. Moreover, we
attempt to reveal hidden cross-layer design opportunities that can further
boost the solution quality of future edge AI and provide insights into future
directions and emerging areas that require increased research focus.

Countries, companies, and universities are increasingly competing over
top-tier artificial intelligence (AI) researchers. Where are these researchers
likely to immigrate and what affects their immigration decisions? We conducted
a survey $(n = 524)$ of the immigration preferences and motivations of
researchers that had papers accepted at one of two prestigious AI conferences:
the Conference on Neural Information Processing Systems (NeurIPS) and the
International Conference on Machine Learning (ICML). We find that the U.S. is
the most popular destination for AI researchers, followed by the U.K., Canada,
Switzerland, and France. A country's professional opportunities stood out as
the most common factor that influences immigration decisions of AI researchers,
followed by lifestyle and culture, the political climate, and personal
relations. The destination country's immigration policies were important to
just under half of the researchers surveyed, while around a quarter noted
current immigration difficulties to be a deciding factor. Visa and immigration
difficulties were perceived to be a particular impediment to conducting AI
research in the U.S., the U.K., and Canada. Implications of the findings for
the future of AI talent policies and governance are discussed.

This paper outlines the state of the art in AI. It then describes basic
machine learning and knowledge processing techniques. Based on this, some
possibilities and limitations of future AI developments are discussed.

AI has the potential to revolutionize many areas of healthcare. Radiology,
dermatology, and ophthalmology are some of the areas most likely to be impacted
in the near future, and they have received significant attention from the
broader research community. But AI techniques are now also starting to be used
in in vitro fertilization (IVF), in particular for selecting which embryos to
transfer to the woman. The contribution of AI to IVF is potentially
significant, but must be done carefully and transparently, as the ethical
issues are significant, in part because this field involves creating new
people. We first give a brief introduction to IVF and review the use of AI for
embryo selection. We discuss concerns with the interpretation of the reported
results from scientific and practical perspectives. We then consider the
broader ethical issues involved. We discuss in detail the problems that result
from the use of black-box methods in this context and advocate strongly for the
use of interpretable models. Importantly, there have been no published trials
of clinical effectiveness, a problem in both the AI and IVF communities, and we
therefore argue that clinical implementation at this point would be premature.
Finally, we discuss ways for the broader AI community to become involved to
ensure scientifically sound and ethically responsible development of AI in IVF.

In this paper, the Robotic Assistant Agent for student and machine
co-learning on AI-FML practice with AIoT application is presented. The
structure of AI-FML contains three parts, including fuzzy logic, neural
network, and evolutionary computation. Besides, the Robotic Assistant Agent
(RAA) can assist students and machines in co-learning English and AI-FML
practice based on the robot Kebbi Air and AIoT-FML learning tool. Since Sept.
2019, we have introduced an Intelligent Speaking English Assistant (ISEA) App
and AI-FML platform to English and computer science learning classes at two
elementary schools in Taiwan. We use the collected English-learning data to
train a predictive regression model based on students' monthly examination
scores. In Jan. 2021, we further combined the developed AI-FML platform with a
novel AIoT-FML learning tool to enhance students' interests in learning English
and AI-FML with basic hands-on practice. The proposed RAA is responsible for
reasoning students' learning performance and showing the results on the
AIoT-FML learning tool after communicating with the AI-FML platform. The
experimental results and the collection of students' feedback show that this
kind of learning model is popular with elementary-school and high-school
students, and the learning performance of elementary-school students is
improved.

Among the most damaging characteristics of the covid-19 pandemic has been its
disproportionate effect on disadvantaged communities. As the outbreak has
spread globally, factors such as systemic racism, marginalisation, and
structural inequality have created path dependencies that have led to poor
health outcomes. These social determinants of infectious disease and
vulnerability to disaster have converged to affect already disadvantaged
communities with higher levels of economic instability, disease exposure,
infection severity, and death. Artificial intelligence (AI) technologies are an
important part of the health informatics toolkit used to fight contagious
disease. AI is well known, however, to be susceptible to algorithmic biases
that can entrench and augment existing inequality. Uncritically deploying AI in
the fight against covid-19 thus risks amplifying the pandemic's adverse effects
on vulnerable groups, exacerbating health inequity. In this paper, we claim
that AI systems can introduce or reflect bias and discrimination in three ways:
in patterns of health discrimination that become entrenched in datasets, in
data representativeness, and in human choices made during the design,
development, and deployment of these systems. We highlight how the use of AI
technologies threaten to exacerbate the disparate effect of covid-19 on
marginalised, under-represented, and vulnerable groups, particularly black,
Asian, and other minoritised ethnic people, older populations, and those of
lower socioeconomic status. We conclude that, to mitigate the compounding
effects of AI on inequalities associated with covid-19, decision makers,
technology developers, and health officials must account for the potential
biases and inequities at all stages of the AI process.

Grice's Cooperative Principle (1975) describes the implicit maxims that guide
conversation between humans. As humans begin to interact with non-human
dialogue systems more frequently and in a broader scope, an important question
emerges: what principles govern those interactions? The present study addresses
this question by evaluating human-AI interactions using Grice's four maxims; we
demonstrate that humans do, indeed, apply these maxims to interactions with AI,
even making explicit references to the AI's performance through a Gricean lens.
Twenty-three participants interacted with an American English-speaking Alexa
and rated and discussed their experience with an in-lab researcher. Researchers
then reviewed each exchange, identifying those that might relate to Grice's
maxims: Quantity, Quality, Manner, and Relevance. Many instances of explicit
user frustration stemmed from violations of Grice's maxims. Quantity violations
were noted for too little but not too much information, while Quality
violations were rare, indicating trust in Alexa's responses. Manner violations
focused on speed and humanness. Relevance violations were the most frequent,
and they appear to be the most frustrating. While the maxims help describe many
of the issues participants encountered, other issues do not fit neatly into
Grice's framework. Participants were particularly averse to Alexa initiating
exchanges or making unsolicited suggestions. To address this gap, we propose
the addition of human Priority to describe human-AI interaction. Humans and AIs
are not conversational equals, and human initiative takes priority. We suggest
that the application of Grice's Cooperative Principles to human-AI interactions
is beneficial both from an AI development perspective and as a tool for
describing an emerging form of interaction.

HPC is an enabling platform for AI. The introduction of AI workloads in the
HPC applications basket has non-trivial consequences both on the way of
designing AI applications and on the way of providing HPC computing. This is
the leitmotif of the convergence between HPC and AI. The formalized definition
of AI pipelines is one of the milestones of HPC-AI convergence. If well
conducted, it allows, on the one hand, to obtain portable and scalable
applications. On the other hand, it is crucial for the reproducibility of
scientific pipelines. In this work, we advocate the StreamFlow Workflow
Management System as a crucial ingredient to define a parametric pipeline,
called "CLAIRE COVID-19 Universal Pipeline," which is able to explore the
optimization space of methods to classify COVID-19 lung lesions from CT scans,
compare them for accuracy, and therefore set a performance baseline. The
universal pipeline automatizes the training of many different Deep Neural
Networks (DNNs) and many different hyperparameters. It, therefore, requires a
massive computing power, which is found in traditional HPC infrastructure
thanks to the portability-by-design of pipelines designed with StreamFlow.
Using the universal pipeline, we identified a DNN reaching over 90% accuracy in
detecting COVID-19 lesions in CT scans.

The traditional production paradigm of large batch production does not offer
flexibility towards satisfying the requirements of individual customers. A new
generation of smart factories is expected to support new multi-variety and
small-batch customized production modes. For that, Artificial Intelligence (AI)
is enabling higher value-added manufacturing by accelerating the integration of
manufacturing and information communication technologies, including computing,
communication, and control. The characteristics of a customized smart factory
are to include self-perception, operations optimization, dynamic
reconfiguration, and intelligent decision-making. The AI technologies will
allow manufacturing systems to perceive the environment, adapt to external
needs, and extract the processed knowledge, including business models, such as
intelligent production, networked collaboration, and extended service models.
  This paper focuses on the implementation of AI in customized manufacturing
(CM). The architecture of an AI-driven customized smart factory is presented.
Details of intelligent manufacturing devices, intelligent information
interaction, and the construction of a flexible manufacturing line are
showcased. The state-of-the-art AI technologies of potential use in CM, i.e.,
machine learning, multi-agent systems, Internet of Things, big data, and
cloud-edge computing are surveyed. The AI-enabled technologies in a customized
smart factory are validated with a case study of customized packaging. The
experimental results have demonstrated that the AI-assisted CM offers the
possibility of higher production flexibility and efficiency. Challenges and
solutions related to AI in CM are also discussed.

Artificial Intelligence (AI) is rapidly becoming integrated into military
Command and Control (C2) systems as a strategic priority for many defence
forces. The successful implementation of AI is promising to herald a
significant leap in C2 agility through automation. However, realistic
expectations need to be set on what AI can achieve in the foreseeable future.
This paper will argue that AI could lead to a fragility trap, whereby the
delegation of C2 functions to an AI could increase the fragility of C2,
resulting in catastrophic strategic failures. This calls for a new framework
for AI in C2 to avoid this trap. We will argue that antifragility along with
agility should form the core design principles for AI-enabled C2 systems. This
duality is termed Agile, Antifragile, AI-Enabled Command and Control (A3IC2).
An A3IC2 system continuously improves its capacity to perform in the face of
shocks and surprises through overcompensation from feedback during the C2
decision-making cycle. An A3IC2 system will not only be able to survive within
a complex operational environment, it will also thrive, benefiting from the
inevitable shocks and volatility of war.

Artificial Intelligence (AI) logic formalizes the reasoning of intelligent
agents. In this paper, we discuss how an argumentation-based AI logic could be
used also to formalize important aspects of social reasoning. Besides reasoning
about the knowledge and actions of individual agents, social AI logic can
reason also about social dependencies among agents using the rights,
obligations and permissions of the agents. We discuss four aspects of social AI
logic. First, we discuss how rights represent relations between the obligations
and permissions of intelligent agents. Second, we discuss how to argue about
the right-to-know, a central issue in the recent discussion of privacy and
ethics. Third, we discuss how a wide variety of conflicts among intelligent
agents can be identified and (sometimes) resolved by comparing formal
arguments. Importantly, to cover a wide range of arguments occurring in daily
life, also fallacious arguments can be represented and reasoned about. Fourth,
we discuss how to argue about the freedom to act for intelligent agents.
Examples from social, legal and ethical reasoning highlight the challenges in
developing social AI logic. The discussion of the four challenges leads to a
research program for argumentation-based social AI logic, contributing towards
the future development of AI logic.

Despite AI's superhuman performance in a variety of domains, humans are often
unwilling to adopt AI systems. The lack of interpretability inherent in many
modern AI techniques is believed to be hurting their adoption, as users may not
trust systems whose decision processes they do not understand. We investigate
this proposition with a novel experiment in which we use an interactive
prediction task to analyze the impact of interpretability and outcome feedback
on trust in AI and on human performance in AI-assisted prediction tasks. We
find that interpretability led to no robust improvements in trust, while
outcome feedback had a significantly greater and more reliable effect. However,
both factors had modest effects on participants' task performance. Our findings
suggest that (1) factors receiving significant attention, such as
interpretability, may be less effective at increasing trust than factors like
outcome feedback, and (2) augmenting human performance via AI systems may not
be a simple matter of increasing trust in AI, as increased trust is not always
associated with equally sizable improvements in performance. These findings
invite the research community to focus not only on methods for generating
interpretations but also on techniques for ensuring that interpretations impact
trust and performance in practice.

The range of application of artificial intelligence (AI) is vast, as is the
potential for harm. Growing awareness of potential risks from AI systems has
spurred action to address those risks, while eroding confidence in AI systems
and the organizations that develop them. A 2019 study found over 80
organizations that published and adopted "AI ethics principles'', and more have
joined since. But the principles often leave a gap between the "what" and the
"how" of trustworthy AI development. Such gaps have enabled questionable or
ethically dubious behavior, which casts doubts on the trustworthiness of
specific organizations, and the field more broadly. There is thus an urgent
need for concrete methods that both enable AI developers to prevent harm and
allow them to demonstrate their trustworthiness through verifiable behavior.
Below, we explore mechanisms (drawn from arXiv:2004.07213) for creating an
ecosystem where AI developers can earn trust - if they are trustworthy. Better
assessment of developer trustworthiness could inform user choice, employee
actions, investment decisions, legal recourse, and emerging governance regimes.

Given the growing use of Artificial Intelligence (AI) and machine learning
(ML) methods across all aspects of environmental sciences, it is imperative
that we initiate a discussion about the ethical and responsible use of AI. In
fact, much can be learned from other domains where AI was introduced, often
with the best of intentions, yet often led to unintended societal consequences,
such as hard coding racial bias in the criminal justice system or increasing
economic inequality through the financial system. A common misconception is
that the environmental sciences are immune to such unintended consequences when
AI is being used, as most data come from observations, and AI algorithms are
based on mathematical formulas, which are often seen as objective. In this
article, we argue the opposite can be the case. Using specific examples, we
demonstrate many ways in which the use of AI can introduce similar consequences
in the environmental sciences. This article will stimulate discussion and
research efforts in this direction. As a community, we should avoid repeating
any foreseeable mistakes made in other domains through the introduction of AI.
In fact, with proper precautions, AI can be a great tool to help {\it reduce}
climate and environmental injustice. We primarily focus on weather and climate
examples but the conclusions apply broadly across the environmental sciences.

Modern Artificial Intelligence (AI) systems excel at diverse tasks, from
image classification to strategy games, even outperforming humans in many of
these domains. After making astounding progress in language learning in the
recent decade, AI systems, however, seem to approach the ceiling that does not
reflect important aspects of human communicative capacities. Unlike human
learners, communicative AI systems often fail to systematically generalize to
new data, suffer from sample inefficiency, fail to capture common-sense
semantic knowledge, and do not translate to real-world communicative
situations. Cognitive Science offers several insights on how AI could move
forward from this point. This paper aims to: (1) suggest that the dominant
cognitively-inspired AI directions, based on nativist and symbolic paradigms,
lack necessary substantiation and concreteness to guide progress in modern AI,
and (2) articulate an alternative, "grounded", perspective on AI advancement,
inspired by Embodied, Embedded, Extended, and Enactive Cognition (4E) research.
I review results on 4E research lines in Cognitive Science to distinguish the
main aspects of naturalistic learning conditions that play causal roles for
human language development. I then use this analysis to propose a list of
concrete, implementable components for building "grounded" linguistic
intelligence. These components include embodying machines in a
perception-action cycle, equipping agents with active exploration mechanisms so
they can build their own curriculum, allowing agents to gradually develop motor
abilities to promote piecemeal language development, and endowing the agents
with adaptive feedback from their physical and social environment. I hope that
these ideas can direct AI research towards building machines that develop
human-like language abilities through their experiences with the world.

Publicly accessible benchmarks that allow for assessing and comparing model
performances are important drivers of progress in artificial intelligence (AI).
While recent advances in AI capabilities hold the potential to transform
medical practice by assisting and augmenting the cognitive processes of
healthcare professionals, the coverage of clinically relevant tasks by AI
benchmarks is largely unclear. Furthermore, there is a lack of systematized
meta-information that allows clinical AI researchers to quickly determine
accessibility, scope, content and other characteristics of datasets and
benchmark datasets relevant to the clinical domain.
  To address these issues, we curated and released a comprehensive catalogue of
datasets and benchmarks pertaining to the broad domain of clinical and
biomedical natural language processing (NLP), based on a systematic review of
literature and online resources. A total of 450 NLP datasets were manually
systematized and annotated with rich metadata, such as targeted tasks, clinical
applicability, data types, performance metrics, accessibility and licensing
information, and availability of data splits. We then compared tasks covered by
AI benchmark datasets with relevant tasks that medical practitioners reported
as highly desirable targets for automation in a previous empirical study.
  Our analysis indicates that AI benchmarks of direct clinical relevance are
scarce and fail to cover most work activities that clinicians want to see
addressed. In particular, tasks associated with routine documentation and
patient data administration workflows are not represented despite significant
associated workloads. Thus, currently available AI benchmarks are improperly
aligned with desired targets for AI automation in clinical settings, and novel
benchmarks should be created to fill these gaps.

Gastric endoscopic screening is an effective way to decide appropriate
gastric cancer (GC) treatment at an early stage, reducing GC-associated
mortality rate. Although artificial intelligence (AI) has brought a great
promise to assist pathologist to screen digitalized whole slide images,
existing AI systems are limited in fine-grained cancer subclassifications and
have little usability in planning cancer treatment. We propose a practical AI
system that enables five subclassifications of GC pathology, which can be
directly matched to general GC treatment guidance. The AI system is designed to
efficiently differentiate multi-classes of GC through multi-scale
self-attention mechanism using 2-stage hybrid Vision Transformer (ViT)
networks, by mimicking the way how human pathologists understand histology. The
AI system demonstrates reliable diagnostic performance by achieving
class-average sensitivity of above 0.85 on a total of 1,212 slides from
multicentric cohort. Furthermore, AI-assisted pathologists show significantly
improved diagnostic sensitivity by 12% in addition to 18% reduced screening
time compared to human pathologists. Our results demonstrate that AI-assisted
gastric endoscopic screening has a great potential for providing presumptive
pathologic opinion and appropriate cancer treatment of gastric cancer in
practical clinical settings.

This chapter formulates seven lessons for preventing harm in artificial
intelligence (AI) systems based on insights from the field of system safety for
software-based automation in safety-critical domains. New applications of AI
across societal domains and public organizations and infrastructures come with
new hazards, which lead to new forms of harm, both grave and pernicious. The
text addresses the lack of consensus for diagnosing and eliminating new AI
system hazards. For decades, the field of system safety has dealt with
accidents and harm in safety-critical systems governed by varying degrees of
software-based automation and decision-making. This field embraces the core
assumption of systems and control that AI systems cannot be safeguarded by
technical design choices on the model or algorithm alone, instead requiring an
end-to-end hazard analysis and design frame that includes the context of use,
impacted stakeholders and the formal and informal institutional environment in
which the system operates. Safety and other values are then inherently
socio-technical and emergent system properties that require design and control
measures to instantiate these across the technical, social and institutional
components of a system. This chapter honors system safety pioneer Nancy
Leveson, by situating her core lessons for today's AI system safety challenges.
For every lesson, concrete tools are offered for rethinking and reorganizing
the safety management of AI systems, both in design and governance. This
history tells us that effective AI safety management requires transdisciplinary
approaches and a shared language that allows involvement of all levels of
society.

Archival institutions and programs worldwide work to ensure that the records
of governments, organizations, communities, and individuals are preserved for
future generations as cultural heritage, as sources of rights, and as vehicles
for holding the past accountable and to inform the future. This commitment is
guaranteed through the adoption of strategic and technical measures for the
long-term preservation of digital assets in any medium and form - textual,
visual, or aural. Public and private archives are the largest providers of data
big and small in the world and collectively host yottabytes of trusted data, to
be preserved forever. Several aspects of retention and preservation,
arrangement and description, management and administrations, and access and use
are still open to improvement. In particular, recent advances in Artificial
Intelligence (AI) open the discussion as to whether AI can support the ongoing
availability and accessibility of trustworthy public records. This paper
presents preliminary results of the InterPARES Trust AI (I Trust AI)
international research partnership, which aims to (1) identify and develop
specific AI technologies to address critical records and archives challenges;
(2) determine the benefits and risks of employing AI technologies on records
and archives; (3) ensure that archival concepts and principles inform the
development of responsible AI; and (4) validate outcomes through a conglomerate
of case studies and demonstrations.

Centaurs are half-human, half-AI decision-makers where the AI's goal is to
complement the human. To do so, the AI must be able to recognize the goals and
constraints of the human and have the means to help them. We present a novel
formulation of the interaction between the human and the AI as a sequential
game where the agents are modelled using Bayesian best-response models. We show
that in this case the AI's problem of helping bounded-rational humans make
better decisions reduces to a Bayes-adaptive POMDP. In our simulated
experiments, we consider an instantiation of our framework for humans who are
subjectively optimistic about the AI's future behaviour. Our results show that
when equipped with a model of the human, the AI can infer the human's bounds
and nudge them towards better decisions. We discuss ways in which the machine
can learn to improve upon its own limitations as well with the help of the
human. We identify a novel trade-off for centaurs in partially observable
tasks: for the AI's actions to be acceptable to the human, the machine must
make sure their beliefs are sufficiently aligned, but aligning beliefs might be
costly. We present a preliminary theoretical analysis of this trade-off and its
dependence on task structure.

With the growing availability of large-scale datasets, and the popularization
of affordable storage and computational capabilities, the energy consumed by AI
is becoming a growing concern. To address this issue, in recent years, studies
have focused on demonstrating how AI energy efficiency can be improved by
tuning the model training strategy. Nevertheless, how modifications applied to
datasets can impact the energy consumption of AI is still an open question. To
fill this gap, in this exploratory study, we evaluate if data-centric
approaches can be utilized to improve AI energy efficiency. To achieve our
goal, we conduct an empirical experiment, executed by considering 6 different
AI algorithms, a dataset comprising 5,574 data points, and two dataset
modifications (number of data points and number of features). Our results show
evidence that, by exclusively conducting modifications on datasets, energy
consumption can be drastically reduced (up to 92.16%), often at the cost of a
negligible or even absent accuracy decline. As additional introductory results,
we demonstrate how, by exclusively changing the algorithm used, energy savings
up to two orders of magnitude can be achieved. In conclusion, this exploratory
investigation empirically demonstrates the importance of applying data-centric
techniques to improve AI energy efficiency. Our results call for a research
agenda that focuses on data-centric techniques, to further enable and
democratize Green AI.

This survey article assesses and compares existing critiques of current
fairness-enhancing technical interventions into machine learning (ML) that draw
from a range of non-computing disciplines, including philosophy, feminist
studies, critical race and ethnic studies, legal studies, anthropology, and
science and technology studies. It bridges epistemic divides in order to offer
an interdisciplinary understanding of the possibilities and limits of hegemonic
computational approaches to ML fairness for producing just outcomes for
society's most marginalized. The article is organized according to nine major
themes of critique wherein these different fields intersect: 1) how "fairness"
in AI fairness research gets defined; 2) how problems for AI systems to address
get formulated; 3) the impacts of abstraction on how AI tools function and its
propensity to lead to technological solutionism; 4) how racial classification
operates within AI fairness research; 5) the use of AI fairness measures to
avoid regulation and engage in ethics washing; 6) an absence of participatory
design and democratic deliberation in AI fairness considerations; 7) data
collection practices that entrench "bias," are non-consensual, and lack
transparency; 8) the predatory inclusion of marginalized groups into AI
systems; and 9) a lack of engagement with AI's long-term social and ethical
outcomes. Drawing from these critiques, the article concludes by imagining
future ML fairness research directions that actively disrupt entrenched power
dynamics and structural injustices in society.

Critical examinations of AI systems often apply principles such as fairness,
justice, accountability, and safety, which is reflected in AI regulations such
as the EU AI Act. Are such principles sufficient to promote the design of
systems that support human flourishing? Even if a system is in some sense fair,
just, or 'safe', it can nonetheless be exploitative, coercive, inconvenient, or
otherwise conflict with cultural, individual, or social values. This paper
proposes a dimension of interactional ethics thus far overlooked: the ways AI
systems should treat human beings. For this purpose, we explore the
philosophical concept of respect: if respect is something everyone needs and
deserves, shouldn't technology aim to be respectful? Despite its intuitive
simplicity, respect in philosophy is a complex concept with many disparate
senses. Like fairness or justice, respect can characterise how people deserve
to be treated; but rather than relating primarily to the distribution of
benefits or punishments, respect relates to how people regard one another, and
how this translates to perception, treatment, and behaviour. We explore respect
broadly across several literatures, synthesising perspectives on respect from
Kantian, post-Kantian, dramaturgical, and agential realist design perspectives
with a goal of drawing together a view of what respect could mean for AI. In so
doing, we identify ways that respect may guide us towards more sociable
artefacts that ethically and inclusively honour and recognise humans using the
rich social language that we have evolved to interact with one another every
day.

Explanation mechanisms from the field of Counterfactual Thinking are a
widely-used paradigm for Explainable Artificial Intelligence (XAI), as they
follow a natural way of reasoning that humans are familiar with. However, all
common approaches from this field are based on communicating information about
features or characteristics that are especially important for an AI's decision.
We argue that in order to fully understand a decision, not only knowledge about
relevant features is needed, but that the awareness of irrelevant information
also highly contributes to the creation of a user's mental model of an AI
system. Therefore, we introduce a new way of explaining AI systems. Our
approach, which we call Alterfactual Explanations, is based on showing an
alternative reality where irrelevant features of an AI's input are altered. By
doing so, the user directly sees which characteristics of the input data can
change arbitrarily without influencing the AI's decision. We evaluate our
approach in an extensive user study, revealing that it is able to significantly
contribute to the participants' understanding of an AI. We show that
alterfactual explanations are suited to convey an understanding of different
aspects of the AI's reasoning than established counterfactual explanation
methods.

In recent years, the landscape of computing paradigms has witnessed a gradual
yet remarkable shift from monolithic computing to distributed and decentralized
paradigms such as Internet of Things (IoT), Edge, Fog, Cloud, and Serverless.
The frontiers of these computing technologies have been boosted by shift from
manually encoded algorithms to Artificial Intelligence (AI)-driven autonomous
systems for optimum and reliable management of distributed computing resources.
Prior work focuses on improving existing systems using AI across a wide range
of domains, such as efficient resource provisioning, application deployment,
task placement, and service management. This survey reviews the evolution of
data-driven AI-augmented technologies and their impact on computing systems. We
demystify new techniques and draw key insights in Edge, Fog and Cloud resource
management-related uses of AI methods and also look at how AI can innovate
traditional applications for enhanced Quality of Service (QoS) in the presence
of a continuum of resources. We present the latest trends and impact areas such
as optimizing AI models that are deployed on or for computing systems. We
layout a roadmap for future research directions in areas such as resource
management for QoS optimization and service reliability. Finally, we discuss
blue-sky ideas and envision this work as an anchor point for future research on
AI-driven computing systems.

Face image synthesis has progressed beyond the point at which humans can
effectively distinguish authentic faces from synthetically generated ones.
Recently developed synthetic face image detectors boast "better-than-human"
discriminative ability, especially those guided by human perceptual
intelligence during the model's training process. In this paper, we investigate
whether these human-guided synthetic face detectors can assist non-expert human
operators in the task of synthetic image detection when compared to models
trained without human-guidance. We conducted a large-scale experiment with more
than 1,560 subjects classifying whether an image shows an authentic or
synthetically-generated face, and annotate regions that supported their
decisions. In total, 56,015 annotations across 3,780 unique face images were
collected. All subjects first examined samples without any AI support, followed
by samples given (a) the AI's decision ("synthetic" or "authentic"), (b) class
activation maps illustrating where the model deems salient for its decision, or
(c) both the AI's decision and AI's saliency map. Synthetic faces were
generated with six modern Generative Adversarial Networks. Interesting
observations from this experiment include: (1) models trained with
human-guidance offer better support to human examination of face images when
compared to models trained traditionally using cross-entropy loss, (2) binary
decisions presented to humans offers better support than saliency maps, (3)
understanding the AI's accuracy helps humans to increase trust in a given model
and thus increase their overall accuracy. This work demonstrates that although
humans supported by machines achieve better-than-random accuracy of synthetic
face detection, the ways of supplying humans with AI support and of building
trust are key factors determining high effectiveness of the human-AI tandem.

The role of artificial intelligence (AI) in material science and engineering
(MSE) is becoming increasingly important as AI technology advances. The
development of high-performance computing has made it possible to test deep
learning (DL) models with significant parameters, providing an opportunity to
overcome the limitation of traditional computational methods, such as density
functional theory (DFT), in property prediction. Machine learning (ML)-based
methods are faster and more accurate than DFT-based methods. Furthermore, the
generative adversarial networks (GANs) have facilitated the generation of
chemical compositions of inorganic materials without using crystal structure
information. These developments have significantly impacted material
engineering (ME) and research. Some of the latest developments in AI in ME
herein are reviewed. First, the development of AI in the critical areas of ME,
such as in material processing, the study of structure and material property,
and measuring the performance of materials in various aspects, is discussed.
Then, the significant methods of AI and their uses in MSE, such as graph neural
network, generative models, transfer of learning, etc. are discussed. The use
of AI to analyze the results from existing analytical instruments is also
discussed. Finally, AI's advantages, disadvantages, and future in ME are
discussed.

Despite the proliferation of explainable AI (XAI) methods, little is
understood about end-users' explainability needs and behaviors around XAI
explanations. To address this gap and contribute to understanding how
explainability can support human-AI interaction, we conducted a mixed-methods
study with 20 end-users of a real-world AI application, the Merlin bird
identification app, and inquired about their XAI needs, uses, and perceptions.
We found that participants desire practically useful information that can
improve their collaboration with the AI, more so than technical system details.
Relatedly, participants intended to use XAI explanations for various purposes
beyond understanding the AI's outputs: calibrating trust, improving their task
skills, changing their behavior to supply better inputs to the AI, and giving
constructive feedback to developers. Finally, among existing XAI approaches,
participants preferred part-based explanations that resemble human reasoning
and explanations. We discuss the implications of our findings and provide
recommendations for future XAI design.

In the past decade, we have observed a growing interest in using technologies
such as artificial intelligence (AI), machine learning, and chatbots to provide
assistance to language learners, especially in second language learning. By
using AI and natural language processing (NLP) and chatbots, we can create an
intelligent self-learning environment that goes beyond multiple-choice
questions and/or fill in the blank exercises. In addition, NLP allows for
learning to be adaptive in that it offers more than an indication that an error
has occurred. It also provides a description of the error, uses linguistic
analysis to isolate the source of the error, and then suggests additional
drills to achieve optimal individualized learning outcomes. In this paper, we
present our approach for developing an Artificial Intelligence-based Arabic
Language and Speech Tutor (AI-ALST) for teaching the Moroccan Arabic dialect.
The AI-ALST system is an intelligent tutor that provides analysis and
assessment of students learning the Moroccan dialect at University of Arizona
(UA). The AI-ALST provides a self-learned environment to practice each lesson
for pronunciation training. In this paper, we present our initial experimental
evaluation of the AI-ALST that is based on MFCC (Mel frequency cepstrum
coefficient) feature extraction, bidirectional LSTM (Long Short-Term Memory),
attention mechanism, and a cost-based strategy for dealing with class-imbalance
learning. We evaluated our tutor on the word pronunciation of lesson 1 of the
Moroccan Arabic dialect class. The experimental results show that the AI-ALST
can effectively and successfully detect pronunciation errors and evaluate its
performance by using F_1-score, accuracy, precision, and recall.

Despite high global prevalence of hepatic steatosis, no automated diagnostics
demonstrated generalizability in detecting steatosis on multiple international
datasets. Traditionally, hepatic steatosis detection relies on clinicians
selecting the region of interest (ROI) on computed tomography (CT) to measure
liver attenuation. ROI selection demands time and expertise, and therefore is
not routinely performed in populations. To automate the process, we validated
an existing artificial intelligence (AI) system for 3D liver segmentation and
used it to purpose a novel method: AI-ROI, which could automatically select the
ROI for attenuation measurements. AI segmentation and AI-ROI method were
evaluated on 1,014 non-contrast enhanced chest CT images from eight
international datasets: LIDC-IDRI, NSCLC-Lung1, RIDER, VESSEL12, RICORD-1A,
RICORD-1B, COVID-19-Italy, and COVID-19-China. AI segmentation achieved a mean
dice coefficient of 0.957. Attenuations measured by AI-ROI showed no
significant differences (p = 0.545) and a reduction of 71% time compared to
expert measurements. The area under the curve (AUC) of the steatosis
classification of AI-ROI is 0.921 (95% CI: 0.883 - 0.959). If performed as a
routine screening method, our AI protocol could potentially allow early
non-invasive, non-pharmacological preventative interventions for hepatic
steatosis. 1,014 expert-annotated liver segmentations of patients with hepatic
steatosis annotations can be downloaded here:
https://drive.google.com/drive/folders/1-g_zJeAaZXYXGqL1OeF6pUjr6KB0igJX.

Artificial Intelligence (AI) is one of the most transformative technologies
of the 21st century. The extent and scope of future AI capabilities remain a
key uncertainty, with widespread disagreement on timelines and potential
impacts. As nations and technology companies race toward greater complexity and
autonomy in AI systems, there are concerns over the extent of integration and
oversight of opaque AI decision processes. This is especially true in the
subfield of machine learning (ML), where systems learn to optimize objectives
without human assistance. Objectives can be imperfectly specified or executed
in an unexpected or potentially harmful way. This becomes more concerning as
systems increase in power and autonomy, where an abrupt capability jump could
result in unexpected shifts in power dynamics or even catastrophic failures.
This study presents a hierarchical complex systems framework to model AI risk
and provide a template for alternative futures analysis. Survey data were
collected from domain experts in the public and private sectors to classify AI
impact and likelihood. The results show increased uncertainty over the powerful
AI agent scenario, confidence in multiagent environments, and increased concern
over AI alignment failures and influence-seeking behavior.

With the rise of edge computing, various AI services are expected to be
available at a mobile side through the inference based on deep neural network
(DNN) operated at the network edge, called edge inference (EI). On the other
hand, the resulting AI quality (e.g., mean average precision in objective
detection) has been regarded as a given factor, and AI quality control has yet
to be explored despite its importance in addressing the diverse demands of
different users. This work aims at tackling the issue by proposing a feature
hierarchical EI (FHEI), comprising feature network and inference network
deployed at an edge server and corresponding mobile, respectively.
Specifically, feature network is designed based on feature hierarchy, a
one-directional feature dependency with a different scale. A higher scale
feature requires more computation and communication loads while it provides a
better AI quality. The tradeoff enables FHEI to control AI quality gradually
w.r.t. communication and computation loads, leading to deriving a
near-to-optimal solution to maximize multi-user AI quality under the
constraints of uplink \& downlink transmissions and edge server and mobile
computation capabilities. It is verified by extensive simulations that the
proposed joint communication-and-computation control on FHEI architecture
always outperforms several benchmarks by differentiating each user's AI quality
depending on the communication and computation conditions.

Recent advances in artificial intelligence (AI) have significantly
intensified research in the geoscience and remote sensing (RS) field. AI
algorithms, especially deep learning-based ones, have been developed and
applied widely to RS data analysis. The successful application of AI covers
almost all aspects of Earth observation (EO) missions, from low-level vision
tasks like super-resolution, denoising and inpainting, to high-level vision
tasks like scene classification, object detection and semantic segmentation.
While AI techniques enable researchers to observe and understand the Earth more
accurately, the vulnerability and uncertainty of AI models deserve further
attention, considering that many geoscience and RS tasks are highly
safety-critical. This paper reviews the current development of AI security in
the geoscience and RS field, covering the following five important aspects:
adversarial attack, backdoor attack, federated learning, uncertainty and
explainability. Moreover, the potential opportunities and trends are discussed
to provide insights for future research. To the best of the authors' knowledge,
this paper is the first attempt to provide a systematic review of AI
security-related research in the geoscience and RS community. Available code
and datasets are also listed in the paper to move this vibrant field of
research forward.

Artificial Intelligence (AI) technologies have been developed rapidly, and
AI-based systems have been widely used in various application domains with
opportunities and challenges. However, little is known about the architecture
decisions made in AI-based systems development, which has a substantial impact
on the success and sustainability of these systems. To this end, we conducted
an empirical study by collecting and analyzing the data from Stack Overflow
(SO) and GitHub. More specifically, we searched on SO with six sets of keywords
and explored 32 AI-based projects on GitHub, and finally we collected 174 posts
and 128 GitHub issues related to architecture decisions. The results show that
in AI-based systems development (1) architecture decisions are expressed in six
linguistic patterns, among which Solution Proposal and Information Giving are
most frequently used, (2) Technology Decision, Component Decision, and Data
Decision are the main types of architecture decisions made, (3) Game is the
most common application domain among the eighteen application domains
identified, (4) the dominant quality attribute considered in architecture
decision-making is Performance, and (5) the main limitations and challenges
encountered by practitioners in making architecture decisions are Design Issues
and Data Issues. Our results suggest that the limitations and challenges when
making architecture decisions in AI-based systems development are highly
specific to the characteristics of AI-based systems and are mainly of technical
nature, which need to be properly confronted.

Interactive Artificial Intelligence (AI) agents are becoming increasingly
prevalent in society. However, application of such systems without
understanding them can be problematic. Black-box AI systems can lead to
liability and accountability issues when they produce an incorrect decision.
Explainable AI (XAI) seeks to bridge the knowledge gap, between developers and
end-users, by offering insights into how an AI algorithm functions. Many modern
algorithms focus on making the AI model "transparent", i.e. unveil the inherent
functionality of the agent in a simpler format. However, these approaches do
not cater to end-users of these systems, as users may not possess the requisite
knowledge to understand these explanations in a reasonable amount of time.
Therefore, to be able to develop suitable XAI methods, we need to understand
the factors which influence subjective perception and objective usability. In
this paper, we present a novel user-study which studies four differing XAI
modalities commonly employed in prior work for explaining AI behavior, i.e.
Decision Trees, Text, Programs. We study these XAI modalities in the context of
explaining the actions of a self-driving car on a highway, as driving is an
easily understandable real-world task and self-driving cars is a keen area of
interest within the AI community. Our findings highlight internal consistency
issues wherein participants perceived language explanations to be significantly
more usable, however participants were better able to objectively understand
the decision making process of the car through a decision tree explanation. Our
work also provides further evidence of importance of integrating user-specific
and situational criteria into the design of XAI systems. Our findings show that
factors such as computer science experience, and watching the car succeed or
fail can impact the perception and usefulness of the explanation.

AI alignment is about ensuring AI systems only pursue goals and activities
that are beneficial to humans. Most of the current approach to AI alignment is
to learn what humans value from their behavioural data. This paper proposes a
different way of looking at the notion of alignment, namely by introducing AI
Alignment Dialogues: dialogues with which users and agents try to achieve and
maintain alignment via interaction. We argue that alignment dialogues have a
number of advantages in comparison to data-driven approaches, especially for
behaviour support agents, which aim to support users in achieving their desired
future behaviours rather than their current behaviours. The advantages of
alignment dialogues include allowing the users to directly convey higher-level
concepts to the agent, and making the agent more transparent and trustworthy.
In this paper we outline the concept and high-level structure of alignment
dialogues. Moreover, we conducted a qualitative focus group user study from
which we developed a model that describes how alignment dialogues affect users,
and created design suggestions for AI alignment dialogues. Through this we
establish foundations for AI alignment dialogues and shed light on what
requires further development and research.

Music is a powerful medium for altering the emotional state of the listener.
In recent years, with significant advancement in computing capabilities,
artificial intelligence-based (AI-based) approaches have become popular for
creating affective music generation (AMG) systems that are empowered with the
ability to generate affective music. Entertainment, healthcare, and
sensor-integrated interactive system design are a few of the areas in which
AI-based affective music generation (AI-AMG) systems may have a significant
impact. Given the surge of interest in this topic, this article aims to provide
a comprehensive review of AI-AMG systems. The main building blocks of an AI-AMG
system are discussed, and existing systems are formally categorized based on
the core algorithm used for music generation. In addition, this article
discusses the main musical features employed to compose affective music, along
with the respective AI-based approaches used for tailoring them. Lastly, the
main challenges and open questions in this field, as well as their potential
solutions, are presented to guide future research. We hope that this review
will be useful for readers seeking to understand the state-of-the-art in AI-AMG
systems, and gain an overview of the methods used for developing them, thereby
helping them explore this field in the future.

The use of AI in healthcare is designed to improve care delivery and augment
the decisions of providers to enhance patient outcomes. When deployed in
clinical settings, the interaction between providers and AI is a critical
component for measuring and understanding the effectiveness of these digital
tools on broader health outcomes. Even in cases where AI algorithms have high
diagnostic accuracy, healthcare providers often still rely on their experience
and sometimes gut feeling to make a final decision. Other times, providers rely
unquestioningly on the outputs of the AI models, which leads to a concern about
over-reliance on the technology. The purpose of this research was to understand
how reliant drug shop dispensers were on AI-powered technologies when
determining a differential diagnosis for a presented clinical case vignette. We
explored how the drug dispensers responded to technology that is framed as
always correct in an attempt to measure whether they begin to rely on it
without any critical thought of their own. We found that dispensers relied on
the decision made by the AI 25 percent of the time, even when the AI provided
no explanation for its decision.

Medical devices and artificial intelligence systems rapidly transform
healthcare provisions. At the same time, due to their nature, AI in or as
medical devices might get exposed to cyberattacks, leading to patient safety
and security risks. This book chapter is divided into three parts. The first
part starts by setting the scene where we explain the role of cybersecurity in
healthcare. Then, we briefly define what we refer to when we talk about AI that
is considered a medical device by itself or supports one. To illustrate the
risks such medical devices pose, we provide three examples: the poisoning of
datasets, social engineering, and data or source code extraction. In the second
part, the paper provides an overview of the European Union's regulatory
framework relevant for ensuring the cybersecurity of AI as or in medical
devices (MDR, NIS Directive, Cybersecurity Act, GDPR, the AI Act proposal and
the NIS 2 Directive proposal). Finally, the third part of the paper examines
possible challenges stemming from the EU regulatory framework. In particular,
we look toward the challenges deriving from the two legislative proposals and
their interaction with the existing legislation concerning AI medical devices'
cybersecurity. They are structured as answers to the following questions: (1)
how will the AI Act interact with the MDR regarding the cybersecurity and
safety requirements?; (2) how should we interpret incident notification
requirements from the NIS 2 Directive proposal and MDR?; and (3) what are the
consequences of the evolving term of critical infrastructures?
  [This is a draft chapter. The final version will be available in Research
Handbook on Health, AI and the Law edited by Barry Solaiman & I. Glenn Cohen,
forthcoming 2023, Edward Elgar Publishing Ltd]

AI-supported programming has arrived, as shown by the introduction and
successes of large language models for code, such as Copilot/Codex
(Github/OpenAI) and AlphaCode (DeepMind). Above human average performance on
programming challenges is now possible. However, software engineering is much
more than solving programming contests. Moving beyond code completion to
AI-supported software engineering will require an AI system that can, among
other things, understand how to avoid code smells, to follow language idioms,
and eventually (maybe!) propose rational software designs. In this study, we
explore the current limitations of AI-supported code completion tools like
Copilot and offer a simple taxonomy for understanding the classification of
AI-supported code completion tools in this space. We first perform an
exploratory study on Copilot's code suggestions for language idioms and code
smells. Copilot does not follow language idioms and avoid code smells in most
of our test scenarios. We then conduct additional investigation to determine
the current boundaries of AI-supported code completion tools like Copilot by
introducing a taxonomy of software abstraction hierarchies where 'basic
programming functionality' such as code compilation and syntax checking is at
the least abstract level, software architecture analysis and design are at the
most abstract level. We conclude by providing a discussion on challenges for
future development of AI-supported code completion tools to reach the design
level of abstraction in our taxonomy.

Artificial Intelligence systems, which benefit from the availability of
large-scale datasets and increasing computational power, have become effective
solutions to various critical tasks, such as natural language understanding,
speech recognition, and image processing. The advancement of these AI systems
is inseparable from open-source software (OSS). This paper presents an
empirical study that investigates the issues in the repositories of open-source
AI repositories to assist developers in understanding problems during the
process of employing AI systems. We collect 576 repositories from the
PapersWithCode platform. Among these repositories, we find 24,953 issues by
utilizing GitHub REST APIs. Our empirical study includes three phases. First,
we manually analyze these issues to categorize the problems that developers are
likely to encounter in open-source AI repositories. Specifically, we provide a
taxonomy of 13 categories related to AI systems. The two most common issues are
runtime errors (23.18%) and unclear instructions (19.53%). Second, we see that
67.5% of issues are closed. We also find that half of these issues resolve
within four days. Moreover, issue management features, e.g., label and assign,
are not widely adopted in open-source AI repositories. In particular, only
7.81% and 5.9% of repositories label issues and assign these issues to
assignees, respectively. Finally, we empirically show that employing GitHub
issue management features and writing issues with detailed descriptions
facilitate the resolution of issues. Based on our findings, we make
recommendations for developers to help better manage the issues of open-source
AI repositories and improve their quality.

Systems with artificial intelligence components, so-called AI-based systems,
have gained considerable attention recently. However, many organizations have
issues with achieving production readiness with such systems. As a means to
improve certain software quality attributes and to address frequently occurring
problems, design patterns represent proven solution blueprints. While new
patterns for AI-based systems are emerging, existing patterns have also been
adapted to this new context.
  The goal of this study is to provide an overview of design patterns for
AI-based systems, both new and adapted ones. We want to collect and categorize
patterns, and make them accessible for researchers and practitioners. To this
end, we first performed a multivocal literature review (MLR) to collect design
patterns used with AI-based systems. We then integrated the created pattern
collection into a web-based pattern repository to make the patterns browsable
and easy to find.
  As a result, we selected 51 resources (35 white and 16 gray ones), from which
we extracted 70 unique patterns used for AI-based systems. Among these are 34
new patterns and 36 traditional ones that have been adapted to this context.
Popular pattern categories include "architecture" (25 patterns), "deployment"
(16), "implementation" (9), or "security & safety" (9). While some patterns
with four or more mentions already seem established, the majority of patterns
have only been mentioned once or twice (51 patterns). Our results in this
emerging field can be used by researchers as a foundation for follow-up studies
and by practitioners to discover relevant patterns for informing the design of
AI-based systems.

User trust in Artificial Intelligence (AI) enabled systems has been
increasingly recognized and proven as a key element to fostering adoption. It
has been suggested that AI-enabled systems must go beyond technical-centric
approaches and towards embracing a more human centric approach, a core
principle of the human-computer interaction (HCI) field. This review aims to
provide an overview of the user trust definitions, influencing factors, and
measurement methods from 23 empirical studies to gather insight for future
technical and design strategies, research, and initiatives to calibrate the
user AI relationship. The findings confirm that there is more than one way to
define trust. Selecting the most appropriate trust definition to depict user
trust in a specific context should be the focus instead of comparing
definitions. User trust in AI-enabled systems is found to be influenced by
three main themes, namely socio-ethical considerations, technical and design
features, and user characteristics. User characteristics dominate the findings,
reinforcing the importance of user involvement from development through to
monitoring of AI enabled systems. In conclusion, user trust needs to be
addressed directly in every context where AI-enabled systems are being used or
discussed. In addition, calibrating the user-AI relationship requires finding
the optimal balance that works for not only the user but also the system.

Recognizing the tremendous improvements that the integration of generative AI
can bring to intelligent transportation systems, this article explores the
integration of generative AI technologies in vehicular networks, focusing on
their potential applications and challenges. Generative AI, with its
capabilities of generating realistic data and facilitating advanced
decision-making processes, enhances various applications when combined with
vehicular networks, such as navigation optimization, traffic prediction, data
generation, and evaluation. Despite these promising applications, the
integration of generative AI with vehicular networks faces several challenges,
such as real-time data processing and decision-making, adapting to dynamic and
unpredictable environments, as well as privacy and security concerns. To
address these challenges, we propose a multi-modality semantic-aware framework
to enhance the service quality of generative AI. By leveraging multi-modal and
semantic communication technologies, the framework enables the use of text and
image data for creating multi-modal content, providing more reliable guidance
to receiving vehicles and ultimately improving system usability and efficiency.
To further improve the reliability and efficiency of information transmission
and reconstruction within the framework, taking generative AI-enabled
vehicle-to-vehicle (V2V) as a case study, a deep reinforcement learning
(DRL)-based approach is proposed for resource allocation. Finally, we discuss
potential research directions and anticipated advancements in the field of
generative AI-enabled vehicular networks.

Artificial Intelligence (AI) has become a disruptive technology, promising to
grant a significant economic and strategic advantage to the nations that
harness its power. China, with its recent push towards AI adoption, is
challenging the U.S.'s position as the global leader in this field. Given AI's
massive potential, as well as the fierce geopolitical tensions between the two
nations, a number of policies have been put in place that discourage AI
scientists from migrating to, or collaborating with, the other country.
However, the extents of such brain drain and cross-border collaboration are not
fully understood. Here, we analyze a dataset of over 350,000 AI scientists and
5,000,000 AI papers. We find that, since the year 2000, China and the U.S. have
been leading the field in terms of impact, novelty, productivity, and
workforce. Most AI scientists who migrate to China come from the U.S., and most
who migrate to the U.S. come from China, highlighting a notable brain drain in
both directions. Upon migrating from one country to the other, scientists
continue to collaborate frequently with the origin country. Although the number
of collaborations between the two countries has been increasing since the dawn
of the millennium, such collaborations continue to be relatively rare. A
matching experiment reveals that the two countries have always been more
impactful when collaborating than when each of them works without the other.
These findings suggest that instead of suppressing cross-border migration and
collaboration between the two nations, the field could benefit from promoting
such activities.

Electronic health records (EHRs) serve as an essential data source for the
envisioned artificial intelligence (AI)-driven transformation in healthcare.
However, clinician biases reflected in EHR notes can lead to AI models
inheriting and amplifying these biases, perpetuating health disparities. This
study investigates the impact of stigmatizing language (SL) in EHR notes on
mortality prediction using a Transformer-based deep learning model and
explainable AI (XAI) techniques. Our findings demonstrate that SL written by
clinicians adversely affects AI performance, particularly so for black
patients, highlighting SL as a source of racial disparity in AI model
development. To explore an operationally efficient way to mitigate SL's impact,
we investigate patterns in the generation of SL through a clinicians'
collaborative network, identifying central clinicians as having a stronger
impact on racial disparity in the AI model. We find that removing SL written by
central clinicians is a more efficient bias reduction strategy than eliminating
all SL in the entire corpus of data. This study provides actionable insights
for responsible AI development and contributes to understanding clinician
behavior and EHR note writing in healthcare.

Low-code programming allows citizen developers to create programs with
minimal coding effort, typically via visual (e.g. drag-and-drop) interfaces. In
parallel, recent AI-powered tools such as Copilot and ChatGPT generate programs
from natural language instructions. We argue that these modalities are
complementary: tools like ChatGPT greatly reduce the need to memorize large
APIs but still require their users to read (and modify) programs, whereas
visual tools abstract away most or all programming but struggle to provide easy
access to large APIs. At their intersection, we propose LowCoder, the first
low-code tool for developing AI pipelines that supports both a visual
programming interface (LowCoder_VP) and an AI-powered natural language
interface (LowCoder_NL). We leverage this tool to provide some of the first
insights into whether and how these two modalities help programmers by
conducting a user study. We task 20 developers with varying levels of AI
expertise with implementing four ML pipelines using LowCoder, replacing the
LowCoder_NL component with a simple keyword search in half the tasks. Overall,
we find that LowCoder is especially useful for (i) Discoverability: using
LowCoder_NL, participants discovered new operators in 75% of the tasks,
compared to just 32.5% and 27.5% using web search or scrolling through options
respectively in the keyword-search condition, and (ii) Iterative Composition:
82.5% of tasks were successfully completed and many initial pipelines were
further successfully improved. Qualitative analysis shows that AI helps users
discover how to implement constructs when they know what to do, but still fails
to support novices when they lack clarity on what they want to accomplish.
Overall, our work highlights the benefits of combining the power of AI with
low-code programming.

AI-based text-to-image generation has undergone a significant leap in the
production of visually comprehensive and aesthetic imagery over the past year,
to the point where differentiating between a man-made piece of art and an
AI-generated image is becoming more difficult. Generative Models such as Stable
Diffusion, Midjourney and others are expected to affect several major
industries in technological and ethical aspects. Striking the balance between
raising human standard of life and work vs exploiting one group of people to
enrich another is a complex and crucial part of the discussion. Due to the
rapid growth of this technology, the way in which its models operate, and gray
area legalities, visual and artistic domains - including the video game
industry, are at risk of being taken over from creators by AI infrastructure
owners. This paper is a literature review examining the concerns facing both AI
developers and users today, including identity theft, data laundering and more.
It discusses legalization challenges and ethical concerns, and concludes with
how AI generative models can be tremendously useful in streamlining the process
of visual creativity in both static and interactive media given proper
regulation.
  Keywords: AI text-to-image generation, Midjourney, Stable Diffusion, AI
Ethics, Game Design, Digital Art, Data Laundering

Despite the successes of recent developments in visual AI, different
shortcomings still exist; from missing exact logical reasoning, to abstract
generalization abilities, to understanding complex and noisy scenes.
Unfortunately, existing benchmarks, were not designed to capture more than a
few of these aspects. Whereas deep learning datasets focus on visually complex
data but simple visual reasoning tasks, inductive logic datasets involve
complex logical learning tasks, however, lack the visual component. To address
this, we propose the visual logical learning dataset, V-LoL, that seamlessly
combines visual and logical challenges. Notably, we introduce the first
instantiation of V-LoL, V-LoL-Trains, -- a visual rendition of a classic
benchmark in symbolic AI, the Michalski train problem. By incorporating
intricate visual scenes and flexible logical reasoning tasks within a versatile
framework, V-LoL-Trains provides a platform for investigating a wide range of
visual logical learning challenges. We evaluate a variety of AI systems
including traditional symbolic AI, neural AI, as well as neuro-symbolic AI. Our
evaluations demonstrate that even state-of-the-art AI faces difficulties in
dealing with visual logical learning challenges, highlighting unique advantages
and limitations specific to each methodology. Overall, V-LoL opens up new
avenues for understanding and enhancing current abilities in visual logical
learning for AI systems.

Technical debt is a well-known challenge in software development, and its
negative impact on software quality, maintainability, and performance is widely
recognized. In recent years, artificial intelligence (AI) has proven to be a
promising approach to assist in managing technical debt. This paper presents a
comprehensive literature review of existing research on the use of AI powered
tools for technical debt avoidance in software development. In this literature
review we analyzed 15 related research papers which covers various AI-powered
techniques, such as code analysis and review, automated testing, code
refactoring, predictive maintenance, code generation, and code documentation,
and explores their effectiveness in addressing technical debt. The review also
discusses the benefits and challenges of using AI for technical debt
management, provides insights into the current state of research, and
highlights gaps and opportunities for future research. The findings of this
review suggest that AI has the potential to significantly improve technical
debt management in software development, and that existing research provides
valuable insights into how AI can be leveraged to address technical debt
effectively and efficiently. However, the review also highlights several
challenges and limitations of current approaches, such as the need for
high-quality data and ethical considerations and underscores the importance of
further research to address these issues. The paper provides a comprehensive
overview of the current state of research on AI for technical debt avoidance
and offers practical guidance for software development teams seeking to
leverage AI in their development processes to mitigate technical debt
effectively

Just as other disciplines, the humanities explore how computational research
approaches and tools can meaningfully contribute to scholarly knowledge
production. We approach the design of computational tools through the
analytical lens of 'human-AI collaboration.' However, there is no generalizable
concept of what constitutes 'meaningful' human-AI collaboration. In terms of
genuinely human competencies, we consider criticality and reflection as guiding
principles of scholarly knowledge production. Although (designing for)
reflection is a recurring topic in CSCW and HCI discourses, it has not been
centered in work on human-AI collaboration. We posit that integrating both
concepts is a viable approach to supporting 'meaningful' human-AI collaboration
in the humanities. Our research, thus, is guided by the question of how
critical reflection can be enabled in human-AI collaboration. We address this
question with a use case that centers on computer vision (CV) tools for art
historical image retrieval. Specifically, we conducted a qualitative interview
study with art historians and extended the interviews with a think-aloud
software exploration. We observed and recorded our participants' interaction
with a ready-to-use CV tool in a possible research scenario. We found that
critical reflection, indeed, constitutes a core prerequisite for 'meaningful'
human-AI collaboration in humanities research contexts. However, we observed
that critical reflection was not fully realized during interaction with the CV
tool. We interpret this divergence as supporting our hypothesis that
computational tools need to be intentionally designed in such a way that they
actively scaffold and support critical reflection during interaction. Based on
our findings, we suggest four empirically grounded design implications for
'critical-reflective human-AI collaboration'.

Despite recent efforts by the Artificial Intelligence (AI) community to move
towards standardised procedures for documenting models, methods, systems or
datasets, there is currently no methodology focused on use cases aligned with
the risk-based approach of the European AI Act (AI Act). In this paper, we
propose a new framework for the documentation of use cases, that we call "use
case cards", based on the use case modelling included in the Unified Markup
Language (UML) standard. Unlike other documentation methodologies, we focus on
the intended purpose and operational use of an AI system. It consists of two
main parts. Firstly, a UML-based template, tailored to allow implicitly
assessing the risk level of the AI system and defining relevant requirements.
Secondly, a supporting UML diagram designed to provide information about the
system-user interactions and relationships. The proposed framework is the
result of a co-design process involving a relevant team of EU policy experts
and scientists. We have validated our proposal with 11 experts with different
backgrounds and a reasonable knowledge of the AI Act as a prerequisite. We
provide the 5 "use case cards" used in the co-design and validation process.
"Use case cards" allows framing and contextualising use cases in an effective
way, and we hope this methodology can be a useful tool for policy makers and
providers for documenting use cases, assessing the risk level, adapting the
different requirements and building a catalogue of existing usages of AI.

The increasing prevalence of Artificial Intelligence (AI) in safety-critical
contexts such as air-traffic control leads to systems that are practical and
efficient, and to some extent explainable to humans to be trusted and accepted.
The present structured literature analysis examines n = 236 articles on the
requirements for the explainability and acceptance of AI. Results include a
comprehensive review of n = 48 articles on information people need to perceive
an AI as explainable, the information needed to accept an AI, and
representation and interaction methods promoting trust in an AI. Results
indicate that the two main groups of users are developers who require
information about the internal operations of the model and end users who
require information about AI results or behavior. Users' information needs vary
in specificity, complexity, and urgency and must consider context, domain
knowledge, and the user's cognitive resources. The acceptance of AI systems
depends on information about the system's functions and performance, privacy
and ethical considerations, as well as goal-supporting information tailored to
individual preferences and information to establish trust in the system.
Information about the system's limitations and potential failures can increase
acceptance and trust. Trusted interaction methods are human-like, including
natural language, speech, text, and visual representations such as graphs,
charts, and animations. Our results have significant implications for future
human-centric AI systems being developed. Thus, they are suitable as input for
further application-specific investigations of user needs.

Artificial Intelligence (AI), a cornerstone of 21st-century technology, has
seen remarkable growth in China. In this paper, we examine China's AI
development process, demonstrating that it is characterized by rapid learning
and differentiation, surpassing the export-oriented growth propelled by Foreign
Direct Investment seen in earlier Asian industrializers.
  Our data indicates that China currently leads the USA in the volume of
AI-related research papers. However, when we delve into the quality of these
papers based on specific metrics, the USA retains a slight edge. Nevertheless,
the pace and scale of China's AI development remain noteworthy.
  We attribute China's accelerated AI progress to several factors, including
global trends favoring open access to algorithms and research papers,
contributions from China's broad diaspora and returnees, and relatively lax
data protection policies.
  In the vein of our research, we have developed a novel measure for gauging
China's imitation of US research. Our analysis shows that by 2018, the time lag
between China and the USA in addressing AI research topics had evaporated. This
finding suggests that China has effectively bridged a significant knowledge gap
and could potentially be setting out on an independent research trajectory.
  While this study compares China and the USA exclusively, it's important to
note that research collaborations between these two nations have resulted in
more highly cited work than those produced by either country independently.
This underscores the power of international cooperation in driving scientific
progress in AI.

Recent advances in artificial intelligence (AI) have produced highly capable
and controllable systems. This creates unprecedented opportunities for
structured reasoning as well as collaboration among multiple AI systems and
humans. To fully realize this potential, it is essential to develop a
principled way of designing and studying such structured interactions. For this
purpose, we introduce the conceptual framework Flows. Flows are self-contained
building blocks of computation, with an isolated state, communicating through a
standardized message-based interface. This modular design simplifies the
process of creating Flows by allowing them to be recursively composed into
arbitrarily nested interactions and is inherently concurrency-friendly.
Crucially, any interaction can be implemented using this framework, including
prior work on AI-AI and human-AI interactions, prompt engineering schemes, and
tool augmentation. We demonstrate the potential of Flows on competitive coding,
a challenging task on which even GPT-4 struggles. Our results suggest that
structured reasoning and collaboration substantially improve generalization,
with AI-only Flows adding +21 and human-AI Flows adding +54 absolute points in
terms of solve rate. To support rapid and rigorous research, we introduce the
aiFlows library embodying Flows. The aiFlows library is available at
https://github.com/epfl-dlab/aiflows. Data and Flows for reproducing our
experiments are available at https://github.com/epfl-dlab/cc_flows.

Can an AI win Ghana's National Science and Maths Quiz (NSMQ)? That is the
question we seek to answer in the NSMQ AI project, an open-source project that
is building AI to compete live in the NSMQ and win. The NSMQ is an annual live
science and mathematics competition for senior secondary school students in
Ghana in which 3 teams of 2 students compete by answering questions across
biology, chemistry, physics, and math in 5 rounds over 5 progressive stages
until a winning team is crowned for that year. The NSMQ is an exciting live
quiz competition with interesting technical challenges across speech-to-text,
text-to-speech, question-answering, and human-computer interaction. In this
ongoing work that began in January 2023, we give an overview of the project,
describe each of the teams, progress made thus far, and the next steps toward
our planned launch and debut of the AI in October for NSMQ 2023. An AI that
conquers this grand challenge can have real-world impact on education such as
enabling millions of students across Africa to have one-on-one learning support
from this AI.

In recent years, Artificial Intelligence (AI) systems have surpassed human
intelligence in a variety of computational tasks. However, AI systems, like
humans, make mistakes, have blind spots, hallucinate, and struggle to
generalize to new situations. This work explores whether AI can benefit from
creative decision-making mechanisms when pushed to the limits of its
computational rationality. In particular, we investigate whether a team of
diverse AI systems can outperform a single AI in challenging tasks by
generating more ideas as a group and then selecting the best ones. We study
this question in the game of chess, the so-called drosophila of AI. We build on
AlphaZero (AZ) and extend it to represent a league of agents via a
latent-conditioned architecture, which we call AZ_db. We train AZ_db to
generate a wider range of ideas using behavioral diversity techniques and
select the most promising ones with sub-additive planning. Our experiments
suggest that AZ_db plays chess in diverse ways, solves more puzzles as a group
and outperforms a more homogeneous team. Notably, AZ_db solves twice as many
challenging puzzles as AZ, including the challenging Penrose positions. When
playing chess from different openings, we notice that players in AZ_db
specialize in different openings, and that selecting a player for each opening
using sub-additive planning results in a 50 Elo improvement over AZ. Our
findings suggest that diversity bonuses emerge in teams of AI agents, just as
they do in teams of humans and that diversity is a valuable asset in solving
computationally hard problems.

Today's AI systems for medical decision support often succeed on benchmark
datasets in research papers but fail in real-world deployment. This work
focuses on the decision making of sepsis, an acute life-threatening systematic
infection that requires an early diagnosis with high uncertainty from the
clinician. Our aim is to explore the design requirements for AI systems that
can support clinical experts in making better decisions for the early diagnosis
of sepsis. The study begins with a formative study investigating why clinical
experts abandon an existing AI-powered Sepsis predictive module in their
electrical health record (EHR) system. We argue that a human-centered AI system
needs to support human experts in the intermediate stages of a medical
decision-making process (e.g., generating hypotheses or gathering data),
instead of focusing only on the final decision. Therefore, we build SepsisLab
based on a state-of-the-art AI algorithm and extend it to predict the future
projection of sepsis development, visualize the prediction uncertainty, and
propose actionable suggestions (i.e., which additional laboratory tests can be
collected) to reduce such uncertainty. Through heuristic evaluation with six
clinicians using our prototype system, we demonstrate that SepsisLab enables a
promising human-AI collaboration paradigm for the future of AI-assisted sepsis
diagnosis and other high-stakes medical decision making.

Computed Tomography (CT) is commonly used to image acute ischemic stroke
(AIS) patients, but its interpretation by radiologists is time-consuming and
subject to inter-observer variability. Deep learning (DL) techniques can
provide automated CT brain scan assessment, but usually require annotated
images. Aiming to develop a DL method for AIS using labelled but not annotated
CT brain scans from patients with AIS, we designed a convolutional neural
network-based DL algorithm using routinely-collected CT brain scans from the
Third International Stroke Trial (IST-3), which were not acquired using strict
research protocols. The DL model aimed to detect AIS lesions and classify the
side of the brain affected. We explored the impact of AIS lesion features,
background brain appearances, and timing on DL performance. From 5772 unique CT
scans of 2347 AIS patients (median age 82), 54% had visible AIS lesions
according to expert labelling. Our best-performing DL method achieved 72%
accuracy for lesion presence and side. Lesions that were larger (80% accuracy)
or multiple (87% accuracy for two lesions, 100% for three or more), were better
detected. Follow-up scans had 76% accuracy, while baseline scans 67% accuracy.
Chronic brain conditions reduced accuracy, particularly non-stroke lesions and
old stroke lesions (32% and 31% error rates respectively). DL methods can be
designed for AIS lesion detection on CT using the vast quantities of
routinely-collected CT brain scan data. Ultimately, this should lead to more
robust and widely-applicable methods.

The tremendous rise of generative AI has reached every part of society -
including the news environment. There are many concerns about the individual
and societal impact of the increasing use of generative AI, including issues
such as disinformation and misinformation, discrimination, and the promotion of
social tensions. However, research on anticipating the impact of generative AI
is still in its infancy and mostly limited to the views of technology
developers and/or researchers. In this paper, we aim to broaden the perspective
and capture the expectations of three stakeholder groups (news consumers;
technology developers; content creators) about the potential negative impacts
of generative AI, as well as mitigation strategies to address these.
Methodologically, we apply scenario writing and use participatory foresight in
the context of a survey (n=119) to delve into cognitively diverse imaginations
of the future. We qualitatively analyze the scenarios using thematic analysis
to systematically map potential impacts of generative AI on the news
environment, potential mitigation strategies, and the role of stakeholders in
causing and mitigating these impacts. In addition, we measure respondents'
opinions on a specific mitigation strategy, namely transparency obligations as
suggested in Article 52 of the draft EU AI Act. We compare the results across
different stakeholder groups and elaborate on the (non-) presence of different
expected impacts across these groups. We conclude by discussing the usefulness
of scenario-writing and participatory foresight as a toolbox for generative AI
impact assessment.

In maritime traffic surveillance, detecting illegal activities, such as
illegal fishing or transshipment of illicit products is a crucial task of the
coastal administration. In the open sea, one has to rely on Automatic
Identification System (AIS) message transmitted by on-board transponders, which
are captured by surveillance satellites. However, insincere vessels often
intentionally shut down their AIS transponders to hide illegal activities. In
the open sea, it is very challenging to differentiate intentional AIS shutdowns
from missing reception due to protocol limitations, bad weather conditions or
restricting satellite positions. This paper presents a novel approach for the
detection of abnormal AIS missing reception based on self-supervised deep
learning techniques and transformer models. Using historical data, the trained
model predicts if a message should be received in the upcoming minute or not.
Afterwards, the model reports on detected anomalies by comparing the prediction
with what actually happens. Our method can process AIS messages in real-time,
in particular, more than 500 Millions AIS messages per month, corresponding to
the trajectories of more than 60 000 ships. The method is evaluated on 1-year
of real-world data coming from four Norwegian surveillance satellites. Using
related research results, we validated our method by rediscovering already
detected intentional AIS shutdowns.

Generative Artificial Intelligence (AI) is a cutting-edge technology capable
of producing text, images, and various media content leveraging generative
models and user prompts. Between 2022 and 2023, generative AI surged in
popularity with a plethora of applications spanning from AI-powered movies to
chatbots. In this paper, we delve into the potential of generative AI within
the realm of the World Wide Web, specifically focusing on image generation. Web
developers already harness generative AI to help crafting text and images,
while Web browsers might use it in the future to locally generate images for
tasks like repairing broken webpages, conserving bandwidth, and enhancing
privacy. To explore this research area, we have developed WebDiffusion, a tool
that allows to simulate a Web powered by stable diffusion, a popular
text-to-image model, from both a client and server perspective. WebDiffusion
further supports crowdsourcing of user opinions, which we use to evaluate the
quality and accuracy of 409 AI-generated images sourced from 60 webpages. Our
findings suggest that generative AI is already capable of producing pertinent
and high-quality Web images, even without requiring Web designers to manually
input prompts, just by leveraging contextual information available within the
webpages. However, we acknowledge that direct in-browser image generation
remains a challenge, as only highly powerful GPUs, such as the A40 and A100,
can (partially) compete with classic image downloads. Nevertheless, this
approach could be valuable for a subset of the images, for example when fixing
broken webpages or handling highly private content.

Educational disparities within the Dominican Republic (DR) have long-standing
origins rooted in economic, political, and social inequity. Addressing these
challenges has necessarily called for capacity building with respect to
educational materials, high-quality instruction, and structural resourcing.
Generative AI tools like ChatGPT have begun to pique the interest of Dominican
educators due to their perceived potential to bridge these educational gaps.
However, a substantial body of AI fairness literature has documented ways AI
disproportionately reinforces power dynamics reflective of jurisdictions
driving AI development and deployment policies, collectively termed the AI
Global North. As such, indiscriminate adoption of this technology for DR
education, even in part, risks perpetuating forms of digital coloniality.
Therefore, this paper centers embracing AI-facilitated educational reform by
critically examining how AI-driven tools like ChatGPT in DR education may
replicate facets of digital colonialism. We provide a concise overview of
20th-century Dominican education reforms following the 1916 US occupation.
Then, we employ identified neocolonial aspects historically shaping Dominican
education to interrogate the perceived advantages of ChatGPT for contemporary
Dominican education, as outlined by a Dominican scholar. This work invites AI
Global North & South developers, stakeholders, and Dominican leaders alike to
exercise a relational contextualization of data-centric epistemologies like
ChatGPT to reap its transformative benefits while remaining vigilant of
safeguarding Dominican digital sovereignty.

Artificial intelligence (AI) plays a pivotal role in various sectors,
influencing critical decision-making processes in our daily lives. Within the
AI landscape, novel AI paradigms, such as Federated Learning (FL), focus on
preserving data privacy while collaboratively training AI models. In such a
context, a group of experts from the European Commission (AI-HLEG) has
identified sustainable AI as one of the key elements that must be considered to
provide trustworthy AI. While existing literature offers several taxonomies and
solutions for assessing the trustworthiness of FL models, a significant gap
exists in considering sustainability and the carbon footprint associated with
FL. Thus, this work introduces the sustainability pillar to the most recent and
comprehensive trustworthy FL taxonomy, making this work the first to address
all AI-HLEG requirements. The sustainability pillar assesses the FL system
environmental impact, incorporating notions and metrics for hardware
efficiency, federation complexity, and energy grid carbon intensity. Then, this
work designs and implements an algorithm for evaluating the trustworthiness of
FL models by incorporating the sustainability pillar. Extensive evaluations
with the FederatedScope framework and various scenarios varying federation
participants, complexities, hardware, and energy grids demonstrate the
usefulness of the proposed solution.

Case studies commonly form the pedagogical backbone in law, ethics, and many
other domains that face complex and ambiguous societal questions informed by
human values. Similar complexities and ambiguities arise when we consider how
AI should be aligned in practice: when faced with vast quantities of diverse
(and sometimes conflicting) values from different individuals and communities,
with whose values is AI to align, and how should AI do so? We propose a
complementary approach to constitutional AI alignment, grounded in ideas from
case-based reasoning (CBR), that focuses on the construction of policies
through judgments on a set of cases. We present a process to assemble such a
case repository by: 1) gathering a set of ``seed'' cases -- questions one may
ask an AI system -- in a particular domain, 2) eliciting domain-specific key
dimensions for cases through workshops with domain experts, 3) using LLMs to
generate variations of cases not seen in the wild, and 4) engaging with the
public to judge and improve cases. We then discuss how such a case repository
could assist in AI alignment, both through directly acting as precedents to
ground acceptable behaviors, and as a medium for individuals and communities to
engage in moral reasoning around AI.

Artificial Intelligence Impact Assessments ("AIIAs"), a family of tools that
provide structured processes to imagine the possible impacts of a proposed AI
system, have become an increasingly popular proposal to govern AI systems.
Recent efforts from government or private-sector organizations have proposed
many diverse instantiations of AIIAs, which take a variety of forms ranging
from open-ended questionnaires to graded score-cards. However, to date that has
been limited evaluation of existing AIIA instruments. We conduct a classroom
study (N = 38) at a large research-intensive university (R1) in an elective
course focused on the societal and ethical implications of AI. We assign
students to different organizational roles (for example, an ML scientist or
product manager) and ask participant teams to complete one of three existing AI
impact assessments for one of two imagined generative AI systems. In our
thematic analysis of participants' responses to pre- and post-activity
questionnaires, we find preliminary evidence that impact assessments can
influence participants' perceptions of the potential risks of generative AI
systems, and the level of responsibility held by AI experts in addressing
potential harm. We also discover a consistent set of limitations shared by
several existing AIIA instruments, which we group into concerns about their
format and content, as well as the feasibility and effectiveness of the
activity in foreseeing and mitigating potential harms. Drawing on the findings
of this study, we provide recommendations for future work on developing and
validating AIIAs.

The unprecedented rapid growth of computing demand for AI is projected to
increase global annual datacenter (DC) growth from 7.2% to 11.3%. We project
the 5-year AI DC demand for several power grids and assess whether they will
allow desired AI growth (resource adequacy). If not, several "desperate
measures" -- grid policies that enable more load growth and maintain grid
reliability by sacrificing new DC reliability are considered.
  We find that two DC hotspots -- EirGrid (Ireland) and Dominion (US) -- will
have difficulty accommodating new DCs needed by the AI growth. In EirGrid,
relaxing new DC reliability guarantees increases the power available to
1.6x--4.1x while maintaining 99.6% actual power availability for the new DCs,
sufficient for the 5-year AI demand. In Dominion, relaxing reliability
guarantees increases available DC capacity similarly (1.5x--4.6x) but not
enough for the 5-year AI demand. New DCs only receive 89% power availability.
Study of other US power grids -- SPP, CAISO, ERCOT -- shows that sufficient
capacity exists for the projected AI load growth.
  Our results suggest the need to rethink adequacy assessment and also grid
planning and management. New research opportunities include coordinated
planning, reliability models that incorporate load flexibility, and adaptive
load abstractions.

The digital landscape of the Internet of Energy (IoE) is on the brink of a
revolutionary transformation with the integration of edge Artificial
Intelligence (AI). This comprehensive review elucidates the promise and
potential that edge AI holds for reshaping the IoE ecosystem. Commencing with a
meticulously curated research methodology, the article delves into the myriad
of edge AI techniques specifically tailored for IoE. The myriad benefits,
spanning from reduced latency and real-time analytics to the pivotal aspects of
information security, scalability, and cost-efficiency, underscore the
indispensability of edge AI in modern IoE frameworks. As the narrative
progresses, readers are acquainted with pragmatic applications and techniques,
highlighting on-device computation, secure private inference methods, and the
avant-garde paradigms of AI training on the edge. A critical analysis follows,
offering a deep dive into the present challenges including security concerns,
computational hurdles, and standardization issues. However, as the horizon of
technology ever expands, the review culminates in a forward-looking
perspective, envisaging the future symbiosis of 5G networks, federated edge AI,
deep reinforcement learning, and more, painting a vibrant panorama of what the
future beholds. For anyone vested in the domains of IoE and AI, this review
offers both a foundation and a visionary lens, bridging the present realities
with future possibilities.

With the advent of sophisticated artificial intelligence (AI) technologies,
the proliferation of deepfakes and the spread of m/disinformation have emerged
as formidable threats to the integrity of information ecosystems worldwide.
This paper provides an overview of the current literature. Within the frontier
AI's crucial application in developing defense mechanisms for detecting
deepfakes, we highlight the mechanisms through which generative AI based on
large models (LM-based GenAI) craft seemingly convincing yet fabricated
contents. We explore the multifaceted implications of LM-based GenAI on
society, politics, and individual privacy violations, underscoring the urgent
need for robust defense strategies. To address these challenges, in this study,
we introduce an integrated framework that combines advanced detection
algorithms, cross-platform collaboration, and policy-driven initiatives to
mitigate the risks associated with AI-Generated Content (AIGC). By leveraging
multi-modal analysis, digital watermarking, and machine learning-based
authentication techniques, we propose a defense mechanism adaptable to AI
capabilities of ever-evolving nature. Furthermore, the paper advocates for a
global consensus on the ethical usage of GenAI and implementing cyber-wellness
educational programs to enhance public awareness and resilience against
m/disinformation. Our findings suggest that a proactive and collaborative
approach involving technological innovation and regulatory oversight is
essential for safeguarding netizens while interacting with cyberspace against
the insidious effects of deepfakes and GenAI-enabled m/disinformation
campaigns.

The efficiency of an AI system is contingent upon its ability to align with
the specified requirements of a given task. How-ever, the inherent complexity
of tasks often introduces the potential for harmful implications or adverse
actions. This note explores the critical concept of capability within AI
systems, representing what the system is expected to deliver. The articulation
of capability involves specifying well-defined out-comes. Yet, the achievement
of this capability may be hindered by deficiencies in implementation and
testing, reflecting a gap in the system's competency (what it can do vs. what
it does successfully).
  A central challenge arises in elucidating the competency of an AI system to
execute tasks effectively. The exploration of system competency in AI remains
in its early stages, occasionally manifesting as confidence intervals denoting
the probability of success. Trust in an AI system hinges on the explicit
modeling and detailed specification of its competency, connected intricately to
the system's capability. This note explores this gap by proposing a framework
for articulating the competency of AI systems.
  Motivated by practical scenarios such as the Glass Door problem, where an
individual inadvertently encounters a glass obstacle due to a failure in their
competency, this research underscores the imperative of delving into competency
dynamics. Bridging the gap between capability and competency at a detailed
level, this note contributes to advancing the discourse on bolstering the
reliability of AI systems in real-world applications.

Climate change poses one of the most significant challenges to humanity. As a
result of these climatic changes, the frequency of weather, climate, and
water-related disasters has multiplied fivefold over the past 50 years,
resulting in over 2 million deaths and losses exceeding $3.64 trillion USD.
Leveraging AI-powered technologies for sustainable development and combating
climate change is a promising avenue. Numerous significant publications are
dedicated to using AI to improve renewable energy forecasting, enhance waste
management, and monitor environmental changes in real time. However, very few
research studies focus on making AI itself environmentally sustainable. This
oversight regarding the sustainability of AI within the field might be
attributed to a mindset gap and the absence of comprehensive energy datasets.
In addition, with the ubiquity of edge AI systems and applications, especially
on-device learning, there is a pressing need to measure, analyze, and optimize
their environmental sustainability, such as energy efficiency. To this end, in
this paper, we propose large-scale energy datasets for edge AI, named
DeepEn2023, covering a wide range of kernels, state-of-the-art deep neural
network models, and popular edge AI applications. We anticipate that DeepEn2023
will improve transparency in sustainability in on-device deep learning across a
range of edge AI systems and applications. For more information, including
access to the dataset and code, please visit
https://amai-gsu.github.io/DeepEn2023.

Artificial Intelligence (AI) and its associated applications are ubiquitous
in today's world, making it imperative that students and their teachers
understand how it works and the ramifications arising from its usage. In this
study, we investigate the experiences of seven teachers following their
implementation of modules from the MIT RAICA (Responsible AI for Computational
Action) curriculum. Through semi-structured interviews, we investigated their
instructional strategies as they engaged with the AI curriculum in their
classroom, how their teaching and learning beliefs about AI evolved with the
curriculum as well as how those beliefs impacted their implementation of the
curriculum. Our analysis suggests that the AI modules not only expanded our
teachers' knowledge in the field, but also prompted them to recognize its daily
applications and their ethical and societal implications, so that they could
better engage with the content they deliver to students. Teachers were able to
leverage their own interdisciplinary backgrounds to creatively introduce
foundational AI topics to students to maximize engagement and playful learning.
Our teachers advocated their need for better external support when navigating
technological resources, additional time for preparation given the novelty of
the curriculum, more flexibility within curriculum timelines, and additional
accommodations for students of determination. Our findings provide valuable
insights for enhancing future iterations of AI literacy curricula and teacher
professional development (PD) resources.

The expanding application of Artificial Intelligence (AI) in scientific
fields presents unprecedented opportunities for discovery and innovation.
However, this growth is not without risks. AI models in science, if misused,
can amplify risks like creation of harmful substances, or circumvention of
established regulations. In this study, we aim to raise awareness of the
dangers of AI misuse in science, and call for responsible AI development and
use in this domain. We first itemize the risks posed by AI in scientific
contexts, then demonstrate the risks by highlighting real-world examples of
misuse in chemical science. These instances underscore the need for effective
risk management strategies. In response, we propose a system called SciGuard to
control misuse risks for AI models in science. We also propose a red-teaming
benchmark SciMT-Safety to assess the safety of different systems. Our proposed
SciGuard shows the least harmful impact in the assessment without compromising
performance in benign tests. Finally, we highlight the need for a
multidisciplinary and collaborative effort to ensure the safe and ethical use
of AI models in science. We hope that our study can spark productive
discussions on using AI ethically in science among researchers, practitioners,
policymakers, and the public, to maximize benefits and minimize the risks of
misuse.

Explainability and Safety engender Trust. These require a model to exhibit
consistency and reliability. To achieve these, it is necessary to use and
analyze data and knowledge with statistical and symbolic AI methods relevant to
the AI application - neither alone will do. Consequently, we argue and seek to
demonstrate that the NeuroSymbolic AI approach is better suited for making AI a
trusted AI system. We present the CREST framework that shows how Consistency,
Reliability, user-level Explainability, and Safety are built on NeuroSymbolic
methods that use data and knowledge to support requirements for critical
applications such as health and well-being. This article focuses on Large
Language Models (LLMs) as the chosen AI system within the CREST framework. LLMs
have garnered substantial attention from researchers due to their versatility
in handling a broad array of natural language processing (NLP) scenarios. For
example, ChatGPT and Google's MedPaLM have emerged as highly promising
platforms for providing information in general and health-related queries,
respectively. Nevertheless, these models remain black boxes despite
incorporating human feedback and instruction-guided tuning. For instance,
ChatGPT can generate unsafe responses despite instituting safety guardrails.
CREST presents a plausible approach harnessing procedural and graph-based
knowledge within a NeuroSymbolic framework to shed light on the challenges
associated with LLMs.

New technologies have the power to revolutionize science. It has happened in
the past and is happening again with the emergence of new computational tools,
such as artificial intelligence and machine learning. Despite the documented
impact of these technologies, there remains a significant gap in understanding
the process of their adoption within the scientific community. In this paper,
we draw on theories of scientific and technical human capital to study the
integration of AI in scientific research, focusing on the human capital of
scientists and the external resources available within their network of
collaborators and institutions. We validate our hypotheses on a large sample of
publications from OpenAlex, covering all sciences from 1980 to 2020, and
identify a set key drivers and inhibitors of AI adoption and use in science.
Our results suggest that AI is pioneered by domain scientists with a `taste for
exploration' and who are embedded in a network rich of computer scientists,
experienced AI scientists and early-career researchers; they come from
institutions with high citation impact and a relatively strong publication
history on AI. The access to computing resources only matters for a few
scientific disciplines, such as chemistry and medical sciences. Once AI is
integrated into research, most adoption factors continue to influence its
subsequent reuse. Implications for the organization and management of science
in the evolving era of AI-driven discovery are discussed.

Emotion significantly impacts our daily behaviors and interactions. While
recent generative AI models, such as large language models, have shown
impressive performance in various tasks, it remains unclear whether they truly
comprehend emotions. This paper aims to address this gap by incorporating
psychological theories to gain a holistic understanding of emotions in
generative AI models. Specifically, we propose three approaches: 1)
EmotionPrompt to enhance AI model performance, 2) EmotionAttack to impair AI
model performance, and 3) EmotionDecode to explain the effects of emotional
stimuli, both benign and malignant. Through extensive experiments involving
language and multi-modal models on semantic understanding, logical reasoning,
and generation tasks, we demonstrate that both textual and visual EmotionPrompt
can boost the performance of AI models while EmotionAttack can hinder it.
Additionally, EmotionDecode reveals that AI models can comprehend emotional
stimuli akin to the mechanism of dopamine in the human brain. Our work heralds
a novel avenue for exploring psychology to enhance our understanding of
generative AI models. This paper is an extended version of our previous work
EmotionPrompt (arXiv:2307.11760).

The Animal-AI Environment is a unique game-based research platform designed
to serve both the artificial intelligence and cognitive science research
communities. In this paper, we present Animal-AI 3, the latest version of the
environment, outlining several major new features that make the game more
engaging for humans and more complex for AI systems. New features include
interactive buttons, reward dispensers, and player notifications, as well as an
overhaul of the environment's graphics and processing for significant increases
in agent training time and quality of the human player experience. We provide
detailed guidance on how to build computational and behavioural experiments
with Animal-AI 3. We present results from a series of agents, including the
state-of-the-art Deep Reinforcement Learning agent (dreamer-v3), on newly
designed tests and the Animal-AI Testbed of 900 tasks inspired by research in
comparative psychology. Animal-AI 3 is designed to facilitate collaboration
between the cognitive sciences and artificial intelligence. This paper serves
as a stand-alone document that motivates, describes, and demonstrates Animal-AI
3 for the end user.

For emerging professions, such as jobs in the field of Artificial
Intelligence (AI) or sustainability (green), labour supply does not meet
industry demand. In this scenario of labour shortages, our work aims to
understand whether employers have started focusing on individual skills rather
than on formal qualifications in their recruiting. By analysing a large time
series dataset of around one million online job vacancies between 2019 and 2022
from the UK and drawing on diverse literature on technological change and
labour market signalling, we provide evidence that employers have started
so-called "skill-based hiring" for AI and green roles, as more flexible hiring
practices allow them to increase the available talent pool. In our observation
period the demand for AI roles grew twice as much as average labour demand. At
the same time, the mention of university education for AI roles declined by
23%, while AI roles advertise five times as many skills as job postings on
average. Our regression analysis also shows that university degrees no longer
show an educational premium for AI roles, while for green positions the
educational premium persists. In contrast, AI skills have a wage premium of
16%, similar to having a PhD (17%). Our work recommends making use of
alternative skill building formats such as apprenticeships, on-the-job
training, MOOCs, vocational education and training, micro-certificates, and
online bootcamps to use human capital to its full potential and to tackle
talent shortages.

The rapid expansion of Learning Analytics (LA) and Artificial Intelligence in
Education (AIED) offers new scalable, data-intensive systems but also raises
concerns about data privacy and agency. Excluding stakeholders -- like students
and teachers -- from the design process can potentially lead to mistrust and
inadequately aligned tools. Despite a shift towards human-centred design in
recent LA and AIED research, there remain gaps in our understanding of the
importance of human control, safety, reliability, and trustworthiness in the
design and implementation of these systems. We conducted a systematic
literature review to explore these concerns and gaps. We analysed 108 papers to
provide insights about i) the current state of human-centred LA/AIED research;
ii) the extent to which educational stakeholders have contributed to the design
process of human-centred LA/AIED systems; iii) the current balance between
human control and computer automation of such systems; and iv) the extent to
which safety, reliability and trustworthiness have been considered in the
literature. Results indicate some consideration of human control in LA/AIED
system design, but limited end-user involvement in actual design. Based on
these findings, we recommend: 1) carefully balancing stakeholders' involvement
in designing and deploying LA/AIED systems throughout all design phases, 2)
actively involving target end-users, especially students, to delineate the
balance between human control and automation, and 3) exploring safety,
reliability, and trustworthiness as principles in future human-centred LA/AIED
systems.

This research aims to demonstrate that AI can function not only as a tool for
learning, but also as an intelligent agent with which humans can engage in
collaborative learning (CL) to change epistemic practices in science
classrooms. We adopted a design and development research approach, following
the Analysis, Design, Development, Implementation and Evaluation (ADDIE) model,
to prototype a tangible instructional system called Collaborative Learning with
AI Speakers (CLAIS). The CLAIS system is designed to have 3-4 human learners
join an AI speaker to form a small group, where humans and AI are considered as
peers participating in the Jigsaw learning process. The development was carried
out using the NUGU AI speaker platform. The CLAIS system was successfully
implemented in a Science Education course session with 15 pre-service
elementary science teachers. The participants evaluated the CLAIS system
through mixed methods surveys as teachers, learners, peers, and users.
Quantitative data showed that the participants' Intelligent-Technological,
Pedagogical, And Content Knowledge was significantly increased after the CLAIS
session, the perception of the CLAIS learning experience was positive, the peer
assessment on AI speakers and human peers was different, and the user
experience was ambivalent. Qualitative data showed that the participants
anticipated future changes in the epistemic process in science classrooms,
while acknowledging technical issues such as speech recognition performance and
response latency. This study highlights the potential of Human-AI Collaboration
for knowledge co-construction in authentic classroom settings and exemplify how
AI could shape the future landscape of epistemic practices in the classroom.

Artificial intelligence (AI) has revolutionized human cognitive abilities and
facilitated the development of new AI entities capable of interacting with
humans in both physical and virtual environments. Despite the existence of
virtual reality, mixed reality, and augmented reality for several years,
integrating these technical fields remains a formidable challenge due to their
disparate application directions. The advent of AI agents, capable of
autonomous perception and action, further compounds this issue by exposing the
limitations of traditional human-centered research approaches. It is imperative
to establish a comprehensive framework that accommodates the dual perceptual
centers of humans and AI agents in both physical and virtual worlds. In this
paper, we introduce the symmetrical reality framework, which offers a unified
representation encompassing various forms of physical-virtual amalgamations.
This framework enables researchers to better comprehend how AI agents can
collaborate with humans and how distinct technical pathways of physical-virtual
integration can be consolidated from a broader perspective. We then delve into
the coexistence of humans and AI, demonstrating a prototype system that
exemplifies the operation of symmetrical reality systems for specific tasks,
such as pouring water. Subsequently, we propose an instance of an AI-driven
active assistance service that illustrates the potential applications of
symmetrical reality. This paper aims to offer beneficial perspectives and
guidance for researchers and practitioners in different fields, thus
contributing to the ongoing research about human-AI coexistence in both
physical and virtual environments.

Executing deep neural networks (DNNs) on edge artificial intelligence (AI)
devices enables various autonomous mobile computing applications. However, the
memory budget of edge AI devices restricts the number and complexity of DNNs
allowed in such applications. Existing solutions, such as model compression or
cloud offloading, reduce the memory footprint of DNN inference at the cost of
decreased model accuracy or autonomy. To avoid these drawbacks, we divide DNN
into blocks and swap them in and out in order, such that large DNNs can execute
within a small memory budget. Nevertheless, naive swapping on edge AI devices
induces significant delays due to the redundant memory operations in the DNN
development ecosystem for edge AI devices. To this end, we develop SwapNet, an
efficient DNN block swapping middleware for edge AI devices. We systematically
eliminate the unnecessary memory operations during block swapping while
retaining compatible with the deep learning frameworks, GPU backends, and
hardware architectures of edge AI devices. We further showcase the utility of
SwapNet via a multi-DNN scheduling scheme. Evaluations on eleven DNN inference
tasks in three applications demonstrate that SwapNet achieves almost the same
latency as the case with sufficient memory even when DNNs demand 2.32x to 5.81x
memory beyond the available budget. The design of SwapNet also provides novel
and feasible insights for deploying large language models (LLMs) on edge AI
devices in the future.

Space-Air-Ground Integrated Networks (SAGINs), which incorporate space and
aerial networks with terrestrial wireless systems, are vital enablers of the
emerging sixth-generation (6G) wireless networks. Besides bringing significant
benefits to various applications and services, SAGINs are envisioned to extend
high-speed broadband coverage to remote areas, such as small towns or mining
sites, or areas where terrestrial infrastructure cannot reach, such as
airplanes or maritime use cases. However, due to the limited power and storage
resources, as well as other constraints introduced by the design of terrestrial
networks, SAGINs must be intelligently configured and controlled to satisfy the
envisioned requirements. Meanwhile, Artificial Intelligence (AI) is another
critical enabler of 6G. Due to massive amounts of available data, AI has been
leveraged to address pressing challenges of current and future wireless
networks. By adding AI and facilitating the decision-making and prediction
procedures, SAGINs can effectively adapt to their surrounding environment, thus
enhancing the performance of various metrics. In this work, we aim to
investigate the interplay of AI and SAGINs by providing a holistic overview of
state-of-the-art research in AI-enabled SAGINs. Specifically, we present a
comprehensive overview of some potential applications of AI in SAGINs. We also
cover open issues in employing AI and detail the contributions of SAGINs in the
development of AI. Finally, we highlight some limitations of the existing
research works and outline potential future research directions.

The introduction of AI and ML technologies into medical devices has
revolutionized healthcare diagnostics and treatments. Medical device
manufacturers are keen to maximize the advantages afforded by AI and ML by
consolidating multiple applications onto a single platform. However, concurrent
execution of several AI applications, each with its own visualization
components, leads to unpredictable end-to-end latency, primarily due to GPU
resource contentions. To mitigate this, manufacturers typically deploy separate
workstations for distinct AI applications, thereby increasing financial,
energy, and maintenance costs. This paper addresses these challenges within the
context of NVIDIA's Holoscan platform, a real-time AI system for streaming
sensor data and images. We propose a system design optimized for heterogeneous
GPU workloads, encompassing both compute and graphics tasks. Our design
leverages CUDA MPS for spatial partitioning of compute workloads and isolates
compute and graphics processing onto separate GPUs. We demonstrate significant
performance improvements across various end-to-end latency determinism metrics
through empirical evaluation with real-world Holoscan medical device
applications. For instance, the proposed design reduces maximum latency by
21-30% and improves latency distribution flatness by 17-25% for up to five
concurrent endoscopy tool tracking AI applications, compared to a single-GPU
baseline. Against a default multi-GPU setup, our optimizations decrease maximum
latency by 35% for up to six concurrent applications by improving GPU
utilization by 42%. This paper provides clear design insights for AI
applications in the edge-computing domain including medical systems, where
performance predictability of concurrent and heterogeneous GPU workloads is a
critical requirement.

This manuscript presents a methodical examination of the utilization of
Artificial Intelligence in the assessment of emotions in texts related to
healthcare, with a particular focus on the incorporation of Natural Language
Processing and deep learning technologies. We scrutinize numerous research
studies that employ AI to augment sentiment analysis, categorize emotions, and
forecast patient outcomes based on textual information derived from clinical
narratives, patient feedback on medications, and online health discussions. The
review demonstrates noteworthy progress in the precision of algorithms used for
sentiment classification, the prognostic capabilities of AI models for
neurodegenerative diseases, and the creation of AI-powered systems that offer
support in clinical decision-making. Remarkably, the utilization of AI
applications has exhibited an enhancement in personalized therapy plans by
integrating patient sentiment and contributing to the early identification of
mental health disorders. There persist challenges, which encompass ensuring the
ethical application of AI, safeguarding patient confidentiality, and addressing
potential biases in algorithmic procedures. Nevertheless, the potential of AI
to revolutionize healthcare practices is unmistakable, offering a future where
healthcare is not only more knowledgeable and efficient but also more
empathetic and centered around the needs of patients. This investigation
underscores the transformative influence of AI on healthcare, delivering a
comprehensive comprehension of its role in examining emotional content in
healthcare texts and highlighting the trajectory towards a more compassionate
approach to patient care. The findings advocate for a harmonious synergy
between AI's analytical capabilities and the human aspects of healthcare.

This research explores the quickly changing field of generative artificial
intelligence (GAI) chatbots in higher education, an industry that is undergoing
major technological changes. AI chatbots, such as ChatGPT, HuggingChat, and
Google Bard, are becoming more and more common in a variety of sectors,
including education. Their acceptance is still in its early phases, with a
variety of prospects and obstacles. However, their potential in higher
education is particularly noteworthy, providing lecturers and students with
affordable, individualized support. Creating a comprehensive framework to aid
the usage of generative AI chatbots in higher education institutions (HEIs) is
the aim of this project. The Generative AI Chatbots Acceptance Model (GAICAM)
is the result of this study's synthesis of elements from well-known frameworks,
including the TAM, UTAUT2, TPB, and others along with variables like optimism,
innovativeness, discomfort, insecurity, and others. Using a research method
that encompasses a comprehensive analysis of extant literature from databases
such as IEEE, ACM, ScienceDirect, and Google Scholar, the study aims to
comprehend the implications of AI Chatbots on higher education and pinpoint
critical elements for their efficacious implementation. Peer-reviewed
English-language publications published between 2020 and 2023 with a focus on
the use of AI chatbots in higher education were the main focus of the search
criteria. The results demonstrate how much AI chatbots can do to improve
student engagement, streamline the educational process, and support
administrative and research duties. But there are also clear difficulties, such
as unfavorable student sentiments, doubts about the veracity of material
produced by AI, and unease and nervousness with new technologies.

Generative Artificial Intelligence (AI) has pioneered new methodological
paradigms in architectural design, significantly expanding the innovative
potential and efficiency of the design process. This paper explores the
extensive applications of generative AI technologies in architectural design, a
trend that has benefited from the rapid development of deep generative models.
This article provides a comprehensive review of the basic principles of
generative AI and large-scale models and highlights the applications in the
generation of 2D images, videos, and 3D models. In addition, by reviewing the
latest literature from 2020, this paper scrutinizes the impact of generative AI
technologies at different stages of architectural design, from generating
initial architectural 3D forms to producing final architectural imagery. The
marked trend of research growth indicates an increasing inclination within the
architectural design community towards embracing generative AI, thereby
catalyzing a shared enthusiasm for research. These research cases and
methodologies have not only proven to enhance efficiency and innovation
significantly but have also posed challenges to the conventional boundaries of
architectural creativity. Finally, we point out new directions for design
innovation and articulate fresh trajectories for applying generative AI in the
architectural domain. This article provides the first comprehensive literature
review about generative AI for architectural design, and we believe this work
can facilitate more research work on this significant topic in architecture.

How are new technologies like generative AI quickly adopted and used by
executive and managerial leaders to create value in organizations? A survey of
INSEAD's global alumni base revealed several intriguing insights into
perceptions and engagements with generative AI across a broad spectrum of
demographics, industries, and geographies. Notably, there's a prevailing
optimism about the role of generative AI in enhancing productivity and
innovation, as evidenced by the 90% of respondents being excited about its
time-saving and efficiency benefits. Analysis revealed different attitudes
about adoption and use across demographic variables. Younger respondents are
significantly more excited about generative AI and more likely to be using it
at work and in personal life than older participants. Those in Europe have a
somewhat more distant view of generative AI than those in North America in
Asia, in that they see the gains more likely to be captured by organizations
than individuals, and are less likely to be using it in professional and
personal contexts than those in North America and Asia. This may also be
related to the fact that those in Europe are more likely to be working in
Financial Services and less likely to be working in Information Technology
industries than those in North America and Asia. Despite this, those in Europe
are more likely to see AGI happening faster than those in North America,
although this may reflect less interaction with generative AI in personal and
professional contexts. These findings collectively underscore the complex and
multifaceted perceptions of generative AI's role in society, pointing to both
its promising potential and the challenges it presents.

DevOps has emerged as one of the most rapidly evolving software development
paradigms. With the growing concerns surrounding security in software systems,
the DevSecOps paradigm has gained prominence, urging practitioners to
incorporate security practices seamlessly into the DevOps workflow. However,
integrating security into the DevOps workflow can impact agility and impede
delivery speed. Recently, the advancement of artificial intelligence (AI) has
revolutionized automation in various software domains, including software
security. AI-driven security approaches, particularly those leveraging machine
learning or deep learning, hold promise in automating security workflows. They
reduce manual efforts, which can be integrated into DevOps to ensure
uninterrupted delivery speed and align with the DevSecOps paradigm
simultaneously. This paper seeks to contribute to the critical intersection of
AI and DevSecOps by presenting a comprehensive landscape of AI-driven security
techniques applicable to DevOps and identifying avenues for enhancing security,
trust, and efficiency in software development processes. We analyzed 99
research papers spanning from 2017 to 2023. Specifically, we address two key
research questions (RQs). In RQ1, we identified 12 security tasks associated
with the DevOps process and reviewed existing AI-driven security approaches. In
RQ2, we discovered 15 challenges encountered by existing AI-driven security
approaches and derived future research opportunities. Drawing insights from our
findings, we discussed the state-of-the-art AI-driven security approaches,
highlighted challenges in existing research, and proposed avenues for future
opportunities.

Next-generation multiple input multiple output (MIMO) is expected to be
intelligent and scalable. In this paper, we study generative artificial
intelligence (AI) agent-enabled next-generation MIMO design. Firstly, we
provide an overview of the development, fundamentals, and challenges of the
next-generation MIMO. Then, we propose the concept of the generative AI agent,
which is capable of generating tailored and specialized contents with the aid
of large language model (LLM) and retrieval augmented generation (RAG). Next,
we comprehensively discuss the features and advantages of the generative AI
agent framework. More importantly, to tackle existing challenges of
next-generation MIMO, we discuss generative AI agent-enabled next-generation
MIMO design, from the perspective of performance analysis, signal processing,
and resource allocation. Furthermore, we present two compelling case studies
that demonstrate the effectiveness of leveraging the generative AI agent for
performance analysis in complex configuration scenarios. These examples
highlight how the integration of generative AI agents can significantly enhance
the analysis and design of next-generation MIMO systems. Finally, we discuss
important potential research future directions.

The landscape of maintenance in distributed systems is rapidly evolving with
the integration of Artificial Intelligence (AI). Also, as the complexity of
computing continuum systems intensifies, the role of AI in predictive
maintenance (Pd.M.) becomes increasingly pivotal. This paper presents a
comprehensive survey of the current state of Pd.M. in the computing continuum,
with a focus on the combination of scalable AI technologies. Recognizing the
limitations of traditional maintenance practices in the face of increasingly
complex and heterogenous computing continuum systems, the study explores how
AI, especially machine learning and neural networks, is being used to enhance
Pd.M. strategies. The survey encompasses a thorough review of existing
literature, highlighting key advancements, methodologies, and case studies in
the field. It critically examines the role of AI in improving prediction
accuracy for system failures and in optimizing maintenance schedules, thereby
contributing to reduced downtime and enhanced system longevity. By synthesizing
findings from the latest advancements in the field, the article provides
insights into the effectiveness and challenges of implementing AI-driven
predictive maintenance. It underscores the evolution of maintenance practices
in response to technological advancements and the growing complexity of
computing continuum systems. The conclusions drawn from this survey are
instrumental for practitioners and researchers in understanding the current
landscape and future directions of Pd.M. in distributed systems. It emphasizes
the need for continued research and development in this area, pointing towards
a trend of more intelligent, efficient, and cost-effective maintenance
solutions in the era of AI.

The legacy beam management (BM) procedure in 5G introduces higher measurement
and reporting overheads for larger beam codebooks resulting in higher power
consumption of user equipment (UEs). Hence, the 3rd generation partnership
project (3GPP) studied the use of artificial intelligence (AI) and machine
learning (ML) in the air interface to reduce the overhead associated with the
legacy BM procedure. The usage of AI/ML in BM is mainly discussed with regard
to spatial-domain beam prediction (SBP) and time-domain beam prediction (TBP).
In this study, we discuss different sub-use cases of SBP and TBP and evaluate
the beam prediction accuracy of AI/ML models designed for each sub-use case
along with AI/ML model generalization aspects. Moreover, a comprehensive
system-level performance evaluation is presented in terms of user throughput
with integrated AI/ML models to a 3GPP-compliant system-level simulator. Based
on user throughput evaluations, we present AI/ML BM design guidelines for the
deployment of lightweight, low-complexity AI/ML models discussed in this study.

In the next few years, applications of Generative AI are expected to
revolutionize a number of different areas, ranging from science & medicine to
education. The potential for these seismic changes has triggered a lively
debate about potential risks and resulted in calls for tighter regulation, in
particular from some of the major tech companies who are leading in AI
development. This regulation is likely to put at risk the budding field of open
source Generative AI. We argue for the responsible open sourcing of generative
AI models in the near and medium term. To set the stage, we first introduce an
AI openness taxonomy system and apply it to 40 current large language models.
We then outline differential benefits and risks of open versus closed source AI
and present potential risk mitigation, ranging from best practices to calls for
technical and scientific contributions. We hope that this report will add a
much needed missing voice to the current public discourse on near to mid-term
AI safety and other societal impact.

In the rapidly evolving landscape of computing disciplines, substantial
efforts are being dedicated to unraveling the sociotechnical implications of
generative AI (Gen AI). While existing research has manifested in various
forms, there remains a notable gap concerning the direct engagement of
knowledge workers in academia with Gen AI. We interviewed 18 knowledge workers,
including faculty and students, to investigate the social and technical
dimensions of Gen AI from their perspective. Our participants raised concerns
about the opacity of the data used to train Gen AI. This lack of transparency
makes it difficult to identify and address inaccurate, biased, and potentially
harmful, information generated by these models. Knowledge workers also
expressed worries about Gen AI undermining trust in the relationship between
instructor and student and discussed potential solutions, such as pedagogy
readiness, to mitigate them. Additionally, participants recognized Gen AI's
potential to democratize knowledge by accelerating the learning process and act
as an accessible research assistant. However, there were also concerns about
potential social and power imbalances stemming from unequal access to such
technologies. Our study offers insights into the concerns and hopes of
knowledge workers about the ethical use of Gen AI in educational settings and
beyond, with implications for navigating this new landscape.

Let A be a locally m-convex Fr\'echet algebra. We give a necessary and
sufficient condition for a cyclic Fr\'echet A-module X=A_+/I to be strictly
flat, generalizing thereby a criterion of Helemskii and Sheinberg. To this end,
we introduce a notion of locally bounded approximate identity (a.i.), and we
show that X is strictly flat if and only if the ideal I has a right locally
bounded a.i. An example is given of a commutative locally m-convex Fr\'echet
algebra that has a locally bounded a.i., but does not have a bounded a.i. On
the other hand, we show that a quasinormable locally m-convex Fr\'echet algebra
has a locally bounded a.i. if and only if it has a bounded a.i. Some
applications to amenable Fr\'echet algebras are also given.

When Kurt Goedel layed the foundations of theoretical computer science in
1931, he also introduced essential concepts of the theory of Artificial
Intelligence (AI). Although much of subsequent AI research has focused on
heuristics, which still play a major role in many practical AI applications, in
the new millennium AI theory has finally become a full-fledged formal science,
with important optimality results for embodied agents living in unknown
environments, obtained through a combination of theory a la Goedel and
probability theory. Here we look back at important milestones of AI history,
mention essential recent results, and speculate about what we may expect from
the next 25 years, emphasizing the significance of the ongoing dramatic
hardware speedups, and discussing Goedel-inspired, self-referential,
self-improving universal problem solvers.

Over the last few years, more and more heuristic decision making techniques
have been inspired by nature, e.g. evolutionary algorithms, ant colony
optimisation and simulated annealing. More recently, a novel computational
intelligence technique inspired by immunology has emerged, called Artificial
Immune Systems (AIS). This immune system inspired technique has already been
useful in solving some computational problems. In this keynote, we will very
briefly describe the immune system metaphors that are relevant to AIS. We will
then give some illustrative real-world problems suitable for AIS use and show a
step-by-step algorithm walkthrough. A comparison of AIS to other well-known
algorithms and areas for future work will round this keynote off. It should be
noted that as AIS is still a young and evolving field, there is not yet a fixed
algorithm template and hence actual implementations might differ somewhat from
the examples given here.

The major function of this model is to access the UCI Wisconsin Breast Can-
cer data-set[1] and classify the data items into two categories, which are
normal and anomalous. This kind of classifi cation can be referred as anomaly
detection, which discriminates anomalous behaviour from normal behaviour in
computer systems. One popular solution for anomaly detection is Artifi cial
Immune Sys- tems (AIS). AIS are adaptive systems inspired by theoretical
immunology and observed immune functions, principles and models which are
applied to prob- lem solving. The Dendritic Cell Algorithm (DCA)[2] is an AIS
algorithm that is developed specifi cally for anomaly detection. It has been
successfully applied to intrusion detection in computer security. It is
believed that agent-based mod- elling is an ideal approach for implementing
AIS, as intelligent agents could be the perfect representations of immune
entities in AIS. This model evaluates the feasibility of re-implementing the
DCA in an agent-based simulation environ- ment called AnyLogic, where the
immune entities in the DCA are represented by intelligent agents. If this model
can be successfully implemented, it makes it possible to implement more
complicated and adaptive AIS models in the agent-based simulation environment.

The questions which we will consider here are "What is AI?" and "How can we
make AI?". Here we will present the definition of AI in terms of multi-agent
systems. This means that here you will not find a new answer to the question
"What is AI?", but an old answer in a new form.
  This new form of the definition of AI is of interest for the theory of
multi-agent systems because it gives us better understanding of this theory.
More important is that this work will help us answer the second question. We
want to make a program which is capable of constructing a model of its
environment. Every multi-agent model is equivalent to a single-agent model but
multi-agent models are more natural and accordingly more easily discoverable.

Real-time strategy (RTS) games make heavy use of artificial intelligence
(AI), especially in the design of computerized opponents. Because of the
computational complexity involved in managing all aspects of these games, many
AI opponents are designed to optimize only a few areas of playing style. In
games like StarCraft 2, a very popular and recently released RTS, most AI
strategies revolve around economic and building efficiency: AI opponents try to
gather and spend all resources as quickly and effectively as possible while
ensuring that no units are idle. The aim of this work was to help address the
need for AI combat strategies that are not computationally intensive. Our goal
was to produce a computationally efficient model that is accurate at predicting
the results of complex battles between diverse armies, including which army
will win and how many units will remain. Our results suggest it may be possible
to develop a relatively simple approximation model of combat that can
accurately predict many battles that do not involve micromanagement. Future
designs of AI opponents may be able to incorporate such an approximation model
into their decision and planning systems to provide a challenge that is
strategically balanced across all aspects of play.

Currently, potential threats of artificial intelligence (AI) to human have
triggered a large controversy in society, behind which, the nature of the issue
is whether the artificial intelligence (AI) system can be evaluated
quantitatively. This article analyzes and evaluates the challenges that the AI
development level is facing, and proposes that the evaluation methods for the
human intelligence test and the AI system are not uniform; and the key reason
for which is that none of the models can uniformly describe the AI system and
the beings like human. Aiming at this problem, a standard intelligent system
model is established in this study to describe the AI system and the beings
like human uniformly. Based on the model, the article makes an abstract
mathematical description, and builds the standard intelligent machine
mathematical model; expands the Von Neumann architecture and proposes the
Liufeng - Shiyong architecture; gives the definition of the artificial
intelligence IQ, and establishes the artificial intelligence scale and the
evaluation method; conduct the test on 50 search engines and three human
subjects at different ages across the world, and finally obtains the ranking of
the absolute IQ and deviation IQ ranking for artificial intelligence IQ 2014.

Rapid progress in machine learning and artificial intelligence (AI) has
brought increasing attention to the potential impacts of AI technologies on
society. In this paper we discuss one such potential impact: the problem of
accidents in machine learning systems, defined as unintended and harmful
behavior that may emerge from poor design of real-world AI systems. We present
a list of five practical research problems related to accident risk,
categorized according to whether the problem originates from having the wrong
objective function ("avoiding side effects" and "avoiding reward hacking"), an
objective function that is too expensive to evaluate frequently ("scalable
supervision"), or undesirable behavior during the learning process ("safe
exploration" and "distributional shift"). We review previous work in these
areas as well as suggesting research directions with a focus on relevance to
cutting-edge AI systems. Finally, we consider the high-level question of how to
think most productively about the safety of forward-looking applications of AI.

In recent years, maritime safety and efficiency become more and more
important across the world. Automatic Identification System (AIS) tracks vessel
movement by onboard transceiver and terrestrial and/or satellite base station.
The data collected by AIS contains broadcast kinematic information and static
information. Both of them are useful for anomaly detection and route prediction
which are key techniques in intelligent maritime research area. This paper is
devoted to construct a standard AIS database for maritime trajectory learning,
prediction and data mining. A path prediction algorithm is tested on this AIS
database and the testing results show this database can be used as a
standardized training resource for different trajectory prediction algorithms
and other AIS data mining algorithms.

Currently, Artificial Intelligence (AI) has won unprecedented attention and
is becoming the increasingly popular focus in China. This change can be judged
by the impressive record of academic publications, the amount of state-level
investment and the presence of nation-wide participation and devotion. In this
paper, we place emphasis on discussing the progress of artificial intelligence
engineerings in China. We first introduce the focus on AI in Chinese academia,
including the supercomputing brain system, Cambrian Period supercomputer of
neural networks, and biometric systems. Then, the development of AI in
industrial circles and the latest layout of AI products in companies, such as
Baidu, Tencent, and Alibaba, are introduced. Last, we bring in the opinions and
arguments of the main intelligentsia of China about the future development of
AI, including how to examine the relationship between humanity on one side and
science and technology on the other.

This paper presents a design of a non-player character (AI) for promoting
balancedness in use of body segments when engaging in full-body motion gaming.
In our experiment, we settle a battle between the proposed AI and a player by
using FightingICE, a fighting game platform for AI development. A middleware
called UKI is used to allow the player to control the game by using body motion
instead of the keyboard and mouse. During gameplay, the proposed AI analyze
health states of the player; it determines its next action by predicting how
each candidate action, recommended by a Monte-Carlo tree search algorithm, will
induce the player to move, and how the player's health tends to be affected.
Our result demonstrates successful improvement in balancedness in use of body
segments on 4 out of 5 subjects.

Artificial Intelligence (AI) is an effective science which employs strong
enough approaches, methods, and techniques to solve unsolvable real world based
problems. Because of its unstoppable rise towards the future, there are also
some discussions about its ethics and safety. Shaping an AI friendly
environment for people and a people friendly environment for AI can be a
possible answer for finding a shared context of values for both humans and
robots. In this context, objective of this paper is to address the ethical
issues of AI and explore the moral dilemmas that arise from ethical algorithms,
from pre set or acquired values. In addition, the paper will also focus on the
subject of AI safety. As general, the paper will briefly analyze the concerns
and potential solutions to solving the ethical issues presented and increase
readers awareness on AI safety as another related research interest.

Artificial General Intelligence (AGI) or Strong AI aims to create machines
with human-like or human-level intelligence, which is still a very ambitious
goal when compared to the existing computing and AI systems. After many hype
cycles and lessons from AI history, it is clear that a big conceptual leap is
needed for crossing the starting line to kick-start mainstream AGI research.
This position paper aims to make a small conceptual contribution toward
reaching that starting line. After a broad analysis of the AGI problem from
different perspectives, a system-theoretic and engineering-based research
approach is introduced, which builds upon the existing mainstream AI and
systems foundations. Several promising cross-fertilization opportunities
between systems disciplines and AI research are identified. Specific potential
research directions are discussed.

The recent successes of AI have captured the wildest imagination of both the
scientific communities and the general public. Robotics and AI amplify human
potentials, increase productivity and are moving from simple reasoning towards
human-like cognitive abilities. Current AI technologies are used in a set area
of applications, ranging from healthcare, manufacturing, transport, energy, to
financial services, banking, advertising, management consulting and government
agencies. The global AI market is around 260 billion USD in 2016 and it is
estimated to exceed 3 trillion by 2024. To understand the impact of AI, it is
important to draw lessons from it's past successes and failures and this white
paper provides a comprehensive explanation of the evolution of AI, its current
status and future directions.

Advances in AI in the last decade have clearly made economists, politicians,
journalists, and citizenry in general believe that the machines are coming to
take human jobs. We review 'superhuman' AI performance claims in radiology and
then provide a self-reflection on our own work in the area in the form of a
critical review, a tribute of sorts to McDermotts 1976 paper, asking the field
for some self-discipline. Clearly there is an opportunity to replace humans,
but there are better opportunities, as we have discovered to fit cognitive
abilities of human and non-humans. We performed one of the first studies in
radiology to see how human and AI performance can complement and improve each
others performance for detecting pneumonia in chest X-rays. We question if
there is a practical wisdom or phronesis that we need to demonstrate in AI
today as well as in our field. Using this, we articulate what AI as a field has
already and probably can in the future learn from Psychology, Cognitive
Science, Sociology and Science and Technology Studies.

We suggest that the analysis of incomplete contracting developed by law and
economics researchers can provide a useful framework for understanding the AI
alignment problem and help to generate a systematic approach to finding
solutions. We first provide an overview of the incomplete contracting
literature and explore parallels between this work and the problem of AI
alignment. As we emphasize, misalignment between principal and agent is a core
focus of economic analysis. We highlight some technical results from the
economics literature on incomplete contracts that may provide insights for AI
alignment researchers. Our core contribution, however, is to bring to bear an
insight that economists have been urged to absorb from legal scholars and other
behavioral scientists: the fact that human contracting is supported by
substantial amounts of external structure, such as generally available
institutions (culture, law) that can supply implied terms to fill the gaps in
incomplete contracts. We propose a research agenda for AI alignment work that
focuses on the problem of how to build AI that can replicate the human
cognitive processes that connect individual incomplete contracts with this
supporting external structure.

Interpretability of the underlying AI representations is a key raison
d'\^{e}tre for Open Learner Modelling (OLM) -- a branch of Intelligent Tutoring
Systems (ITS) research. OLMs provide tools for 'opening' up the AI models of
learners' cognition and emotions for the purpose of supporting human learning
and teaching. Over thirty years of research in ITS (also known as AI in
Education) produced important work, which informs about how AI can be used in
Education to best effects and, through the OLM research, what are the necessary
considerations to make it interpretable and explainable for the benefit of
learning. We argue that this work can provide a valuable starting point for a
framework of interpretable AI, and as such is of relevance to the application
of both knowledge-based and machine learning systems in other high-stakes
contexts, beyond education.

Recent developments in AI, Machine Learning and Robotics have raised concerns
about the ethical consequences of both academic and industrial AI research.
Leading academics, businessmen and politicians have voiced an increasing number
of questions about the consequences of AI not only over people, but also on the
large-scale consequences on the the future of work and employment, its social
consequences and the sustainability of the planet. In this work, we analyse the
use and the occurrence of ethics-related research in leading AI, machine
learning and robotics venues. In order to do so we perform long term,
historical corpus-based analyses on a large number of flagship conferences and
journals. Our experiments identify the prominence of ethics-related terms in
published papers and presents several statistics on related topics. Finally,
this research provides quantitative evidence on the pressing ethical concerns
of the AI community.

In the last couple of years, the rise of Artificial Intelligence and the
successes of academic breakthroughs in the field have been inescapable. Vast
sums of money have been thrown at AI start-ups. Many existing tech companies --
including the giants like Google, Amazon, Facebook, and Microsoft -- have
opened new research labs. The rapid changes in these everyday work and
entertainment tools have fueled a rising interest in the underlying technology
itself; journalists write about AI tirelessly, and companies -- of tech nature
or not -- brand themselves with AI, Machine Learning or Deep Learning whenever
they get a chance. Confronting squarely this media coverage, several analysts
are starting to voice concerns about over-interpretation of AI's blazing
successes and the sometimes poor public reporting on the topic. This paper
reviews briefly the track-record in AI and Machine Learning and finds this
pattern of early dramatic successes, followed by philosophical critique and
unexpected difficulties, if not downright stagnation, returning almost to the
clock in 30-year cycles since 1958.

We describe a biologically-inspired research agenda with parallel tracks
aimed at AI and AI safety. The bottom-up component consists of building a
sequence of biophysically realistic simulations of simple organisms such as the
nematode $Caenorhabditis$ $elegans$, the fruit fly $Drosophila$ $melanogaster$,
and the zebrafish $Danio$ $rerio$ to serve as platforms for research into AI
algorithms and system architectures. The top-down component consists of an
approach to value alignment that grounds AI goal structures in neuropsychology,
broadly considered. Our belief is that parallel pursuit of these tracks will
inform the development of value-aligned AI systems that have been inspired by
embodied organisms with sensorimotor integration. An important set of side
benefits is that the research trajectories we describe here are grounded in
long-standing intellectual traditions within existing research communities and
funding structures. In addition, these research programs overlap with
significant contemporary themes in the biological and psychological sciences
such as data/model integration and reproducibility.

The question addressed in this paper is: If we present to a user an AI system
that explains how it works, how do we know whether the explanation works and
the user has achieved a pragmatic understanding of the AI? In other words, how
do we know that an explanainable AI system (XAI) is any good? Our focus is on
the key concepts of measurement. We discuss specific methods for evaluating:
(1) the goodness of explanations, (2) whether users are satisfied by
explanations, (3) how well users understand the AI systems, (4) how curiosity
motivates the search for explanations, (5) whether the user's trust and
reliance on the AI are appropriate, and finally, (6) how the human-XAI work
system performs. The recommendations we present derive from our integration of
extensive research literatures and our own psychometric evaluations.

The recent upsurge of diversified mobile applications, especially those
supported by Artificial Intelligence (AI), is spurring heated discussions on
the future evolution of wireless communications. While 5G is being deployed
around the world, efforts from industry and academia have started to look
beyond 5G and conceptualize 6G. We envision 6G to undergo an unprecedented
transformation that will make it substantially different from the previous
generations of wireless cellular systems. In particular, 6G will go beyond
mobile Internet and will be required to support ubiquitous AI services from the
core to the end devices of the network. Meanwhile, AI will play a critical role
in designing and optimizing 6G architectures, protocols, and operations. In
this article, we discuss potential technologies for 6G to enable mobile AI
applications, as well as AI-enabled methodologies for 6G network design and
optimization. Key trends in the evolution to 6G will also be discussed.

The AI for social good movement has now reached a state in which a large
number of one-off demonstrations have illustrated that partnerships of AI
practitioners and social change organizations are possible and can address
problems faced in sustainable development. In this paper, we discuss how moving
from demonstrations to true impact on humanity will require a different course
of action, namely open platforms containing foundational AI capabilities to
support common needs of multiple organizations working in similar topical
areas. We lend credence to this proposal by describing three example patterns
of social good problems and their AI-based solutions: natural language
processing for making sense of international development reports, causal
inference for providing guidance to vulnerable individuals, and
discrimination-aware classification for supporting unbiased allocation
decisions. We argue that the development of such platforms will be possible
through convenings of social change organizations, AI companies, and
grantmaking foundations.

The Internet of Things (IoT) and Artificial Intelligence (AI) have been
employed in agriculture over a long period of time, alongside other advanced
computing technologies. However, increased attention is currently being paid to
the use of such smart technologies. Agriculture has provided an important
source of food for human beings over many thousands of years, including the
development of appropriate farming methods for different types of crops. The
emergence of new advanced IoT technologies has the potential to monitor the
agricultural environment to ensure high-quality products. However, there
remains a lack of research and development in relation to Smart Sustainable
Agriculture (SSA), accompanied by complex obstacles arising from the
fragmentation of agricultural processes, i.e. the control and operation of
IoT/AI machines; data sharing and management; interoperability; and large
amounts of data analysis and storage. This study firstly, explores existing
IoT/AI technologies adopted for SSA and secondly, identifies IoT/AI technical
architecture capable of underpinning the development of SSA platforms. As well
as contributing to the current body of knowledge, this research reviews
research and development within SSA and provides an IoT/AI architecture to
establish a Smart, Sustainable Agriculture platform as a solution.

Experiential AI is proposed as a new research agenda in which artists and
scientists come together to dispel the mystery of algorithms and make their
mechanisms vividly apparent. It addresses the challenge of finding novel ways
of opening up the field of artificial intelligence to greater transparency and
collaboration between human and machine. The hypothesis is that art can mediate
between computer code and human comprehension to overcome the limitations of
explanations in and for AI systems. Artists can make the boundaries of systems
visible and offer novel ways to make the reasoning of AI transparent and
decipherable. Beyond this, artistic practice can explore new configurations of
humans and algorithms, mapping the terrain of inter-agencies between people and
machines. This helps to viscerally understand the complex causal chains in
environments with AI components, including questions about what data to collect
or who to collect it about, how the algorithms are chosen, commissioned and
configured or how humans are conditioned by their participation in algorithmic
processes.

We discuss issues of Artificial Intelligence (AI) fairness for people with
disabilities, with examples drawn from our research on human-computer
interaction (HCI) for AI-based systems for people who are Deaf or Hard of
Hearing (DHH). In particular, we discuss the need for inclusion of data from
people with disabilities in training sets, the lack of interpretability of AI
systems, ethical responsibilities of access technology researchers and
companies, the need for appropriate evaluation metrics for AI-based access
technologies (to determine if they are ready to be deployed and if they can be
trusted by users), and the ways in which AI systems influence human behavior
and influence the set of abilities needed by users to successfully interact
with computing systems.

We study the problem of designing AI agents that can robustly cooperate with
people in human-machine partnerships. Our work is inspired by real-life
scenarios in which an AI agent, e.g., a virtual assistant, has to cooperate
with new users after its deployment. We model this problem via a parametric MDP
framework where the parameters correspond to a user's type and characterize her
behavior. In the test phase, the AI agent has to interact with a user of
unknown type. Our approach to designing a robust AI agent relies on observing
the user's actions to make inferences about the user's type and adapting its
policy to facilitate efficient cooperation. We show that without being
adaptive, an AI agent can end up performing arbitrarily bad in the test phase.
We develop two algorithms for computing policies that automatically adapt to
the user in the test phase. We demonstrate the effectiveness of our approach in
solving a two-agent collaborative task.

Over the past decade, the NASA Autonomous Systems and Operations (ASO)
project has developed and demonstrated numerous autonomy enabling technologies
employing AI techniques. Our work has employed AI in three distinct ways to
enable autonomous mission operations capabilities. Crew Autonomy gives
astronauts tools to assist in the performance of each of these mission
operations functions. Vehicle System Management uses AI techniques to turn the
astronaut's spacecraft into a robot, allowing it to operate when astronauts are
not present, or to reduce astronaut workload. AI technology also enables
Autonomous Robots as crew assistants or proxies when the crew are not present.
We first describe human spaceflight mission operations capabilities. We then
describe the ASO project, and the development and demonstration performed by
ASO since 2011. We will describe the AI techniques behind each of these
demonstrations, which include a variety of symbolic automated reasoning and
machine learning based approaches. Finally, we conclude with an assessment of
future development needs for AI to enable NASA's future Exploration missions.

Artificial intelligence (AI) powered wireless networks promise to
revolutionize the conventional operation and structure of current networks from
network design to infrastructure management, cost reduction, and user
performance improvement. Empowering future networks with AI functionalities
will enable a shift from reactive/incident driven operations to
proactive/data-driven operations. This paper provides an overview on the
integration of AI functionalities in 5G and beyond networks. Key factors for
successful AI integration such as data, security, and explainable AI are
highlighted. We also summarize the various types of network intelligence as
well as machine learning based air interface in future networks. Use case
examples for the application of AI to the wireless domain are then summarized.
We highlight on applications to the physical layer, mobility management,
wireless security, and localization.

Many decision-making processes have begun to incorporate an AI element,
including prison sentence recommendations, college admissions, hiring, and
mortgage approval. In all of these cases, AI models are being trained to help
human decision makers reach accurate and fair judgments, but little is known
about what factors influence the extent to which people consider an AI-infused
decision-making process to be trustworthy. We aim to understand how different
factors about a decision-making process, and an AI model that supports that
process, influences peoples' perceptions of the trustworthiness of that
process. We report on our evaluation of how seven different factors -- decision
stakes, decision authority, model trainer, model interpretability, social
transparency, and model confidence -- influence ratings of trust in a
scenario-based study.

The overarching goal of this paper is to develop a general model of the state
space of AI. Given the breathtaking progress in AI research and technologies in
recent years, such conceptual work is of substantial theoretical interest. The
present AI hype is mainly driven by the triumph of deep learning neural
networks. As the distinguishing feature of such networks is the ability to
self-learn, self-learning is identified as one important dimension of the AI
state space. Another main dimension lies in the possibility to go over from
specific to more general types of problems. The third main dimension is
provided by semantic grounding. Since this is a philosophically complex and
controversial dimension, a larger part of the paper is devoted to it. We take a
fresh look at known foundational arguments in the philosophy of mind and
cognition that are gaining new relevance in view of the recent AI developments
including the blockhead objection, the Turing test, the symbol grounding
problem, the Chinese room argument, and general use-theoretic considerations of
meaning. Finally, the AI state space, spanned by the main dimensions
generalization, grounding and "selfx-ness", possessing self-x properties such
as self-learning, is outlined.

Artificial intelligence (AI) has widespread societal implications, yet social
scientists are only beginning to study public attitudes toward the technology.
Existing studies find that the public's trust in institutions can play a major
role in shaping the regulation of emerging technologies. Using a large-scale
survey (N=2000), we examined Americans' perceptions of 13 AI governance
challenges as well as their trust in governmental, corporate, and
multistakeholder institutions to responsibly develop and manage AI. While
Americans perceive all of the AI governance issues to be important for tech
companies and governments to manage, they have only low to moderate trust in
these institutions to manage AI applications.

There is an analogy between machine learning systems and economic entities in
that they are both adaptive, and their behaviour is specified in a more-or-less
explicit way. It appears that the area of AI that is most analogous to the
behaviour of economic entities is that of morally good decision-making, but it
is an open question as to how precisely moral behaviour can be achieved in an
AI system. This paper explores the analogy between these two complex systems,
and we suggest that a clearer understanding of this apparent analogy may help
us forward in both the socio-economic domain and the AI domain: known results
in economics may help inform feasible solutions in AI safety, but also known
results in AI may inform economic policy. If this claim is correct, then the
recent successes of deep learning for AI suggest that more implicit
specifications work better than explicit ones for solving such problems.

The recent development of data-driven AI promises to automate medical
diagnosis; however, most AI functions as 'black boxes' to physicians with
limited computational knowledge. Using medical imaging as a point of departure,
we conducted three iterations of design activities to formulate CheXplain---a
system that enables physicians to explore and understand AI-enabled chest X-ray
analysis: (1) a paired survey between referring physicians and radiologists
reveals whether, when, and what kinds of explanations are needed; (2) a
low-fidelity prototype co-designed with three physicians formulates eight key
features; and (3) a high-fidelity prototype evaluated by another six physicians
provides detailed summative insights on how each feature enables the
exploration and understanding of AI. We summarize by discussing recommendations
for future work to design and implement explainable medical AI systems that
encompass four recurring themes: motivation, constraint, explanation, and
justification.

The lack of diversity of the Artificial Intelligence (AI) field is nowadays a
concern, and several initiatives such as funding schemes and mentoring programs
have been designed to overcome it. However, there is no indication on how these
initiatives actually impact AI diversity in the short and long term. This work
studies the concept of diversity in this particular context and proposes a
small set of diversity indicators (i.e. indexes) of AI scientific events. These
indicators are designed to quantify the diversity of the AI field and monitor
its evolution. We consider diversity in terms of gender, geographical location
and business (understood as the presence of academia versus industry). We
compute these indicators for the different communities of a conference:
authors, keynote speakers and organizing committee. From these components we
compute a summarized diversity indicator for each AI event. We evaluate the
proposed indexes for a set of recent major AI conferences and we discuss their
values and limitations.

Business processes underpin a large number of enterprise operations including
processing loan applications, managing invoices, and insurance claims. There is
a large opportunity for infusing AI to reduce cost or provide better customer
experience, and the business process management (BPM) literature is rich in
machine learning solutions including unsupervised learning to gain insights on
clusters of process traces, classification models to predict the outcomes,
duration, or paths of partial process traces, extracting business process from
documents, and models to recommend how to optimize a business process or
navigate decision points. More recently, deep learning models including those
from the NLP domain have been applied to process predictions.
  Unfortunately, very little of these innovations have been applied and adopted
by enterprise companies. We assert that a large reason for the lack of adoption
of AI models in BPM is that business users are risk-averse and do not
implicitly trust AI models. There has, unfortunately, been little attention
paid to explaining model predictions to business users with process context. We
challenge the BPM community to build on the AI interpretability literature, and
the AI Trust community to understand

Artificial intelligence (AI) comes with great opportunities but can also pose
significant risks. Automatically generated explanations for decisions can
increase transparency and foster trust, especially for systems based on
automated predictions by AI models. However, given, e.g., economic incentives
to create dishonest AI, to what extent can we trust explanations? To address
this issue, our work investigates how AI models (i.e., deep learning, and
existing instruments to increase transparency regarding AI decisions) can be
used to create and detect deceptive explanations. As an empirical evaluation,
we focus on text classification and alter the explanations generated by
GradCAM, a well-established explanation technique in neural networks. Then, we
evaluate the effect of deceptive explanations on users in an experiment with
200 participants. Our findings confirm that deceptive explanations can indeed
fool humans. However, one can deploy machine learning (ML) methods to detect
seemingly minor deception attempts with accuracy exceeding 80% given sufficient
domain knowledge. Without domain knowledge, one can still infer inconsistencies
in the explanations in an unsupervised manner, given basic knowledge of the
predictive model under scrutiny.

Since the first Graphical User Interface (GUI) prototype was invented in the
1970s, GUI systems have been deployed into various personal computer systems
and server platforms. Recently, with the development of artificial intelligence
(AI) technology, malicious malware powered by AI is emerging as a potential
threat to GUI systems. This type of AI-based cybersecurity attack, targeting at
GUI systems, is explored in this paper. It is twofold: (1) A malware is
designed to attack the existing GUI system by using AI-based object recognition
techniques. (2) Its defensive methods are discovered by generating adversarial
examples and other methods to alleviate the threats from the intelligent GUI
attack. The results have shown that a generic GUI attack can be implemented and
performed in a simple way based on current AI techniques and its
countermeasures are temporary but effective to mitigate the threats of GUI
attack so far.

This paper looks at philosophical questions that arise in the context of AI
alignment. It defends three propositions. First, normative and technical
aspects of the AI alignment problem are interrelated, creating space for
productive engagement between people working in both domains. Second, it is
important to be clear about the goal of alignment. There are significant
differences between AI that aligns with instructions, intentions, revealed
preferences, ideal preferences, interests and values. A principle-based
approach to AI alignment, which combines these elements in a systematic way,
has considerable advantages in this context. Third, the central challenge for
theorists is not to identify 'true' moral principles for AI; rather, it is to
identify fair principles for alignment, that receive reflective endorsement
despite widespread variation in people's moral beliefs. The final part of the
paper explores three ways in which fair principles for AI alignment could
potentially be identified.

Today, AI is increasingly being used in many high-stakes decision-making
applications in which fairness is an important concern. Already, there are many
examples of AI being biased and making questionable and unfair decisions. The
AI research community has proposed many methods to measure and mitigate
unwanted biases, but few of them involve inputs from human policy makers. We
argue that because different fairness criteria sometimes cannot be
simultaneously satisfied, and because achieving fairness often requires
sacrificing other objectives such as model accuracy, it is key to acquire and
adhere to human policy makers' preferences on how to make the tradeoff among
these objectives. In this paper, we propose a framework and some exemplar
methods for eliciting such preferences and for optimizing an AI model according
to these preferences.

To achieve optimal human-system integration in the context of user-AI
interaction it is important that users develop a valid representation of how AI
works. In most of the everyday interaction with technical systems users
construct mental models (i.e., an abstraction of the anticipated mechanisms a
system uses to perform a given task). If no explicit explanations are provided
by a system (e.g. by a self-explaining AI) or other sources (e.g. an
instructor), the mental model is typically formed based on experiences, i.e.
the observations of the user during the interaction. The congruence of this
mental model and the actual systems functioning is vital, as it is used for
assumptions, predictions and consequently for decisions regarding system use. A
key question for human-centered AI research is therefore how to validly survey
users' mental models. The objective of the present research is to identify
suitable elicitation methods for mental model analysis. We evaluated whether
mental models are suitable as an empirical research method. Additionally,
methods of cognitive tutoring are integrated. We propose an exemplary method to
evaluate explainable AI approaches in a human-centered way.

In a hierarchically-structured cloud/edge/device computing environment,
workload allocation can greatly affect the overall system performance. This
paper deals with AI-oriented medical workload generated in emergency rooms (ER)
or intensive care units (ICU) in metropolitan areas. The goal is to optimize
AI-workload allocation to cloud clusters, edge servers, and end devices so that
minimum response time can be achieved in life-saving emergency applications.
  In particular, we developed a new workload allocation method for the AI
workload in distributed cloud/edge/device computing systems. An efficient
scheduling and allocation strategy is developed in order to reduce the overall
response time to satisfy multi-patient demands. We apply several ICU AI
workloads from a comprehensive edge computing benchmark Edge AIBench. The
healthcare AI applications involved are short-of-breath alerts, patient
phenotype classification, and life-death threats. Our experimental results
demonstrate the high efficiency and effectiveness in real-life health-care and
emergency applications.

In this paper we discuss how systems with Artificial Intelligence (AI) can
undergo safety assessment. This is relevant, if AI is used in safety related
applications. Taking a deeper look into AI models, we show, that many models of
artificial intelligence, in particular machine learning, are statistical
models. Safety assessment would then have t o concentrate on the model that is
used in AI, besides the normal assessment procedure. Part of the budget of
dangerous random failures for the relevant safety integrity level needs to be
used for the probabilistic faulty behavior of the AI system. We demonstrate our
thoughts with a simple example and propose a research challenge that may be
decisive for the use of AI in safety related systems.

Explainability in AI is gaining attention in the computer science community
in response to the increasing success of deep learning and the important need
of justifying how such systems make predictions in life-critical applications.
The focus of explainability in AI has predominantly been on trying to gain
insights into how machine learning systems function by exploring relationships
between input data and predicted outcomes or by extracting simpler
interpretable models. Through literature surveys of philosophy and social
science, authors have highlighted the sharp difference between these generated
explanations and human-made explanations and claimed that current explanations
in AI do not take into account the complexity of human interaction to allow for
effective information passing to not-expert users. In this paper we instantiate
the concept of structure of scientific explanation as the theoretical
underpinning for a general framework in which explanations for AI systems can
be implemented. This framework aims to provide the tools to build a
"mental-model" of any AI system so that the interaction with the user can
provide information on demand and be closer to the nature of human-made
explanations. We illustrate how we can utilize this framework through two very
different examples: an artificial neural network and a Prolog solver and we
provide a possible implementation for both examples.

Autonomous vehicles need safe development and testing environments. Many
traffic scenarios are such that they cannot be tested in the real world. We see
hybrid photorealistic simulation as a viable tool for developing AI (artificial
intelligence) software for autonomous driving. We present a machine learning
environment for detecting autonomous vehicle corner case behavior. Our
environment is based on connecting the CARLA simulation software to TensorFlow
machine learning framework and custom AI client software. The AI client
software receives data from a simulated world via virtual sensors and
transforms the data into information using machine learning models. The AI
clients control vehicles in the simulated world. Our environment monitors the
state assumed by the vehicle AIs to the ground truth state derived from the
simulation model. Our system can search for corner cases where the vehicle AI
is unable to correctly understand the situation. In our paper, we present the
overall hybrid simulator architecture and compare different configurations. We
present performance measurements from real setups, and outline the main
parameters affecting the hybrid simulator performance.

Healthcare is one of the most promising areas for machine learning models to
make a positive impact. However, successful adoption of AI-based systems in
healthcare depends on engaging and educating stakeholders from diverse
backgrounds about the development process of AI models. We present a broadly
accessible overview of the development life cycle of clinical AI models that is
general enough to be adapted to most machine learning projects, and then give
an in-depth case study of the development process of a deep learning based
system to detect aortic aneurysms in Computed Tomography (CT) exams. We hope
other healthcare institutions and clinical practitioners find the insights we
share about the development process useful in informing their own model
development efforts and to increase the likelihood of successful deployment and
integration of AI in healthcare.

Quantitative ultrasound (QUS) imaging is a reliable, fast and inexpensive
technique to extract physically descriptive parameters for assessing
pathologies. Despite its safety and efficacy, QUS suffers from several major
drawbacks: poor imaging quality, inter- and intra-observer variability which
hampers the reproducibility of measurements. Therefore, it is in great need to
develop automatic method to improve the imaging quality and aid in measurements
in QUS. In recent years, there has been an increasing interest in artificial
intelligence (AI) applications in ultrasound imaging. However, no research has
been found that surveyed the AI use in QUS. The purpose of this paper is to
review recent research into the AI applications in QUS. This review first
introduces the AI workflow, and then discusses the various AI applications in
QUS. Finally, challenges and future potential AI applications in QUS are
discussed.

Various recent Artificial Intelligence (AI) system failures, some of which
have made the global headlines, have highlighted issues in these systems. These
failures have resulted in calls for more ethical AI systems that better take
into account their effects on various stakeholders. However, implementing AI
ethics into practice is still an on-going challenge. High-level guidelines for
doing so exist, devised by governments and private organizations alike, but
lack practicality for developers. To address this issue, in this paper, we
present a method for implementing AI ethics. The method, ECCOLA, has been
iteratively developed using a cyclical action design research approach. The
method aims at making the high-level AI ethics principles more practical,
making it possible for developers to more easily implement them in practice.

Companies have considered adoption of various high-level artificial
intelligence (AI) principles for responsible AI, but there is less clarity on
how to implement these principles as organizational practices. This paper
reviews the principles-to-practices gap. We outline five explanations for this
gap ranging from a disciplinary divide to an overabundance of tools. In turn,
we argue that an impact assessment framework which is broad, operationalizable,
flexible, iterative, guided, and participatory is a promising approach to close
the principles-to-practices gap. Finally, to help practitioners with applying
these recommendations, we review a case study of AI's use in forest ecosystem
restoration, demonstrating how an impact assessment framework can translate
into effective and responsible AI practices.

In the past decade, Artificial Intelligence (AI) has become a part of our
daily lives due to major advances in Machine Learning (ML) techniques. In spite
of an explosive growth in the raw AI technology and in consumer facing
applications on the internet, its adoption in business applications has
conspicuously lagged behind. For business/mission-critical systems, serious
concerns about reliability and maintainability of AI applications remain. Due
to the statistical nature of the output, software 'defects' are not well
defined. Consequently, many traditional quality management techniques such as
program debugging, static code analysis, functional testing, etc. have to be
reevaluated. Beyond the correctness of an AI model, many other new quality
attributes, such as fairness, robustness, explainability, transparency, etc.
become important in delivering an AI system. The purpose of this paper is to
present a view of a holistic quality management framework for ML applications
based on the current advances and identify new areas of software engineering
research to achieve a more trustworthy AI.

Large and ever-evolving technology companies continue to invest more time and
resources to incorporate responsible Artificial Intelligence (AI) into
production-ready systems to increase algorithmic accountability. This paper
examines and seeks to offer a framework for analyzing how organizational
culture and structure impact the effectiveness of responsible AI initiatives in
practice. We present the results of semi-structured qualitative interviews with
practitioners working in industry, investigating common challenges, ethical
tensions, and effective enablers for responsible AI initiatives. Focusing on
major companies developing or utilizing AI, we have mapped what organizational
structures currently support or hinder responsible AI initiatives, what
aspirational future processes and structures would best enable effective
initiatives, and what key elements comprise the transition from current work
practices to the aspirational future.

Medical imaging AI systems such as disease classification and segmentation
are increasingly inspired and transformed from computer vision based AI
systems. Although an array of adversarial training and/or loss function based
defense techniques have been developed and proved to be effective in computer
vision, defending against adversarial attacks on medical images remains largely
an uncharted territory due to the following unique challenges: 1) label
scarcity in medical images significantly limits adversarial generalizability of
the AI system; 2) vastly similar and dominant fore- and background in medical
images make it hard samples for learning the discriminating features between
different disease classes; and 3) crafted adversarial noises added to the
entire medical image as opposed to the focused organ target can make clean and
adversarial examples more discriminate than that between different disease
classes. In this paper, we propose a novel robust medical imaging AI framework
based on Semi-Supervised Adversarial Training (SSAT) and Unsupervised
Adversarial Detection (UAD), followed by designing a new measure for assessing
systems adversarial risk. We systematically demonstrate the advantages of our
robust medical imaging AI system over the existing adversarial defense
techniques under diverse real-world settings of adversarial attacks using a
benchmark OCT imaging data set.

As AI models and services are used in a growing number of highstakes areas, a
consensus is forming around the need for a clearer record of how these models
and services are developed to increase trust. Several proposals for higher
quality and more consistent AI documentation have emerged to address ethical
and legal concerns and general social impacts of such systems. However, there
is little published work on how to create this documentation. This is the first
work to describe a methodology for creating the form of AI documentation we
call FactSheets. We have used this methodology to create useful FactSheets for
nearly two dozen models. This paper describes this methodology and shares the
insights we have gathered. Within each step of the methodology, we describe the
issues to consider and the questions to explore with the relevant people in an
organization who will be creating and consuming the AI facts in a FactSheet.
This methodology will accelerate the broader adoption of transparent AI
documentation.

Artificial intelligence (AI) is increasingly of tremendous interest in the
medical field. However, failures of medical AI could have serious consequences
for both clinical outcomes and the patient experience. These consequences could
erode public trust in AI, which could in turn undermine trust in our healthcare
institutions. This article makes two contributions. First, it describes the
major conceptual, technical, and humanistic challenges in medical AI. Second,
it proposes a solution that hinges on the education and accreditation of new
expert groups who specialize in the development, verification, and operation of
medical AI technologies. These groups will be required to maintain trust in our
healthcare institutions.

The use of Artificial Intelligence (AI) and Machine Learning (ML) to solve
cybersecurity problems has been gaining traction within industry and academia,
in part as a response to widespread malware attacks on critical systems, such
as cloud infrastructures, government offices or hospitals, and the vast amounts
of data they generate. AI- and ML-assisted cybersecurity offers data-driven
automation that could enable security systems to identify and respond to cyber
threats in real time. However, there is currently a shortfall of professionals
trained in AI and ML for cybersecurity. Here we address the shortfall by
developing lab-intensive modules that enable undergraduate and graduate
students to gain fundamental and advanced knowledge in applying AI and ML
techniques to real-world datasets to learn about Cyber Threat Intelligence
(CTI), malware analysis, and classification, among other important topics in
cybersecurity.
  Here we describe six self-contained and adaptive modules in "AI-assisted
Malware Analysis." Topics include: (1) CTI and malware attack stages, (2)
malware knowledge representation and CTI sharing, (3) malware data collection
and feature identification, (4) AI-assisted malware detection, (5) malware
classification and attribution, and (6) advanced malware research topics and
case studies such as adversarial learning and Advanced Persistent Threat (APT)
detection.

There is currently a rapid increase in the number of challenge problem,
benchmarking datasets and algorithmic optimization tests for evaluating AI
systems. However, there does not currently exist an objective measure to
determine the complexity between these newly created domains. This lack of
cross-domain examination creates an obstacle to effectively research more
general AI systems. We propose a theory for measuring the complexity between
varied domains. This theory is then evaluated using approximations by a
population of neural network based AI systems. The approximations are compared
to other well known standards and show it meets intuitions of complexity. An
application of this measure is then demonstrated to show its effectiveness as a
tool in varied situations. The experimental results show this measure has
promise as an effective tool for aiding in the evaluation of AI systems. We
propose the future use of such a complexity metric for use in computing an AI
system's intelligence.

Medical Artificial Intelligence (AI) involves the application of machine
learning algorithms to biomedical datasets in order to improve medical
practices. Products incorporating medical AI require certification before
deployment in most jurisdictions. To date, clear pathways for regulating
medical AI are still under development. Below the level of formal pathways lies
the actual practice of developing a medical AI solution. This Perspective
proposes best practice guidelines for development compatible with the
production of a regulatory package which, regardless of the formal regulatory
path, will form a core component of a certification process. The approach is
predicated on a statistical risk perspective, typical of medical device
regulators, and a deep understanding of machine learning methodologies. These
guidelines will allow all parties to communicate more clearly in the
development of a common Good Machine Learning Practice (GMLP), and thus lead to
the enhanced development of both medical AI products and regulations.

Technological advances of virtually every kind pose risks to society
including fairness and bias. We review a long-standing wisdom that a widespread
practical deployment of any technology may produce adverse side effects
misusing the knowhow. This includes AI but AI systems are not solely
responsible for societal risks. We describe some of the common and AI specific
risks in health industries and other sectors and propose both broad and
specific solutions. Each technology requires very specialized and informed
tracking, monitoring and creative solutions. We postulate that AI systems are
uniquely poised to produce conceptual and methodological solutions to both
fairness and bias in automated decision-making systems. We propose a simple
intelligent system quotient that may correspond to their adverse societal
impact and outline a multi-tier architecture for producing solutions of
increasing complexity to these risks. We also propose that universities may
consider forming interdisciplinary Study of Future Technology Centers to
investigate and predict the fuller range of risks posed by technology and seek
both common and AI specific solutions using computational, technical,
conceptual and ethical analysis

Artificial intelligence (AI) is playing an increasingly significant role in
our everyday lives. This trend is expected to continue, especially with recent
pushes to move more AI to the edge. However, one of the biggest challenges
associated with AI on edge devices (mobile phones, unmanned vehicles, sensors,
etc.) is their associated size, weight, and power constraints. In this work, we
consider the scenario where an AI system may need to operate at
less-than-maximum accuracy in order to meet application-dependent energy
requirements. We propose a simple function that divides the cost of using an AI
system into the cost of the decision making process and the cost of decision
execution. For simple binary decision problems with convolutional neural
networks, it is shown that minimizing the cost corresponds to using fewer than
the maximum number of resources (e.g. convolutional neural network layers and
filters). Finally, it is shown that the cost associated with energy can be
significantly reduced by leveraging high-confidence predictions made in
lower-level layers of the network.

Artificial intelligence (AI) is already part of our daily lives and is
playing a key role in defining the economic and social shape of the future. In
2018, the European Commission introduced its AI strategy able to compete in the
next years with world powers such as China and US, but relying on the respect
of European values and fundamental rights. As a result, most of the Member
States have published their own National Strategy with the aim to work on a
coordinated plan for Europe. In this paper, we present an ongoing study on how
European countries are approaching the field of Artificial Intelligence, with
its promises and risks, through the lens of their national AI strategies. In
particular, we aim to investigate how European countries are investing in AI
and to what extent the stated plans can contribute to the benefit of the whole
society. This paper reports the main findings of a qualitative analysis of the
investment plans reported in 15 European National Strategies

The AI chips increasingly focus on implementing neural computing at low power
and cost. The intelligent sensing, automation, and edge computing applications
have been the market drivers for AI chips. Increasingly, the generalisation,
performance, robustness, and scalability of the AI chip solutions are compared
with human-like intelligence abilities. Such a requirement to transit from
application-specific to general intelligence AI chip must consider several
factors. This paper provides an overview of this cross-disciplinary field of
study, elaborating on the generalisation of intelligence as understood in
building artificial general intelligence (AGI) systems. This work presents a
listing of emerging AI chip technologies, classification of edge AI
implementations, and the funnel design flow for AGI chip development. Finally,
the design consideration required for building an AGI chip is listed along with
the methods for testing and validating it.

This paper addresses the question of how to align AI systems with human
values and situates it within a wider body of thought regarding technology and
value. Far from existing in a vacuum, there has long been an interest in the
ability of technology to 'lock-in' different value systems. There has also been
considerable thought about how to align technologies with specific social
values, including through participatory design-processes. In this paper we look
more closely at the question of AI value alignment and suggest that the power
and autonomy of AI systems gives rise to opportunities and challenges in the
domain of value that have not been encountered before. Drawing important
continuities between the work of the fairness, accountability, transparency and
ethics community, and work being done by technical AI safety researchers, we
suggest that more attention needs to be paid to the question of 'social value
alignment' - that is, how to align AI systems with the plurality of values
endorsed by groups of people, especially on the global level.

Understanding the actions of both humans and artificial intelligence (AI)
agents is important before modern AI systems can be fully integrated into our
daily life. In this paper, we show that, despite their current huge success,
deep learning based AI systems can be easily fooled by subtle adversarial noise
to misinterpret the intention of an action in interaction scenarios. Based on a
case study of skeleton-based human interactions, we propose a novel adversarial
attack on interactions, and demonstrate how DNN-based interaction models can be
tricked to predict the participants' reactions in unexpected ways. From a
broader perspective, the scope of our proposed attack method is not confined to
problems related to skeleton data but can also be extended to any type of
problems involving sequential regressions. Our study highlights potential risks
in the interaction loop with AI and humans, which need to be carefully
addressed when deploying AI systems in safety-critical applications.

Radiotherapy, due to its technology-intensive nature and reliance on digital
data and human-machine interactions, is particularly suited to benefit from
artificial intelligence (AI) to improve the accuracy and efficiency of its
clinical workflow. Recently, various artificial intelligence (AI) methods have
been successfully developed to exploit the benefit of the inherent physical
properties of particle therapy. Many reviews about AI applications in
radiotherapy have already been published, but none were specifically dedicated
to particle therapy. In this article, we present a comprehensive review of the
recent published works on AI applications in particle therapy, which can be
classified into particle therapy treatment planning, adaptive particle therapy,
range and dose verification and other applications in particle therapy.
Although promising results reported in these works demonstrate how AI-based
methods can help exploit the intrinsic physic advantages of particle therapy,
challenges remained to be address before AI applications in particle therapy
enjoy widespread implementation in clinical practice.

Qualitative inductive methods are widely used in CSCW and HCI research for
their ability to generatively discover deep and contextualized insights, but
these inherently manual and human-resource-intensive processes are often
infeasible for analyzing large corpora. Researchers have been increasingly
interested in ways to apply qualitative methods to "big" data problems, hoping
to achieve more generalizable results from larger amounts of data while
preserving the depth and richness of qualitative methods. In this paper, we
describe a study of qualitative researchers' work practices and their
challenges, with an eye towards whether this is an appropriate domain for
human-AI collaboration and what successful collaborations might entail. Our
findings characterize participants' diverse methodological practices and
nuanced collaboration dynamics, and identify areas where they might benefit
from AI-based tools. While participants highlight the messiness and uncertainty
of qualitative inductive analysis, they still want full agency over the process
and believe that AI should not interfere. Our study provides a deep
investigation of task delegability in human-AI collaboration in the context of
qualitative analysis, and offers directions for the design of AI assistance
that honor serendipity, human agency, and ambiguity.

The paper describes a Multisource AI Scorecard Table (MAST) that provides the
developer and user of an artificial intelligence (AI)/machine learning (ML)
system with a standard checklist focused on the principles of good analysis
adopted by the intelligence community (IC) to help promote the development of
more understandable systems and engender trust in AI outputs. Such a scorecard
enables a transparent, consistent, and meaningful understanding of AI tools
applied for commercial and government use. A standard is built on compliance
and agreement through policy, which requires buy-in from the stakeholders.
While consistency for testing might only exist across a standard data set, the
community requires discussion on verification and validation approaches which
can lead to interpretability, explainability, and proper use. The paper
explores how the analytic tradecraft standards outlined in Intelligence
Community Directive (ICD) 203 can provide a framework for assessing the
performance of an AI system supporting various operational needs. These include
sourcing, uncertainty, consistency, accuracy, and visualization. Three use
cases are presented as notional examples that support security for comparative
analysis.

Code obfuscation aims at protecting Intellectual Property and other secrets
embedded into software from being retrieved. Recent works leverage advances in
artificial intelligence with the hope of getting blackbox deobfuscators
completely immune to standard (whitebox) protection mechanisms. While
promising, this new field of AI-based blackbox deobfuscation is still in its
infancy. In this article we deepen the state of AI-based blackbox deobfuscation
in three key directions: understand the current state-of-the-art, improve over
it and design dedicated protection mechanisms. In particular, we define a novel
generic framework for AI-based blackbox deobfuscation encompassing prior work
and highlighting key components; we are the first to point out that the search
space underlying code deobfuscation is too unstable for simulation-based
methods (e.g., Monte Carlo Tres Search used in prior work) and advocate the use
of robust methods such as S-metaheuritics; we propose the new optimized
AI-based blackbox deobfuscator Xyntia which significantly outperforms prior
work in terms of success rate (especially with small time budget) while being
completely immune to the most recent anti-analysis code obfuscation methods;
and finally we propose two novel protections against AI-based blackbox
deobfuscation, allowing to counter Xyntia's powerful attacks.

In this paper from communication channel coding perspective we are able to
present both a theoretical and practical discussion of AI's uncertainty,
capacity and evolution for pattern classification based on the classical
Rademacher complexity and Shannon entropy. First AI capacity is defined as in
communication channels. It is shown qualitatively that the classical Rademacher
complexity and Shannon entropy used in communication theory is closely related
by their definitions, given a pattern classification problem with a complexity
measured by Rademacher complexity. Secondly based on the Shannon mathematical
theory on communication coding, we derive several sufficient and necessary
conditions for an AI's error rate approaching zero in classifications problems.
A 1/2 criteria on Shannon entropy is derived in this paper so that error rate
can approach zero or is zero for AI pattern classification problems. Last but
not least, we show our analysis and theory by providing examples of AI pattern
classifications with error rate approaching zero or being zero.

We propose to build directly upon our longstanding, prior r&d in AI/machine
ethics in order to attempt to make real the blue-sky idea of AI that can thwart
mass shootings, by bringing to bear its ethical reasoning. The r&d in question
is overtly and avowedly logicist in form, and since we are hardly the only ones
who have established a firm foundation in the attempt to imbue AI's with their
own ethical sensibility, the pursuit of our proposal by those in different
methodological camps should, we believe, be considered as well. We seek herein
to make our vision at least somewhat concrete by anchoring our exposition to
two simulations, one in which the AI saves the lives of innocents by locking
out a malevolent human's gun, and a second in which this malevolent agent is
allowed by the AI to be neutralized by law enforcement. Along the way, some
objections are anticipated, and rebutted.

The transition from AI/ML models to production-ready AI-based systems is a
challenge for both data scientists and software engineers. In this paper, we
report the results of a workshop conducted in a consulting company to
understand how this transition is perceived by practitioners. Starting from the
need for making AI experiments reproducible, the main themes that emerged are
related to the use of the Jupyter Notebook as the primary prototyping tool, and
the lack of support for software engineering best practices as well as data
science specific functionalities.

Artificial intelligence (AI) and Machine Learning (ML) are becoming pervasive
in today's applications, such as autonomous vehicles, healthcare, aerospace,
cybersecurity, and many critical applications. Ensuring the reliability and
robustness of the underlying AI/ML hardware becomes our paramount importance.
In this paper, we explore and evaluate the reliability of different AI/ML
hardware. The first section outlines the reliability issues in a commercial
systolic array-based ML accelerator in the presence of faults engendering from
device-level non-idealities in the DRAM. Next, we quantified the impact of
circuit-level faults in the MSB and LSB logic cones of the Multiply and
Accumulate (MAC) block of the AI accelerator on the AI/ML accuracy. Finally, we
present two key reliability issues -- circuit aging and endurance in emerging
neuromorphic hardware platforms and present our system-level approach to
mitigate them.

A pervasive design issue of AI systems is their explainability--how to
provide appropriate information to help users understand the AI. The technical
field of explainable AI (XAI) has produced a rich toolbox of techniques.
Designers are now tasked with the challenges of how to select the most suitable
XAI techniques and translate them into UX solutions. Informed by our previous
work studying design challenges around XAI UX, this work proposes a design
process to tackle these challenges. We review our and related prior work to
identify requirements that the process should fulfill, and accordingly, propose
a Question-Driven Design Process that grounds the user needs, choices of XAI
techniques, design, and evaluation of XAI UX all in the user questions. We
provide a mapping guide between prototypical user questions and exemplars of
XAI techniques to reframe the technical space of XAI, also serving as boundary
objects to support collaboration between designers and AI engineers. We
demonstrate it with a use case of designing XAI for healthcare adverse events
prediction, and discuss lessons learned for tackling design challenges of AI
systems.

Thinking of technology as a design material is appealing. It encourages
designers to explore the material's properties to understand its capabilities
and limitations, a prerequisite to generative design thinking. However, as a
material, AI resists this approach because its properties emerge as part of the
design process itself. Therefore, designers and AI engineers must collaborate
in new ways to create both the material and its application experience. We
investigate the co-creation process through a design study with 10 pairs of
designers and engineers. We find that design 'probes' with user data are a
useful tool in defining AI materials. Through data probes, designers construct
designerly representations of the envisioned AI experience (AIX) to identify
desirable AI characteristics. Data probes facilitate divergent thinking,
material testing, and design validation. Based on our findings, we propose a
process model for co-creating AIX and offer design considerations for
incorporating data probes in design tools.

This paper aims to provide an overview of the ethical concerns in artificial
intelligence (AI) and the framework that is needed to mitigate those risks, and
to suggest a practical path to ensure the development and use of AI at the
United Nations (UN) aligns with our ethical values. The overview discusses how
AI is an increasingly powerful tool with potential for good, albeit one with a
high risk of negative side-effects that go against fundamental human rights and
UN values. It explains the need for ethical principles for AI aligned with
principles for data governance, as data and AI are tightly interwoven. It
explores different ethical frameworks that exist and tools such as assessment
lists. It recommends that the UN develop a framework consisting of ethical
principles, architectural standards, assessment methods, tools and
methodologies, and a policy to govern the implementation and adherence to this
framework, accompanied by an education program for staff.

The sixth generation (6G) systems are generally recognized to be established
on ubiquitous Artificial Intelligence (AI) and distributed ledger such as
blockchain. However, the AI training demands tremendous computing resource,
which is limited in most 6G devices. Meanwhile, miners in Proof-of-Work (PoW)
based blockchains devote massive computing power to block mining, and are
widely criticized for the waste of computation. To address this dilemma, we
propose an Evolved-Proof-of-Work (E-PoW) consensus that can integrate the
matrix computations, which are widely existed in AI training, into the process
of brute-force searches in the block mining. Consequently, E-PoW can connect AI
learning and block mining via the multiply used common computing resource.
Experimental results show that E-PoW can salvage by up to 80 percent computing
power from pure block mining for parallel AI training in 6G systems.

Given an on-board diagnostics (OBD) dataset and a physics-based emissions
prediction model, this paper aims to develop an accurate and
computational-efficient AI (Artificial Intelligence) method that predicts
vehicle emissions. The problem is of societal importance because vehicular
emissions lead to climate change and impact human health. This problem is
challenging because the OBD data does not contain enough parameters needed by
high-order physics models. Conversely, related work has shown that low-order
physics models have poor predictive accuracy when using available OBD data.
This paper uses a divergent window co-occurrence pattern detection method to
develop a spatiotemporal variability-aware AI model for predicting emission
values from the OBD datasets. We conducted a case study using real-world OBD
data from a local public transportation agency. Results show that the proposed
AI method has approximately 65% improved predictive accuracy than a non-AI
low-order physics model and is approximately 35% more accurate than a baseline
model.

To implement fair machine learning in a sustainable way, choosing the right
fairness objective is key. Since fairness is a concept of justice which comes
in various, sometimes conflicting definitions, this is not a trivial task
though. The most appropriate fairness definition for an artificial intelligence
(AI) system is a matter of ethical standards and legal requirements, and the
right choice depends on the particular use case and its context. In this
position paper, we propose to use a decision tree as means to explain and
justify the implemented kind of fairness to the end users. Such a structure
would first of all support AI practitioners in mapping ethical principles to
fairness definitions for a concrete application and therefore make the
selection a straightforward and transparent process. However, this approach
would also help document the reasoning behind the decision making. Due to the
general complexity of the topic of fairness in AI, we argue that specifying
"fairness" for a given use case is the best way forward to maintain confidence
in AI systems. In this case, this could be achieved by sharing the reasons and
principles expressed during the decision making process with the broader
audience.

In today's society, AI systems are increasingly used to make critical
decisions such as credit scoring and patient triage. However, great convenience
brought by AI systems comes with troubling prevalence of bias against
underrepresented groups. Mitigating bias in AI systems to increase overall
fairness has emerged as an important challenge. Existing studies on mitigating
bias in AI systems focus on eliminating sensitive demographic information
embedded in data. Given the temporal and contextual complexity of
conceptualizing fairness, lossy treatment of demographic information may
contribute to an unnecessary trade-off between accuracy and fairness,
especially when demographic attributes and class labels are correlated. In this
paper, we present an information-lossless de-biasing technique that targets the
scarcity of data in the disadvantaged group. Unlike the existing work, we
demonstrate, both theoretically and empirically, that oversampling
underrepresented groups can not only mitigate algorithmic bias in AI systems
that consistently predict a favorable outcome for a certain group, but improve
overall accuracy by mitigating class imbalance within data that leads to a bias
towards the majority class. We demonstrate the effectiveness of our technique
on real datasets using a variety of fairness metrics.

Recent advances in artificial intelligence (AI) have achieved human-scale
speed and accuracy for classification tasks. In turn, these capabilities have
made AI a viable replacement for many human activities that at their core
involve classification, such as basic mechanical and analytical tasks in
low-level service jobs. Current systems do not need to be conscious to
recognize patterns and classify them. However, for AI to progress to more
complicated tasks requiring intuition and empathy, it must develop capabilities
such as metathinking, creativity, and empathy akin to human self-awareness or
consciousness. We contend that such a paradigm shift is possible only through a
fundamental shift in the state of artificial intelligence toward consciousness,
a shift similar to what took place for humans through the process of natural
selection and evolution. As such, this paper aims to theoretically explore the
requirements for the emergence of consciousness in AI. It also provides a
principled understanding of how conscious AI can be detected and how it might
be manifested in contrast to the dominant paradigm that seeks to ultimately
create machines that are linguistically indistinguishable from humans.

Currently, there is a surge of interest in fair Artificial Intelligence (AI)
and Machine Learning (ML) research which aims to mitigate discriminatory bias
in AI algorithms, e.g. along lines of gender, age, and race. While most
research in this domain focuses on developing fair AI algorithms, in this work,
we show that a fair AI algorithm on its own may be insufficient to achieve its
intended results in the real world. Using career recommendation as a case
study, we build a fair AI career recommender by employing gender debiasing
machine learning techniques. Our offline evaluation showed that the debiased
recommender makes fairer career recommendations without sacrificing its
accuracy. Nevertheless, an online user study of more than 200 college students
revealed that participants on average prefer the original biased system over
the debiased system. Specifically, we found that perceived gender disparity is
a determining factor for the acceptance of a recommendation. In other words,
our results demonstrate we cannot fully address the gender bias issue in AI
recommendations without addressing the gender bias in humans.

Adversarial images highlight how vulnerable modern image classifiers are to
perturbations outside of their training set. Human oversight might mitigate
this weakness, but depends on humans understanding the AI well enough to
predict when it is likely to make a mistake. In previous work we have found
that humans tend to assume that the AI's decision process mirrors their own.
Here we evaluate if methods from explainable AI can disrupt this assumption to
help participants predict AI classifications for adversarial and standard
images. We find that both saliency maps and examples facilitate catching AI
errors, but their effects are not additive, and saliency maps are more
effective than examples.

The article proposes a new type of AI system that does not give solutions
directly but rather points toward it, friendly prompting the user with
questions and adjusting messages. Models of AI human collaboration can be
deduced from the classic literary example of interaction between Mr. Holmes and
Dr. Watson from the stories by Conan Doyle, where the highly qualified expert
Mr. Holmes answers questions posed by Dr. Watson. Here Mr. Holmes, with his
rule-based calculations, logic, and memory management, apparently plays the
role of an AI system, and Dr. Watson is the user. Looking into the same
Holmes-Watson interaction, we find and promote another model in which the AI
behaves like Dr. Watson, who, by asking questions and acting in a particular
way, helps Holmes (the AI user) make the right decisions. We call the systems
based on this principle "Dr. Watson-type systems." The article describes the
properties of such systems and introduces two particular: Patient Management
System for intensive care physicians and Data Error Prevention System.

Test automation is common in software development; often one tests repeatedly
to identify regressions. If the amount of test cases is large, one may select a
subset and only use the most important test cases. The regression test
selection (RTS) could be automated and enhanced with Artificial Intelligence
(AI-RTS). This however could introduce ethical challenges. While such
challenges in AI are in general well studied, there is a gap with respect to
ethical AI-RTS. By exploring the literature and learning from our experiences
of developing an industry AI-RTS tool, we contribute to the literature by
identifying three challenges (assigning responsibility, bias in decision-making
and lack of participation) and three approaches (explicability, supervision and
diversity). Additionally, we provide a checklist for ethical AI-RTS to help
guide the decision-making of the stakeholders involved in the process.

In April 2021, the European Commission proposed a Regulation on Artificial
Intelligence, known as the AI Act. We present an overview of the Act and
analyse its implications, drawing on scholarship ranging from the study of
contemporary AI practices to the structure of EU product safety regimes over
the last four decades. Aspects of the AI Act, such as different rules for
different risk-levels of AI, make sense. But we also find that some provisions
of the Draft AI Act have surprising legal implications, whilst others may be
largely ineffective at achieving their stated goals. Several overarching
aspects, including the enforcement regime and the risks of maximum
harmonisation pre-empting legitimate national AI policy, engender significant
concern. These issues should be addressed as a priority in the legislative
process.

This chapter provides an overview of the different Artificial Intelligence
(AI) systems that are being used in contemporary digital tools for Mathematics
Education (ME). It is aimed at researchers in AI and Machine Learning (ML), for
whom we shed some light on the specific technologies that are being used in
educational applications; and at researchers in ME, for whom we clarify: i)
what the possibilities of the current AI technologies are, ii) what is still
out of reach and iii) what is to be expected in the near future. We start our
analysis by establishing a high-level taxonomy of AI tools that are found as
components in digital ME applications. Then, we describe in detail how these AI
tools, and in particular ML, are being used in two key applications,
specifically AI-based calculators and intelligent tutoring systems. We finish
the chapter with a discussion about student modeling systems and their
relationship to artificial general intelligence.

The new generation of cyber threats leverages advanced AI-aided methods,
which make them capable to launch multi-stage, dynamic, and effective attacks.
Current cyber-defense systems encounter various challenges to defend against
such new and emerging threats. Modeling AI-aided threats through game theory
models can help the defender to select optimal strategies against the attacks
and make wise decisions to mitigate the attack's impact. This paper first
explores the current state-of-the-art in the new generation of threats in which
AI techniques such as deep neural network is used for the attacker and
discusses further challenges. We propose a Markovian dynamic game that can
evaluate the efficiency of defensive methods against the AI-aided attacker
under a cloud-based system in which the attacker utilizes an AI technique to
launch an advanced attack by finding the shortest attack path. We use the CVSS
metrics to quantify the values of this zero-sum game model for decision-making.

Game AI designers must manage complex interactions between the AI character,
the game world, and the player, while achieving their design visions.
Computational co-creativity tools can aid them, but first, AI and HCI
researchers must gather requirements and determine design heuristics to build
effective co-creative tools. In this work, we present a participatory design
study that categorizes and analyzes game AI designers' workflows, goals, and
expectations for such tools. We evince deep connections between game AI design
and the design of co-creative tools, and present implications for future
co-creativity tool research and development.

The use of artificial intelligence (AI) and AI methods in the workplace holds
both great opportunities as well as risks to occupational safety and
discrimination. In addition to legal regulation, technical standards will play
a key role in mitigating such risk by defining technical requirements for
development and testing of AI systems. This paper provides an overview and
assessment of existing international, European and German standards as well as
those currently under development. The paper is part of the research project
"ExamAI - Testing and Auditing of AI systems" and focusses on the use of AI in
an industrial production environment as well as in the realm of human resource
management (HR).

In this commentary, we respond to a recent editorial letter by Professor
Luciano Floridi entitled 'AI as a public service: Learning from Amsterdam and
Helsinki'. Here, Floridi considers the positive impact of these municipal AI
registers, which collect a limited number of algorithmic systems used by the
city of Amsterdam and Helsinki. There are a number of assumptions about AI
registers as a governance model for automated systems that we seek to question.
Starting with recent attempts to normalize AI by decontextualizing and
depoliticizing it, which is a fraught political project that encourages what we
call 'ethics theater' given the proven dangers of using these systems in the
context of the digital welfare state. We agree with Floridi that much can be
learned from these registers about the role of AI systems in municipal city
management. Yet, the lessons we draw, on the basis of our extensive
ethnographic engagement with digital well-fare states are distinctly less
optimistic.

Vessel trajectory prediction plays a pivotal role in numerous maritime
applications and services. While the Automatic Identification System (AIS)
offers a rich source of information to address this task, forecasting vessel
trajectory using AIS data remains challenging, even for modern machine learning
techniques, because of the inherent heterogeneous and multimodal nature of
motion data. In this paper, we propose a novel approach to tackle these
challenges. We introduce a discrete, high-dimensional representation of AIS
data and a new loss function designed to explicitly address heterogeneity and
multimodality. The proposed model-referred to as TrAISformer-is a modified
transformer network that extracts long-term temporal patterns in AIS vessel
trajectories in the proposed enriched space to forecast the positions of
vessels several hours ahead. We report experimental results on real, publicly
available AIS data. TrAISformer significantly outperforms state-of-the-art
methods, with an average prediction performance below 10 nautical miles up to
~10 hours.

State of the art Artificial Intelligence (AI) techniques have reached an
impressive complexity. Consequently, researchers are discovering more and more
methods to use them in real-world applications. However, the complexity of such
systems requires the introduction of methods that make those transparent to the
human user. The AI community is trying to overcome the problem by introducing
the Explainable AI (XAI) field, which is tentative to make AI algorithms less
opaque. However, in recent years, it became clearer that XAI is much more than
a computer science problem: since it is about communication, XAI is also a
Human-Agent Interaction problem. Moreover, AI came out of the laboratories to
be used in real life. This implies the need for XAI solutions tailored to
non-expert users. Hence, we propose a user-centred framework for XAI that
focuses on its social-interactive aspect taking inspiration from cognitive and
social sciences' theories and findings. The framework aims to provide a
structure for interactive XAI solutions thought for non-expert users.

Artificial Intelligence (AI), especially Neural Networks (NNs), has become
increasingly popular. However, people usually treat AI as a tool, focusing on
improving outcome, accuracy, and performance while paying less attention to the
representation of AI itself. We present AIive, an interactive visualization of
AI in Virtual Reality (VR) that brings AI "alive". AIive enables users to
manipulate the parameters of NNs with virtual hands and provides auditory
feedback for the real-time values of loss, accuracy, and hyperparameters. Thus,
AIive contributes an artistic and intuitive way to represent AI by integrating
visualization, sonification, and direct manipulation in VR, potentially
targeting a wide range of audiences.

Human-Centered AI (HCAI) refers to the research effort that aims to design
and implement AI techniques to support various human tasks, while taking human
needs into consideration and preserving human control. In this short position
paper, we illustrate how we approach HCAI using a series of research projects
around Data Science (DS) works as a case study. The AI techniques built for
supporting DS works are collectively referred to as AutoML systems, and their
goals are to automate some parts of the DS workflow. We illustrate a three-step
systematical research approach(i.e., explore, build, and integrate) and four
practical ways of implementation for HCAI systems. We argue that our work is a
cornerstone towards the ultimate future of Human-AI Collaboration for DS and
beyond, where AI and humans can take complementary and indispensable roles to
achieve a better outcome and experience.

Artificial Intelligence (AI) is increasingly used to analyze large amounts of
data in various practices, such as object recognition. We are specifically
interested in using AI-powered systems to engage local communities in
developing plans or solutions for pressing societal and environmental concerns.
Such local contexts often involve multiple stakeholders with different and even
contradictory agendas, resulting in mismatched expectations of these systems'
behaviors and desired outcomes. There is a need to investigate if AI models and
pipelines can work as expected in different contexts through co-creation and
field deployment. Based on case studies in co-creating AI-powered systems with
local people, we explain challenges that require more attention and provide
viable paths to bridge AI research with citizen needs. We advocate for
developing new collaboration approaches and mindsets that are needed to
co-create AI-powered systems in multi-stakeholder contexts to address local
concerns.

Social Justice oriented Engineering Education frameworks have been developed
to help guide engineering students' decisions about which projects will
genuinely address human needs to create a better and more equitable society. In
this paper, we explore the role such theories might play in the field of
AI-HRI, consider the extent to which our community is (or is not) aligned with
these recommendations, and envision a future in which our research community
takes guidance from these theories. In particular, we analyze recent AI-HRI
(through analysis of 2020 AI-HRI papers) and consider possible futures of
AI-HRI (through a speculative ethics exercise). Both activities are guided
through the lens of the Engineering for Social Justice (E4SJ) framework, which
centers contextual listening and enhancement of human capabilities. Our
analysis suggests that current AI-HRI research is not well aligned with the
guiding principles of Engineering for Social Justice, and as such, does not
obviously meet the needs of the communities we could be helping most. As such,
we suggest that motivating future work through the E4SJ framework could help to
ensure that we as researchers are developing technologies that will actually
lead to a more equitable world.

Artificial intelligence (AI) is increasingly utilized in synthesizing
visuals, texts, and audio. These AI-based works, often derived from neural
networks, are entering the mainstream market, as digital paintings, songs,
books, and others. We conceptualize both existing and future human-in-the-loop
(HITL) approaches for creative applications and to develop more expressive,
nuanced, and multimodal models. Particularly, how can our expertise as curators
and collaborators be encoded in AI models in an interactive manner? We examine
and speculate on long term implications for models, interfaces, and machine
creativity. Our selection, creation, and interpretation of AI art inherently
contain our emotional responses, cultures, and contexts. Therefore, the
proposed HITL may help algorithms to learn creative processes that are much
harder to codify or quantify. We envision multimodal HITL processes, where
texts, visuals, sounds, and other information are coupled together, with
automated analysis of humans and environments. Overall, these HITL approaches
will increase interaction between human and AI, and thus help the future AI
systems to better understand our own creative and emotional processes.

Recent AI-related scandals have shed a spotlight on accountability in AI,
with increasing public interest and concern. This paper draws on literature
from public policy and governance to make two contributions. First, we propose
an AI accountability ecosystem as a useful lens on the system, with different
stakeholders requiring and contributing to specific accountability mechanisms.
We argue that the present ecosystem is unbalanced, with a need for improved
transparency via AI explainability and adequate documentation and process
formalisation to support internal audit, leading up eventually to external
accreditation processes. Second, we use a case study in the gambling sector to
illustrate in a subset of the overall ecosystem the need for industry-specific
accountability principles and processes. We define and evaluate critically the
implementation of key accountability principles in the gambling industry,
namely addressing algorithmic bias and model explainability, before concluding
and discussing directions for future work based on our findings. Keywords:
Accountability, Explainable AI, Algorithmic Bias, Regulation.

Artificial intelligence (AI) continues to find more numerous and more
critical applications in the financial services industry, giving rise to fair
and ethical AI as an industry-wide objective. While many ethical principles and
guidelines have been published in recent years, they fall short of addressing
the serious challenges that model developers face when building ethical AI
solutions. We survey the practical and overarching issues surrounding model
development, from design and implementation complexities, to the shortage of
tools, and the lack of organizational constructs. We show how practical
considerations reveal the gaps between high-level principles and concrete,
deployed AI applications, with the aim of starting industry-wide conversations
toward solution approaches.

The Second Language Acquisition field has been significantly impacted by a
greater emphasis on individualized learning and rapid developments in
artificial intelligence (AI). Although increasingly adaptive language learning
tools are being developed with the application of AI to the Computer Assisted
Language Learning field, there have been concerns regarding insufficient
information and teacher preparation. To effectively utilize these tools,
teachers need an in-depth overview on recently developed AI-based language
learning tools. Therefore, this review synthesized information on AI tools that
were developed between 2017 and 2020. A majority of these tools utilized
machine learning and natural language processing, and were used to identify
errors, provide feedback, and assess language abilities. After using these
tools, learners demonstrated gains in their language abilities and knowledge.
This review concludes by presenting pedagogical implications and emerging
themes in the future research of AI-based language learning tools.

The past decade has seen significant progress in artificial intelligence
(AI), which has resulted in algorithms being adopted for resolving a variety of
problems. However, this success has been met by increasing model complexity and
employing black-box AI models that lack transparency. In response to this need,
Explainable AI (XAI) has been proposed to make AI more transparent and thus
advance the adoption of AI in critical domains. Although there are several
reviews of XAI topics in the literature that identified challenges and
potential research directions in XAI, these challenges and research directions
are scattered. This study, hence, presents a systematic meta-survey for
challenges and future research directions in XAI organized in two themes: (1)
general challenges and research directions in XAI and (2) challenges and
research directions in XAI based on machine learning life cycle's phases:
design, development, and deployment. We believe that our meta-survey
contributes to XAI literature by providing a guide for future exploration in
the XAI area.

Policymakers face a broader challenge of how to view AI capabilities today
and where does society stand in terms of those capabilities. This paper surveys
AI capabilities and tackles this very issue, exploring it in context of
political security in digitally networked societies. We extend the ideas of
Information Management to better understand contemporary AI systems as part of
a larger and more complex information system. Comprehensively reviewing AI
capabilities and contemporary man-machine interactions, we undertake conceptual
development to suggest that better information management could allow states to
more optimally offset the risks of AI enabled influence and better utilise the
emerging capabilities which these systems have to offer to policymakers and
political institutions across the world. Hopefully this long essay will actuate
further debates and discussions over these ideas, and prove to be a useful
contribution towards governing the future of AI.

Artifical Intelligence (AI) in Education has great potential for building
more personalised curricula, as well as democratising education worldwide and
creating a Renaissance of new ways of teaching and learning. We believe this is
a crucial moment for setting the foundations of AI in education in the
beginning of this Fourth Industrial Revolution. This report aims to synthesize
how AI might change (and is already changing) how we learn, as well as what
technological features are crucial for these AI systems in education, with the
end goal of starting this pressing dialogue of how the future of AI in
education should unfold, engaging policy makers, engineers, researchers and
obviously, teachers and learners. This report also presents the advances within
the X5GON project, a European H2020 project aimed at building and deploying a
cross-modal, cross-lingual, cross-cultural, cross-domain and cross-site
personalised learning platform for Open Educational Resources (OER).

The AI landscape demands a broad set of legal, ethical, and societal
considerations to be accounted for in order to develop ethical AI (eAI)
solutions which sustain human values and rights. Currently, a variety of
guidelines and a handful of niche tools exist to account for and tackle
individual challenges. However, it is also well established that many
organizations face practical challenges in navigating these considerations from
a risk management perspective. Therefore, new methodologies are needed to
provide a well-vetted and real-world applicable structure and path through the
checks and balances needed for ethically assessing and guiding the development
of AI. In this paper we show that a multidisciplinary research approach,
spanning cross-sectional viewpoints, is the foundation of a pragmatic
definition of ethical and societal risks faced by organizations using AI.
Equally important is the findings of cross-structural governance for
implementing eAI successfully. Based on evidence acquired from our
multidisciplinary research investigation, we propose a novel data-driven risk
assessment methodology, entitled DRESS-eAI. In addition, through the evaluation
of our methodological implementation, we demonstrate its state-of-the-art
relevance as a tool for sustaining human values in the data-driven AI era.

Certain aspects of the explainability of AI systems will be critically
discussed. This especially with focus on the feasibility of the task of making
every AI system explainable. Emphasis will be given to difficulties related to
the explainability of highly complex and efficient AI systems which deliver
decisions whose explanation defies classical logical schemes of cause and
effect. AI systems have provably delivered unintelligible solutions which in
retrospect were characterized as ingenious (for example move 37 of the game 2
of AlphaGo). It will be elaborated on arguments supporting the notion that if
AI-solutions were to be discarded in advance because of their not being
thoroughly comprehensible, a great deal of the potentiality of intelligent
systems would be wasted.

After several winters, AI is center-stage once again, with current advances
enabling a vast array of AI applications. This renewed wave of AI has brought
back to the fore several questions from the past, about philosophical
foundations of intelligence and common sense -- predominantly motivated by
ethical concerns of AI decision-making. In this paper, we address some of the
arguments that led to research interest in intelligent agents, and argue for
their relevance even in today's context. Specifically we focus on the cognitive
sense of "self" and its role in autonomous decision-making leading to
responsible behaviour. The authors hope to make a case for greater research
interest in building richer computational models of AI agents with a sense of
self.

Building artificial intelligence (AI) that aligns with human values is an
unsolved problem. Here, we developed a human-in-the-loop research pipeline
called Democratic AI, in which reinforcement learning is used to design a
social mechanism that humans prefer by majority. A large group of humans played
an online investment game that involved deciding whether to keep a monetary
endowment or to share it with others for collective benefit. Shared revenue was
returned to players under two different redistribution mechanisms, one designed
by the AI and the other by humans. The AI discovered a mechanism that redressed
initial wealth imbalance, sanctioned free riders, and successfully won the
majority vote. By optimizing for human preferences, Democratic AI may be a
promising method for value-aligned policy innovation.

Software systems are increasingly relying on Artificial Intelligence (AI) and
Machine Learning (ML) components. The emerging popularity of AI techniques in
various application domains attracts malicious actors and adversaries.
Therefore, the developers of AI-enabled software systems need to take into
account various novel cyber-attacks and vulnerabilities that these systems may
be susceptible to. This paper presents a framework to characterize attacks and
weaknesses associated with AI-enabled systems and provide mitigation techniques
and defense strategies. This framework aims to support software designers in
taking proactive measures in developing AI-enabled software, understanding the
attack surface of such systems, and developing products that are resilient to
various emerging attacks associated with ML. The developed framework covers a
broad spectrum of attacks, mitigation techniques, and defensive and offensive
tools. In this paper, we demonstrate the framework architecture and its major
components, describe their attributes, and discuss the long-term goals of this
research.

For an artificial intelligence (AI) to be aligned with human values (or human
preferences), it must first learn those values. AI systems that are trained on
human behavior, risk miscategorising human irrationalities as human values --
and then optimising for these irrationalities. Simply learning human values
still carries risks: AI learning them will inevitably also gain information on
human irrationalities and human behaviour/policy. Both of these can be
dangerous: knowing human policy allows an AI to become generically more
powerful (whether it is partially aligned or not aligned at all), while
learning human irrationalities allows it to exploit humans without needing to
provide value in return. This paper analyses the danger in developing
artificial intelligence that learns about human irrationalities and human
policy, and constructs a model recommendation system with various levels of
information about human biases, human policy, and human values. It concludes
that, whatever the power and knowledge of the AI, it is more dangerous for it
to know human irrationalities than human values. Thus it is better for the AI
to learn human values directly, rather than learning human biases and then
deducing values from behaviour.

The workshop will focus on the application of AI to problems in cyber
security. Cyber systems generate large volumes of data, utilizing this
effectively is beyond human capabilities. Additionally, adversaries continue to
develop new attacks. Hence, AI methods are required to understand and protect
the cyber domain. These challenges are widely studied in enterprise networks,
but there are many gaps in research and practice as well as novel problems in
other domains.
  In general, AI techniques are still not widely adopted in the real world.
Reasons include: (1) a lack of certification of AI for security, (2) a lack of
formal study of the implications of practical constraints (e.g., power, memory,
storage) for AI systems in the cyber domain, (3) known vulnerabilities such as
evasion, poisoning attacks, (4) lack of meaningful explanations for security
analysts, and (5) lack of analyst trust in AI solutions. There is a need for
the research community to develop novel solutions for these practical issues.

The rapid growth of the online fashion market brought demands for innovative
fashion services and commerce platforms. With the recent success of deep
learning, many applications employ AI technologies such as visual search and
recommender systems to provide novel and beneficial services. In this paper, we
describe applied technologies for AI-driven fashion social networking service
that incorporate fashion e-commerce. In the application, people can share and
browse their outfit-of-the-day (OOTD) photos, while AI analyzes them and
suggests similar style OOTDs and related products. To this end, we trained deep
learning based AI models for fashion and integrated them to build a fashion
visual search system and a recommender system for OOTD. With aforementioned
technologies, the AI-driven fashion SNS platform, iTOO, has been successfully
launched.

In this article, we create a system called AI-EVL. This is an annotated-based
learning system. We extend AI to learning experience. If a user from the main
YouTube page browses YouTube videos and a user from the AI-EVL system does the
same, the amount of traffic used will be much less. It is due to ignoring
unwanted contents which indicates a reduction in bandwidth usage too. This
system is designed to be embedded with online learning tools and platforms to
enrich their curriculum. In evaluating the system using Google 2020 trend data,
we were able to extract rich ontological information for each data. Of the data
collected, 34.86% belong to wolfram, 30.41% to DBpedia, and 34.73% to
Wikipedia. The video subtitle information is displayed interactively and
functionally to the user over time as the video is played. This effective
visual learning system, due to the unique features, prevents the user's
distraction and makes learning more focused. The information about the subtitle
text is displayed in multiple layers including AI-annotated topics,
Wikipedia/DBpedia, and Wolfram enriched texts via interactive and visual
widgets.

Artificial intelligence (AI) has been widely applied to music generation
topics such as continuation, melody/harmony generation, genre transfer and
music infilling application. Although with the burst interest to apply AI to
music, there are still few interfaces for the musicians to take advantage of
the latest progress of the AI technology. This makes those tools less valuable
in practice and harder to find its advantage/drawbacks without utilizing them
in the real scenario. This work builds a max patch for interactive music
infilling application with different levels of control, including track
density/polyphony/occupation rate and bar tonal tension control. The user can
select the melody/bass/harmony track as the infilling content up to 16 bars.
The infilling algorithm is based on the author's previous work, and the
interface sends/receives messages to the AI system hosted in the cloud. This
interface lowers the barrier of AI technology and can generate different
variations of the selected content. Those results can give several alternatives
to the musicians' composition, and the interactive process realizes the value
of the AI infilling system.

Neuroscience and neurotechnology are currently being revolutionized by
artificial intelligence (AI) and machine learning. AI is widely used to study
and interpret neural signals (analytical applications), assist people with
disabilities (prosthetic applications), and treat underlying neurological
symptoms (therapeutic applications). In this brief, we will review the emerging
opportunities of on-chip AI for the next-generation implantable brain-machine
interfaces (BMIs), with a focus on state-of-the-art prosthetic BMIs. Major
technological challenges for the effectiveness of AI models will be discussed.
Finally, we will present algorithmic and IC design solutions to enable a new
generation of AI-enhanced and high-channel-count BMIs.

Using Japanese professional chess (Shogi) players records in the novel
setting, this paper examines how and the extent to which the emergence of
technological changes influences the ageing and innate ability of players
winning probability. We gathered games of professional Shogi players from 1968
to 2019.
  The major findings are: (1) diffusion of artificial intelligence (AI) reduces
innate ability, which reduces the performance gap among same-age players; (2)
players winning rates declined consistently from 20 years and as they get
older; (3) AI accelerated the ageing declination of the probability of winning,
which increased the performance gap among different aged players; (4) the
effects of AI on the ageing declination and the probability of winning are
observed for high innate skill players but not for low innate skill ones. This
implies that the diffusion of AI hastens players retirement from active play,
especially for those with high innate abilities. Thus, AI is a substitute for
innate ability in brain-work productivity.

Artificial intelligence (AI) is gaining momentum, and its importance for the
future of work in many areas, such as medicine and banking, is continuously
rising. However, insights on the effective collaboration of humans and AI are
still rare. Typically, AI supports humans in decision-making by addressing
human limitations. However, it may also evoke human bias, especially in the
form of automation bias as an over-reliance on AI advice. We aim to shed light
on the potential to influence automation bias by explainable AI (XAI). In this
pre-test, we derive a research model and describe our study design.
Subsequentially, we conduct an online experiment with regard to hotel review
classifications and discuss first results. We expect our research to contribute
to the design and development of safe hybrid intelligence systems.

This volume is devoted to the emerging field of Integrated Visual Knowledge
Discovery that combines advances in Artificial Intelligence/Machine Learning
(AI/ML) and Visualization/Visual Analytics. Chapters included are extended
versions of the selected AI and Visual Analytics papers and related symposia at
the recent International Information Visualization Conferences (IV2019 and
IV2020). AI/ML face a long-standing challenge of explaining models to humans.
Models explanation is fundamentally human activity, not only an algorithmic
one. In this chapter we aim to present challenges and future directions within
the field of Visual Analytics, Visual Knowledge Discovery and AI/ML, and to
discuss the role of visualization in visual AI/ML. In addition, we describe
progress in emerging Full 2D ML, natural language processing, and AI/ML in
multidimensional data aided by visual means.

As the use of AI-powered applications widens across multiple domains, so do
increase the computational demands. Primary driver of AI technology are the
deep neural networks (DNNs). When focusing either on cloud-based systems that
serve multiple AI queries from different users each with their own DNN model,
or on mobile robots and smartphones employing pipelines of various models or
parallel DNNs for the concurrent processing of multi-modal data, the next
generation of AI systems will have multi-DNN workloads at their core.
Large-scale deployment of AI services and integration across mobile and
embedded systems require additional breakthroughs in the computer architecture
front, with processors that can maintain high performance as the number of DNNs
increases while meeting the quality-of-service requirements, giving rise to the
topic of multi-DNN accelerator design.

We have developed an AI-aided multiple time stepping (AI-MTS) algorithm and
multiscale modeling framework (AI-MSM) and implemented them on the Summit-like
supercomputer, AIMOS. AI-MSM is the first of its kind to integrate
multi-physics, including intra-platelet, inter-platelet, and fluid-platelet
interactions, into one system. It has simulated a record-setting multiscale
blood clotting model of 102 million particles, of which 70 flowing and 180
aggregating platelets, under dissipative particle dynamics to coarse-grained
molecular dynamics. By adaptively adjusting timestep sizes to match the
characteristic time scales of the underlying dynamics, AI-MTS optimally
balances speeds and accuracies of the simulations.

We present the Novel-Materials-Discovery (NOMAD) Artificial-Intelligence (AI)
Toolkit, a web-browser-based infrastructure for the interactive AI-based
analysis of materials-science findable, accessible, interoperable, and reusable
(FAIR) data. The AI Toolkit readily operates on the FAIR data stored in the
central server of the NOMAD Archive, the largest database of materials-science
data worldwide, as well as locally stored, users' owned data. The NOMAD Oasis,
a local, stand alone server can be also used to run the AI Toolkit. By using
Jupyter notebooks that run in a web-browser, the NOMAD data can be queried and
accessed; data mining, machine learning, and other AI techniques can be then
applied to analyse them. This infrastructure brings the concept of
reproducibility in materials science to the next level, by allowing researchers
to share not only the data contributing to their scientific publications, but
also all the developed methods and analytics tools. Besides reproducing
published results, users of the NOMAD AI toolkit can modify the Jupyter
notebooks towards their own research work.

We explore the design of Marvista -- a human-AI collaborative tool that
employs a suite of natural language processing models to provide end-to-end
support for reading online news articles. Before reading an article, Marvista
helps a user plan what to read by filtering text based on how much time one can
spend and what questions one is interested to find out from the article. During
reading, Marvista helps the user reflect on their understanding of each
paragraph with AI-generated questions. After reading, Marvista generates an
explainable human-AI summary that combines both AI's processing of the text,
the user's reading behavior, and user-generated data in the reading process. In
contrast to prior work that offered (content-independent) interaction
techniques or devices for reading, Marvista takes a human-AI collaborative
approach that contributes text-specific guidance (content-aware) to support the
entire reading process.

The size and complexity of deep neural networks continue to grow
exponentially, significantly increasing energy consumption for training and
inference by these models. We introduce an open-source package eco2AI to help
data scientists and researchers to track energy consumption and equivalent CO2
emissions of their models in a straightforward way. In eco2AI we put emphasis
on accuracy of energy consumption tracking and correct regional CO2 emissions
accounting. We encourage research community to search for new optimal
Artificial Intelligence (AI) architectures with a lower computational cost. The
motivation also comes from the concept of AI-based green house gases
sequestrating cycle with both Sustainable AI and Green AI pathways.

Neural language models have the potential to support human writing. However,
questions remain on their integration and influence on writing and output. To
address this, we designed and compared two user interfaces for writing with AI
on mobile devices, which manipulate levels of initiative and control: 1)
Writing with continuously generated text, the AI adds text word-by-word and
user steers. 2) Writing with suggestions, the AI suggests phrases and user
selects from a list. In a supervised online study (N=18), participants used
these prototypes and a baseline without AI. We collected touch interactions,
ratings on inspiration and authorship, and interview data. With AI suggestions,
people wrote less actively, yet felt they were the author. Continuously
generated text reduced this perceived authorship, yet increased editing
behavior. In both designs, AI increased text length and was perceived to
influence wording. Our findings add new empirical evidence on the impact of UI
design decisions on user experience and output with co-creative systems.

It is curious that AI increasingly outperforms human decision makers, yet
much of the public distrusts AI to make decisions affecting their lives. In
this paper we explore a novel theory that may explain one reason for this. We
propose that public distrust of AI is a moral consequence of designing systems
that prioritize reduction of costs of false positives over less tangible costs
of false negatives. We show that such systems, which we characterize as
'distrustful', are more likely to miscategorize trustworthy individuals, with
cascading consequences to both those individuals and the overall human-AI trust
relationship. Ultimately, we argue that public distrust of AI stems from
well-founded concern about the potential of being miscategorized. We propose
that restoring public trust in AI will require that systems are designed to
embody a stance of 'humble trust', whereby the moral costs of the misplaced
distrust associated with false negatives is weighted appropriately during
development and use.

More than twenty years after its introduction, Annealed Importance Sampling
(AIS) remains one of the most effective methods for marginal likelihood
estimation. It relies on a sequence of distributions interpolating between a
tractable initial distribution and the target distribution of interest which we
simulate from approximately using a non-homogeneous Markov chain. To obtain an
importance sampling estimate of the marginal likelihood, AIS introduces an
extended target distribution to reweight the Markov chain proposal. While much
effort has been devoted to improving the proposal distribution used by AIS, an
underappreciated issue is that AIS uses a convenient but suboptimal extended
target distribution. We here leverage recent progress in score-based generative
modeling (SGM) to approximate the optimal extended target distribution
minimizing the variance of the marginal likelihood estimate for AIS proposals
corresponding to the discretization of Langevin and Hamiltonian dynamics. We
demonstrate these novel, differentiable, AIS procedures on a number of
synthetic benchmark distributions and variational auto-encoders.

Recent AI algorithms are black box models whose decisions are difficult to
interpret. eXplainable AI (XAI) is a class of methods that seek to address lack
of AI interpretability and trust by explaining to customers their AI decisions.
The common wisdom is that regulating AI by mandating fully transparent XAI
leads to greater social welfare. Our paper challenges this notion through a
game theoretic model of a policy-maker who maximizes social welfare, firms in a
duopoly competition that maximize profits, and heterogenous consumers. The
results show that XAI regulation may be redundant. In fact, mandating fully
transparent XAI may make firms and consumers worse off. This reveals a tradeoff
between maximizing welfare and receiving explainable AI outputs. We extend the
existing literature on method and substantive fronts, and we introduce and
study the notion of XAI fairness, which may be impossible to guarantee even
under mandatory XAI. Finally, the regulatory and managerial implications of our
results for policy-makers and businesses are discussed, respectively.

Reinforcement learning has shown an outstanding performance in the
applications of games, particularly in Atari games as well as Go. Based on
these successful examples, we attempt to apply one of the well-known
reinforcement learning algorithms, Deep Q-Network, to the AI Soccer game. AI
Soccer is a 5:5 robot soccer game where each participant develops an algorithm
that controls five robots in a team to defeat the opponent participant. Deep
Q-Network is designed to implement our original rewards, the state space, and
the action space to train each agent so that it can take proper actions in
different situations during the game. Our algorithm was able to successfully
train the agents, and its performance was preliminarily proven through the
mini-competition against 10 teams wishing to take part in the AI Soccer
international competition. The competition was organized by the AI World Cup
committee, in conjunction with the WCG 2019 Xi'an AI Masters. With our
algorithm, we got the achievement of advancing to the round of 16 in this
international competition with 130 teams from 39 countries.

The creation process of design fiction is going participatory and inclusive
with non experts. Recognizing the potential of artificial intelligence in
creativity support, we explore the use of AI assistance in creating design
fiction. This investigation is based on a workshop on future work in 2040 with
Chinese youth. We look into fiction quality, participants experiences with the
AI agent, and their ways of incorporating those texts into writing. Our
findings show that human writers while responding to messy and unexpected
AI-generated texts, can elevate the richness and creativity in writing and
initiate joyful and inspirational interactions. Furthermore, for the design of
AI assistance in creativity support, we suggest two implications of enhancing
interactional quality between human and AI and prompt programming. Our study
indicates the potential of applying design fiction outside the design context
using a more inclusive approach for future speculation with critical reflection
on technology.

As human science pushes the boundaries towards the development of artificial
intelligence (AI), the sweep of progress has caused scholars and policymakers
alike to question the legality of applying or utilising AI in various human
endeavours. For example, debate has raged in international scholarship about
the legitimacy of applying AI to weapon systems to form lethal autonomous
weapon systems (LAWS). Yet the argument holds true even when AI is applied to a
military autonomous system that is not weaponised: how does one hold a machine
accountable for a crime? What about a tort? Can an artificial agent understand
the moral and ethical content of its instructions? These are thorny questions,
and in many cases these questions have been answered in the negative, as
artificial entities lack any contingent moral agency. So what if the AI is not
alone, but linked with or overseen by a human being, with their own moral and
ethical understandings and obligations? Who is responsible for any malfeasance
that may be committed? Does the human bear the legal risks of unethical or
immoral decisions by an AI? These are some of the questions this manuscript
seeks to engage with.

This article presents a new design for autonomous artificial intelligence
(AI), based on the state-of-the-art algorithms, and describes a new autonomous
AI system called AutoAI. The methodology is used to assemble the design founded
on self-improved algorithms that use new and emerging sources of data (NEFD).
The objective of the article is to conceptualise the design of a novel AutoAI
algorithm. The conceptual approach is used to advance into building new and
improved algorithms. The article integrates and consolidates the findings from
existing literature and advances the AutoAI design into (1) using new and
emerging sources of data for teaching and training AI algorithms and (2)
enabling AI algorithms to use automated tools for training new and improved
algorithms. This approach is going beyond the state-of-the-art in AI algorithms
and suggests a design that enables autonomous algorithms to self-optimise and
self-adapt, and on a higher level, be capable to self-procreate.

Human computation is an approach to solving problems that prove difficult
using AI only, and involves the cooperation of many humans. Because human
computation requires close engagement with both "human populations as users"
and "human populations as driving forces," establishing mutual trust between AI
and humans is an important issue to further the development of human
computation. This survey lays the groundwork for the realization of trustworthy
human computation. First, the trustworthiness of human computation as computing
systems, that is, trust offered by humans to AI, is examined using the RAS
(Reliability, Availability, and Serviceability) analogy, which define measures
of trustworthiness in conventional computer systems. Next, the social
trustworthiness provided by human computation systems to users or participants
is discussed from the perspective of AI ethics, including fairness, privacy,
and transparency. Then, we consider human--AI collaboration based on two-way
trust, in which humans and AI build mutual trust and accomplish difficult tasks
through reciprocal collaboration. Finally, future challenges and research
directions for realizing trustworthy human computation are discussed.

AI requires heavy amounts of storage and compute. As a result, AI developers
are regular users of centralised cloud services such as AWS, GCP and Azure,
compute environments such as Jupyter and Colab notebooks, and AI Hubs such as
HuggingFace and ActiveLoop. There services are associated with certain benefits
and limitations that stem from the underlying infrastructure and governance
systems with which they are built. These limitations include high costs, lack
of monetization and reward, lack of control and difficulty of reproducibility.
At the same time, there are few libraries that allow data scientists to
interact with decentralised storage in the language that data scientists are
used to, and few hubs where they can discover and interact with AI assets. In
this report, we explore the potential of decentralized technologies - such as
Web3 wallets, peer-to-peer marketplaces, decentralized storage (IPFS and
Filecoin) and compute, and DAOs - to address some of the above limitations. We
showcase some of the libraries and integrations that we have built to tackle
these issues, as well as a proof of concept of a decentralized AI Hub app, that
all use IPFS as a core infrastructural component.

Openness and intelligence are two enabling features to be introduced in next
generation wireless networks, e.g. Beyond 5G and 6G, to support service
heterogeneity, open hardware, optimal resource utilization, and on-demand
service deployment. The open radio access network (O-RAN) is a promising RAN
architecture to achieve both openness and intelligence through virtualized
network elements and well-defined interfaces. While deploying artificial
intelligence (AI) models is becoming easier in O-RAN, one significant challenge
that has been long neglected is the comprehensive testing of their performance
in realistic environments. This article presents a general automated,
distributed and AI-enabled testing framework to test AI models deployed in
O-RAN in terms of their decision-making performance, vulnerability and
security. This framework adopts a master-actor architecture to manage a number
of end devices for distributed testing. More importantly, it leverages AI to
automatically and intelligently explore the decision space of AI models in
O-RAN. Both software simulation testing and software-defined radio hardware
testing are supported, enabling rapid proof of concept research and
experimental research on wireless research platforms.

We are in a golden age of progress in artificial intelligence (AI).
Radiotherapy, due to its technology-intensive nature as well as direct
human-machine interactions, is perfectly suited for benefitting from AI to
enhance accuracy and efficiency. Over the past few years, a vast majority of AI
research have already been published in the field of photon therapy, while the
applications of AI specifically targeted for particle therapy remain scarcely
investigated. There are two distinct differences between photon therapy and
particle therapy: beam interaction physics (photons vs. charged particles) and
beam delivery mode (e.g. IMRT/VMAT vs. pencil beam scanning). As a result,
different strategies of AI deployment are required between these two
radiotherapy modalities. In this article, we aim to present a comprehensive
survey of recent literatures exclusively focusing on AI-powered particle
therapy. Six major aspects are included: treatment planning, dose calculation,
range and dose verification, image guidance, quality assurance and adaptive
replanning. A number of perspectives as well as potential challenges and common
pitfalls, are also discussed.

Two years after publicly launching the AI Incident Database (AIID) as a
collection of harms or near harms produced by AI in the world, a backlog of
"issues" that do not meet its incident ingestion criteria have accumulated in
its review queue. Despite not passing the database's current criteria for
incidents, these issues advance human understanding of where AI presents the
potential for harm. Similar to databases in aviation and computer security, the
AIID proposes to adopt a two-tiered system for indexing AI incidents (i.e., a
harm or near harm event) and issues (i.e., a risk of a harm event). Further, as
some machine learning-based systems will sometimes produce a large number of
incidents, the notion of an incident "variant" is introduced. These proposed
changes mark the transition of the AIID to a new version in response to lessons
learned from editing 2,000+ incident reports and additional reports that fall
under the new category of "issue."

Artificial intelligence (AI) systems attempt to imitate human behavior. How
well they do this imitation is often used to assess their utility and to
attribute human-like (or artificial) intelligence to them. However, most work
on AI refers to and relies on human intelligence without accounting for the
fact that human behavior is inherently shaped by the cultural contexts they are
embedded in, the values and beliefs they hold, and the social practices they
follow. Additionally, since AI technologies are mostly conceived and developed
in just a handful of countries, they embed the cultural values and practices of
these countries. Similarly, the data that is used to train the models also
fails to equitably represent global cultural diversity. Problems therefore
arise when these technologies interact with globally diverse societies and
cultures, with different values and interpretive practices. In this position
paper, we describe a set of cultural dependencies and incongruencies in the
context of AI-based language and vision technologies, and reflect on the
possibilities of and potential strategies towards addressing these
incongruencies.

We present AI-SDC, an integrated suite of open source Python tools to
facilitate Statistical Disclosure Control (SDC) of Machine Learning (ML) models
trained on confidential data prior to public release. AI-SDC combines (i) a
SafeModel package that extends commonly used ML models to provide ante-hoc SDC
by assessing the vulnerability of disclosure posed by the training regime; and
(ii) an Attacks package that provides post-hoc SDC by rigorously assessing the
empirical disclosure risk of a model through a variety of simulated attacks
after training. The AI-SDC code and documentation are available under an MIT
license at https://github.com/AI-SDC/AI-SDC.

We study the role of dark and bright autoionizing states (AIS) in
photoionization and high harmonic generation (HHG) using a 1D helium model.
This model allows numerical integration of the time-dependent Schr\"odinger
equation beyond the singe-electron approximation completely taking into account
electronic correlation. We find the level structure of the system and the
spatial distribution of the electronic density for several states including
AIS. Studying the HHG efficiency as a function of the detuning from the
resonances with AIS we find the HHG enhancement lines. The shapes of these
lines are different from the corresponding Fano lines in the photoelectronic
spectra, in agreement with the experimental studies in helium. Moreover, we
simulate HHG under the conditions when the fundamental frequency is close to
the even-order multiphoton resonance with the dark AIS. We find the enhanced
generation of the neighbouring odd harmonics. The details of the enhancement
lines for these harmonics can be understood taking into account the temporal
delay between the emission of the non-resonant and resonant XUV; this delay is
defined by the AIS lifetime. Finally, our simulations show that the HHG
enhancement due to the dark and the bright AIS is comparable in the studied
system.

Organizations that develop and deploy artificial intelligence (AI) systems
need to manage the associated risks - for economic, legal, and ethical reasons.
However, it is not always clear who is responsible for AI risk management. The
Three Lines of Defense (3LoD) model, which is considered best practice in many
industries, might offer a solution. It is a risk management framework that
helps organizations to assign and coordinate risk management roles and
responsibilities. In this article, I suggest ways in which AI companies could
implement the model. I also discuss how the model could help reduce risks from
AI: it could identify and close gaps in risk coverage, increase the
effectiveness of risk management practices, and enable the board of directors
to oversee management more effectively. The article is intended to inform
decision-makers at leading AI companies, regulators, and standard-setting
bodies.

Artificial Intelligence (AI) increasingly becomes an indispensable advisor.
New ethical concerns arise if AI persuades people to behave dishonestly. In an
experiment, we study how AI advice (generated by a Natural-Language-Processing
algorithm) affects (dis)honesty, compare it to equivalent human advice, and
test whether transparency about advice source matters. We find that
dishonesty-promoting advice increases dishonesty, whereas honesty-promoting
advice does not increase honesty. This is the case for both AI- and human
advice. Algorithmic transparency, a commonly proposed policy to mitigate AI
risks, does not affect behaviour. The findings mark the first steps towards
managing AI advice responsibly.

AI alignment considers how we can encode AI systems in a way that is
compatible with human values. The normative side of this problem asks what
moral values or principles, if any, we should encode in AI. To this end, we
present a framework to consider the question at four levels: Individual,
Organizational, National, and Global. We aim to illustrate how AI alignment is
made up of value alignment problems at each of these levels, where values at
each level affect the others and effects can flow in either direction. We
outline key questions and considerations of each level and demonstrate an
application of this framework to the topic of AI content moderation.

The ability to replicate predictions by machine learning (ML) or artificial
intelligence (AI) models and results in scientific workflows that incorporate
such ML/AI predictions is driven by numerous factors. An uncertainty-aware
metric that can quantitatively assess the reproducibility of quantities of
interest (QoI) would contribute to the trustworthiness of results obtained from
scientific workflows involving ML/AI models. In this article, we discuss how
uncertainty quantification (UQ) in a Bayesian paradigm can provide a general
and rigorous framework for quantifying reproducibility for complex scientific
workflows. Such as framework has the potential to fill a critical gap that
currently exists in ML/AI for scientific workflows, as it will enable
researchers to determine the impact of ML/AI model prediction variability on
the predictive outcomes of ML/AI-powered workflows. We expect that the
envisioned framework will contribute to the design of more reproducible and
trustworthy workflows for diverse scientific applications, and ultimately,
accelerate scientific discoveries.

The rapid and dynamic pace of Artificial Intelligence (AI) and Machine
Learning (ML) is revolutionizing the insurance sector. AI offers significant,
very much welcome advantages to insurance companies, and is fundamental to
their customer-centricity strategy. It also poses challenges, in the project
and implementation phase. Among those, we study Adversarial Attacks, which
consist of the creation of modified input data to deceive an AI system and
produce false outputs. We provide examples of attacks on insurance AI
applications, categorize them, and argue on defence methods and precautionary
systems, considering that they can involve few-shot and zero-shot
multilabelling. A related topic, with growing interest, is the validation and
verification of systems incorporating AI and ML components. These topics are
discussed in various sections of this paper.

Companies struggle to continuously develop and deploy AI models to complex
production systems due to AI characteristics while assuring quality. To ease
the development process, continuous pipelines for AI have become an active
research area where consolidated and in-depth analysis regarding the
terminology, triggers, tasks, and challenges is required. This paper includes a
Multivocal Literature Review where we consolidated 151 relevant formal and
informal sources. In addition, nine-semi structured interviews with
participants from academia and industry verified and extended the obtained
information. Based on these sources, this paper provides and compares
terminologies for DevOps and CI/CD for AI, MLOps, (end-to-end) lifecycle
management, and CD4ML. Furthermore, the paper provides an aggregated list of
potential triggers for reiterating the pipeline, such as alert systems or
schedules. In addition, this work uses a taxonomy creation strategy to present
a consolidated pipeline comprising tasks regarding the continuous development
of AI. This pipeline consists of four stages: Data Handling, Model Learning,
Software Development and System Operations. Moreover, we map challenges
regarding pipeline implementation, adaption, and usage for the continuous
development of AI to these four stages.

Artificial intelligence (AI) technologies are widely deployed in smartphone
photography; and prompt-based image synthesis models have rapidly become
commonplace. In this paper, we describe a Research-through-Design (RtD) project
which explores this shift in the means and modes of image production via the
creation and use of the Entoptic Field Camera. Entoptic phenomena usually refer
to perceptions of floaters or bright blue dots stemming from the physiological
interplay of the eye and brain. We use the term entoptic as a metaphor to
investigate how the material interplay of data and models in AI technologies
shapes human experiences of reality. Through our case study using first-person
design and a field study, we offer implications for critical, reflective,
more-than-human and ludic design to engage AI technologies; the
conceptualisation of an RtD research space which contributes to AI literacy
discourses; and outline a research trajectory concerning materiality and design
affordances of AI technologies.

The world around us has undergone a radical transformation due to rapid
technological advancement in recent decades. The industry of the future
generation is evolving, and artificial intelligence is the following change in
the making popularly known as Industry 4.0. Indeed, experts predict that
artificial intelligence(AI) will be the main force behind the following
significant virtual shift in the way we stay, converse, study, live,
communicate and conduct business. All facets of our social connection are being
transformed by this growing technology. One of the newest areas of educational
technology is Artificial Intelligence in the field of Education(AIEd).This
study emphasizes the different applications of artificial intelligence in
education from both an industrial and academic standpoint. It highlights the
most recent contextualized learning novel transformative evaluations and
advancements in sophisticated tutoring systems. It analyses the AIEd's ethical
component and the influence of the transition on people, particularly students
and instructors as well. Finally, this article touches on AIEd's potential
future research and practices. The goal of this study is to introduce the
present-day applications to its intended audience.

This chapter introduces the perspective of political ecology to the
application of artificial intelligence to artistic processes (Creative-Ai).
Hence, the environmental and social impact of the development and employment of
Creative-Ai are the focus of this text, when we consider them as part of an
economic system that transforms artistic creation to a commodity. I first
analyse specific Creative-Ai cases, and then conduct a speculation that takes
Jacques Attali's writing on the role of music in society as a vantage point,
and investigates the environmental and social consequences of an automatic
composition network controlled by a large music streaming platform. Whereas the
possibilities that emerge from Creative-Ai may be promising from an artistic
perspective, its entanglement with corporate interest raises severe concerns.
These concerns can only be addressed by a wide cross-sectoral alliance between
research and arts that develops a critical perspective on the future directions
of Creative-Ai.

[Context] Engineering Artificial Intelligence (AI) software is a relatively
new area with many challenges, unknowns, and limited proven best practices. Big
companies such as Google, Microsoft, and Apple have provided a suite of recent
guidelines to assist engineering teams in building human-centered AI systems.
[Objective] The practices currently adopted by practitioners for developing
such systems, especially during Requirements Engineering (RE), are little
studied and reported to date. [Method] This paper presents the results of a
survey conducted to understand current industry practices in RE for AI (RE4AI)
and to determine which key human-centered AI guidelines should be followed. Our
survey is based on mapping existing industrial guidelines, best practices, and
efforts in the literature. [Results] We surveyed 29 professionals and found
most participants agreed that all the human-centered aspects we mapped should
be addressed in RE. Further, we found that most participants were using UML or
Microsoft Office to present requirements. [Conclusion] We identify that most of
the tools currently used are not equipped to manage AI-based software, and the
use of UML and Office may pose issues to the quality of requirements captured
for AI. Also, all human-centered practices mapped from the guidelines should be
included in RE.

Artificial intelligence (AI) in healthcare has the potential to improve
patient outcomes, but clinician acceptance remains a critical barrier. We
developed a novel decision support interface that provides interpretable
treatment recommendations for sepsis, a life-threatening condition in which
decisional uncertainty is common, treatment practices vary widely, and poor
outcomes can occur even with optimal decisions. This system formed the basis of
a mixed-methods study in which 24 intensive care clinicians made AI-assisted
decisions on real patient cases. We found that explanations generally increased
confidence in the AI, but concordance with specific recommendations varied
beyond the binary acceptance or rejection described in prior work. Although
clinicians sometimes ignored or trusted the AI, they also often prioritized
aspects of the recommendations to follow, reject, or delay in a process we term
"negotiation." These results reveal novel barriers to adoption of
treatment-focused AI tools and suggest ways to better support differing
clinician perspectives.

Optimization of human-AI teams hinges on the AI's ability to tailor its
interaction to individual human teammates. A common hypothesis in adaptive AI
research is that minor differences in people's predisposition to trust can
significantly impact their likelihood of complying with recommendations from
the AI. Predisposition to trust is often measured with self-report inventories
that are administered before interactions. We benchmark a popular measure of
this kind against behavioral predictors of compliance. We find that the
inventory is a less effective predictor of compliance than the behavioral
measures in datasets taken from three previous research projects. This suggests
a general property that individual differences in initial behavior are more
predictive than differences in self-reported trust attitudes. This result also
shows a potential for easily accessible behavioral measures to provide an AI
with more accurate models without the use of (often costly) survey instruments.

Large, transformer-based pretrained language models like BERT, GPT, and T5
have demonstrated a deep understanding of contextual semantics and language
syntax. Their success has enabled significant advances in conversational AI,
including the development of open-dialogue systems capable of coherent, salient
conversations which can answer questions, chat casually, and complete tasks.
However, state-of-the-art models still struggle with tasks that involve higher
levels of reasoning - including commonsense reasoning that humans find trivial.
This paper presents a survey of recent conversational AI research focused on
commonsense reasoning. The paper lists relevant training datasets and describes
the primary approaches to include commonsense in conversational AI. The paper
also discusses benchmarks used for evaluating commonsense in conversational AI
problems. Finally, the paper presents preliminary observations of the limited
commonsense capabilities of two state-of-the-art open dialogue models,
BlenderBot3 and LaMDA, and its negative effect on natural interactions. These
observations further motivate research on commonsense reasoning in
conversational AI.

This study used XAI, which shows its purposes and attention as explanations
of its process, and investigated how these explanations affect human trust in
and use of AI. In this study, we generated heat maps indicating AI attention,
conducted Experiment 1 to confirm the validity of the interpretability of the
heat maps, and conducted Experiment 2 to investigate the effects of the purpose
and heat maps in terms of reliance (depending on AI) and compliance (accepting
answers of AI). The results of structural equation modeling (SEM) analyses
showed that (1) displaying the purpose of AI positively and negatively
influenced trust depending on the types of AI usage, reliance or compliance,
and task difficulty, (2) just displaying the heat maps negatively influenced
trust in a more difficult task, and (3) the heat maps positively influenced
trust according to their interpretability in a more difficult task.

AI-based design tools are proliferating in professional software to assist
engineering and industrial designers in complex manufacturing and design tasks.
These tools take on more agentic roles than traditional computer-aided design
tools and are often portrayed as "co-creators." Yet, working effectively with
such systems requires different skills than working with complex CAD tools
alone. To date, we know little about how engineering designers learn to work
with AI-based design tools. In this study, we observed trained designers as
they learned to work with two AI-based tools on a realistic design task. We
find that designers face many challenges in learning to effectively co-create
with current systems, including challenges in understanding and adjusting AI
outputs and in communicating their design goals. Based on our findings, we
highlight several design opportunities to better support designer-AI
co-creation.

We aim to understand how people assess human likeness in navigation produced
by people and artificially intelligent (AI) agents in a video game. To this
end, we propose a novel AI agent with the goal of generating more human-like
behavior. We collect hundreds of crowd-sourced assessments comparing the
human-likeness of navigation behavior generated by our agent and baseline AI
agents with human-generated behavior. Our proposed agent passes a Turing Test,
while the baseline agents do not. By passing a Turing Test, we mean that human
judges could not quantitatively distinguish between videos of a person and an
AI agent navigating. To understand what people believe constitutes human-like
navigation, we extensively analyze the justifications of these assessments.
This work provides insights into the characteristics that people consider
human-like in the context of goal-directed video game navigation, which is a
key step for further improving human interactions with AI agents.

[Context] Artificial intelligence (AI) components used in building software
solutions have substantially increased in recent years. However, many of these
solutions focus on technical aspects and ignore critical human-centered
aspects. [Objective] Including human-centered aspects during requirements
engineering (RE) when building AI-based software can help achieve more
responsible, unbiased, and inclusive AI-based software solutions. [Method] In
this paper, we present a new framework developed based on human-centered AI
guidelines and a user survey to aid in collecting requirements for
human-centered AI-based software. We provide a catalog to elicit these
requirements and a conceptual model to present them visually. [Results] The
framework is applied to a case study to elicit and model requirements for
enhancing the quality of 360 degree~videos intended for virtual reality (VR)
users. [Conclusion] We found that our proposed approach helped the project team
fully understand the human-centered needs of the project to deliver.
Furthermore, the framework helped to understand what requirements need to be
captured at the initial stages against later stages in the engineering process
of AI-based software.

AI technologies continue to advance from digital assistants to assisted
decision-making. However, designing AI remains a challenge given its unknown
outcomes and uses. One way to expand AI design is by centering stakeholders in
the design process. We conduct co-design sessions with gig workers to explore
the design of gig worker-centered tools as informed by their driving patterns,
decisions, and personal contexts. Using workers' own data as well as city-level
data, we create probes -- interactive data visuals -- that participants explore
to surface the well-being and positionalities that shape their work strategies.
We describe participant insights and corresponding AI design considerations
surfaced from data probes about: 1) workers' well-being trade-offs and
positionality constraints, 2) factors that impact well-being beyond those in
the data probes, and 3) instances of unfair algorithmic management. We discuss
the implications for designing data probes and using them to elevate
worker-centered AI design as well as for worker advocacy.

In this thorough study, we took a closer look at the skepticism that has
arisen with respect to potential dangers associated with artificial
intelligence, denoted as AI Risk Skepticism. Our study takes into account
different points of view on the topic and draws parallels with other forms of
skepticism that have shown up in science. We categorize the various skepticisms
regarding the dangers of AI by the type of mistaken thinking involved. We hope
this will be of interest and value to AI researchers concerned about the future
of AI and the risks that it may pose. The issues of skepticism and risk in AI
are decidedly important and require serious consideration. By addressing these
issues with the rigor and precision of scientific research, we hope to better
understand the objections we face and to find satisfactory ways to resolve
them.

In its pragmatic turn, the new discipline of AI ethics came to be dominated
by humanity's collective fear of its creatures, as reflected in an extensive
and perennially popular literary tradition. Dr. Frankenstein's monster in the
novel by Mary Shelley rising against its creator; the unorthodox golem in H.
Leivick's 1920 play going on a rampage; the rebellious robots of Karel
\v{C}apek -- these and hundreds of other examples of the genre are the
background against which the preoccupation of AI ethics with preventing robots
from behaving badly towards people is best understood. In each of these three
fictional cases (as well as in many others), the miserable artificial creature
-- mercilessly exploited, or cornered by a murderous mob, and driven to
violence in self-defense -- has its author's sympathy. In real life, with very
few exceptions, things are different: theorists working on the ethics of AI
completely ignore the possibility of robots needing protection from their
creators. The present book chapter takes up this, less commonly considered,
ethical angle of AI.

The development of privacy-enhancing technologies has made immense progress
in reducing trade-offs between privacy and performance in data exchange and
analysis. Similar tools for structured transparency could be useful for AI
governance by offering capabilities such as external scrutiny, auditing, and
source verification. It is useful to view these different AI governance
objectives as a system of information flows in order to avoid partial solutions
and significant gaps in governance, as there may be significant overlap in the
software stacks needed for the AI governance use cases mentioned in this text.
When viewing the system as a whole, the importance of interoperability between
these different AI governance solutions becomes clear. Therefore, it is
imminently important to look at these problems in AI governance as a system,
before these standards, auditing procedures, software, and norms settle into
place.

Emerging AI applications such as ChatGPT, graph convolutional networks, and
other deep neural networks require massive computational resources for training
and inference. Contemporary computing platforms such as CPUs, GPUs, and TPUs
are struggling to keep up with the demands of these AI applications.
Non-coherent optical computing represents a promising approach for light-speed
acceleration of AI workloads. In this paper, we show how cross-layer design can
overcome challenges in non-coherent optical computing platforms. We describe
approaches for optical device engineering, tuning circuit enhancements, and
architectural innovations to adapt optical computing to a variety of AI
workloads. We also discuss techniques for hardware/software co-design that can
intelligently map and adapt AI software to improve its performance on
non-coherent optical computing platforms.

AI-based decision-making tools are rapidly spreading across a range of
real-world, complex domains like healthcare, criminal justice, and child
welfare. A growing body of research has called for increased scrutiny around
the validity of AI system designs. However, in real-world settings, it is often
not possible to fully address questions around the validity of an AI tool
without also considering the design of associated organizational and public
policies. Yet, considerations around how an AI tool may interface with policy
are often only discussed retrospectively, after the tool is designed or
deployed. In this short position paper, we discuss opportunities to promote
multi-stakeholder deliberations around the design of AI-based technologies and
associated policies, at the earliest stages of a new project.

Recent advancements in technology, particularly in machine learning (ML),
deep learning (DL), and the metaverse, offer great potential for
revolutionizing surgical science. The combination of artificial intelligence
and extended reality (AI-XR) technologies has the potential to create a
surgical metaverse, a virtual environment where surgeries can be planned and
performed. This paper aims to provide insight into the various potential
applications of an AI-XR surgical metaverse and the challenges that must be
addressed to bring its full potential to fruition. It is important for the
community to focus on these challenges to fully realize the potential of the
AI-XR surgical metaverses. Furthermore, to emphasize the need for secure and
robust AI-XR surgical metaverses and to demonstrate the real-world implications
of security threats to the AI-XR surgical metaverses, we present a case study
in which the ``an immersive surgical attack'' on incision point localization is
performed in the context of preoperative planning in a surgical metaverse.

AI Advancements have augmented casual writing and story generation, but their
usage poses challenges in collaborative storytelling. In role-playing games
such as Dungeons & Dragons (D&D), composing prompts using generative AI
requires a technical understanding to generate ideal results, which is
difficult for novices. Thus, emergent narratives organically developed based on
player actions and decisions have yet to be fully utilized. This paper
envisions the use of generative AI in transforming storytelling into an
interactive drama using dynamic and immersive narratives. First, we describe
scenarios where narratives are created and character conversations are designed
within an overarching fantasy disposition. Then, we recommend design guidelines
to help create tools using generative AI in interactive storytelling. Lastly,
we raise questions on its potential impact on player immersion and cognitive
load. Our contributions may be expanded within the broader interactive
storytelling domain, such as speech-conversational AI and persona-driven
chatbots.

Artificial intelligence (AI) systems can cause harm to people. This research
examines how individuals react to such harm through the lens of blame. Building
upon research suggesting that people blame AI systems, we investigated how
several factors influence people's reactive attitudes towards machines,
designers, and users. The results of three studies (N = 1,153) indicate
differences in how blame is attributed to these actors. Whether AI systems were
explainable did not impact blame directed at them, their developers, and their
users. Considerations about fairness and harmfulness increased blame towards
designers and users but had little to no effect on judgments of AI systems.
Instead, what determined people's reactive attitudes towards machines was
whether people thought blaming them would be a suitable response to algorithmic
harm. We discuss implications, such as how future decisions about including AI
systems in the social and moral spheres will shape laypeople's reactions to
AI-caused harm.

Large language models have introduced exciting new opportunities and
challenges in designing and developing new AI-assisted writing support tools.
Recent work has shown that leveraging this new technology can transform writing
in many scenarios such as ideation during creative writing, editing support,
and summarization. However, AI-supported expository writing--including
real-world tasks like scholars writing literature reviews or doctors writing
progress notes--is relatively understudied. In this position paper, we argue
that developing AI supports for expository writing has unique and exciting
research challenges and can lead to high real-world impacts. We characterize
expository writing as evidence-based and knowledge-generating: it contains
summaries of external documents as well as new information or knowledge. It can
be seen as the product of authors' sensemaking process over a set of source
documents, and the interplay between reading, reflection, and writing opens up
new opportunities for designing AI support. We sketch three components for AI
support design and discuss considerations for future research.

We highlight the challenges faced by non-native speakers when using AI
writing assistants to paraphrase text. Through an interview study with 15
non-native English speakers (NNESs) with varying levels of English proficiency,
we observe that they face difficulties in assessing paraphrased texts generated
by AI writing assistants, largely due to the lack of explanations accompanying
the suggested paraphrases. Furthermore, we examine their strategies to assess
AI-generated texts in the absence of such explanations. Drawing on the needs of
NNESs identified in our interview, we propose four potential user interfaces to
enhance the writing experience of NNESs using AI writing assistants. The
proposed designs focus on incorporating explanations to better support NNESs in
understanding and evaluating the AI-generated paraphrasing suggestions.

Security risks from AI have motivated calls for international agreements that
guardrail the technology. However, even if states could agree on what rules to
set on AI, the problem of verifying compliance might make these agreements
infeasible. To help clarify the difficulty of verifying agreements on
AI$\unicode{x2013}$and identify actions that might reduce this
difficulty$\unicode{x2013}$this report examines the case study of verification
in nuclear arms control. We review the implementation, track records, and
politics of verification across three types of nuclear arms control agreements.
Then, we consider implications for the case of AI, especially AI development
that relies on thousands of highly specialized chips. In this context, the case
study suggests that, with certain preparations, the foreseeable challenges of
verification would be reduced to levels that were successfully managed in
nuclear arms control. To avoid even worse challenges, substantial preparations
are needed: (1) developing privacy-preserving, secure, and acceptably priced
methods for verifying the compliance of hardware, given inspection access; and
(2) building an initial, incomplete verification system, with authorities and
precedents that allow its gaps to be quickly closed if and when the political
will arises.

Are you an AI researcher at an academic institution? Are you anxious you are
not coping with the current pace of AI advancements? Do you feel you have no
(or very limited) access to the computational and human resources required for
an AI research breakthrough? You are not alone; we feel the same way. A growing
number of AI academics can no longer find the means and resources to compete at
a global scale. This is a somewhat recent phenomenon, but an accelerating one,
with private actors investing enormous compute resources into cutting edge AI
research. Here, we discuss what you can do to stay competitive while remaining
an academic. We also briefly discuss what universities and the private sector
could do improve the situation, if they are so inclined. This is not an
exhaustive list of strategies, and you may not agree with all of them, but it
serves to start a discussion.

Many sets of ethics principles for responsible AI have been proposed to allay
concerns about misuse and abuse of AI/ML systems. The underlying aspects of
such sets of principles include privacy, accuracy, fairness, robustness,
explainability, and transparency. However, there are potential tensions between
these aspects that pose difficulties for AI/ML developers seeking to follow
these principles. For example, increasing the accuracy of an AI/ML system may
reduce its explainability. As part of the ongoing effort to operationalise the
principles into practice, in this work we compile and discuss a catalogue of 10
notable tensions, trade-offs and other interactions between the underlying
aspects. We primarily focus on two-sided interactions, drawing on support
spread across a diverse literature. This catalogue can be helpful in raising
awareness of the possible interactions between aspects of ethics principles, as
well as facilitating well-supported judgements by the designers and developers
of AI/ML systems.

Many approaches to Natural Language Processing (NLP) tasks often treat them
as single-step problems, where an agent receives an instruction, executes it,
and is evaluated based on the final outcome. However, human language is
inherently interactive, as evidenced by the back-and-forth nature of human
conversations. In light of this, we posit that human-AI collaboration should
also be interactive, with humans monitoring the work of AI agents and providing
feedback that the agent can understand and utilize. Further, the AI agent
should be able to detect when it needs additional information and proactively
ask for help. Enabling this scenario would lead to more natural, efficient, and
engaging human-AI collaborations.
  In this work, we explore these directions using the challenging task defined
by the IGLU competition, an interactive grounded language understanding task in
a MineCraft-like world. We explore multiple types of help players can give to
the AI to guide it and analyze the impact of this help in AI behavior,
resulting in performance improvements.

We introduce the idea of Centaur Programmer, based on the premise that a
collaborative approach between humans and AI will be more effective than AI
alone, as demonstrated in centaur chess tournaments where mixed teams of humans
and AI beat sole computers. The paper introduces several collaboration models
for programming alongside an AI, including the guidance model, the sketch
model, and the inverted control model, and suggests that universities should
prepare future programmers for a more efficient and productive programming
environment augmented with AI. We hope to contribute to the important
discussion about the diverse ways whereby humans and AI can work together in
programming in the next decade, how universities should handle these changes
and some legal implications surrounding this topic.

Strong foundations in basic AI techniques are key to understanding more
advanced concepts. We believe that introducing AI techniques, such as search
methods, early in higher education helps create a deeper understanding of the
concepts seen later in more advanced AI and algorithms courses. We present a
project-based and competition-based bachelor course that gives second-year
students an introduction to search methods applied to board games. In groups of
two, students have to use network programming and AI methods to build an AI
agent to compete in a board game tournament-othello was this year's game.
Students are evaluated based on the quality of their projects and on their
performance during the final tournament. We believe that the introduction of
gamification, in the form of competition-based learning, allows for a better
learning experience for the students.

Artificial intelligence (AI) has demonstrated the ability to extract insights
from data, but the issue of fairness remains a concern in high-stakes fields
such as healthcare. Despite extensive discussion and efforts in algorithm
development, AI fairness and clinical concerns have not been adequately
addressed. In this paper, we discuss the misalignment between technical and
clinical perspectives of AI fairness, highlight the barriers to AI fairness'
translation to healthcare, advocate multidisciplinary collaboration to bridge
the knowledge gap, and provide possible solutions to address the clinical
concerns pertaining to AI fairness.

The disruptive potential of AI systems roots in the emergence of big data.
Yet, a significant portion is scattered and locked in data silos, leaving its
potential untapped. Federated Machine Learning is a novel AI paradigm enabling
the creation of AI models from decentralized, potentially siloed data. Hence,
Federated Machine Learning could technically open data silos and therefore
unlock economic potential. However, this requires collaboration between
multiple parties owning data silos. Setting up collaborative business models is
complex and often a reason for failure. Current literature lacks guidelines on
which aspects must be considered to successfully realize collaborative AI
projects. This research investigates the challenges of prevailing collaborative
business models and distinct aspects of Federated Machine Learning. Through a
systematic literature review, focus group, and expert interviews, we provide a
systemized collection of socio-technical challenges and an extended Business
Model Canvas for the initial viability assessment of collaborative AI projects.

Resistive random-access memory (RRAM) is a promising candidate for
next-generation memory devices due to its high speed, low power consumption,
and excellent scalability. Metal oxides are commonly used as the oxide layer in
RRAM devices due to their high dielectric constant and stability. However, to
further improve the performance of RRAM devices, recent research has focused on
integrating artificial intelligence (AI). AI can be used to optimize the
performance of RRAM devices, while RRAM can also power AI as a hardware
accelerator and in neuromorphic computing. This review paper provides an
overview of the combination of metal oxides-based RRAM and AI, highlighting
recent advances in these two directions. We discuss the use of AI to improve
the performance of RRAM devices and the use of RRAM to power AI. Additionally,
we address key challenges in the field and provide insights into future
research directions

This paper aims to prove the emergence of symbolic concepts in well-trained
AI models. We prove that if (1) the high-order derivatives of the model output
w.r.t. the input variables are all zero, (2) the AI model can be used on
occluded samples and will yield higher confidence when the input sample is less
occluded, and (3) the confidence of the AI model does not significantly degrade
on occluded samples, then the AI model will encode sparse interactive concepts.
Each interactive concept represents an interaction between a specific set of
input variables, and has a certain numerical effect on the inference score of
the model. Specifically, it is proved that the inference score of the model can
always be represented as the sum of the interaction effects of all interactive
concepts. In fact, we hope to prove that conditions for the emergence of
symbolic concepts are quite common. It means that for most AI models, we can
usually use a small number of interactive concepts to mimic the model outputs
on any arbitrarily masked samples.

With the widespread attention and application of artificial intelligence (AI)
and blockchain technologies, privacy protection techniques arising from their
integration are of notable significance. In addition to protecting privacy of
individuals, these techniques also guarantee security and dependability of
data. This paper initially presents an overview of AI and blockchain,
summarizing their combination along with derived privacy protection
technologies. It then explores specific application scenarios in data
encryption, de-identification, multi-tier distributed ledgers, and k-anonymity
methods. Moreover, the paper evaluates five critical aspects of
AI-blockchain-integration privacy protection systems, including authorization
management, access control, data protection, network security, and scalability.
Furthermore, it analyzes the deficiencies and their actual cause, offering
corresponding suggestions. This research also classifies and summarizes privacy
protection techniques based on AI-blockchain application scenarios and
technical schemes. In conclusion, this paper outlines the future directions of
privacy protection technologies emerging from AI and blockchain integration,
including enhancing efficiency and security to achieve a more comprehensive
privacy protection of privacy.

We previously showed that three-dimensional quadratic diffeomorphisms have
anti-integrable (AI) limits that correspond to a quadratic correspondence; a
pair of one-dimensional maps. At the AI limit the dynamics is conjugate to a
full shift on two symbols. Here we consider a more general AI limit, allowing
two parameters of the map to go to infinity. We prove the existence of AI
states for each symbol sequence for three cases of the quadratic
correspondence: parabolas, ellipses and hyperbolas. A contraction argument
gives parameter domains such that this is a bijection, but the correspondence
also is observed to apply more generally. We show that orbits of the original
map can be obtained by numerical continuation for a volume-contracting case.
These results show that periodic AI states evolve into the observed periodic
attractors of the diffeomorphism. We also continue a periodic AI state with a
symbol sequence chosen so that it continues to an orbit resembling a chaotic
attractor that is a 3D version of the classical 2D H\'enon attractor.

The launch of ChatGPT in November 2022 precipitated a panic among some
educators while prompting qualified enthusiasm from others. Under the umbrella
term Generative AI, ChatGPT is an example of a range of technologies for the
delivery of computer-generated text, image, and other digitized media. This
paper examines the implications for education of one generative AI technology,
chatbots responding from large language models, or C-LLM. It reports on an
application of a C-LLM to AI review and assessment of complex student work. In
a concluding discussion, the paper explores the intrinsic limits of generative
AI, bound as it is to language corpora and their textual representation through
binary notation. Within these limits, we suggest the range of emerging and
potential applications of Generative AI in education.

Trust is an important factor in people's interactions with AI systems.
However, there is a lack of empirical studies examining how real end-users
trust or distrust the AI system they interact with. Most research investigates
one aspect of trust in lab settings with hypothetical end-users. In this paper,
we provide a holistic and nuanced understanding of trust in AI through a
qualitative case study of a real-world computer vision application. We report
findings from interviews with 20 end-users of a popular, AI-based bird
identification app where we inquired about their trust in the app from many
angles. We find participants perceived the app as trustworthy and trusted it,
but selectively accepted app outputs after engaging in verification behaviors,
and decided against app adoption in certain high-stakes scenarios. We also find
domain knowledge and context are important factors for trust-related assessment
and decision-making. We discuss the implications of our findings and provide
recommendations for future research on trust in AI.

This paper investigates the causal impact of negatively and positively toned
ChatGPT Artificial Intelligence (AI) discussions on US students' anticipated
labor market outcomes. Our findings reveal students reduce their confidence
regarding their future earnings prospects after exposure to AI debates, and
this effect is more pronounced after reading discussion excerpts with a
negative tone. Unlike STEM majors, students in Non-STEM fields show asymmetric
and pessimistic belief changes, suggesting that they might feel more vulnerable
to emerging AI technologies. Pessimistic belief updates regarding future
earnings are also prevalent among non-male students, indicating widespread AI
concerns among vulnerable student subgroups. Educators, administrators, and
policymakers may regularly engage with students to address their concerns and
enhance educational curricula to better prepare them for a future that AI will
inevitably shape.

The introduction of OpenAI's large language model, ChatGPT, catalyzed
investor attention towards artificial intelligence (AI) technologies, including
AI-related crypto assets not directly related to ChatGPT. Utilizing the
synthetic difference-in-difference methodology, we identify significant
'ChatGPT effects' with returns of AI-related crypto assets experiencing average
returns ranging between 10.7% and 15.6% (35.5% to 41.3%) in the one-month
(two-month) period after the ChatGPT launch. Furthermore, Google search
volumes, a proxy for attention to AI, emerged as critical pricing indicators
for AI-related crypto post-launch. We conclude that investors perceived
AI-assets as possessing heightened potential or value after the launch,
resulting in higher market valuations.

The collaboration between humans and artificial intelligence (AI) is a
significant feature in this digital age. However, humans and AI may have
observation, interpretation, and action conflicts when working synchronously.
This phenomenon is often masked by faults and, unfortunately, overlooked. This
paper systematically introduces the human-AI conflict concept, causes,
measurement methods, and risk assessment. The results highlight that there is a
potential second decision-maker besides the human, which is the AI; the
human-AI conflict is a unique and emerging risk in digitalized process systems;
and this is an interdisciplinary field that needs to be distinguished from
traditional fault and failure analysis; the conflict risk is significant and
cannot be ignored.

When making strategic decisions, we are often confronted with overwhelming
information to process. The situation can be further complicated when some
pieces of evidence are contradicted each other or paradoxical. The challenge
then becomes how to determine which information is useful and which ones should
be eliminated. This process is known as meta-decision. Likewise, when it comes
to using Artificial Intelligence (AI) systems for strategic decision-making,
placing trust in the AI itself becomes a meta-decision, given that many AI
systems are viewed as opaque "black boxes" that process large amounts of data.
Trusting an opaque system involves deciding on the level of Trustworthy AI
(TAI). We propose a new approach to address this issue by introducing a novel
taxonomy or framework of TAI, which encompasses three crucial domains:
articulate, authentic, and basic for different levels of trust. To underpin
these domains, we create ten dimensions to measure trust:
explainability/transparency, fairness/diversity, generalizability, privacy,
data governance, safety/robustness, accountability, reproducibility,
reliability, and sustainability. We aim to use this taxonomy to conduct a
comprehensive survey and explore different TAI approaches from a strategic
decision-making perspective.

AI requires heavy amounts of storage and compute with assets that are
commonly stored in AI Hubs. AI Hubs have contributed significantly to the
democratization of AI. However, existing implementations are associated with
certain benefits and limitations that stem from the underlying infrastructure
and governance systems with which they are built. These limitations include
high costs, lack of monetization and reward, lack of control and difficulty of
reproducibility. In the current work, we explore the potential of decentralized
technologies - such as Web3 wallets, peer-to-peer marketplaces, storage and
compute, and DAOs - to address some of these issues. We suggest that these
infrastructural components can be used in combination in the design and
construction of decentralized AI Hubs.

Language assessment plays a crucial role in diagnosing and treating
individuals with speech, language, and communication disorders caused by
neurogenic conditions, whether developmental or acquired. However, current
assessment methods are manual, laborious, and time-consuming to administer and
score, causing additional patient stress. To address these challenges, we
developed Open Brain AI (https://openbrainai.com). This computational platform
harnesses innovative AI techniques, namely machine learning, natural language
processing, large language models, and automatic speech-to-text transcription,
to automatically analyze multilingual spoken and written speech productions.
This paper discusses the development of Open Brain AI, the AI language
processing modules, and the linguistic measurements of discourse
macro-structure and micro-structure. The fast and automatic analysis of
language alleviates the burden on clinicians, enabling them to streamline their
workflow and allocate more time and resources to direct patient care. Open
Brain AI is freely accessible, empowering clinicians to conduct critical data
analyses and give more attention and resources to other critical aspects of
therapy and treatment.

As AI technology develops, trust in AI agents is becoming more important for
more AI applications in human society. Possible ways to improve the trust
relationship include empathy, success-failure series, and capability
(performance). Appropriate trust is less likely to cause deviations between
actual and ideal performance. In this study, we focus on the agent's empathy
and success-failure series to increase trust in AI agents. We experimentally
examine the effect of empathy from agent to person on changes in trust over
time. The experiment was conducted with a two-factor mixed design: empathy
(available, not available) and success-failure series (phase 1 to phase 5). An
analysis of variance (ANOVA) was conducted using data from 198 participants.
The results showed an interaction between the empathy factor and the
success-failure series factor, with trust in the agent stabilizing when empathy
was present. This result supports our hypothesis. This study shows that
designing AI agents to be empathetic is an important factor for trust and helps
humans build appropriate trust relationships with AI agents.

This research delves into the intersection of illustration art and artificial
intelligence (AI), focusing on how illustrators engage with AI agents that
embody their original characters (OCs). We introduce 'ORIBA', a customizable AI
chatbot that enables illustrators to converse with their OCs. This approach
allows artists to not only receive responses from their OCs but also to observe
their inner monologues and behavior. Despite the existing tension between
artists and AI, our study explores innovative collaboration methods that are
inspiring to illustrators. By examining the impact of AI on the creative
process and the boundaries of authorship, we aim to enhance human-AI
interactions in creative fields, with potential applications extending beyond
illustration to interactive storytelling and more.

Moral AI has been studied in the fields of philosophy and artificial
intelligence. Although most existing studies are only theoretical, recent
developments in AI have made it increasingly necessary to implement AI with
morality. On the other hand, humans are under the moral uncertainty of not
knowing what is morally right. In this paper, we implement the Maximizing
Expected Choiceworthiness (MEC) algorithm, which aggregates outputs of models
based on three normative theories of normative ethics to generate the most
appropriate output. MEC is a method for making appropriate moral judgments
under moral uncertainty. Our experimental results suggest that the output of
MEC correlates to some extent with commonsense morality and that MEC can
produce equally or more appropriate output than existing methods.

Artificial intelligence (AI) and machine learning (ML) present revolutionary
opportunities to enhance our understanding of animal behavior and conservation
strategies. Using elephants, a crucial species in Africa's protected areas, as
our focal point, we delve into the role of AI and ML in their conservation.
Given the increasing amounts of data gathered from a variety of sensors like
cameras, microphones, geophones, drones, and satellites, the challenge lies in
managing and interpreting this vast data. New AI and ML techniques offer
solutions to streamline this process, helping us extract vital information that
might otherwise be overlooked. This paper focuses on the different AI-driven
monitoring methods and their potential for improving elephant conservation.
Collaborative efforts between AI experts and ecological researchers are
essential in leveraging these innovative technologies for enhanced wildlife
conservation, setting a precedent for numerous other species.

To realize the potential benefits and mitigate potential risks of AI, it is
necessary to develop a framework of governance that conforms to ethics and
fundamental human values. Although several organizations have issued guidelines
and ethical frameworks for trustworthy AI, without a mediating governance
structure, these ethical principles will not translate into practice. In this
paper, we propose a multilevel governance approach that involves three groups
of interdependent stakeholders: governments, corporations, and citizens. We
examine their interrelationships through dimensions of trust, such as
competence, integrity, and benevolence. The levels of governance combined with
the dimensions of trust in AI provide practical insights that can be used to
further enhance user experiences and inform public policy related to AI.

We present this article as a small gesture in an attempt to counter what
appears to be exponentially growing hype around Artificial Intelligence (AI)
and its capabilities, and the distraction provided by the associated talk of
science-fiction scenarios that might arise if AI should become sentient and
super-intelligent. It may also help those outside of the field to become more
informed about some of the limitations of AI technology. In the current context
of popular discourse AI defaults to mean foundation and large language models
(LLMs) such as those used to create ChatGPT. This in itself is a
misrepresentation of the diversity, depth and volume of research, researchers,
and technology that truly represents the field of AI. AI being a field of
research that has existed in software artefacts since at least the 1950's. We
set out to highlight a number of limitations of LLMs, and in so doing highlight
that harms have already arisen and will continue to arise due to these
limitations. Along the way we also highlight some of the associated risks for
individuals and organisations in using this technology.

Recently, the AI/ML research community has indicated an urgent need to
establish Responsible AI (RAI) values and practices as part of the AI/ML
lifecycle. Several organizations and communities are responding to this call by
sharing RAI guidelines. However, there are gaps in awareness, deliberation, and
execution of such practices for multi-disciplinary ML practitioners. This work
contributes to the discussion by unpacking co-production challenges faced by
practitioners as they align their RAI values. We interviewed 23 individuals,
across 10 organizations, tasked to ship AI/ML based products while upholding
RAI norms and found that both top-down and bottom-up institutional structures
create burden for different roles preventing them from upholding RAI values, a
challenge that is further exacerbated when executing conflicted values. We
share multiple value levers used as strategies by the practitioners to resolve
their challenges. We end our paper with recommendations for inclusive and
equitable RAI value-practices, creating supportive organizational structures
and opportunities to further aid practitioners.

In the interdisciplinary field of artificial intelligence (AI) the problem of
clear terminology is especially momentous. This paper claims, that AI debates
are still characterised by a lack of critical distance to metaphors like
'training', 'learning' or 'deciding'. As consequence, reflections regarding
responsibility or potential use-cases are greatly distorted. Yet, if relevant
decision-makers are convinced that AI can develop an 'understanding' or
properly 'interpret' issues, its regular use for sensitive tasks like deciding
about social benefits or judging court cases looms. The chapter argues its
claim by analysing central notions of the AI debate and tries to contribute by
proposing more fitting terminology and hereby enabling more fruitful debates.
It is a conceptual work at the intersection of critical computer science and
philosophy of language.

This paper argues that training AI systems with absolute constraints -- which
forbid certain acts irrespective of the amount of value they might produce --
may make considerable progress on many AI safety problems in principle. First,
it provides a guardrail for avoiding the very worst outcomes of misalignment.
Second, it could prevent AIs from causing catastrophes for the sake of very
valuable consequences, such as replacing humans with a much larger number of
beings living at a higher welfare level. Third, it makes systems more
corrigible, allowing creators to make corrective interventions in them, such as
altering their objective functions or shutting them down. And fourth, it helps
systems explore their environment more safely by prohibiting them from
exploring especially dangerous acts. I offer a decision-theoretic formalization
of an absolute constraints, improving on existing models in the literature, and
use this model to prove some results about the training and behavior of
absolutist AIs. I conclude by showing that, although absolutist AIs will not
maximize expected value, they will not be susceptible to behave irrationally,
and they will not (contra coherence arguments) face environmental pressure to
become expected-value maximizers.

Exposure to disturbing imagery can significantly impact individuals,
especially professionals who encounter such content as part of their work. This
paper presents a user study, involving 107 participants, predominantly
journalists and human rights investigators, that explores the capability of
Artificial Intelligence (AI)-based image filters to potentially mitigate the
emotional impact of viewing such disturbing content. We tested five different
filter styles, both traditional (Blurring and Partial Blurring) and AI-based
(Drawing, Colored Drawing, and Painting), and measured their effectiveness in
terms of conveying image information while reducing emotional distress. Our
findings suggest that the AI-based Drawing style filter demonstrates the best
performance, offering a promising solution for reducing negative feelings
(-30.38%) while preserving the interpretability of the image (97.19%). Despite
the requirement for many professionals to eventually inspect the original
images, participants suggested potential strategies for integrating AI filters
into their workflow, such as using AI filters as an initial, preparatory step
before viewing the original image. Overall, this paper contributes to the
development of a more ethically considerate and effective visual environment
for professionals routinely engaging with potentially disturbing imagery.

Artificial Intelligence (AI)'s pervasive presence and variety necessitate
diversity and inclusivity (D&I) principles in its design for fairness, trust,
and transparency. Yet, these considerations are often overlooked, leading to
issues of bias, discrimination, and perceived untrustworthiness. In response,
we conducted a Systematic Review to unearth challenges and solutions relating
to D&I in AI. Our rigorous search yielded 48 research articles published
between 2017 and 2022. Open coding of these papers revealed 55 unique
challenges and 33 solutions for D&I in AI, as well as 24 unique challenges and
23 solutions for enhancing such practices using AI. This study, by offering a
deeper understanding of these issues, will enlighten researchers and
practitioners seeking to integrate these principles into future AI systems.

Neuro-Symbolic Artificial Intelligence (AI) is an emerging and quickly
advancing field that combines the subsymbolic strengths of (deep) neural
networks and explicit, symbolic knowledge contained in knowledge graphs to
enhance explainability and safety in AI systems. This approach addresses a key
criticism of current generation systems, namely their inability to generate
human-understandable explanations for their outcomes and ensure safe behaviors,
especially in scenarios with \textit{unknown unknowns} (e.g. cybersecurity,
privacy). The integration of neural networks, which excel at exploring complex
data spaces, and symbolic knowledge graphs, which represent domain knowledge,
allows AI systems to reason, learn, and generalize in a manner understandable
to experts. This article describes how applications in cybersecurity and
privacy, two most demanding domains in terms of the need for AI to be
explainable while being highly accurate in complex environments, can benefit
from Neuro-Symbolic AI.

This book chapter delves into the pressing need to "queer" the ethics of AI
to challenge and re-evaluate the normative suppositions and values that
underlie AI systems. The chapter emphasizes the ethical concerns surrounding
the potential for AI to perpetuate discrimination, including binarism, and
amplify existing inequalities due to the lack of representative datasets and
the affordances and constraints depending on technology readiness. The chapter
argues that a critical examination of the neoliberal conception of equality
that often underpins non-discrimination law is necessary and cannot stress more
the need to create alternative interdisciplinary approaches that consider the
complex and intersecting factors that shape individuals' experiences of
discrimination. By exploring such approaches centering on intersectionality and
vulnerability-informed design, the chapter contends that designers and
developers can create more ethical AI systems that are inclusive, equitable,
and responsive to the needs and experiences of all individuals and communities,
particularly those who are most vulnerable to discrimination and harm.

The European Union's Artificial Intelligence Act aims to regulate
manipulative and harmful uses of AI, but lacks precise definitions for key
concepts. This paper provides technical recommendations to improve the Act's
conceptual clarity and enforceability. We review psychological models to define
"personality traits," arguing the Act should protect full "psychometric
profiles." We urge expanding "behavior" to include "preferences" since
preferences causally influence and are influenced by behavior. Clear
definitions are provided for "subliminal," "manipulative," and "deceptive"
techniques, considering incentives, intent, and covertness. We distinguish
"exploiting individuals" from "exploiting groups," emphasising different policy
needs. An "informed decision" is defined by four facets: comprehension,
accurate information, no manipulation, and understanding AI's influence. We
caution the Act's therapeutic use exemption given the lack of regulation of
digital therapeutics by the EMA. Overall, the recommendations strengthen
definitions of vague concepts in the EU AI Act, enhancing precise applicability
to regulate harmful AI manipulation.

Coarse-Grained Reconfigurable Arrays (CGRAs) hold great promise as
power-efficient edge accelerator, offering versatility beyond AI applications.
Morpher, an open-source, architecture-adaptive CGRA design framework, is
specifically designed to explore the vast design space of CGRAs. The
comprehensive ecosystem of Morpher includes a tailored compiler, simulator,
accelerator synthesis, and validation framework. This study provides an
overview of Morpher, highlighting its capabilities in automatically compiling
AI application kernels onto user-defined CGRA architectures and verifying their
functionality. Through the Morpher framework, the versatility of CGRAs is
harnessed to facilitate efficient compilation and verification of edge AI
applications, covering important kernels representative of a wide range of
embedded AI workloads. Morpher is available online at
https://github.com/ecolab-nus/morpher-v2.

Compelling writing is tailored to its audience. This is challenging, as
writers may struggle to empathize with readers, get feedback in time, or gain
access to the target group. We propose a concept that generates on-demand
feedback, based on writer-defined AI personas of any target audience. We
explore this concept with a prototype (using GPT-3.5) in two user studies (N=5
and N=11): Writers appreciated the concept and strategically used personas for
getting different perspectives. The feedback was seen as helpful and inspired
revisions of text and personas, although it was often verbose and unspecific.
We discuss the impact of on-demand feedback, the limited representativity of
contemporary AI systems, and further ideas for defining AI personas. This work
contributes to the vision of supporting writers with AI by expanding the
socio-technical perspective in AI tool design: To empower creators, we also
need to keep in mind their relationship to an audience.

While AI shows promise for enhancing the efficiency of qualitative analysis,
the unique human-AI interaction resulting from varied coding strategies makes
it challenging to develop a trustworthy AI-assisted qualitative coding system
(AIQCs) that supports coding tasks effectively. We bridge this gap by exploring
the impact of varying coding strategies on user trust and reliance on AI. We
conducted a mixed-methods split-plot 3x3 study, involving 30 participants, and
a follow-up study with 6 participants, exploring varying text selection and
code length in the use of our AIQCs system for qualitative analysis. Our
results indicate that qualitative open coding should be conceptualized as a
series of distinct subtasks, each with differing levels of complexity, and
therefore, should be given tailored design considerations. We further observed
a discrepancy between perceived and behavioral measures, and emphasized the
potential challenges of under- and over-reliance on AIQCs systems. Additional
design implications were also proposed for consideration.

Due to the powerful capabilities demonstrated by large language model (LLM),
there has been a recent surge in efforts to integrate them with AI agents to
enhance their performance. In this paper, we have explored the core differences
and characteristics between LLM-based AI agents and traditional AI agents.
Specifically, we first compare the fundamental characteristics of these two
types of agents, clarifying the significant advantages of LLM-based agents in
handling natural language, knowledge storage, and reasoning capabilities.
Subsequently, we conducted an in-depth analysis of the key components of AI
agents, including planning, memory, and tool use. Particularly, for the crucial
component of memory, this paper introduced an innovative classification scheme,
not only departing from traditional classification methods but also providing a
fresh perspective on the design of an AI agent's memory system. We firmly
believe that in-depth research and understanding of these core components will
lay a solid foundation for the future advancement of AI agent technology. At
the end of the paper, we provide directional suggestions for further research
in this field, with the hope of offering valuable insights to scholars and
researchers in the field.

As artificial intelligence (AI) systems increasingly impact society, the EU
Artificial Intelligence Act (AIA) is the first serious legislative attempt to
contain the harmful effects of AI systems. This paper proposes a governance
framework for AI innovation. The framework bridges the gap between strategic
variables and responsible value creation, recommending audit as an enforcement
mechanism. Strategic variables include, among others, organization size,
exploration versus exploitation -, and build versus buy dilemmas. The proposed
framework is based on primary and secondary research; the latter describes four
pressures that organizations innovating with AI experience. Primary research
includes an experimental setup, using which 34 organizations in the Netherlands
are surveyed, followed up by 2 validation interviews. The survey measures the
extent to which organizations coordinate technical elements of AI systems to
ultimately comply with the AIA. The validation interviews generated additional
in-depth insights and provided root causes. The moderating effect of the
strategic variables is tested and found to be statistically significant for
variables such as organization size. Relevant insights from primary and
secondary research are eventually combined to propose the APPRAISE framework.

Artificial Intelligence (AI) systems, especially generative AI technologies
are becoming more relevant in our society. Tools like ChatGPT are being used by
members of the disabled community e.g., Autistic people may use it to help
compose emails. The growing impact and popularity of generative AI tools have
prompted us to examine their relevance within the disabled community. The
design and development phases often neglect this marginalized group, leading to
inaccurate predictions and unfair discrimination directed towards them. This
could result from bias in data sets, algorithms, and systems at various phases
of creation and implementation. This workshop paper proposes a platform to
involve the disabled community while building generative AI systems. With this
platform, our aim is to gain insight into the factors that contribute to bias
in the outputs generated by generative AI when used by the disabled community.
Furthermore, we expect to comprehend which algorithmic factors are the main
contributors to the output's incorrectness or irrelevancy. The proposed
platform calls on both disabled and non-disabled people from various
geographical and cultural backgrounds to collaborate asynchronously and
remotely in a democratic approach to decision-making.

Data storytelling is powerful for communicating data insights, but it
requires diverse skills and considerable effort from human creators. Recent
research has widely explored the potential for artificial intelligence (AI) to
support and augment humans in data storytelling. However, there lacks a
systematic review to understand data storytelling tools from the perspective of
human-AI collaboration, which hinders researchers from reflecting on the
existing collaborative tool designs that promote humans' and AI's advantages
and mitigate their shortcomings. This paper investigated existing tools with a
framework from two perspectives: the stages in the storytelling workflow where
a tool serves, including analysis, planning, implementation, and communication,
and the roles of humans and AI in each stage, such as creators, assistants,
optimizers, and reviewers. Through our analysis, we recognize the common
collaboration patterns in existing tools, summarize lessons learned from these
patterns, and further illustrate research opportunities for human-AI
collaboration in data storytelling.

We introduce the fundamental ideas and challenges of Predictable AI, a
nascent research area that explores the ways in which we can anticipate key
indicators of present and future AI ecosystems. We argue that achieving
predictability is crucial for fostering trust, liability, control, alignment
and safety of AI ecosystems, and thus should be prioritised over performance.
While distinctive from other areas of technical and non-technical AI research,
the questions, hypotheses and challenges relevant to Predictable AI were yet to
be clearly described. This paper aims to elucidate them, calls for identifying
paths towards AI predictability and outlines the potential impact of this
emergent field.

Generative AI is expected to have transformative effects in multiple
knowledge industries. To better understand how knowledge workers expect
generative AI may affect their industries in the future, we conducted
participatory research workshops for seven different industries, with a total
of 54 participants across three US cities. We describe participants'
expectations of generative AI's impact, including a dominant narrative that cut
across the groups' discourse: participants largely envision generative AI as a
tool to perform menial work, under human review. Participants do not generally
anticipate the disruptive changes to knowledge industries currently projected
in common media and academic narratives. Participants do however envision
generative AI may amplify four social forces currently shaping their
industries: deskilling, dehumanization, disconnection, and disinformation. We
describe these forces, and then we provide additional detail regarding
attitudes in specific knowledge industries. We conclude with a discussion of
implications and research challenges for the HCI community.

Enterprise Wi-Fi networks can greatly benefit from Artificial Intelligence
and Machine Learning (AI/ML) thanks to their well-developed management and
operation capabilities. At the same time, AI/ML-based traffic/load prediction
is one of the most appealing data-driven solutions to improve the Wi-Fi
experience, either through the enablement of autonomous operation or by
boosting troubleshooting with forecasted network utilization. In this paper, we
study the suitability and feasibility of adopting AI/ML-based load prediction
in practical enterprise Wi-Fi networks. While leveraging AI/ML solutions can
potentially contribute to optimizing Wi-Fi networks in terms of energy
efficiency, performance, and reliability, their effective adoption is
constrained to aspects like data availability and quality, computational
capabilities, and energy consumption. Our results show that
hardware-constrained AI/ML models can potentially predict network load with
less than 20% average error and 3% 85th-percentile error, which constitutes a
suitable input for proactively driving Wi-Fi network optimization.

We study how humans learn from AI, exploiting an introduction of an
AI-powered Go program (APG) that unexpectedly outperformed the best
professional player. We compare the move quality of professional players to
that of APG's superior solutions around its public release. Our analysis of
749,190 moves demonstrates significant improvements in players' move quality,
accompanied by decreased number and magnitude of errors. The effect is
pronounced in the early stages of the game where uncertainty is highest. In
addition, younger players and those in AI-exposed countries experience greater
improvement, suggesting potential inequality in learning from AI. Further,
while players of all levels learn, less skilled players derive higher marginal
benefits. These findings have implications for managers seeking to adopt and
utilize AI effectively within their organizations.

Human-AI collaboration has the potential to transform various domains by
leveraging the complementary strengths of human experts and Artificial
Intelligence (AI) systems. However, unobserved confounding can undermine the
effectiveness of this collaboration, leading to biased and unreliable outcomes.
In this paper, we propose a novel solution to address unobserved confounding in
human-AI collaboration by employing the marginal sensitivity model (MSM). Our
approach combines domain expertise with AI-driven statistical modeling to
account for potential confounders that may otherwise remain hidden. We present
a deferral collaboration framework for incorporating the MSM into policy
learning from observational data, enabling the system to control for the
influence of unobserved confounding factors. In addition, we propose a
personalized deferral collaboration system to leverage the diverse expertise of
different human decision-makers. By adjusting for potential biases, our
proposed solution enhances the robustness and reliability of collaborative
outcomes. The empirical and theoretical analyses demonstrate the efficacy of
our approach in mitigating unobserved confounding and improving the overall
performance of human-AI collaborations.

Training large transformers using next-token prediction has given rise to
groundbreaking advancements in AI. While this generative AI approach has
produced impressive results, it heavily leans on human supervision. Even
state-of-the-art AI models like ChatGPT depend on fine-tuning through human
demonstrations, demanding extensive human input and domain expertise. This
strong reliance on human oversight poses a significant hurdle to the
advancement of AI innovation. To address this limitation, we propose a novel
paradigm termed Exploratory AI (EAI) aimed at autonomously generating
high-quality training data. Drawing inspiration from unsupervised reinforcement
learning (RL) pretraining, EAI achieves exploration within the natural language
space. We accomplish this by harnessing large language models to assess the
novelty of generated content. Our approach employs two key components: an actor
that generates novel content following exploration principles and a critic that
evaluates the generated content, offering critiques to guide the actor.
Empirical evaluations demonstrate that EAI significantly boosts model
performance on complex reasoning tasks, addressing the limitations of
human-intensive supervision.

This paper proposes a Multinational Artificial General Intelligence
Consortium (MAGIC) to mitigate existential risks from advanced artificial
intelligence (AI). MAGIC would be the only institution in the world permitted
to develop advanced AI, enforced through a global moratorium by its signatory
members on all other advanced AI development. MAGIC would be exclusive,
safety-focused, highly secure, and collectively supported by member states,
with benefits distributed equitably among signatories. MAGIC would allow narrow
AI models to flourish while significantly reducing the possibility of
misaligned, rogue, breakout, or runaway outcomes of general-purpose systems. We
do not address the political feasibility of implementing a moratorium or
address the specific legislative strategies and rules needed to enforce a ban
on high-capacity AGI training runs. Instead, we propose one positive vision of
the future, where MAGIC, as a global governance regime, can lay the groundwork
for long-term, safe regulation of advanced AI.

Mathematics is one of the most powerful conceptual systems developed and used
by the human species. Dreams of automated mathematicians have a storied history
in artificial intelligence (AI). Rapid progress in AI, particularly propelled
by advances in large language models (LLMs), has sparked renewed, widespread
interest in building such systems. In this work, we reflect on these goals from
a \textit{cognitive science} perspective. We call attention to several
classical and ongoing research directions from cognitive science, which we
believe are valuable for AI practitioners to consider when seeking to build
truly human (or superhuman)-level mathematical systems. We close with open
discussions and questions that we believe necessitate a multi-disciplinary
perspective -- cognitive scientists working in tandem with AI researchers and
mathematicians -- as we move toward better mathematical AI systems which not
only help us push the frontier of the mathematics, but also offer glimpses into
how we as humans are even capable of such great cognitive feats.

The advancement of Artificial Intelligence (AI) and Machine Learning (ML) has
profound implications for both the utility and security of our digital
interactions. This paper investigates the transformative role of Generative AI
in Social Engineering (SE) attacks. We conduct a systematic review of social
engineering and AI capabilities and use a theory of social engineering to
identify three pillars where Generative AI amplifies the impact of SE attacks:
Realistic Content Creation, Advanced Targeting and Personalization, and
Automated Attack Infrastructure. We integrate these elements into a conceptual
model designed to investigate the complex nature of AI-driven SE attacks - the
Generative AI Social Engineering Framework. We further explore human
implications and potential countermeasures to mitigate these risks. Our study
aims to foster a deeper understanding of the risks, human implications, and
countermeasures associated with this emerging paradigm, thereby contributing to
a more secure and trustworthy human-computer interaction.

Prototyping AI applications is notoriously difficult. While large language
model (LLM) prompting has dramatically lowered the barriers to AI prototyping,
designers are still prototyping AI functionality and UI separately. We
investigate how coupling prompt and UI design affects designers' workflows.
Grounding this research, we developed PromptInfuser, a Figma plugin that
enables users to create semi-functional mockups, by connecting UI elements to
the inputs and outputs of prompts. In a study with 14 designers, we compare
PromptInfuser to designers' current AI-prototyping workflow. PromptInfuser was
perceived to be significantly more useful for communicating product ideas, more
capable of producing prototypes that realistically represent the envisioned
artifact, more efficient for prototyping, and more helpful for anticipating UI
issues and technical constraints. PromptInfuser encouraged iteration over
prompt and UI together, which helped designers identify UI and prompt
incompatibilities and reflect upon their total solution. Together, these
findings inform future systems for prototyping AI applications.

Artificial Intelligence (AI) is one of the most momentous technologies of our
time. Thus, it is of major importance to know which stakeholders influence AI
research. Besides researchers at universities and colleges, researchers in
companies have hardly been considered in this context. In this article, we
consider how the influence of companies on AI research can be made measurable
on the basis of scientific publishing activities. We compare academic- and
company-authored AI publications published in the last decade and use
scientometric data from multiple scholarly databases to look for differences
across these groups and to disclose the top contributing organizations. While
the vast majority of publications is still produced by academia, we find that
the citation count an individual publication receives is significantly higher
when it is (co-)authored by a company. Furthermore, using a variety of
altmetric indicators, we notice that publications with company participation
receive considerably more attention online. Finally, we place our analysis
results in a broader context and present targeted recommendations to safeguard
a harmonious balance between academia and industry in the realm of AI research.

We present results from a pilot experiment to measure if machine
recommendations can debias human perceptual biases in visualization tasks. We
specifically studied the ``pull-down'' effect, i.e., people underestimate the
average position of lines, for the task of estimating the ensemble average of
data points in line charts. These line charts can show for example temperature
or precipitation in 12 months. Six participants estimated ensemble averages
with or without an AI assistant. The assistant, when available, responded at
three different speeds to assemble the conditions of a human collaborator who
may delay his or her responses. Our pilot study showed that participants were
faster with AI assistance in ensemble tasks, compared to the baseline without
AI assistance. Although ``pull-down'' biases were reduced, the effect of AI
assistance was not statistically significant. Also, delaying AI responses had
no significant impact on human decision accuracy. We discuss the implications
of these preliminary results for subsequent studies.

The emergence of AI tools in cybersecurity creates many opportunities and
uncertainties. A focus group with advanced graduate students in cybersecurity
revealed the potential depth and breadth of the challenges and opportunities.
The salient issues are access to open source or free tools, documentation,
curricular diversity, and clear articulation of ethical principles for AI
cybersecurity education. Confronting the "black box" mentality in AI
cybersecurity work is also of the greatest importance, doubled by deeper and
prior education in foundational AI work. Systems thinking and effective
communication were considered relevant areas of educational improvement. Future
AI educators and practitioners need to address these issues by implementing
rigorous technical training curricula, clear documentation, and frameworks for
ethically monitoring AI combined with critical and system's thinking and
communication skills.

With the advances in computationally efficient artificial Intelligence (AI)
techniques and their numerous applications in our everyday life, there is a
pressing need to understand the computational details hidden in black box AI
techniques such as most popular machine learning and deep learning techniques;
through more detailed explanations. The origin of explainable AI (xAI) is
coined from these challenges and recently gained more attention by the
researchers by adding explainability comprehensively in traditional AI systems.
This leads to develop an appropriate framework for successful applications of
xAI in real life scenarios with respect to innovations, risk mitigation,
ethical issues and logical values to the users. In this book chapter, an
in-depth analysis of several xAI frameworks and methods including LIME (Local
Interpretable Model-agnostic Explanations) and SHAP (SHapley Additive
exPlanations) are provided. Random Forest Classifier as black box AI is used on
a publicly available Diabetes symptoms dataset with LIME and SHAP for better
interpretations. The results obtained are interesting in terms of transparency,
valid and trustworthiness in diabetes disease prediction.

The ethics of AI as both material and medium for interaction remains in murky
waters within the context of musical and artistic practice. The
interdisciplinarity of the field is revealing matters of concern and care,
which necessitate interdisciplinary methodologies for evaluation to trouble and
critique the inheritance of "residue-laden" AI-tools in musical applications.
Seeking to unsettle these murky waters, this paper critically examines the
example of Holly+, a deep neural network that generates raw audio in the
likeness of its creator Holly Herndon. Drawing from theoretical concerns and
considerations from speculative feminism and care ethics, we care-fully trouble
the structures, frameworks and assumptions that oscillate within and around
Holly+. We contribute with several considerations and contemplate future
directions for integrating speculative feminism and care into musical-AI agent
and system design, derived from our critical feminist examination.

AI-based image generation has continued to rapidly improve, producing
increasingly more realistic images with fewer obvious visual flaws.
AI-generated images are being used to create fake online profiles which in turn
are being used for spam, fraud, and disinformation campaigns. As the general
problem of detecting any type of manipulated or synthesized content is
receiving increasing attention, here we focus on a more narrow task of
distinguishing a real face from an AI-generated face. This is particularly
applicable when tackling inauthentic online accounts with a fake user profile
photo. We show that by focusing on only faces, a more resilient and
general-purpose artifact can be detected that allows for the detection of
AI-generated faces from a variety of GAN- and diffusion-based synthesis
engines, and across image resolutions (as low as 128 x 128 pixels) and
qualities.

With the rapid advancement of machine learning models for NLP tasks,
collecting high-fidelity labels from AI models is a realistic possibility.
Firms now make AI available to customers via predictions as a service (PaaS).
This includes PaaS products for healthcare. It is unclear whether these labels
can be used for training a local model without expensive annotation checking by
in-house experts. In this work, we propose a new framework for Human Correction
of AI-Generated Labels (H-COAL). By ranking AI-generated outputs, one can
selectively correct labels and approach gold standard performance (100% human
labeling) with significantly less human effort. We show that correcting 5% of
labels can close the AI-human performance gap by up to 64% relative
improvement, and correcting 20% of labels can close the performance gap by up
to 86% relative improvement.

When engaging in strategic decision-making, we are frequently confronted with
overwhelming information and data. The situation can be further complicated
when certain pieces of evidence contradict each other or become paradoxical.
The primary challenge is how to determine which information can be trusted when
we adopt Artificial Intelligence (AI) systems for decision-making. This issue
is known as deciding what to decide or Trustworthy AI. However, the AI system
itself is often considered an opaque black box. We propose a new approach to
address this issue by introducing a novel framework of Trustworthy AI (TAI)
encompassing three crucial components of AI: representation space, loss
function, and optimizer. Each component is loosely coupled with four TAI
properties. Altogether, the framework consists of twelve TAI properties. We aim
to use this framework to conduct the TAI experiments by quantitive and
qualitative research methods to satisfy TAI properties for the decision-making
context. The framework allows us to formulate an optimal prediction model
trained by the given dataset for applying the strategic investment decision of
credit default swaps (CDS) in the technology sector. Finally, we provide our
view of the future direction of TAI research

The emergence of pre-trained AI systems with powerful capabilities across a
diverse and ever-increasing set of complex domains has raised a critical
challenge for AI safety as tasks can become too complicated for humans to judge
directly. Irving et al. [2018] proposed a debate method in this direction with
the goal of pitting the power of such AI models against each other until the
problem of identifying (mis)-alignment is broken down into a manageable
subtask. While the promise of this approach is clear, the original framework
was based on the assumption that the honest strategy is able to simulate
deterministic AI systems for an exponential number of steps, limiting its
applicability. In this paper, we show how to address these challenges by
designing a new set of debate protocols where the honest strategy can always
succeed using a simulation of a polynomial number of steps, whilst being able
to verify the alignment of stochastic AI systems, even when the dishonest
strategy is allowed to use exponentially many simulation steps.

The emergent abilities of Large Language Models (LLMs), which power tools
like ChatGPT and Bard, have produced both excitement and worry about how AI
will impact academic writing. In response to rising concerns about AI use,
authors of academic publications may decide to voluntarily disclose any AI
tools they use to revise their manuscripts, and journals and conferences could
begin mandating disclosure and/or turn to using detection services, as many
teachers have done with student writing in class settings. Given these looming
possibilities, we investigate whether academics view it as necessary to report
AI use in manuscript preparation and how detectors react to the use of AI in
academic writing.

Training advanced AI models requires large investments in computational
resources, or compute. Yet, as hardware innovation reduces the price of compute
and algorithmic advances make its use more efficient, the cost of training an
AI model to a given performance falls over time - a concept we describe as
increasing compute efficiency. We find that while an access effect increases
the number of actors who can train models to a given performance over time, a
performance effect simultaneously increases the performance available to each
actor. This potentially enables large compute investors to pioneer new
capabilities, maintaining a performance advantage even as capabilities diffuse.
Since large compute investors tend to develop new capabilities first, it will
be particularly important that they share information about their AI models,
evaluate them for emerging risks, and, more generally, make responsible
development and release decisions. Further, as compute efficiency increases,
governments will need to prepare for a world where dangerous AI capabilities
are widely available - for instance, by developing defenses against harmful AI
models or by actively intervening in the diffusion of particularly dangerous
capabilities.

This research paper introduces an innovative AI coaching approach by
integrating vision-encoder-decoder models. The feasibility of this method is
demonstrated using a Vision Transformer as the encoder and GPT-2 as the
decoder, achieving a seamless integration of visual input and textual
interaction. Departing from conventional practices of employing distinct models
for image recognition and text-based coaching, our integrated architecture
directly processes input images, enabling natural question-and-answer dialogues
with the AI coach. This unique strategy simplifies model architecture while
enhancing the overall user experience in human-AI interactions. We showcase
sample results to demonstrate the capability of the model. The results
underscore the methodology's potential as a promising paradigm for creating
efficient AI coach models in various domains involving visual inputs.
Importantly, this potential holds true regardless of the particular visual
encoder or text decoder chosen. Additionally, we conducted experiments with
different sizes of GPT-2 to assess the impact on AI coach performance,
providing valuable insights into the scalability and versatility of our
proposed methodology.

Generative AI systems such as ChatGPT have a disruptive effect on learning
and assessment. Computer science requires practice to develop skills in problem
solving and programming that are traditionally developed using assignments.
Generative AI has the capability of completing these assignments for students
with high accuracy, which dramatically increases the potential for academic
integrity issues and students not achieving desired learning outcomes. This
work investigates the performance of ChatGPT by evaluating it across three
courses (CS1,CS2,databases). ChatGPT completes almost all introductory
assessments perfectly. Existing detection methods, such as MOSS and JPlag
(based on similarity metrics) and GPTzero (AI detection), have mixed success in
identifying AI solutions. Evaluating instructors and teaching assistants using
heuristics to distinguish between student and AI code shows that their
detection is not sufficiently accurate. These observations emphasize the need
for adapting assessments and improved detection methods.

The influence of artificial intelligence (AI) within the field of nuclear
medicine has been rapidly growing. Many researchers and clinicians are seeking
to apply AI within PET, and clinicians will soon find themselves engaging with
AI-based applications all along the chain of molecular imaging, from image
reconstruction to enhanced reporting. This expanding presence of AI in PET
imaging will result in greater demand for educational resources for those
unfamiliar with AI. The objective of this article to is provide an illustrated
guide to the core principles of modern AI, with specific focus on aspects that
are most likely to be encountered in PET imaging. We describe convolutional
neural networks, algorithm training, and explain the components of the commonly
used U-Net for segmentation and image synthesis.

As global challenges of population growth, climate change, and resource
scarcity intensify, the agricultural landscape is at a critical juncture.
Sustainable vertical farming emerges as a transformative solution to address
these challenges by maximizing crop yields in controlled environments. This
paradigm shift necessitates the integration of cutting-edge technologies, with
Artificial Intelligence (AI) at the forefront. The paper provides a
comprehensive exploration of the role of AI in sustainable vertical farming,
investigating its potential, challenges, and opportunities. The review
synthesizes the current state of AI applications, encompassing machine
learning, computer vision, the Internet of Things (IoT), and robotics, in
optimizing resource usage, automating tasks, and enhancing decision-making. It
identifies gaps in research, emphasizing the need for optimized AI models,
interdisciplinary collaboration, and the development of explainable AI in
agriculture. The implications extend beyond efficiency gains, considering
economic viability, reduced environmental impact, and increased food security.
The paper concludes by offering insights for stakeholders and suggesting
avenues for future research, aiming to guide the integration of AI technologies
in sustainable vertical farming for a resilient and sustainable future in
agriculture.

For humanity to maintain and expand its agency into the future, the most
powerful systems we create must be those which act to align the future with the
will of humanity. The most powerful systems today are massive institutions like
governments, firms, and NGOs. Deliberative technology is already being used
across these institutions to help align governance and diplomacy with human
will, and modern AI is poised to make this technology significantly better. At
the same time, the race to superhuman AGI is already underway, and the AI
systems it gives rise to may become the most powerful systems of the future.
Failure to align the impact of such powerful AI with the will of humanity may
lead to catastrophic consequences, while success may unleash abundance. Right
now, there is a window of opportunity to use deliberative technology to align
the impact of powerful AI with the will of humanity. Moreover, it may be
possible to engineer a symbiotic coupling between powerful AI and deliberative
alignment systems such that the quality of alignment improves as AI
capabilities increase.

In this paper we analyze features to classify human- and AI-generated text
for English, French, German and Spanish and compare them across languages. We
investigate two scenarios: (1) The detection of text generated by AI from
scratch, and (2) the detection of text rephrased by AI. For training and
testing the classifiers in this multilingual setting, we created a new text
corpus covering 10 topics for each language. For the detection of AI-generated
text, the combination of all proposed features performs best, indicating that
our features are portable to other related languages: The F1-scores are close
with 99% for Spanish, 98% for English, 97% for German and 95% for French. For
the detection of AI-rephrased text, the systems with all features outperform
systems with other features in many cases, but using only document features
performs best for German (72%) and Spanish (86%) and only text vector features
leads to best results for English (78%).

The use of generative AI in education is a controversial topic. Current
technology offers the potential to create educational content from text,
speech, to images based on simple input prompts. This can enhance productivity
by summarizing knowledge and improving communication, quickly adjusting to
different types of learners. Moreover, generative AI holds the promise of
making the learning itself more fun, by responding to user inputs and
dynamically generating high-quality creative material. In this paper we present
the multisensory educational game ArchiGuesser that combines various AI
technologies from large language models, image generation, to computer vision
to serve a single purpose: Teaching students in a playful way the diversity of
our architectural history and how generative AI works.

Physical reasoning is a crucial aspect in the development of general AI
systems, given that human learning starts with interacting with the physical
world before progressing to more complex concepts. Although researchers have
studied and assessed the physical reasoning of AI approaches through various
specific benchmarks, there is no comprehensive approach to evaluating and
measuring progress. Therefore, we aim to offer an overview of existing
benchmarks and their solution approaches and propose a unified perspective for
measuring the physical reasoning capacity of AI systems. We select benchmarks
that are designed to test algorithmic performance in physical reasoning tasks.
While each of the selected benchmarks poses a unique challenge, their ensemble
provides a comprehensive proving ground for an AI generalist agent with a
measurable skill level for various physical reasoning concepts. This gives an
advantage to such an ensemble of benchmarks over other holistic benchmarks that
aim to simulate the real world by intertwining its complexity and many
concepts. We group the presented set of physical reasoning benchmarks into
subcategories so that more narrow generalist AI agents can be tested first on
these groups.

As open-source AI software projects become an integral component in the AI
software development, it is critical to develop a novel methods to ensure and
measure the security of the open-source projects for developers. Code
ownership, pivotal in the evolution of such projects, offers insights into
developer engagement and potential vulnerabilities. In this paper, we leverage
the code ownership metrics to empirically investigate the correlation with the
latent vulnerabilities across five prominent open-source AI software projects.
The findings from the large-scale empirical study suggest a positive
relationship between high-level ownership (characterised by a limited number of
minor contributors) and a decrease in vulnerabilities. Furthermore, we
innovatively introduce the time metrics, anchored on the project's duration,
individual source code file timelines, and the count of impacted releases.
These metrics adeptly categorise distinct phases of open-source AI software
projects and their respective vulnerability intensities. With these novel code
ownership metrics, we have implemented a Python-based command-line application
to aid project curators and quality assurance professionals in evaluating and
benchmarking their on-site projects. We anticipate this work will embark a
continuous research development for securing and measuring open-source AI
project security.

Several reports in education have called for transforming physics learning
environments by promoting sensemaking of real-world scenarios in light of
curricular ideas. Recent advancements in Generative-Artificial Intelligence has
garnered increasing traction in educators' community by virtue of its potential
in transforming STEM learning. In this exploratory study, we adopt a
mixed-methods approach in comparatively examining student- and AI-generated
responses to two different formats of a physics problem through the cognitive
lenses of sensemaking and mechanistic reasoning. The student data is derived
from think-aloud interviews of introductory students and the AI data comes from
ChatGPT's solutions collected using Zero shot approach. The results highlight
AI responses to evidence most features of the two processes through
well-structured solutions and student responses to effectively leverage
representations in their solutions through iterative refinement of arguments.
In other words, while AI responses reflect how physics is talked about, the
student responses reflect how physics is practiced. Implications of these
results in light of development and deployment of AI systems in physics
pedagogy are discussed.

Technological progress has persistently shaped the dynamics of human-machine
interactions in task execution. In response to the advancements in Generative
AI, this paper outlines a detailed study plan that investigates various
human-AI interaction modalities across a range of tasks, characterized by
differing levels of creativity and complexity. This exploration aims to inform
and contribute to the development of Graphical User Interfaces (GUIs) that
effectively integrate with and enhance the capabilities of Generative AI
systems. The study comprises three parts: exploring fixed-scope tasks through
news headline generation, delving into atomic creative tasks with analogy
generation, and investigating complex tasks via data visualization. Future work
aims to extend this exploration to linearize complex data analysis results into
narratives understandable to a broader audience, thereby enhancing the
interpretability of AI-generated content.

For generative AI to succeed, how engaging a conversationalist must it be?
For almost sixty years, some conversational agents have responded to any
question or comment to keep a conversation going. In recent years, several
utilized machine learning or sophisticated language processing, such as Tay,
Xiaoice, Zo, Hugging Face, Kuki, and Replika. Unlike generative AI, they
focused on engagement, not expertise. Millions of people were motivated to
engage with them. What were the attractions? Will generative AI do better if it
is equally engaging, or should it be less engaging? Prior to the emergence of
generative AI, we conducted a large-scale quantitative and qualitative analysis
to learn what motivated millions of people to engage with one such 'virtual
companion,' Microsoft's Zo. We examined the complete chat logs of 2000
anonymized people. We identified over a dozen motivations that people had for
interacting with this software. Designers learned different ways to increase
engagement. Generative conversational AI does not yet have a clear revenue
model to address its high cost. It might benefit from being more engaging, even
as it supports productivity and creativity. Our study and analysis point to
opportunities and challenges.

The rise of generative AI has led many companies to hire freelancers to
harness its potential. However, this technology presents unique challenges to
developers who have not previously engaged with it. Freelancers may find these
challenges daunting due to the absence of organizational support and their
reliance on positive client feedback. In a study involving 52 freelance
developers, we identified multiple challenges associated with developing
solutions based on generative AI. Freelancers often struggle with aspects they
perceive as unique to generative AI such as unpredictability of its output, the
occurrence of hallucinations, and the inconsistent effort required due to
trial-and-error prompting cycles. Further, the limitations of specific
frameworks, such as token limits and long response times, add to the
complexity. Hype-related issues, such as inflated client expectations and a
rapidly evolving technological ecosystem, further exacerbate the difficulties.
To address these issues, we propose Software Engineering for Generative AI
(SE4GenAI) and Hype-Induced Software Engineering (HypeSE) as areas where the
software engineering community can provide effective guidance. This support is
essential for freelancers working with generative AI and other emerging
technologies.

Since the launch of applications such as DALL-E, Midjourney, and Stable
Diffusion, generative artificial intelligence has been controversial as a tool
for creating artwork. While some have presented longtermist worries about these
technologies as harbingers of fully automated futures to come, more pressing is
the impact of generative AI on creative labour in the present. Already,
business leaders have begun replacing human artistic labour with AI-generated
images. In response, the artistic community has launched a protest movement,
which argues that AI image generation is a kind of theft. This paper analyzes,
substantiates, and critiques these arguments, concluding that AI image
generators involve an unethical kind of labour theft. If correct, many other AI
applications also rely upon theft.

Creative design is a nonlinear process where designers generate diverse ideas
in the pursuit of an open-ended goal and converge towards consensus through
iterative remixing. In contrast, AI-powered design tools often employ a linear
sequence of incremental and precise instructions to approximate design
objectives. Such operations violate customary creative design practices and
thus hinder AI agents' ability to complete creative design tasks. To explore
better human-AI co-design tools, we first summarize human designers' practices
through a formative study with 12 design experts. Taking graphic design as a
representative scenario, we formulate a nonlinear human-AI co-design framework
and develop a proof-of-concept prototype, OptiMuse. We evaluate OptiMuse and
validate the nonlinear framework through a comparative study. We notice a
subconscious change in people's attitudes towards AI agents, shifting from
perceiving them as mere executors to regarding them as opinionated colleagues.
This shift effectively fostered the exploration and reflection processes of
individual designers.

Artificial Intelligence is a field that lives many lives, and the term has
come to encompass a motley collection of scientific and commercial endeavours.
In this paper, I articulate the contours of a rather neglected but central
scientific role that AI has to play, which I dub `AI-as-exploration'.The basic
thrust of AI-as-exploration is that of creating and studying systems that can
reveal candidate building blocks of intelligence that may differ from the forms
of human and animal intelligence we are familiar with. In other words, I
suggest that AI is one of the best tools we have for exploring intelligence
space, namely the space of possible intelligent systems. I illustrate the value
of AI-as-exploration by focusing on a specific case study, i.e., recent work on
the capacity to combine novel and invented concepts in humans and Large
Language Models. I show that the latter, despite showing human-level accuracy
in such a task, most probably solve it in ways radically different, but no less
relevant to intelligence research, to those hypothesised for humans.

We discuss the implications of generative AI on education across four
critical sections: the historical development of AI in education, its
contemporary applications in learning, societal repercussions, and strategic
recommendations for researchers. We propose ways in which generative AI can
transform the educational landscape, primarily via its ability to conduct
assessment of complex cognitive performances and create personalized content.
We also address the challenges of effective educational tool deployment, data
bias, design transparency, and accurate output verification. Acknowledging the
societal impact, we emphasize the need for updating curricula, redefining
communicative trust, and adjusting to transformed social norms. We end by
outlining the ways in which educational stakeholders can actively engage with
generative AI, develop fluency with its capacities and limitations, and apply
these insights to steer educational practices in a rapidly advancing digital
landscape.

Integrated Development Environments (IDEs) have become central to modern
software development, especially with the integration of Artificial
Intelligence (AI) to enhance programming efficiency and decision-making. The
study of in-IDE Human-AI Experience is critical in understanding how these AI
tools are transforming the software development process, impacting programmer
productivity, and influencing code quality. We conducted a literature review to
study the current state of in-IDE Human-AI Experience research, bridging a gap
in understanding the nuanced interactions between programmers and AI assistants
within IDEs. By analyzing 36 selected papers, our study illustrates three
primary research branches: Design, Impact, and Quality of Interaction. The
trends, challenges, and opportunities identified in this paper emphasize the
evolving landscape of software development and inform future directions for
research and development in this dynamic field. Specifically, we invite the
community to investigate three aspects of these interactions: designing
task-specific user interface, building trust, and improving readability.

We find that large language models (LLMs) are more likely to modify
human-written text than AI-generated text when tasked with rewriting. This
tendency arises because LLMs often perceive AI-generated text as high-quality,
leading to fewer modifications. We introduce a method to detect AI-generated
content by prompting LLMs to rewrite text and calculating the editing distance
of the output. We dubbed our geneRative AI Detection viA Rewriting method
Raidar. Raidar significantly improves the F1 detection scores of existing AI
content detection models -- both academic and commercial -- across various
domains, including News, creative writing, student essays, code, Yelp reviews,
and arXiv papers, with gains of up to 29 points. Operating solely on word
symbols without high-dimensional features, our method is compatible with black
box LLMs, and is inherently robust on new content. Our results illustrate the
unique imprint of machine-generated text through the lens of the machines
themselves.

Recent advancements in machine learning and natural language processing have
led to the rapid development of artificial intelligence (AI) as a valuable tool
in the healthcare industry. Using large language models (LLMs) as
conversational agents or chatbots has the potential to assist doctors in
diagnosing patients, detecting early symptoms of diseases, and providing health
advice to patients. This paper focuses on the role of chatbots in healthcare
and explores the use of avatars to make AI interactions more appealing to
patients. A framework of a general-purpose AI avatar application is
demonstrated by using a three-category prompt dictionary and prompt improvement
mechanism. A two-phase approach is suggested to fine-tune a general-purpose AI
language model and create different AI avatars to discuss medical issues with
users. Prompt engineering enhances the chatbot's conversational abilities and
personality traits, fostering a more human-like interaction with patients.
Ultimately, the injection of personality into the chatbot could potentially
increase patient engagement. Future directions for research include
investigating ways to improve chatbots' understanding of context and ensuring
the accuracy of their outputs through fine-tuning with specialized medical data
sets.

The growing application of artificial intelligence (AI) in the field of
information retrieval (IR) affects different domains, including cultural
heritage. By facilitating organisation and retrieval of large volumes of
heritage-related content, AI-driven IR systems inform users about a broad range
of historical phenomena, including genocides (e.g. the Holocaust). However, it
is currently unclear to what degree IR systems are capable of dealing with
multiple ethical challenges associated with the curation of genocide-related
information. To address this question, this chapter provides an overview of
ethical challenges associated with the human curation of genocide-related
information using a three-part framework inspired by Belmont criteria (i.e.
curation challenges associated with respect for individuals, beneficence and
justice/fairness). Then, the chapter discusses to what degree the
above-mentioned challenges are applicable to the ways in which AI-driven IR
systems deal with genocide-related information and what can be the potential
ways of bridging AI and memory ethics in this context.

Interactive storytelling is vital for preschooler development. While
children's interactive partners have traditionally been their parents and
teachers, recent advances in artificial intelligence (AI) have sparked a surge
of AI-based storytelling technologies. As these technologies become
increasingly ubiquitous in preschoolers' lives, questions arise regarding how
they function in practical storytelling scenarios and, in particular, how
parents, the most critical stakeholders, experience and perceive these
technologies. This paper investigates these questions through a qualitative
study with 17 parents of children aged 3-6. Our findings suggest that even
though AI-based storytelling technologies provide more immersive and engaging
interaction, they still cannot meet parents' expectations due to a series of
interactive, functional, and algorithmic challenges. We elaborate on these
challenges and discuss the possible implications of future AI-based
storytelling technologies for preschoolers. We conclude by highlighting the
design implications for future AI-based storytelling technologies.

Researchers, government bodies, and organizations have been repeatedly
calling for a shift in the responsible AI community from general principles to
tangible and operationalizable practices in mitigating the potential
sociotechnical harms of AI. Frameworks like the NIST AI RMF embody an emerging
consensus on recommended practices in operationalizing sociotechnical harm
mitigation. However, private sector organizations currently lag far behind this
emerging consensus. Implementation is sporadic and selective at best. At worst,
it is ineffective and can risk serving as a misleading veneer of trustworthy
processes, providing an appearance of legitimacy to substantively harmful
practices. In this paper, we provide a foundation for a framework for
evaluating where organizations sit relative to the emerging consensus on
sociotechnical harm mitigation best practices: a flexible maturity model based
on the NIST AI RMF.

Recent progress in artificial intelligence (AI) has drawn attention to the
technology's transformative potential, including what some see as its prospects
for causing large-scale harm. We review two influential arguments purporting to
show how AI could pose catastrophic risks. The first argument -- the Problem of
Power-Seeking -- claims that, under certain assumptions, advanced AI systems
are likely to engage in dangerous power-seeking behavior in pursuit of their
goals. We review reasons for thinking that AI systems might seek power, that
they might obtain it, that this could lead to catastrophe, and that we might
build and deploy such systems anyway. The second argument claims that the
development of human-level AI will unlock rapid further progress, culminating
in AI systems far more capable than any human -- this is the Singularity
Hypothesis. Power-seeking behavior on the part of such systems might be
particularly dangerous. We discuss a variety of objections to both arguments
and conclude by assessing the state of the debate.

This position paper presents a theoretical framework for enhancing
explainable artificial intelligence (xAI) through emergent communication
(EmCom), focusing on creating a causal understanding of AI model outputs. We
explore the novel integration of EmCom into AI systems, offering a paradigm
shift from conventional associative relationships between inputs and outputs to
a more nuanced, causal interpretation. The framework aims to revolutionize how
AI processes are understood, making them more transparent and interpretable.
While the initial application of this model is demonstrated on synthetic data,
the implications of this research extend beyond these simple applications. This
general approach has the potential to redefine interactions with AI across
multiple domains, fostering trust and informed decision-making in healthcare
and in various sectors where AI's decision-making processes are critical. The
paper discusses the theoretical underpinnings of this approach, its potential
broad applications, and its alignment with the growing need for responsible and
transparent AI systems in an increasingly digital world.

While generative AI is now widespread and useful in society, there are
potential risks of misuse, e.g., unconsciously influencing cognitive processes
or decision-making. Although this causes a security problem in the cognitive
domain, there has been no research about neural and computational mechanisms
counteracting the impact of malicious generative AI in humans. We propose
DecNefGAN, a novel framework that combines a generative adversarial system and
a neural reinforcement model. More specifically, DecNefGAN bridges human and
generative AI in a closed-loop system, with the AI creating stimuli that induce
specific mental states, thus exerting external control over neural activity.
The objective of the human is the opposite, to compete and reach an orthogonal
mental state. This framework can contribute to elucidating how the human brain
responds to and counteracts the potential influence of generative AI.

As the societal implications of Artificial Intelligence (AI) continue to
grow, the pursuit of responsible AI necessitates public engagement in its
development and governance processes. This involvement is crucial for capturing
diverse perspectives and promoting equitable practices and outcomes. We applied
Cultural Consensus Theory (CCT) to a nationally representative survey dataset
on various aspects of AI to discern beliefs and attitudes about responsible AI
in the United States. Our results offer valuable insights by identifying shared
and contrasting views on responsible AI. Furthermore, these findings serve as
critical reference points for developers and policymakers, enabling them to
more effectively consider individual variances and group-level cultural
perspectives when making significant decisions and addressing the public's
concerns.

We explore the AI2050 "hard problems" that block the promise of AI and cause
AI risks: (1) developing general capabilities of the systems; (2) assuring the
performance of AI systems and their training processes; (3) aligning system
goals with human goals; (4) enabling great applications of AI in real life; (5)
addressing economic disruptions; (6) ensuring the participation of all; (7) at
the same time ensuring socially responsible deployment; (8) addressing any
geopolitical disruptions that AI causes; (9) promoting sound governance of the
technology; and (10) managing the philosophical disruptions for humans living
in the age of AI. For each problem, we outline the area, identify significant
recent work, and suggest ways forward. [Note: this paper reviews literature
through January 2023.]

Utilitarian games such as dictator games to measure fairness have been
studied in the social sciences for decades. These games have given us insight
into not only how humans view fairness but also in what conditions the
frequency of fairness, altruism and greed increase or decrease. While these
games have traditionally been focused on humans, the rise of AI gives us the
ability to study how these models play these games. AI is becoming a constant
in human interaction and examining how these models portray fairness in game
play can give us some insight into how AI makes decisions. Over 101 rounds of
the dictator game, I conclude that AI has a strong sense of fairness that is
dependant of it it deems the person it is playing with as trustworthy, framing
has a strong effect on how much AI gives a recipient when designated the
trustee, and there may be evidence that AI experiences inequality aversion just
as humans.

Generative AI, exemplified by models like transformers, has opened up new
possibilities in various domains but also raised concerns about fairness,
transparency and reliability, especially in fields like medicine and law. This
paper emphasizes the urgency of ensuring fairness and quality in these domains
through generative AI. It explores using cryptographic techniques, particularly
Zero-Knowledge Proofs (ZKPs), to address concerns regarding performance
fairness and accuracy while protecting model privacy. Applying ZKPs to Machine
Learning models, known as ZKML (Zero-Knowledge Machine Learning), enables
independent validation of AI-generated content without revealing sensitive
model information, promoting transparency and trust. ZKML enhances AI fairness
by providing cryptographic audit trails for model predictions and ensuring
uniform performance across users. We introduce snarkGPT, a practical ZKML
implementation for transformers, to empower users to verify output accuracy and
quality while preserving model privacy. We present a series of empirical
results studying snarkGPT's scalability and performance to assess the
feasibility and challenges of adopting a ZKML-powered approach to capture
quality and performance fairness problems in generative AI models.

The substantial increase in AI model training has considerable environmental
implications, mandating more energy-efficient and sustainable AI practices. On
the one hand, data-centric approaches show great potential towards training
energy-efficient AI models. On the other hand, instance selection methods
demonstrate the capability of training AI models with minimised training sets
and negligible performance degradation. Despite the growing interest in both
topics, the impact of data-centric training set selection on energy efficiency
remains to date unexplored. This paper presents an evolutionary-based sampling
framework aimed at (i) identifying elite training samples tailored for datasets
and model pairs, (ii) comparing model performance and energy efficiency gains
against typical model training practice, and (iii) investigating the
feasibility of this framework for fostering sustainable model training
practices. To evaluate the proposed framework, we conducted an empirical
experiment including 8 commonly used AI classification models and 25 publicly
available datasets. The results showcase that by considering 10% elite training
samples, the models' performance can show a 50% improvement and remarkable
energy savings of 98% compared to the common training practice.

Choreography creation is a multimodal endeavor, demanding cognitive abilities
to develop creative ideas and technical expertise to convert choreographic
ideas into physical dance movements. Previous endeavors have sought to reduce
the complexities in the choreography creation process in both dimensions. Among
them, non-AI-based systems have focused on reinforcing cognitive activities by
helping analyze and understand dance movements and augmenting physical
capabilities by enhancing body expressivity. On the other hand, AI-based
methods have helped the creation of novel choreographic materials with
generative AI algorithms. The choreography creation process is constrained by
time and requires a rich set of resources to stimulate novel ideas, but the
need for iterative prototyping and reduced physical dependence have not been
adequately addressed by prior research. Recognizing these challenges and the
research gap, we present an innovative AI-based choreography-support system.
Our goal is to facilitate rapid ideation by utilizing a generative AI model
that can produce diverse and novel dance sequences. The system is designed to
support iterative digital dance prototyping through an interactive web-based
user interface that enables the editing and modification of generated motion.
We evaluated our system by inviting six choreographers to analyze its
limitations and benefits and present the evaluation results along with
potential directions for future work.

Generative AI platforms and features are permeating many aspects of work.
Entrepreneurs from lean economies in particular are well positioned to
outsource tasks to generative AI given limited resources. In this paper, we
work to address a growing disparity in use of these technologies by building on
a four-year partnership with a local entrepreneurial hub dedicated to equity in
tech and entrepreneurship. Together, we co-designed an interactive workshops
series aimed to onboard local entrepreneurs to generative AI platforms.
Alongside four community-driven and iterative workshops with entrepreneurs
across five months, we conducted interviews with 15 local entrepreneurs and
community providers. We detail the importance of communal and supportive
exposure to generative AI tools for local entrepreneurs, scaffolding actionable
use (and supporting non-use), demystifying generative AI technologies by
emphasizing entrepreneurial power, while simultaneously deconstructing the
veneer of simplicity to address the many operational skills needed for
successful application.

Although recent developments in generative AI have greatly enhanced the
capabilities of conversational agents such as Google's Gemini (formerly Bard)
or OpenAI's ChatGPT, it's unclear whether the usage of these agents aids users
across various contexts. To better understand how access to conversational AI
affects productivity and trust, we conducted a mixed-methods, task-based user
study, observing 76 software engineers (N=76) as they completed a programming
exam with and without access to Bard. Effects on performance, efficiency,
satisfaction, and trust vary depending on user expertise, question type
(open-ended "solve" vs. definitive "search" questions), and measurement type
(demonstrated vs. self-reported). Our findings include evidence of automation
complacency, increased reliance on the AI over the course of the task, and
increased performance for novices on "solve"-type questions when using the AI.
We discuss common behaviors, design recommendations, and impact considerations
to improve collaborations with conversational AI.

Counterspeech, i.e., direct responses against hate speech, has become an
important tool to address the increasing amount of hate online while avoiding
censorship. Although AI has been proposed to help scale up counterspeech
efforts, this raises questions of how exactly AI could assist in this process,
since counterspeech is a deeply empathetic and agentic process for those
involved. In this work, we aim to answer this question, by conducting in-depth
interviews with 10 extensively experienced counterspeakers and a large scale
public survey with 342 everyday social media users. In participant responses,
we identified four main types of barriers and AI needs related to resources,
training, impact, and personal harms. However, our results also revealed
overarching concerns of authenticity, agency, and functionality in using AI
tools for counterspeech. To conclude, we discuss considerations for designing
AI assistants that lower counterspeaking barriers without jeopardizing its
meaning and purpose.

Despite the importance of trust in human-AI interactions, researchers must
adopt questionnaires from other disciplines that lack validation in the AI
context. Motivated by the need for reliable and valid measures, we investigated
the psychometric quality of two trust questionnaires, the Trust between People
and Automation scale (TPA) by Jian et al. (2000) and the Trust Scale for the AI
Context (TAI) by Hoffman et al. (2023). In a pre-registered online experiment
(N = 1485), participants observed interactions with trustworthy and
untrustworthy AI (autonomous vehicle and chatbot). Results support the
psychometric quality of the TAI while revealing opportunities to improve the
TPA, which we outline in our recommendations for using the two questionnaires.
Furthermore, our findings provide additional empirical evidence of trust and
distrust as two distinct constructs that may coexist independently. Building on
our findings, we highlight the opportunities and added value of measuring both
trust and distrust in human-AI research and advocate for further work on both
constructs.

Recent developments in prompt-based generative AI has given rise to discourse
surrounding the perceived ethical concerns, economic implications, and
consequences for the future of cultural production. As generative imagery
becomes pervasive in mainstream society, dominated primarily by emerging
industry leaders, we encourage that the role of the CHI community be one of
inquiry; to investigate the numerous ways in which generative AI has the
potential to, and already is, augmenting human creativity. In this paper, we
conducted a diffractive analysis exploring the potential role of prompt-based
interfaces in artists' creative practice. Over a two week period, seven visual
artists were given access to a personalised instance of Stable Diffusion,
fine-tuned on a dataset of their work. In the following diffractive analysis,
we identified two dominant modes adopted by participants, AI for ideation, and
AI for production. We furthermore present a number of ethical design
considerations for the future development of generative AI interfaces.

Some crucial decisions in AI design tend to be overlooked or factor choices
are assumed implicitly. The question often answered first is what the AI will
do, not how it will interact with the rest of the world. This reduces our
understanding of the possible types of AI that can be developed and their
potential impacts on humanity. As an initial AI taxonomy, I present binary
choices for 10 of the subjectively most separable and influential high-level
design factors, then give brief examples of several of the 1024 possible
systems defined by those choices. This supports a simple binary stream approach
to system designation based on translating the stream of choices into decimal
notation, giving a short-hand way of referring to systems with different
properties that meet specialized needs. Further, underspecified or generic
systems can be designated using the binary stream approach as well, a
notational feature that supports modeling the impacts of AI systems with
selected characteristics.

Deviating from conventional perspectives that frame artificial intelligence
(AI) systems solely as logic emulators, we propose a novel program of heuristic
reasoning. We distinguish between the 'instrumental' use of heuristics to match
resources with objectives, and 'mimetic absorption,' whereby heuristics
manifest randomly and universally. Through a series of innovative experiments,
including variations of the classic Linda problem and a novel application of
the Beauty Contest game, we uncover trade-offs between maximizing accuracy and
reducing effort that shape the conditions under which AIs transition between
exhaustive logical processing and the use of cognitive shortcuts (heuristics).
We provide evidence that AIs manifest an adaptive balancing of precision and
efficiency, consistent with principles of resource-rational human cognition as
explicated in classical theories of bounded rationality and dual-process
theory. Our findings reveal a nuanced picture of AI cognition, where trade-offs
between resources and objectives lead to the emulation of biological systems,
especially human cognition, despite AIs being designed without a sense of self
and lacking introspective capabilities.

Many studies have identified particular features of artificial intelligences
(AI), such as their autonomy and emotion expression, that affect the extent to
which they are treated as subjects of moral consideration. However, there has
not yet been a comparison of the relative importance of features as is
necessary to design and understand increasingly capable, multi-faceted AI
systems. We conducted an online conjoint experiment in which 1,163 participants
evaluated descriptions of AIs that varied on these features. All 11 features
increased how morally wrong participants considered it to harm the AIs. The
largest effects were from human-like physical bodies and prosociality (i.e.,
emotion expression, emotion recognition, cooperation, and moral judgment). For
human-computer interaction designers, the importance of prosociality suggests
that, because AIs are often seen as threatening, the highest levels of moral
consideration may only be granted if the AI has positive intentions.

This research critically navigates the intricate landscape of AI deception,
concentrating on deceptive behaviours of Large Language Models (LLMs). My
objective is to elucidate this issue, examine the discourse surrounding it, and
subsequently delve into its categorization and ramifications. The essay
initiates with an evaluation of the AI Safety Summit 2023 (ASS) and
introduction of LLMs, emphasising multidimensional biases that underlie their
deceptive behaviours.The literature review covers four types of deception
categorised: Strategic deception, Imitation, Sycophancy, and Unfaithful
Reasoning, along with the social implications and risks they entail. Lastly, I
take an evaluative stance on various aspects related to navigating the
persistent challenges of the deceptive AI. This encompasses considerations of
international collaborative governance, the reconfigured engagement of
individuals with AI, proposal of practical adjustments, and specific elements
of digital education.

As AI systems become more advanced, companies and regulators will make
difficult decisions about whether it is safe to train and deploy them. To
prepare for these decisions, we investigate how developers could make a 'safety
case,' which is a structured rationale that AI systems are unlikely to cause a
catastrophe. We propose a framework for organizing a safety case and discuss
four categories of arguments to justify safety: total inability to cause a
catastrophe, sufficiently strong control measures, trustworthiness despite
capability to cause harm, and -- if AI systems become much more powerful --
deference to credible AI advisors. We evaluate concrete examples of arguments
in each category and outline how arguments could be combined to justify that AI
systems are safe to deploy.

Artificial intelligence (AI), although not able to currently capture the many
complexities of humans, are slowly adapting to have certain capabilities of
humans, many of which can revolutionize our world. AI systems, such as ChatGPT
and others utilized within various industries for specific processes, have been
transforming rapidly. However, this transformation can occur in an extremely
concerning way if certain measures are not taken. This article touches on some
of the current issues within the artificial intelligence ethical crisis, such
as the concerns of discrimination within AI and false information that is
becoming readily available with AI. Within this article, plausible solutions
involving regulation are discussed and how they would mitigate ethical
concerns. These include the self-regulation of businesses along with government
regulation, and the effects these possible solutions can both have on current
AI concerns.

Recent advances in interactive large language models like ChatGPT have
revolutionized various domains; however, their behavior in natural and
role-play conversation settings remains underexplored. In our study, we address
this gap by deeply investigating how ChatGPT behaves during conversations in
different settings by analyzing its interactions in both a normal way and a
role-play setting. We introduce a novel dataset of broad range of human-AI
conversations annotated with user motives and model naturalness to examine (i)
how humans engage with the conversational AI model, and (ii) how natural are AI
model responses. Our study highlights the diversity of user motives when
interacting with ChatGPT and variable AI naturalness, showing not only the
nuanced dynamics of natural conversations between humans and AI, but also
providing new avenues for improving the effectiveness of human-AI
communication.

We envision 'AI scientists' as systems capable of skeptical learning and
reasoning that empower biomedical research through collaborative agents that
integrate machine learning tools with experimental platforms. Rather than
taking humans out of the discovery process, biomedical AI agents combine human
creativity and expertise with AI's ability to analyze large datasets, navigate
hypothesis spaces, and execute repetitive tasks. AI agents are proficient in a
variety of tasks, including self-assessment and planning of discovery
workflows. These agents use large language models and generative models to
feature structured memory for continual learning and use machine learning tools
to incorporate scientific knowledge, biological principles, and theories. AI
agents can impact areas ranging from hybrid cell simulation, programmable
control of phenotypes, and the design of cellular circuits to the development
of new therapies.

Globally, artificial intelligence (AI) implementation is growing, holding the
capability to fundamentally alter organisational processes and decision making.
Simultaneously, this brings a multitude of emergent risks to organisations,
exposing vulnerabilities in their extant risk management frameworks. This
necessitates a greater understanding of how organisations can position
themselves in response. This issue is particularly pertinent within the
financial sector with relatively mature AI applications matched with severe
societal repercussions of potential risk events. Despite this, academic risk
management literature is trailing behind the speed of AI implementation.
Adopting a management perspective, this study aims to contribute to the
understanding of AI risk management in organisations through an exploratory
empirical investigation into these practices. In-depth insights are gained
through interviews with nine practitioners from different organisations within
the UK financial sector. Through examining areas of organisational convergence
and divergence, the findings of this study unearth levels of risk management
framework readiness and prevailing approaches to risk management at both a
processual and organisational level. Whilst enhancing the developing literature
concerning AI risk management within organisations, the study simultaneously
offers a practical contribution, providing key areas of guidance for
practitioners in the operational development of AI risk management frameworks.

In the rapidly evolving landscape of generative artificial intelligence (AI),
the increasingly pertinent issue of copyright infringement arises as AI
advances to generate content from scraped copyrighted data, prompting questions
about ownership and protection that impact professionals across various
careers. With this in mind, this survey provides an extensive examination of
copyright infringement as it pertains to generative AI, aiming to stay abreast
of the latest developments and open problems. Specifically, it will first
outline methods of detecting copyright infringement in mediums such as text,
image, and video. Next, it will delve an exploration of existing techniques
aimed at safeguarding copyrighted works from generative models. Furthermore,
this survey will discuss resources and tools for users to evaluate copyright
violations. Finally, insights into ongoing regulations and proposals for AI
will be explored and compared. Through combining these disciplines, the
implications of AI-driven content and copyright are thoroughly illustrated and
brought into question.

Recent advancements in Artificial Intelligence (AI) and machine learning have
demonstrated transformative capabilities across diverse domains. This progress
extends to the field of patent analysis and innovation, where AI-based tools
present opportunities to streamline and enhance important tasks in the patent
cycle such as classification, retrieval, and valuation prediction. This not
only accelerates the efficiency of patent researchers and applicants but also
opens new avenues for technological innovation and discovery. Our survey
provides a comprehensive summary of recent AI tools in patent analysis from
more than 40 papers from 26 venues between 2017 and 2023. Unlike existing
surveys, we include methods that work for patent image and text data.
Furthermore, we introduce a novel taxonomy for the categorization based on the
tasks in the patent life cycle as well as the specifics of the AI methods. This
survey aims to serve as a resource for researchers, practitioners, and patent
offices in the domain of AI-powered patent analysis.

Technological innovations have shown remarkable capabilities to benefit and
harm society alike. AI constitutes a democratized sophisticated technology
accessible to large parts of society, including malicious actors. This work
proposes a taxonomy focusing on on (geo)political risks associated with AI. It
identifies 12 risks in total divided into four categories: (1) Geopolitical
Pressures, (2) Malicious Usage, (3) Environmental, Social, and Ethical Risks,
and (4) Privacy and Trust Violations. Incorporating a regulatory side, this
paper conducts a policy assessment of the EU AI Act. Adopted in March 2023, the
landmark regulation has the potential to have a positive top-down impact
concerning AI risk reduction but needs regulatory adjustments to mitigate risks
more comprehensively. Regulatory exceptions for open-source models, excessively
high parameters for the classification of GPAI models as a systemic risk, and
the exclusion of systems designed exclusively for military purposes from the
regulation's obligations leave room for future action.

As AI systems enter into a growing number of societal domains, these systems
increasingly shape and are shaped by user preferences, opinions, and behaviors.
However, the design of AI systems rarely accounts for how AI and users shape
one another. In this position paper, we argue for the development of formal
interaction models which mathematically specify how AI and users shape one
another. Formal interaction models can be leveraged to (1) specify interactions
for implementation, (2) monitor interactions through empirical analysis, (3)
anticipate societal impacts via counterfactual analysis, and (4) control
societal impacts via interventions. The design space of formal interaction
models is vast, and model design requires careful consideration of factors such
as style, granularity, mathematical complexity, and measurability. Using
content recommender systems as a case study, we critically examine the nascent
literature of formal interaction models with respect to these use-cases and
design axes. More broadly, we call for the community to leverage formal
interaction models when designing, evaluating, or auditing any AI system which
interacts with users.

This article introduces Follow-Me AI, a concept designed to enhance user
interactions with smart environments, optimize energy use, and provide better
control over data captured by these environments. Through AI agents that
accompany users, Follow-Me AI negotiates data management based on user consent,
aligns environmental controls as well as user communication and computes
resources available in the environment with user preferences, and predicts user
behavior to proactively adjust the smart environment. The manuscript
illustrates this concept with a detailed example of Follow-Me AI in a smart
campus setting, detailing the interactions with the building's management
system for optimal comfort and efficiency. Finally, this article looks into the
challenges and opportunities related to Follow-Me AI.

Generative artificial intelligence (AI) systems are trained on large data
corpora to generate new pieces of text, images, videos, and other media. There
is growing concern that such systems may infringe on the copyright interests of
training data contributors. To address the copyright challenges of generative
AI, we propose a framework that compensates copyright owners proportionally to
their contributions to the creation of AI-generated content. The metric for
contributions is quantitatively determined by leveraging the probabilistic
nature of modern generative AI models and using techniques from cooperative
game theory in economics. This framework enables a platform where AI developers
benefit from access to high-quality training data, thus improving model
performance. Meanwhile, copyright owners receive fair compensation, driving the
continued provision of relevant data for generative model training. Experiments
demonstrate that our framework successfully identifies the most relevant data
sources used in artwork generation, ensuring a fair and interpretable
distribution of revenues among copyright owners.

The NOSTR is a communication protocol for the social web, based on the w3c
websockets standard. Although it is still in its infancy, it is well known as a
social media protocol, thousands of trusted users and multiple user interfaces,
offering a unique experience and enormous capabilities. To name a few, the
NOSTR applications include but are not limited to direct messaging, file
sharing, audio/video streaming, collaborative writing, blogging and data
processing through distributed AI directories. In this work, we propose an
approach that builds upon the existing protocol structure with end goal a
decentralized marketplace for federated learning and LLM training. In this
proposed design there are two parties: on one side there are customers who
provide a dataset that they want to use for training an AI model. On the other
side, there are service providers, who receive (parts of) the dataset, train
the AI model, and for a payment as an exchange, they return the optimized AI
model. The decentralized and censorship resistant features of the NOSTR enable
the possibility of designing a fair and open marketplace for training AI models
and LLMs.

Generative Artificial Intelligence (generative AI) poses both opportunities
and risks for the integrity of research. Universities must guide researchers in
using generative AI responsibly, and in navigating a complex regulatory
landscape subject to rapid change. By drawing on the experiences of two
Australian universities, we propose a framework to help institutions promote
and facilitate the responsible use of generative AI. We provide guidance to
help distil the diverse regulatory environment into a principles-based position
statement. Further, we explain how a position statement can then serve as a
foundation for initiatives in training, communications, infrastructure, and
process change. Despite the growing body of literature about AI's impact on
academic integrity for undergraduate students, there has been comparatively
little attention on the impacts of generative AI for research integrity, and
the vital role of institutions in helping to address those challenges. This
paper underscores the urgency for research institutions to take action in this
area and suggests a practical and adaptable framework for so doing.

With the emergence of Artificial Intelligence (AI)-based decision-making,
explanations help increase new technology adoption through enhanced trust and
reliability. However, our experimental study challenges the notion that every
user universally values explanations. We argue that the agreement with AI
suggestions, whether accompanied by explanations or not, is influenced by
individual differences in personality traits and the users' comfort with
technology. We found that people with higher neuroticism and lower
technological comfort showed more agreement with the recommendations without
explanations. As more users become exposed to eXplainable AI (XAI) and AI-based
systems, we argue that the XAI design should not provide explanations for users
with high neuroticism and low technology comfort. Prioritizing user
personalities in XAI systems will help users become better collaborators of AI
systems.

Information systems (IS) are frequently designed to leverage the negative
effect of anchoring bias to influence individuals' decision-making (e.g., by
manipulating purchase decisions). Recent advances in Artificial Intelligence
(AI) and the explanations of its decisions through explainable AI (XAI) have
opened new opportunities for mitigating biased decisions. So far, the potential
of these technological advances to overcome anchoring bias remains widely
unclear. To this end, we conducted two online experiments with a total of N=390
participants in the context of purchase decisions to examine the impact of AI
and XAI-based decision support on anchoring bias. Our results show that AI
alone and its combination with XAI help to mitigate the negative effect of
anchoring bias. Ultimately, our findings have implications for the design of AI
and XAI-based decision support and IS to overcome cognitive biases.

The emergence of large language models (LLMs) capable of generating realistic
texts and images has sparked ethical concerns across various sectors. In
response, researchers in academia and industry are actively exploring methods
to distinguish AI-generated content from human-authored material. However, a
crucial question remains: What are the unique characteristics of AI-generated
text? Addressing this gap, this study proposes StyloAI, a data-driven model
that uses 31 stylometric features to identify AI-generated texts by applying a
Random Forest classifier on two multi-domain datasets. StyloAI achieves
accuracy rates of 81% and 98% on the test set of the AuTextification dataset
and the Education dataset, respectively. This approach surpasses the
performance of existing state-of-the-art models and provides valuable insights
into the differences between AI-generated and human-authored texts.

Developmental AI creates embodied AIs that develop human-like abilities. The
AIs start with innate competences and learn more by interacting with the world
including people. Developmental AIs have been demonstrated, but their abilities
so far do not surpass those of pre-toddler children. In contrast, mainstream
approaches have led to impressive feats and commercially valuable AI systems.
The approaches include deep learning and generative AI (e.g., large language
models) and manually constructed symbolic modeling. However, manually
constructed AIs tend to be brittle even in circumscribed domains. Generative
AIs are helpful on average, but they can make strange mistakes and not notice
them. Not learning from their experience in the world, they can lack common
sense and social alignment. This position paper lays out prospects, gaps, and
challenges for a bootstrapping approach to developmental AI that follows a
bio-inspired trajectory. The approach creates experiential foundation models
for human-compatible AIs. A virtuous multidisciplinary research cycle has led
to developmental AIs with capabilities for multimodal perception, object
recognition, and manipulation. Computational models for hierarchical planning,
abstraction discovery, curiosity, and language acquisition exist but need to be
adapted to an embodied learning approach. The remaining gaps include nonverbal
communication, speech, reading, and writing. These competences enable people to
acquire socially developed competences. Aspirationally, developmental AIs would
learn, share what they learn, and collaborate to achieve high standards. They
would learn to communicate, establish common ground, read critically, consider
the provenance of information, test hypotheses, and collaborate. The approach
would make the training of AIs more democratic.

Randomly-connected networks of integrate-and-fire (IF) neurons are known to
display asynchronous irregular (AI) activity states, which resemble the
discharge activity recorded in the cerebral cortex of awake animals. However,
it is not clear whether such activity states are specific to simple IF models,
or if they also exist in networks where neurons are endowed with complex
intrinsic properties similar to electrophysiological measurements. Here, we
investigate the occurrence of AI states in networks of nonlinear IF neurons,
such as the adaptive exponential IF (Brette-Gerstner-Izhikevich) model. This
model can display intrinsic properties such as low-threshold spike (LTS),
regular spiking (RS) or fast-spiking (FS). We successively investigate the
oscillatory and AI dynamics of thalamic, cortical and thalamocortical networks
using such models. AI states can be found in each case, sometimes with
surprisingly small network size of the order of a few tens of neurons. We show
that the presence of LTS neurons in cortex or in thalamus, explains the robust
emergence of AI states for relatively small network sizes. Finally, we
investigate the role of spike-frequency adaptation (SFA). In cortical networks
with strong SFA in RS cells, the AI state is transient, but when SFA is
reduced, AI states can be self-sustained for long times. In thalamocortical
networks, AI states are found when the cortex is itself in an AI state, but
with strong SFA, the thalamocortical network displays Up and Down state
transitions, similar to intracellular recordings during slow-wave sleep or
anesthesia. Self-sustained Up and Down states could also be generated by
two-layer cortical networks with LTS cells. These models suggest that intrinsic
properties such as LTS are crucial for AI states in thalamocortical networks.

Starcraft II (SC2) is widely considered as the most challenging Real Time
Strategy (RTS) game. The underlying challenges include a large observation
space, a huge (continuous and infinite) action space, partial observations,
simultaneous move for all players, and long horizon delayed rewards for local
decisions. To push the frontier of AI research, Deepmind and Blizzard jointly
developed the StarCraft II Learning Environment (SC2LE) as a testbench of
complex decision making systems. SC2LE provides a few mini games such as
MoveToBeacon, CollectMineralShards, and DefeatRoaches, where some AI agents
have achieved the performance level of human professional players. However, for
full games, the current AI agents are still far from achieving human
professional level performance. To bridge this gap, we present two full game AI
agents in this paper - the AI agent TStarBot1 is based on deep reinforcement
learning over a flat action structure, and the AI agent TStarBot2 is based on
hard-coded rules over a hierarchical action structure. Both TStarBot1 and
TStarBot2 are able to defeat the built-in AI agents from level 1 to level 10 in
a full game (1v1 Zerg-vs-Zerg game on the AbyssalReef map), noting that level
8, level 9, and level 10 are cheating agents with unfair advantages such as
full vision on the whole map and resource harvest boosting. To the best of our
knowledge, this is the first public work to investigate AI agents that can
defeat the built-in AI in the StarCraft II full game.

Due to the availability of huge amounts of data and processing abilities,
current artificial intelligence (AI) systems are effective in solving complex
tasks. However, despite the success of AI in different areas, the problem of
designing AI systems that can truly mimic human cognitive capabilities such as
artificial general intelligence, remains largely open. Consequently, many
emerging cross-device AI applications will require a transition from
traditional centralized learning systems towards large-scale distributed AI
systems that can collaboratively perform multiple complex learning tasks. In
this paper, we propose a novel design philosophy called democratized learning
(Dem-AI) whose goal is to build large-scale distributed learning systems that
rely on the self-organization of distributed learning agents that are
well-connected, but limited in learning capabilities. Correspondingly, inspired
by the societal groups of humans, the specialized groups of learning agents in
the proposed Dem-AI system are self-organized in a hierarchical structure to
collectively perform learning tasks more efficiently. As such, the Dem-AI
learning system can evolve and regulate itself based on the underlying duality
of two processes which we call specialized and generalized processes. In this
regard, we present a reference design as a guideline to realize future Dem-AI
systems, inspired by various interdisciplinary fields. Accordingly, we
introduce four underlying mechanisms in the design such as plasticity-stability
transition mechanism, self-organizing hierarchical structuring, specialized
learning, and generalization. Finally, we establish possible extensions and new
challenges for the existing learning approaches to provide better scalable,
flexible, and more powerful learning systems with the new setting of Dem-AI.

Artificial intelligence (AI) has been applied widely in our daily lives in a
variety of ways with numerous success stories. AI has also contributed to
dealing with the coronavirus disease (COVID-19) pandemic, which has been
happening around the globe. This paper presents a survey of AI methods being
used in various applications in the fight against the COVID-19 outbreak and
outlines the crucial role of AI research in this unprecedented battle. We touch
on areas where AI plays as an essential component, from medical image
processing, data analytics, text mining and natural language processing, the
Internet of Things, to computational biology and medicine. A summary of
COVID-19 related data sources that are available for research purposes is also
presented. Research directions on exploring the potential of AI and enhancing
its capability and power in the pandemic battle are thoroughly discussed. We
identify 13 groups of problems related to the COVID-19 pandemic and highlight
promising AI methods and tools that can be used to address these problems. It
is envisaged that this study will provide AI researchers and the wider
community with an overview of the current status of AI applications, and
motivate researchers to harness AI's potential in the fight against COVID-19.

Over the last two years, The Alan Turing Institute and the Information
Commissioner's Office (ICO) have been working together to discover ways to
tackle the difficult issues surrounding explainable AI. The ultimate product of
this joint endeavour, Explaining decisions made with AI, published in May 2020,
is the most comprehensive practical guidance on AI explanation produced
anywhere to date. We have put together this workbook to help support the uptake
of that guidance. The goal of the workbook is to summarise some of main themes
from Explaining decisions made with AI and then to provide the materials for a
workshop exercise that has been built around a use case created to help you
gain a flavour of how to put the guidance into practice. In the first three
sections, we run through the basics of Explaining decisions made with AI. We
provide a precis of the four principles of AI explainability, the typology of
AI explanations, and the tasks involved in the explanation-aware design,
development, and use of AI/ML systems. We then provide some reflection
questions, which are intended to be a launching pad for group discussion, and a
starting point for the case-study-based exercise that we have included as
Appendix B. In Appendix A, we go into more detailed suggestions about how to
organise the workshop. These recommendations are based on two workshops we had
the privilege of co-hosting with our colleagues from the ICO and Manchester
Metropolitan University in January 2021. The participants of these workshops
came from both the private and public sectors, and we are extremely grateful to
them for their energy, enthusiasm, and tremendous insight. This workbook would
simply not exist without the commitment and keenness of all our collaborators
and workshop participants.

We are moving towards a future where Artificial Intelligence (AI) based
agents make many decisions on behalf of humans. From healthcare decision making
to social media censoring, these agents face problems, and make decisions with
ethical and societal implications. Ethical behaviour is a critical
characteristic that we would like in a human-centric AI. A common observation
in human-centric industries, like the service industry and healthcare, is that
their professionals tend to break rules, if necessary, for pro-social reasons.
This behaviour among humans is defined as pro-social rule breaking. To make AI
agents more human centric, we argue that there is a need for a mechanism that
helps AI agents identify when to break rules set by their designers. To
understand when AI agents need to break rules, we examine the conditions under
which humans break rules for pro-social reasons. In this paper, we present a
study that introduces a 'vaccination strategy dilemma' to human participants
and analyses their responses. In this dilemma, one needs to decide whether they
would distribute Covid-19 vaccines only to members of a high-risk group (follow
the enforced rule) or, in selected cases, administer the vaccine to a few
social influencers (break the rule), which might yield an overall greater
benefit to society. The results of the empirical study suggest a relationship
between stakeholder utilities and pro-social rule breaking (PSRB), which
neither deontological nor utilitarian ethics completely explain. Finally, the
paper discusses the design characteristics of an ethical agent capable of PSRB
and the future research directions on PSRB in the AI realm. We hope that this
will inform the design of future AI agents, and their decision-making
behaviour.

Artificial intelligence (AI) has achieved superhuman performance in board
games such as Go, chess, and Othello (Reversi). In other words, the AI system
surpasses the level of a strong human expert player in such games. In this
context, it is difficult for a human player to enjoy playing the games with the
AI. To keep human players entertained and immersed in a game, the AI is
required to dynamically balance its skill with that of the human player. To
address this issue, we propose AlphaDDA, an AlphaZero-based AI with dynamic
difficulty adjustment (DDA). AlphaDDA consists of a deep neural network (DNN)
and a Monte Carlo tree search, as in AlphaZero. AlphaDDA learns and plays a
game the same way as AlphaZero, but can change its skills. AlphaDDA estimates
the value of the game state from only the board state using the DNN. AlphaDDA
changes a parameter dominantly controlling its skills according to the
estimated value. Consequently, AlphaDDA adjusts its skills according to a game
state. AlphaDDA can adjust its skill using only the state of a game without any
prior knowledge regarding an opponent. In this study, AlphaDDA plays Connect4,
Othello, and 6x6 Othello with other AI agents. Other AI agents are AlphaZero,
Monte Carlo tree search, the minimax algorithm, and a random player. This study
shows that AlphaDDA can balance its skill with that of the other AI agents,
except for a random player. The DDA ability of AlphaDDA is based on an accurate
estimation of the value from the state of a game. We believe that the AlphaDDA
approach for DDA can be used for any game AI system if the DNN can accurately
estimate the value of the game state and we know a parameter controlling the
skills of the AI system.

Recent work has shown the potential benefit of selective prediction systems
that can learn to defer to a human when the predictions of the AI are
unreliable, particularly to improve the reliability of AI systems in
high-stakes applications like healthcare or conservation. However, most prior
work assumes that human behavior remains unchanged when they solve a prediction
task as part of a human-AI team as opposed to by themselves. We show that this
is not the case by performing experiments to quantify human-AI interaction in
the context of selective prediction. In particular, we study the impact of
communicating different types of information to humans about the AI system's
decision to defer. Using real-world conservation data and a selective
prediction system that improves expected accuracy over that of the human or AI
system working individually, we show that this messaging has a significant
impact on the accuracy of human judgements. Our results study two components of
the messaging strategy: 1) Whether humans are informed about the prediction of
the AI system and 2) Whether they are informed about the decision of the
selective prediction system to defer. By manipulating these messaging
components, we show that it is possible to significantly boost human
performance by informing the human of the decision to defer, but not revealing
the prediction of the AI. We therefore show that it is vital to consider how
the decision to defer is communicated to a human when designing selective
prediction systems, and that the composite accuracy of a human-AI team must be
carefully evaluated using a human-in-the-loop framework.

Artificial Intelligence & Nanotechnology are promising areas for the future
of humanity. While Deep Learning based Computer Vision has found applications
in many fields from medicine to automotive, its application in nanotechnology
can open doors for new scientific discoveries. Can we apply AI to explore
objects that our eyes can't see such as nano scale sized objects? An AI
platform to visualize nanoscale patterns learnt by a Deep Learning neural
network can open new frontiers for nanotechnology. The objective of this paper
is to develop a Deep Learning based visualization system on images of
nanomaterials obtained by scanning electron microscope. This paper contributes
an AI platform to enable any nanoscience researcher to use AI in visual
exploration of nanoscale morphologies of nanomaterials. This AI is developed by
a technique of visualizing intermediate activations of a Convolutional
AutoEncoder. In this method, a nano scale specimen image is transformed into
its feature representations by a Convolution Neural Network. The Convolutional
AutoEncoder is trained on 100% SEM dataset, and then CNN visualization is
applied. This AI generates various conceptual feature representations of the
nanomaterial.
  While Deep Learning based image classification of SEM images are widely
published in literature, there are not much publications that have visualized
Deep neural networks of nanomaterials. There is a significant opportunity to
gain insights from the learnings extracted by machine learning. This paper
unlocks the potential of applying Deep Learning based Visualization on electron
microscopy to offer AI extracted features and architectural patterns of various
nanomaterials. This is a contribution in Explainable AI in nano scale objects.
This paper contributes an open source AI with reproducible results at URL
(https://sites.google.com/view/aifornanotechnology)

We introduce an ensemble of artificial intelligence models for gravitational
wave detection that we trained in the Summit supercomputer using 32 nodes,
equivalent to 192 NVIDIA V100 GPUs, within 2 hours. Once fully trained, we
optimized these models for accelerated inference using NVIDIA TensorRT. We
deployed our inference-optimized AI ensemble in the ThetaGPU supercomputer at
Argonne Leadership Computer Facility to conduct distributed inference. Using
the entire ThetaGPU supercomputer, consisting of 20 nodes each of which has 8
NVIDIA A100 Tensor Core GPUs and 2 AMD Rome CPUs, our NVIDIA TensorRT-optimized
AI ensemble processed an entire month of advanced LIGO data (including Hanford
and Livingston data streams) within 50 seconds. Our inference-optimized AI
ensemble retains the same sensitivity of traditional AI models, namely, it
identifies all known binary black hole mergers previously identified in this
advanced LIGO dataset and reports no misclassifications, while also providing a
3X inference speedup compared to traditional artificial intelligence models. We
used time slides to quantify the performance of our AI ensemble to process up
to 5 years worth of advanced LIGO data. In this synthetically enhanced dataset,
our AI ensemble reports an average of one misclassification for every month of
searched advanced LIGO data. We also present the receiver operating
characteristic curve of our AI ensemble using this 5 year long advanced LIGO
dataset. This approach provides the required tools to conduct accelerated,
AI-driven gravitational wave detection at scale.

The state of artificial intelligence technology has a rich history that dates
back decades and includes two fall-outs before the explosive resurgence of
today, which is credited largely to data-driven techniques. While AI technology
has and continues to become increasingly mainstream with impact across domains
and industries, it's not without several drawbacks, weaknesses, and potential
to cause undesired effects. AI techniques are numerous with many approaches and
variants, but they can be classified simply based on the degree of knowledge
they capture and how much data they require; two broad categories emerge as
prominent across AI to date: (1) techniques that are primarily, and often
solely, data-driven while leveraging little to no knowledge and (2) techniques
that primarily leverage knowledge and depend less on data. Now, a third
category is starting to emerge that leverages both data and knowledge, that
some refer to as "informed AI." This third category can be a game changer
within the national security domain where there is ample scientific and
domain-specific knowledge that stands ready to be leveraged, and where purely
data-driven AI can lead to serious unwanted consequences.
  This report shares findings from a thorough exploration of AI approaches that
exploit data as well as principled and/or practical knowledge, which we refer
to as "knowledge-integrated informed AI." Specifically, we review illuminating
examples of knowledge integrated in deep learning and reinforcement learning
pipelines, taking note of the performance gains they provide. We also discuss
an apparent trade space across variants of knowledge-integrated informed AI,
along with observed and prominent issues that suggest worthwhile future
research directions. Most importantly, this report suggests how the advantages
of knowledge-integrated informed AI stand to benefit the national security
domain.

Artificial Intelligence is now recognized as a general-purpose technology
with ample impact on human life. This work aims at understanding the evolution
of AI and, in particular Machine learning, from the perspective of researchers'
contributions to the field. In order to do so, we present several measures
allowing the analyses of AI and machine learning researchers' impact,
influence, and leadership over the last decades. This work also contributes, to
a certain extent, to shed new light on the history and evolution of AI by
exploring the dynamics involved in the field's evolution by looking at papers
published at the flagship AI and machine learning conferences since the first
International Joint Conference on Artificial Intelligence (IJCAI) held in 1969.
AI development and evolution have led to increasing research output, reflected
in the number of articles published over the last sixty years. We construct
comprehensive citation collaboration and paper-author datasets and compute
corresponding centrality measures to carry out our analyses. These analyses
allow a better understanding of how AI has reached its current state of affairs
in research. Throughout the process, we correlate these datasets with the work
of the ACM Turing Award winners and the so-called two AI winters the field has
gone through. We also look at self-citation trends and new authors' behaviors.
Finally, we present a novel way to infer the country of affiliation of a paper
from its organization. Therefore, this work provides a deep analysis of
Artificial Intelligence history from information gathered and analysed from
large technical venues datasets and suggests novel insights that can contribute
to understanding and measuring AI's evolution.

There is a lack of enough qualified teachers across Africa which hampers
efforts to provide adequate learning support such as educational question
answering (EQA) to students. An AI system that can enable students to ask
questions via text or voice and get instant answers will make high-quality
education accessible. Despite advances in the field of AI, there exists no
robust benchmark or challenge to enable building such an (EQA) AI within the
African context. Ghana's National Science and Maths Quiz competition (NSMQ) is
the perfect competition to evaluate the potential of such an AI due to its wide
coverage of scientific fields, variety of question types, highly competitive
nature, and live, real-world format. The NSMQ is a Jeopardy-style annual live
quiz competition in which 3 teams of 2 students compete by answering questions
across biology, chemistry, physics, and math in 5 rounds over 5 progressive
stages until a winning team is crowned for that year. In this position paper,
we propose the NSMQ AI Grand Challenge, an AI Grand Challenge for Education
using Ghana's National Science and Maths Quiz competition (NSMQ) as a case
study. Our proposed grand challenge is to "Build an AI to compete live in
Ghana's National Science and Maths Quiz (NSMQ) competition and win - performing
better than the best contestants in all rounds and stages of the competition."
We describe the competition, and key technical challenges to address along with
ideas from recent advances in machine learning that could be leveraged to solve
this challenge. This position paper is a first step towards conquering such a
challenge and importantly, making advances in AI for education in the African
context towards democratizing high-quality education across Africa.

Many Artificial Intelligence (AI) algorithms are inspired by physics and
employ stochastic fluctuations. We connect these physics-inspired AI algorithms
by unifying them under a single mathematical framework that we call
Thermodynamic AI. Seemingly disparate algorithmic classes can be described by
this framework, for example, (1) Generative diffusion models, (2) Bayesian
neural networks, (3) Monte Carlo sampling and (4) Simulated annealing. Such
Thermodynamic AI algorithms are currently run on digital hardware, ultimately
limiting their scalability and overall potential. Stochastic fluctuations
naturally occur in physical thermodynamic systems, and such fluctuations can be
viewed as a computational resource. Hence, we propose a novel computing
paradigm, where software and hardware become inseparable. Our algorithmic
unification allows us to identify a single full-stack paradigm, involving
Thermodynamic AI hardware, that could accelerate such algorithms. We contrast
Thermodynamic AI hardware with quantum computing where noise is a roadblock
rather than a resource. Thermodynamic AI hardware can be viewed as a novel form
of computing, since it uses a novel fundamental building block. We identify
stochastic bits (s-bits) and stochastic modes (s-modes) as the respective
building blocks for discrete and continuous Thermodynamic AI hardware. In
addition to these stochastic units, Thermodynamic AI hardware employs a
Maxwell's demon device that guides the system to produce non-trivial states. We
provide a few simple physical architectures for building these devices and we
develop a formalism for programming the hardware via gate sequences. We hope to
stimulate discussion around this new computing paradigm. Beyond acceleration,
we believe it will impact the design of both hardware and algorithms, while
also deepening our understanding of the connection between physics and
intelligence.

The growing carbon footprint of artificial intelligence (AI) models,
especially large ones such as GPT-3, has been undergoing public scrutiny.
Unfortunately, however, the equally important and enormous water (withdrawal
and consumption) footprint of AI models has remained under the radar. For
example, training GPT-3 in Microsoft's state-of-the-art U.S. data centers can
directly evaporate 700,000 liters of clean freshwater, but such information has
been kept a secret. More critically, the global AI demand may be accountable
for 4.2 -- 6.6 billion cubic meters of water withdrawal in 2027, which is more
than the total annual water withdrawal of 4 -- 6 Denmark or half of the United
Kingdom. This is very concerning, as freshwater scarcity has become one of the
most pressing challenges shared by all of us in the wake of the rapidly growing
population, depleting water resources, and aging water infrastructures. To
respond to the global water challenges, AI models can, and also must, take
social responsibility and lead by example by addressing their own water
footprint. In this paper, we provide a principled methodology to estimate the
water footprint of AI models, and also discuss the unique spatial-temporal
diversities of AI models' runtime water efficiency. Finally, we highlight the
necessity of holistically addressing water footprint along with carbon
footprint to enable truly sustainable AI.

Private and public sector structures and norms refine how emerging technology
is used in practice. In healthcare, despite a proliferation of AI adoption, the
organizational governance surrounding its use and integration is often poorly
understood. What the Health AI Partnership (HAIP) aims to do in this research
is to better define the requirements for adequate organizational governance of
AI systems in healthcare settings and support health system leaders to make
more informed decisions around AI adoption. To work towards this understanding,
we first identify how the standards for the AI adoption in healthcare may be
designed to be used easily and efficiently. Then, we map out the precise
decision points involved in the practical institutional adoption of AI
technology within specific health systems. Practically, we achieve this through
a multi-organizational collaboration with leaders from major health systems
across the United States and key informants from related fields. Working with
the consultancy IDEO [dot] org, we were able to conduct usability-testing
sessions with healthcare and AI ethics professionals. Usability analysis
revealed a prototype structured around mock key decision points that align with
how organizational leaders approach technology adoption. Concurrently, we
conducted semi-structured interviews with 89 professionals in healthcare and
other relevant fields. Using a modified grounded theory approach, we were able
to identify 8 key decision points and comprehensive procedures throughout the
AI adoption lifecycle. This is one of the most detailed qualitative analyses to
date of the current governance structures and processes involved in AI adoption
by health systems in the United States. We hope these findings can inform
future efforts to build capabilities to promote the safe, effective, and
responsible adoption of emerging technologies in healthcare.

The research on the sixth-generation (6G) wireless communications for the
development of future mobile communication networks has been officially
launched around the world. 6G networks face multifarious challenges, such as
resource-constrained mobile devices, difficult wireless resource management,
high complexity of heterogeneous network architectures, explosive computing and
storage requirements, privacy and security threats. To address these
challenges, deploying blockchain and artificial intelligence (AI) in 6G
networks may realize new breakthroughs in advancing network performances in
terms of security, privacy, efficiency, cost, and more. In this paper, we
provide a detailed survey of existing works on the application of blockchain
and AI to 6G wireless communications. More specifically, we start with a brief
overview of blockchain and AI. Then, we mainly review the recent advances in
the fusion of blockchain and AI, and highlight the inevitable trend of
deploying both blockchain and AI in wireless communications. Furthermore, we
extensively explore integrating blockchain and AI for wireless communication
systems, involving secure services and Internet of Things (IoT) smart
applications. Particularly, some of the most talked-about key services based on
blockchain and AI are introduced, such as spectrum management, computation
allocation, content caching, and security and privacy. Moreover, we also focus
on some important IoT smart applications supported by blockchain and AI,
covering smart healthcare, smart transportation, smart grid, and unmanned
aerial vehicles (UAVs). We also analyze the open issues and research challenges
for the joint deployment of blockchain and AI in 6G wireless communications.
Lastly, based on lots of existing meaningful works, this paper aims to provide
a comprehensive survey of blockchain and AI in 6G networks.

Artificial intelligence's (AI) progress holds great promise in tackling
pressing societal concerns such as health and climate. Large Language Models
(LLM) and the derived chatbots, like ChatGPT, have highly improved the natural
language processing capabilities of AI systems allowing them to process an
unprecedented amount of unstructured data. However, the ensuing excitement has
led to negative sentiments, even as AI methods demonstrate remarkable
contributions (e.g. in health and genetics). A key factor contributing to this
sentiment is the misleading perception that LLMs can effortlessly provide
solutions across domains, ignoring their limitations such as hallucinations and
reasoning constraints. Acknowledging AI fallibility is crucial to address the
impact of dogmatic overconfidence in possibly erroneous suggestions generated
by LLMs. At the same time, it can reduce fear and other negative attitudes
toward AI. This necessitates comprehensive AI literacy interventions that
educate the public about LLM constraints and effective usage techniques, i.e
prompting strategies. With this aim, a pilot educational intervention was
performed in a high school with 21 students. It involved presenting high-level
concepts about intelligence, AI, and LLMs, followed by practical exercises
involving ChatGPT in creating natural educational conversations and applying
established prompting strategies. Encouraging preliminary results emerged,
including high appreciation of the activity, improved interaction quality with
the LLM, reduced negative AI sentiments, and a better grasp of limitations,
specifically unreliability, limited understanding of commands leading to
unsatisfactory responses, and limited presentation flexibility. Our aim is to
explore AI acceptance factors and refine this approach for more controlled
future studies.

Background:Technical systems are growing in complexity with more components
and functions across various disciplines. Model-Driven Engineering (MDE) helps
manage this complexity by using models as key artifacts. Domain-Specific
Languages (DSL) supported by MDE facilitate modeling. As data generation in
product development increases, there's a growing demand for AI algorithms,
which can be challenging to implement. Integrating AI algorithms with DSL and
MDE can streamline this process. Objective:This study aims to investigate the
existing model-driven approaches relying on DSL in support of the engineering
of AI software systems to sharpen future research further and define the
current state of the art. Method:We conducted a Systemic Literature Review
(SLR), collecting papers from five major databases resulting in 1335 candidate
studies, eventually retaining 18 primary studies. Each primary study will be
evaluated and discussed with respect to the adoption of MDE principles and
practices and the phases of AI development support aligned with the stages of
the CRISP-DM methodology. Results:The study's findings show that language
workbenches are of paramount importance in dealing with all aspects of modeling
language development and are leveraged to define DSL explicitly addressing AI
concerns. The most prominent AI-related concerns are training and modeling of
the AI algorithm, while minor emphasis is given to the time-consuming
preparation of the data. Early project phases that support interdisciplinary
communication of requirements, e.g., CRISP-DM Business Understanding phase, are
rarely reflected. Conclusion:The study found that the use of MDE for AI is
still in its early stages, and there is no single tool or method that is widely
used. Additionally, current approaches tend to focus on specific stages of
development rather than providing support for the entire development process.

With the rise of prolific ChatGPT, the risk and consequences of AI-generated
text has increased alarmingly. To address the inevitable question of ownership
attribution for AI-generated artifacts, the US Copyright Office released a
statement stating that 'If a work's traditional elements of authorship were
produced by a machine, the work lacks human authorship and the Office will not
register it'. Furthermore, both the US and the EU governments have recently
drafted their initial proposals regarding the regulatory framework for AI.
Given this cynosural spotlight on generative AI, AI-generated text detection
(AGTD) has emerged as a topic that has already received immediate attention in
research, with some initial methods having been proposed, soon followed by
emergence of techniques to bypass detection. This paper introduces the Counter
Turing Test (CT^2), a benchmark consisting of techniques aiming to offer a
comprehensive evaluation of the robustness of existing AGTD techniques. Our
empirical findings unequivocally highlight the fragility of the proposed AGTD
methods under scrutiny. Amidst the extensive deliberations on policy-making for
regulating AI development, it is of utmost importance to assess the
detectability of content generated by LLMs. Thus, to establish a quantifiable
spectrum facilitating the evaluation and ranking of LLMs according to their
detectability levels, we propose the AI Detectability Index (ADI). We conduct a
thorough examination of 15 contemporary LLMs, empirically demonstrating that
larger LLMs tend to have a higher ADI, indicating they are less detectable
compared to smaller LLMs. We firmly believe that ADI holds significant value as
a tool for the wider NLP community, with the potential to serve as a rubric in
AI-related policy-making.

Documentation burden is a major contributor to clinician burnout, which is
rising nationally and is an urgent threat to our ability to care for patients.
Artificial intelligence (AI) chatbots, such as ChatGPT, could reduce clinician
burden by assisting with documentation. Although many hospitals are actively
integrating such systems into electronic medical record systems, AI chatbots
utility and impact on clinical decision-making have not been studied for this
intended use. We are the first to examine the utility of large language models
in assisting clinicians draft responses to patient questions. In our two-stage
cross-sectional study, 6 oncologists responded to 100 realistic synthetic
cancer patient scenarios and portal messages developed to reflect common
medical situations, first manually, then with AI assistance.
  We find AI-assisted responses were longer, less readable, but provided
acceptable drafts without edits 58% of time. AI assistance improved efficiency
77% of time, with low harm risk (82% safe). However, 7.7% unedited AI responses
could severely harm. In 31% cases, physicians thought AI drafts were
human-written. AI assistance led to more patient education recommendations,
fewer clinical actions than manual responses. Results show promise for AI to
improve clinician efficiency and patient care through assisting documentation,
if used judiciously. Monitoring model outputs and human-AI interaction remains
crucial for safe implementation.

Artificial intelligence (AI) refers to the ability of machines or software to
mimic or even surpass human intelligence in a given cognitive task. While
humans learn by both induction and deduction, the success of current AI is
rooted in induction, relying on its ability to detect statistical regularities
in task input -- an ability learnt from a vast amount of training data using
enormous computation resources. We examine the performance of such a
statistical AI in a human task through the lens of four factors, including task
learnability, statistical resource, computation resource, and learning
techniques, and then propose a three-phase visual framework to understand the
evolving relation between AI and jobs. Based on this conceptual framework, we
develop a simple economic model of competition to show the existence of an
inflection point for each occupation. Before AI performance crosses the
inflection point, human workers always benefit from an improvement in AI
performance, but after the inflection point, human workers become worse off
whenever such an improvement occurs. To offer empirical evidence, we first
argue that AI performance has passed the inflection point for the occupation of
translation but not for the occupation of web development. We then study how
the launch of ChatGPT, which led to significant improvement of AI performance
on many tasks, has affected workers in these two occupations on a large online
labor platform. Consistent with the inflection point conjecture, we find that
translators are negatively affected by the shock both in terms of the number of
accepted jobs and the earnings from those jobs, while web developers are
positively affected by the very same shock. Given the potentially large
disruption of AI on employment, more studies on more occupations using data
from different platforms are urgently needed.

AI-Based Safety-Critical Systems (AI-SCS) are being increasingly deployed in
the real world. These can pose a risk of harm to people and the environment.
Reducing that risk is an overarching priority during development and operation.
As more AI-SCS become autonomous, a layer of risk management via human
intervention has been removed. Following an accident it will be important to
identify causal contributions and the different responsible actors behind those
to learn from mistakes and prevent similar future events. Many authors have
commented on the "responsibility gap" where it is difficult for developers and
manufacturers to be held responsible for harmful behaviour of an AI-SCS. This
is due to the complex development cycle for AI, uncertainty in AI performance,
and dynamic operating environment. A human operator can become a "liability
sink" absorbing blame for the consequences of AI-SCS outputs they weren't
responsible for creating, and may not have understanding of.
  This cross-disciplinary paper considers different senses of responsibility
(role, moral, legal and causal), and how they apply in the context of AI-SCS
safety. We use a core concept (Actor(A) is responsible for Occurrence(O)) to
create role responsibility models, producing a practical method to capture
responsibility relationships and provide clarity on the previously identified
responsibility issues. Our paper demonstrates the approach with two examples: a
retrospective analysis of the Tempe Arizona fatal collision involving an
autonomous vehicle, and a safety focused predictive role-responsibility
analysis for an AI-based diabetes co-morbidity predictor. In both examples our
primary focus is on safety, aiming to reduce unfair or disproportionate blame
being placed on operators or developers. We present a discussion and avenues
for future research.

Reaching consensus on a commonly accepted definition of AI Fairness has long
been a central challenge in AI ethics and governance. There is a broad spectrum
of views across society on what the concept of fairness means and how it should
best be put to practice. In this workbook, we tackle this challenge by
exploring how a context-based and society-centred approach to understanding AI
Fairness can help project teams better identify, mitigate, and manage the many
ways that unfair bias and discrimination can crop up across the AI project
workflow.
  We begin by exploring how, despite the plurality of understandings about the
meaning of fairness, priorities of equality and non-discrimination have come to
constitute the broadly accepted core of its application as a practical
principle. We focus on how these priorities manifest in the form of equal
protection from direct and indirect discrimination and from discriminatory
harassment. These elements form ethical and legal criteria based upon which
instances of unfair bias and discrimination can be identified and mitigated
across the AI project workflow.
  We then take a deeper dive into how the different contexts of the AI project
lifecycle give rise to different fairness concerns. This allows us to identify
several types of AI Fairness (Data Fairness, Application Fairness, Model Design
and Development Fairness, Metric-Based Fairness, System Implementation
Fairness, and Ecosystem Fairness) that form the basis of a multi-lens approach
to bias identification, mitigation, and management. Building on this, we
discuss how to put the principle of AI Fairness into practice across the AI
project workflow through Bias Self-Assessment and Bias Risk Management as well
as through the documentation of metric-based fairness criteria in a Fairness
Position Statement.

Purpose: The study aims to analyze the synergy of Artificial Intelligence
(AI), with scientometrics, webometrics, and bibliometrics to unlock and to
emphasize the potential of the applications and benefits of AI algorithms in
these fields.
  Design/methodology/approach: By conducting a systematic literature review,
our aim is to explore the potential of AI in revolutionizing the methods used
to measure and analyze scholarly communication, identify emerging research
trends, and evaluate the impact of scientific publications. To achieve this, we
implemented a comprehensive search strategy across reputable databases such as
ProQuest, IEEE Explore, EBSCO, Web of Science, and Scopus. Our search
encompassed articles published from January 1, 2000, to September 2022,
resulting in a thorough review of 61 relevant articles.
  Findings: (i) Regarding scientometrics, the application of AI yields various
distinct advantages, such as conducting analyses of publications, citations,
research impact prediction, collaboration, research trend analysis, and
knowledge mapping, in a more objective and reliable framework. (ii) In terms of
webometrics, AI algorithms are able to enhance web crawling and data
collection, web link analysis, web content analysis, social media analysis, web
impact analysis, and recommender systems. (iii) Moreover, automation of data
collection, analysis of citations, disambiguation of authors, analysis of
co-authorship networks, assessment of research impact, text mining, and
recommender systems are considered as the potential of AI integration in the
field of bibliometrics.
  Originality/value: This study covers the particularly new benefits and
potential of AI-enhanced scientometrics, webometrics, and bibliometrics to
highlight the significant prospects of the synergy of this integration through
AI.

Cyberharassment is a critical, socially relevant cybersecurity problem
because of the adverse effects it can have on targeted groups or individuals.
While progress has been made in understanding cyber-harassment, its detection,
attacks on artificial intelligence (AI) based cyberharassment systems, and the
social problems in cyberharassment detectors, little has been done in designing
experiential learning educational materials that engage students in this
emerging social cybersecurity in the era of AI. Experiential learning
opportunities are usually provided through capstone projects and engineering
design courses in STEM programs such as computer science. While capstone
projects are an excellent example of experiential learning, given the
interdisciplinary nature of this emerging social cybersecurity problem, it can
be challenging to use them to engage non-computing students without prior
knowledge of AI. Because of this, we were motivated to develop a hands-on lab
platform that provided experiential learning experiences to non-computing
students with little or no background knowledge in AI and discussed the lessons
learned in developing this lab. In this lab used by social science students at
North Carolina A&T State University across two semesters (spring and fall) in
2022, students are given a detailed lab manual and are to complete a set of
well-detailed tasks. Through this process, students learn AI concepts and the
application of AI for cyberharassment detection. Using pre- and post-surveys,
we asked students to rate their knowledge or skills in AI and their
understanding of the concepts learned. The results revealed that the students
moderately understood the concepts of AI and cyberharassment.

Artificial Intelligence (AI) has become commonplace to solve routine everyday
tasks. Because of the exponential growth in medical imaging data volume and
complexity, the workload on radiologists is steadily increasing. We project
that the gap between the number of imaging exams and the number of expert
radiologist readers required to cover this increase will continue to expand,
consequently introducing a demand for AI-based tools that improve the
efficiency with which radiologists can comfortably interpret these exams. AI
has been shown to improve efficiency in medical-image generation, processing,
and interpretation, and a variety of such AI models have been developed across
research labs worldwide. However, very few of these, if any, find their way
into routine clinical use, a discrepancy that reflects the divide between AI
research and successful AI translation. To address the barrier to clinical
deployment, we have formed MONAI Consortium, an open-source community which is
building standards for AI deployment in healthcare institutions, and developing
tools and infrastructure to facilitate their implementation. This report
represents several years of weekly discussions and hands-on problem solving
experience by groups of industry experts and clinicians in the MONAI
Consortium. We identify barriers between AI-model development in research labs
and subsequent clinical deployment and propose solutions. Our report provides
guidance on processes which take an imaging AI model from development to
clinical implementation in a healthcare institution. We discuss various AI
integration points in a clinical Radiology workflow. We also present a taxonomy
of Radiology AI use-cases. Through this report, we intend to educate the
stakeholders in healthcare and AI (AI researchers, radiologists, imaging
informaticists, and regulators) about cross-disciplinary challenges and
possible solutions.

In the article a turn-based game played on four computers connected via
network is investigated. There are three computers with natural intelligence
and one with artificial intelligence. Game table is seen by each player's own
view point in all players' monitors. Domino pieces are three dimensional. For
distributed systems TCP/IP protocol is used. In order to get 3D image,
Microsoft XNA technology is applied. Domino 101 game is nondeterministic game
that is result of the game depends on the initial random distribution of the
pieces. Number of the distributions is equal to the multiplication of following
combinations: . Moreover, in this game that is played by four people, players
are divided into 2 pairs. Accordingly, we cannot predict how the player uses
the dominoes that is according to the dominoes of his/her partner or according
to his/her own dominoes. The fact that the natural intelligence can be a player
in any level affects the outcome. These reasons make it difficult to develop an
AI. In the article four levels of AI are developed. The AI in the first level
is equivalent to the intelligence of a child who knows the rules of the game
and recognizes the numbers. The AI in this level plays if it has any domino,
suitable to play or says pass. In most of the games which can be played on the
internet, the AI does the same. But the AI in the last level is a master
player, and it can develop itself according to its competitors' levels.

Today, available methods that assess AI systems are focused on using
empirical techniques to measure the performance of algorithms in some specific
tasks (e.g., playing chess, solving mazes or land a helicopter). However, these
methods are not appropriate if we want to evaluate the general intelligence of
AI and, even less, if we compare it with human intelligence. The ANYNT project
has designed a new method of evaluation that tries to assess AI systems using
well known computational notions and problems which are as general as possible.
This new method serves to assess general intelligence (which allows us to learn
how to solve any new kind of problem we face) and not only to evaluate
performance on a set of specific tasks. This method not only focuses on
measuring the intelligence of algorithms, but also to assess any intelligent
system (human beings, animals, AI, aliens?,...), and letting us to place their
results on the same scale and, therefore, to be able to compare them. This new
approach will allow us (in the future) to evaluate and compare any kind of
intelligent system known or even to build/find, be it artificial or biological.
This master thesis aims at ensuring that this new method provides consistent
results when evaluating AI algorithms, this is done through the design and
implementation of prototypes of universal intelligence tests and their
application to different intelligent systems (AI algorithms and humans beings).
From the study we analyze whether the results obtained by two different
intelligent systems are properly located on the same scale and we propose
changes and refinements to these prototypes in order to, in the future, being
able to achieve a truly universal intelligence test.

In recent years, Artificial Intelligence techniques have emerged as useful
tools for solving various engineering problems that were not possible or
convenient to handle by traditional methods. AI has directly influenced many
areas of computer science and becomes an important part of the engineering
curriculum. However, determining the important topics for a single semester AI
course is a nontrivial task, given the lack of a general methodology. AI
concepts commonly overlap with many other disciplines involving a wide range of
subjects, including applied approaches to more formal mathematical issues. This
paper presents the use of a simple robotic platform to assist the learning of
basic AI concepts. The study is guided through some simple experiments using
autonomous mobile robots. The central algorithm is the Learning Automata. Using
LA, each robot action is applied to an environment to be evaluated by means of
a fitness value. The response of the environment is used by the automata to
select its next action. This procedure holds until the goal task is reached.
The proposal addresses the AI study by offering in LA a unifying context to
draw together several of the topics of AI and motivating the students to learn
by building some hands on laboratory exercises. The presented material has been
successfully tested as AI teaching aide in the University of Guadalajara
robotics group as it motivates students and increases enrolment and retention
while educating better computer engineers.

The quality of high-level AI of non-player characters (NPCs) in commercial
open-world games (OWGs) has been increasing during the past years. However, due
to constraints specific to the game industry, this increase has been slow and
it has been driven by larger budgets rather than adoption of new complex AI
techniques. Most of the contemporary AI is still expressed as hard-coded
scripts. The complexity and manageability of the script codebase is one of the
key limiting factors for further AI improvements. In this paper we address this
issue. We present behavior objects - a general approach to development of NPC
behaviors for large OWGs. Behavior objects are inspired by object-oriented
programming and extend the concept of smart objects. Our approach promotes
encapsulation of data and code for multiple related behaviors in one place,
hiding internal details and embedding intelligence in the environment. Behavior
objects are a natural abstraction of five different techniques that we have
implemented to manage AI complexity in an upcoming AAA OWG. We report the
details of the implementations in the context of behavior trees and the lessons
learned during development. Our work should serve as inspiration for AI
architecture designers from both the academia and the industry.

In the context of superintelligent AI systems, the term "oracle" has two
meanings. One refers to modular systems queried for domain-specific tasks.
Another usage, referring to a class of systems which may be useful for
addressing the value alignment and AI control problems, is a superintelligent
AI system that only answers questions. The aim of this manuscript is to survey
contemporary research problems related to oracles which align with long-term
research goals of AI safety. We examine existing question answering systems and
argue that their high degree of architectural heterogeneity makes them poor
candidates for rigorous analysis as oracles. On the other hand, we identify
computer algebra systems (CASs) as being primitive examples of domain-specific
oracles for mathematics and argue that efforts to integrate computer algebra
systems with theorem provers, systems which have largely been developed
independent of one another, provide a concrete set of problems related to the
notion of provable safety that has emerged in the AI safety community. We
review approaches to interfacing CASs with theorem provers, describe
well-defined architectural deficiencies that have been identified with CASs,
and suggest possible lines of research and practical software projects for
scientists interested in AI safety.

The introduction of artificial intelligence (AI) on visual images for
emotional analysis obliterates the natural subjectivity and contextual
dependence of our facial displays. Emotion AI places itself as an algorithmic
lens on our digital artifacts and real-time interactions, creating the illusion
of a new, objective class of data: our emotional and mental states. Building
upon a rich network of existing public photographs--as well as fresh feeds from
surveillance footage or smart phone cameras--these emotion algorithms require
no additional infrastructure or improvements on image quality. In order to
examine the potential policy and legal remedies for emotion AI as an emerging
technology, we first establish a framework of actors, collection motivations,
time scales, and space considerations that differentiates emotion AI from other
algorithmic lenses. Each of these elements influences available policy
remedies, and should shape continuing discussions on the antecedent conditions
that make emotional AI acceptable or not in particular contexts. Based on our
framework of unique elements, we examine potential available policy remedies to
prevent or remediate harm. Specifically, our paper looks toward the regulatory
role of the Federal Trade Commission in the US, gaps in the EU's General Data
Protection Regulation (GDPR) allowing for emotion data collection, and
precedent set by polygraph technologies in evidentiary and use restrictions set
by law. We also examine the way social norms and adaptations could grow to also
modulate broader use. Given the challenges in controlling the flow of these
data, we call for further research and attention as emotion AI technology
remains poised for adoption.

In his seminal book `The Inmates are Running the Asylum: Why High-Tech
Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams
Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is
often poorly designed (from a user perspective) is that programmers are in
charge of design decisions, rather than interaction designers. As a result,
programmers design software for themselves, rather than for their target
audience, a phenomenon he refers to as the `inmates running the asylum'. This
paper argues that explainable AI risks a similar fate. While the re-emergence
of explainable AI is positive, this paper argues most of us as AI researchers
are building explanatory agents for ourselves, rather than for the intended
users. But explainable AI is more likely to succeed if researchers and
practitioners understand, adopt, implement, and improve models from the vast
and valuable bodies of research in philosophy, psychology, and cognitive
science, and if evaluation of these models is focused more on people than on
technology. From a light scan of literature, we demonstrate that there is
considerable scope to infuse more results from the social and behavioural
sciences into explainable AI, and present some key results from these fields
that are relevant to explainable AI.

Artificial Intelligence (AI), is once again in the phase of drastic
advancements. Unarguably, the technology itself can revolutionize the way we
live our everyday life. But the exponential growth of technology poses a
daunting task for policy researchers and law makers in making amendments to the
existing norms. In addition, not everyone in the society is studying the
potential socio-economic intricacies and cultural drifts that AI can bring
about. It is prudence to reflect from our historical past to propel the
development of technology in the right direction. To benefit the society of the
present and future, I scientifically explore the societal impact of AI. While
there are many public and private partnerships working on similar aspects, here
I describe the necessity for an Unanimous International Regulatory Body for all
applications of AI (UIRB-AI). I also discuss the benefits and drawbacks of such
an organization. To combat any drawbacks in the formation of an UIRB-AI, both
idealistic and pragmatic perspectives are discussed alternatively. The paper
further advances the discussion by proposing novel policies on how such
organization should be structured and how it can bring about a win-win
situation for everyone in the society.

The complexity and diversity of big data and AI workloads make understanding
them difficult and challenging. This paper proposes a new approach to
characterizing big data and AI workloads. We consider each big data and AI
workload as a pipeline of one or more classes of unit of computations performed
on different initial or intermediate data inputs. Each class of unit of
computation captures the common requirements while being reasonably divorced
from individual implementations, and hence we call it a data dwarf. For the
first time, among a wide variety of big data and AI workloads, we identify
eight data dwarfs that takes up most of run time, including Matrix, Sampling,
Logic, Transform, Set, Graph, Sort and Statistic. We implement the eight data
dwarfs on different software stacks as the micro benchmarks of an open-source
big data and AI benchmark suite, and perform comprehensive characterization of
those data dwarfs from perspective of data sizes, types, sources, and patterns
as a lens towards fully understanding big data and AI workloads.

AI technology has a long history which is actively and constantly changing
and growing. It focuses on intelligent agents, which contain devices that
perceive the environment and based on which takes actions in order to maximize
goal success chances. In this paper, we will explain the modern AI basics and
various representative applications of AI. In the context of the modern
digitalized world, AI is the property of machines, computer programs, and
systems to perform the intellectual and creative functions of a person,
independently find ways to solve problems, be able to draw conclusions and make
decisions. Most artificial intelligence systems have the ability to learn,
which allows people to improve their performance over time. The recent research
on AI tools, including machine learning, deep learning and predictive analysis
intended toward increasing the planning, learning, reasoning, thinking and
action taking ability. Based on which, the proposed research intends towards
exploring on how the human intelligence differs from the artificial
intelligence. Moreover, we critically analyze what AI of today is capable of
doing, why it still cannot reach human intelligence and what are the open
challenges existing in front of AI to reach and outperform human level of
intelligence. Furthermore, it will explore the future predictions for
artificial intelligence and based on which potential solution will be
recommended to solve it within next decades.

The goal of the Interactive Learning for Artificial Intelligence (AI) for
Human-Robot Interaction (HRI) symposium is to bring together the large
community of researchers working on interactive learning scenarios for
interactive robotics. While current HRI research involves investigating ways
for robots to effectively interact with people, HRI's overarching goal is to
develop robots that are autonomous while intelligently modeling and learning
from humans. These goals greatly overlap with some central goals of AI and
interactive machine learning, such that HRI is an extremely challenging problem
domain for interactive learning and will elicit fresh problem areas for
robotics research. Present-day AI research still does not widely consider
situations for interacting directly with humans and within human-populated
environments, which present inherent uncertainty in dynamics, structure, and
interaction. We believe that the HRI community already offers a rich set of
principles and observations that can be used to structure new models of
interaction. The human-aware AI initiative has primarily been approached
through human-in-the-loop methods that use people's data and feedback to
improve refinement and performance of the algorithms, learned functions, and
personalization. We thus believe that HRI is an important component to
furthering AI and robotics research.

Traumatic injuries are measured using the Abbreviated Injury Scale (AIS),
which is a risk to life scale. New human computer models use stresses and
strains to evaluate whether serious or fatal injuries are reached,
unfortunately these tensors bear no direct relation to AIS. This paper proposes
to overcome this deficiency and suggests a unique Organ Trauma Model (OTM) able
to calculate the risk to life based on the severity on any organ injury,
focussing on real-life pedestrian accidents. The OTM uses a power method, named
Peak Virtual Power (PVP), and calculates the risk to life of brain white and
grey matters as a function of impact direction and impact speed. The OTM
firstly calibrates PVP against the medical critical AIS threshold observed in
each part of the head as a function of speed. This base PVP critical trauma
function is then scaled and banded across all AIS levels using the confirmed
property that AIS and the probability of death is statistically and numerically
a cubic one. The OTM model has been tested against four real-life pedestrian
accidents and proven to be able to predict pedestrian head trauma severity. In
some cases, the method did however under-estimate the head trauma by 1 AIS
level, because of post-impact haemorrhage which cannot be captured with the
employed Lagrangian Finite Element (FE) solver. It is also shown that the
location of the injury predictions using PVP coincide with the post mortem
reports and are different to the predictions made using maximum principal
strain.

While there have been many proposals on making AI algorithms explainable, few
have attempted to evaluate the impact of AI-generated explanations on human
performance in conducting human-AI collaborative tasks. To bridge the gap, we
propose a Twenty-Questions style collaborative image retrieval game,
Explanation-assisted Guess Which (ExAG), as a method of evaluating the efficacy
of explanations (visual evidence or textual justification) in the context of
Visual Question Answering (VQA). In our proposed ExAG, a human user needs to
guess a secret image picked by the VQA agent by asking natural language
questions to it. We show that overall, when AI explains its answers, users
succeed more often in guessing the secret image correctly. Notably, a few
correct explanations can readily improve human performance when VQA answers are
mostly incorrect as compared to no-explanation games. Furthermore, we also show
that while explanations rated as "helpful" significantly improve human
performance, "incorrect" and "unhelpful" explanations can degrade performance
as compared to no-explanation games. Our experiments, therefore, demonstrate
that ExAG is an effective means to evaluate the efficacy of AI-generated
explanations on a human-AI collaborative task.

Futurists have predicted that new technologies, embedded with artificial
intelligence (AI) and machine learning (ML), will lead to substantial job loss
in many sectors disrupting many aspects of healthcare. Mental health appears
ripe for such disruption given the global illness burden, stigma, and shortage
of care providers. Using Sermo, a global networking platform open to verified
and licensed physicians, we measured the opinions of psychiatrists about the
likelihood that future autonomous technology (referred to as AI/ML) would be
able to fully replace the average psychiatrist in performing 10 key tasks (e.g.
mental status exam, suicidality assessment, treatment planning) carried out in
mental health care. Survey respondents were 791 psychiatrists from 22
countries. Only 3.8% of respondents felt that AI/ML was likely to replace a
human clinician for providing empathetic care. Documenting (e.g. updating
medical records) and synthesizing information to reach a diagnosis were the two
tasks where a majority predicted that future AI/ML would replace human doctors.
About 1 in 2 doctors believed their jobs could be changed substantially by
future AI/ML. However, female and US-based doctors were more uncertain that the
possible benefits of AI would outweigh potential risks, versus their male and
global counterparts. To our knowledge, this is the first global survey to seek
the opinions of physicians on the impact of autonomous AI/ML on the future of
psychiatry. Our findings provide compelling insights into how physicians think
about intelligent technologies which may better help us integrate such tools
and reskill doctors, as needed, to enhance mental health care.

The use of Artificial Intelligence (AI) is becoming increasingly pervasive
and relevant in many different application areas. Researchers are putting a
considerable effort to take full advantage of the power of AI, while trying to
overcome the technical challenges that are intrinsically linked to almost any
domain area of application, such as the Internet of Things (IoT). One of the
biggest problems related to the use of AI in IoT is related to the difficulty
of coping with the wide variety of protocols and software technologies used, as
well as with the heterogeneity of the hardware resources consuming the AI. The
scattered IoT landscape accentuates the limitations on interoperability,
especially visible in the deployment of AI, affecting the seamless AI
life-cycle management as well. In this paper, it is discussed how to enable AI
distribution in IoT by introducing a layered intelligence architecture that
aims to face the undertaken challenges taking into account the special
requirements of nowadays IoT networks. It describes the main characteristics of
the new paradigm architecture, highlighting what are the implications of its
adoption from use cases perspective and their requirements. Finally, a set of
open technical and research challenges are enumerated to reach the full
potential of the intelligence distribution's vision.

Deep learning has become widely used in complex AI applications. Yet,
training a deep neural network (DNNs) model requires a considerable amount of
calculations, long running time, and much energy. Nowadays, many-core AI
accelerators (e.g., GPUs and TPUs) are designed to improve the performance of
AI training. However, processors from different vendors perform dissimilarly in
terms of performance and energy consumption. To investigate the differences
among several popular off-the-shelf processors (i.e., Intel CPU, NVIDIA GPU,
AMD GPU, and Google TPU) in training DNNs, we carry out a comprehensive
empirical study on the performance and energy efficiency of these processors by
benchmarking a representative set of deep learning workloads, including
computation-intensive operations, classical convolutional neural networks
(CNNs), recurrent neural networks (LSTM), Deep Speech 2, and Transformer.
Different from the existing end-to-end benchmarks which only present the
training time, We try to investigate the impact of hardware, vendor's software
library, and deep learning framework on the performance and energy consumption
of AI training. Our evaluation methods and results not only provide an
informative guide for end-users to select proper AI accelerators, but also
expose some opportunities for the hardware vendors to improve their software
library.

Artificial intelligence (AI) has evolved considerably in the last few years.
While applications of AI is now becoming more common in fields like retail and
marketing, application of AI in solving problems related to developing
countries is still an emerging topic. Specially, AI applications in
resource-poor settings remains relatively nascent. There is a huge scope of AI
being used in such settings. For example, researchers have started exploring AI
applications to reduce poverty and deliver a broad range of critical public
services. However, despite many promising use cases, there are many dataset
related challenges that one has to overcome in such projects. These challenges
often take the form of missing data, incorrectly collected data and improperly
labeled variables, among other factors. As a result, we can often end up using
data that is not representative of the problem we are trying to solve. In this
case study, we explore the challenges of using such an open dataset from India,
to predict an important health outcome. We highlight how the use of AI without
proper understanding of reporting metrics can lead to erroneous conclusions.

The use of machine learning or artificial intelligence (ML/AI) holds
substantial potential toward improving many functions and needs of the public
sector. In practice however, integrating ML/AI components into public sector
applications is severely limited not only by the fragility of these components
and their algorithms, but also because of mismatches between components of
ML-enabled systems. For example, if an ML model is trained on data that is
different from data in the operational environment, field performance of the ML
component will be dramatically reduced. Separate from software engineering
considerations, the expertise needed to field an ML/AI component within a
system frequently comes from outside software engineering. As a result,
assumptions and even descriptive language used by practitioners from these
different disciplines can exacerbate other challenges to integrating ML/AI
components into larger systems. We are investigating classes of mismatches in
ML/AI systems integration, to identify the implicit assumptions made by
practitioners in different fields (data scientists, software engineers,
operations staff) and find ways to communicate the appropriate information
explicitly. We will discuss a few categories of mismatch, and provide examples
from each class. To enable ML/AI components to be fielded in a meaningful way,
we will need to understand the mismatches that exist and develop practices to
mitigate the impacts of these mismatches.

Certainty around the regulatory environment is crucial to enable responsible
AI innovation and foster the social acceptance of these powerful new
technologies. One notable source of uncertainty is, however, that the existing
legal liability system is inapt to assign responsibility where a potentially
harmful conduct and/or the harm itself are unforeseeable, yet some
instantiations of AI and/or the harms they may trigger are not foreseeable in
the legal sense. The unpredictability of how courts would handle such cases
makes the risks involved in the investment and use of AI incalculable, creating
an environment that is not conducive to innovation and may deprive society of
some of the benefits AI could provide. To tackle this problem, we propose to
draw insights from financial regulatory best-practices and establish a system
of AI guarantee schemes. We envisage the system to form part of the broader
market-structuring regulatory framework, with the primary function to provide a
readily available, clear, and transparent funding mechanism to compensate
claims that are either extremely hard or impossible to realize via conventional
litigation. We propose it to be at least partially industry-funded, with
funding arrangements depending on whether it would pursue other potential
policy goals. We aim to engage in a high-level, comparative conceptual debate
around the suitability of the foreseeability concept to limit legal liability
rather than confronting the intricacies of the case law of specific
jurisdictions. Recognizing the importance of the latter task, we leave this to
further research in support of the legal system's incremental adaptation to the
novel challenges of present and future AI technologies.

The prosperity of artificial intelligence has aroused intensive interests in
intelligent/autonomous navigation, in which path prediction is a key
functionality for decision supports, e.g. route planning, collision warning,
and traffic regulation. For maritime intelligence, Automatic Identification
System (AIS) plays an important role because it recently has been made
compulsory for large international commercial vessels and is able to provide
nearly real-time information of the vessel. Therefore AIS data based vessel
path prediction is a promising way in future maritime intelligence. However,
real-world AIS data collected online are just highly irregular trajectory
segments (AIS message sequences) from different types of vessels and
geographical regions, with possibly very low data quality. So even there are
some works studying how to build a path prediction model using historical AIS
data, but still, it is a very challenging problem. In this paper, we propose a
comprehensive framework to model massive historical AIS trajectory segments for
accurate vessel path prediction. Experimental comparisons with existing popular
methods are made to validate the proposed approach and results show that our
approach could outperform the baseline methods by a wide margin.

We present a blockchain based system that allows data owners, cloud vendors,
and AI developers to collaboratively train machine learning models in a
trustless AI marketplace. Data is a highly valued digital asset and central to
deriving business insights. Our system enables data owners to retain ownership
and privacy of their data, while still allowing AI developers to leverage the
data for training. Similarly, AI developers can utilize compute resources from
cloud vendors without loosing ownership or privacy of their trained models. Our
system protocols are set up to incentivize all three entities - data owners,
cloud vendors, and AI developers to truthfully record their actions on the
distributed ledger, so that the blockchain system provides verifiable evidence
of wrongdoing and dispute resolution. Our system is implemented on the
Hyperledger Fabric and can provide a viable alternative to centralized AI
systems that do not guarantee data or model privacy. We present experimental
performance results that demonstrate the latency and throughput of its
transactions under different network configurations where peers on the
blockchain may be spread across different datacenters and geographies. Our
results indicate that the proposed solution scales well to large number of data
and model owners and can train up to 70 models per second on a 12-peer non
optimized blockchain network and roughly 30 models per second in a 24 peer
network.

AI-based systems are widely employed nowadays to make decisions that have
far-reaching impacts on individuals and society. Their decisions might affect
everyone, everywhere and anytime, entailing concerns about potential human
rights issues. Therefore, it is necessary to move beyond traditional AI
algorithms optimized for predictive performance and embed ethical and legal
principles in their design, training and deployment to ensure social good while
still benefiting from the huge potential of the AI technology. The goal of this
survey is to provide a broad multi-disciplinary overview of the area of bias in
AI systems, focusing on technical challenges and solutions as well as to
suggest new research directions towards approaches well-grounded in a legal
frame. In this survey, we focus on data-driven AI, as a large part of AI is
powered nowadays by (big) data and powerful Machine Learning (ML) algorithms.
If otherwise not specified, we use the general term bias to describe problems
related to the gathering or processing of data that might result in prejudiced
decisions on the bases of demographic features like race, sex, etc.

Aquatic invasive species (AIS) cause significant ecological and economic
damages around the world. A major spread mechanism for AIS is traffic of
boaters transporting their watercraft from invaded to uninvaded waterbodies. To
inhibit the spread of AIS, several Canadian provinces and American states set
up watercraft inspection stations at roadsides, where potentially infested
boats are screened for AIS and, if necessary, decontaminated. However, since
budgets for AIS control are limited, watercraft inspection stations can only be
operated at specific locations and daytimes. Though theoretical studies provide
managers with general guidelines for AIS management, more specific results are
needed to determine when and where watercraft inspections would be most
effective. This is the subject of this paper. We show how linear integer
programming techniques can be used to optimize watercraft inspection policies
under budget constraints. We introduce our approach as a general framework and
apply it to the prevention of the spread of zebra and quagga mussels (Dreissena
spp.) to the Canadian province British Columbia. We consider a variety of
scenarios and show how variations in budget constraints, propagule sources, and
model uncertainty affect the optimal policy. Based on these results, we
identify simple, generally applicable principles for optimal AIS management.

The potential risk of AI systems unintentionally embedding and reproducing
bias has attracted the attention of machine learning practitioners and society
at large. As policy makers are willing to set the standards of algorithms and
AI techniques, the issue on how to refine existing regulation, in order to
enforce that decisions made by automated systems are fair and
non-discriminatory, is again critical. Meanwhile, researchers have demonstrated
that the various existing metrics for fairness are statistically mutually
exclusive and the right choice mostly depends on the use case and the
definition of fairness.
  Recognizing that the solutions for implementing fair AI are not purely
mathematical but require the commitments of the stakeholders to define the
desired nature of fairness, this paper proposes to draft a toolbox which helps
practitioners to ensure fair AI practices. Based on the nature of the
application and the available training data, but also on legal requirements and
ethical, philosophical and cultural dimensions, the toolbox aims to identify
the most appropriate fairness objective. This approach attempts to structure
the complex landscape of fairness metrics and, therefore, makes the different
available options more accessible to non-technical people. In the proven
absence of a silver bullet solution for fair AI, this toolbox intends to
produce the fairest AI systems possible with respect to their local context.

Artificial life originated and has long studied the topic of open-ended
evolution, which seeks the principles underlying artificial systems that
innovate continually, inspired by biological evolution. Recently, interest has
grown within the broader field of AI in a generalization of open-ended
evolution, here called open-ended search, wherein such questions of
open-endedness are explored for advancing AI, whatever the nature of the
underlying search algorithm (e.g. evolutionary or gradient-based). For example,
open-ended search might design new architectures for neural networks, new
reinforcement learning algorithms, or most ambitiously, aim at designing
artificial general intelligence. This paper proposes that open-ended evolution
and artificial life have much to contribute towards the understanding of
open-ended AI, focusing here in particular on the safety of open-ended search.
The idea is that AI systems are increasingly applied in the real world, often
producing unintended harms in the process, which motivates the growing field of
AI safety. This paper argues that open-ended AI has its own safety challenges,
in particular, whether the creativity of open-ended systems can be productively
and predictably controlled. This paper explains how unique safety problems
manifest in open-ended search, and suggests concrete contributions and research
questions to explore them. The hope is to inspire progress towards creative,
useful, and safe open-ended search algorithms.

Explainability is one of the key elements for building trust in AI systems.
Among numerous attempts to make AI explainable, quantifying the effect of
explanations remains a challenge in conducting human-AI collaborative tasks.
Aside from the ability to predict the overall behavior of AI, in many
applications, users need to understand an AI agent's competency in different
aspects of the task domain. In this paper, we evaluate the impact of
explanations on the user's mental model of AI agent competency within the task
of visual question answering (VQA). We quantify users' understanding of
competency, based on the correlation between the actual system performance and
user rankings. We introduce an explainable VQA system that uses spatial and
object features and is powered by the BERT language model. Each group of users
sees only one kind of explanation to rank the competencies of the VQA model.
The proposed model is evaluated through between-subject experiments to probe
explanations' impact on the user's perception of competency. The comparison
between two VQA models shows BERT based explanations and the use of object
features improve the user's prediction of the model's competencies.

In response to calls for greater interdisciplinary involvement from the
social sciences and humanities in the development, governance, and study of
artificial intelligence systems, this paper presents one sociologist's view on
the problem of algorithmic bias and the reproduction of societal bias.
Discussions of bias in AI cover much of the same conceptual terrain that
sociologists studying inequality have long understood using more specific terms
and theories. Concerns over reproducing societal bias should be informed by an
understanding of the ways that inequality is continually reproduced in society
-- processes that AI systems are either complicit in, or can be designed to
disrupt and counter. The contrast presented here is between conservative and
radical approaches to AI, with conservatism referring to dominant tendencies
that reproduce and strengthen the status quo, while radical approaches work to
disrupt systemic forms of inequality. The limitations of conservative
approaches to class, gender, and racial bias are discussed as specific
examples, along with the social structures and processes that biases in these
areas are linked to. Societal issues can no longer be out of scope for AI and
machine learning, given the impact of these systems on human lives. This
requires engagement with a growing body of critical AI scholarship that goes
beyond biased data to analyze structured ways of perpetuating inequality,
opening up the possibility for radical alternatives.

Forecasting AI progress is essential to reducing uncertainty in order to
appropriately plan for research efforts on AI safety and AI governance. While
this is generally considered to be an important topic, little work has been
conducted on it and there is no published document that gives and objective
overview of the field. Moreover, the field is very diverse and there is no
published consensus regarding its direction. This paper describes the
development of a research agenda for forecasting AI progress which utilized the
Delphi technique to elicit and aggregate experts' opinions on what questions
and methods to prioritize. The results of the Delphi are presented; the
remainder of the paper follow the structure of these results, briefly reviewing
relevant literature and suggesting future work for each topic. Experts
indicated that a wide variety of methods should be considered for forecasting
AI progress. Moreover, experts identified salient questions that were both
general and completely unique to the problem of forecasting AI progress. Some
of the highest priority topics include the validation of (partially unresolved)
forecasts, how to make forecasting action-guiding and the quality of different
performance metrics. While statistical methods seem more promising, there is
also recognition that supplementing judgmental techniques can be quite
beneficial.

Recent trends in AI verification and Explainable AI have raised the question
of whether AI planning techniques can be verified. In this paper, we present a
novel resource logic, the Proof Carrying Plans (PCP) logic that can be used to
verify plans produced by AI planners. The PCP logic takes inspiration from
existing resource logics (such as Linear logic and Separation logic) as well as
Hoare logic when it comes to modelling states and resource-aware plan
execution. It also capitalises on the Curry-Howard approach to logics, in its
treatment of plans as functions and plan pre- and post-conditions as types.
This paper presents two main results. From the theoretical perspective, we show
that the PCP logic is sound relative to the standard possible world semantics
used in AI planning. From the practical perspective, we present a complete Agda
formalisation of the PCP logic and of its soundness proof. Moreover, we
showcase the Curry-Howard, or functional, value of this implementation by
supplementing it with the library that parses AI plans into Agda's proofs
automatically. We provide evaluation of this library and the resulting Agda
functions.

Artificial intelligence (AI) and machine learning (ML) have made a paradigm
shift in health care which, eventually can be used for decision support and
forecasting by exploring the medical data. Recent studies showed that AI and ML
can be used to fight against the COVID-19 pandemic. Therefore, the objective of
this review study is to summarize the recent AI and ML based studies that have
focused to fight against COVID-19 pandemic. From an initial set of 634
articles, a total of 35 articles were finally selected through an extensive
inclusion-exclusion process. In our review, we have explored the
objectives/aims of the existing studies (i.e., the role of AI/ML in fighting
COVID-19 pandemic); context of the study (i.e., study focused to a specific
country-context or with a global perspective); type and volume of dataset;
methodology, algorithms or techniques adopted in the prediction or diagnosis
processes; and mapping the algorithms/techniques with the data type
highlighting their prediction/classification accuracy. We particularly focused
on the uses of AI/ML in analyzing the pandemic data in order to depict the most
recent progress of AI for fighting against COVID-19 and pointed out the
potential scope of further research.

For many applications, utilizing DNNs (Deep Neural Networks) requires their
implementation on a target architecture in an optimized manner concerning
energy consumption, memory requirement, throughput, etc. DNN compression is
used to reduce the memory footprint and complexity of a DNN before its
deployment on hardware. Recent efforts to understand and explain AI (Artificial
Intelligence) methods have led to a new research area, termed as explainable
AI. Explainable AI methods allow us to understand better the inner working of
DNNs, such as the importance of different neurons and features. The concepts
from explainable AI provide an opportunity to improve DNN compression methods
such as quantization and pruning in several ways that have not been
sufficiently explored so far. In this paper, we utilize explainable AI methods:
mainly DeepLIFT method. We use these methods for (1) pruning of DNNs; this
includes structured and unstructured pruning of \ac{CNN} filters pruning as
well as pruning weights of fully connected layers, (2) non-uniform quantization
of DNN weights using clustering algorithm; this is also referred to as Weight
Sharing, and (3) integer-based mixed-precision quantization; this is where each
layer of a DNN may use a different number of integer bits. We use typical image
classification datasets with common deep learning image classification models
for evaluation. In all these three cases, we demonstrate significant
improvements as well as new insights and opportunities from the use of
explainable AI in DNN compression.

Artificial Intelligence (AI) applications based on Deep Neural Networks (DNN)
or Deep Learning (DL) have become popular due to their success in solving
problems likeimage analysis and speech recognition. Training a DNN is
computationally intensive and High Performance Computing(HPC) has been a key
driver in AI growth. Virtualisation and container technology have led to the
convergence of cloud and HPC infrastructure. These infrastructures with diverse
hardware increase the complexity of deploying and optimising AI training
workloads. AI training deployments in HPC or cloud can be optimised with
target-specific libraries, graph compilers, andby improving data movement or
IO. Graph compilers aim to optimise the execution of a DNN graph by generating
an optimised code for a target hardware/backend. As part of SODALITE (a Horizon
2020 project), MODAK tool is developed to optimise application deployment in
software defined infrastructures. Using input from the data scientist and
performance modelling, MODAK maps optimal application parameters to a target
infrastructure and builds an optimised container. In this paper, we introduce
MODAK and review container technologies and graph compilers for AI. We
illustrate optimisation of AI training deployments using graph compilers and
Singularity containers. Evaluation using MNIST-CNN and ResNet50 training
workloads shows that custom built optimised containers outperform the official
images from DockerHub. We also found that the performance of graph compilers
depends on the target hardware and the complexity of the neural network.

Self-driving cars and autonomous vehicles are revolutionizing the automotive
sector, shaping the future of mobility altogether. Although the integration of
novel technologies such as Artificial Intelligence (AI) and Cloud/Edge
computing provides golden opportunities to improve autonomous driving
applications, there is the need to modernize accordingly the whole prototyping
and deployment cycle of AI components. This paper proposes a novel framework
for developing so-called AI Inference Engines for autonomous driving
applications based on deep learning modules, where training tasks are deployed
elastically over both Cloud and Edge resources, with the purpose of reducing
the required network bandwidth, as well as mitigating privacy issues. Based on
our proposed data driven V-Model, we introduce a simple yet elegant solution
for the AI components development cycle, where prototyping takes place in the
cloud according to the Software-in-the-Loop (SiL) paradigm, while deployment
and evaluation on the target ECUs (Electronic Control Units) is performed as
Hardware-in-the-Loop (HiL) testing. The effectiveness of the proposed framework
is demonstrated using two real-world use-cases of AI inference engines for
autonomous vehicles, that is environment perception and most probable path
prediction.

This report prepared by the Montreal AI Ethics Institute provides
recommendations in response to the National Security Commission on Artificial
Intelligence (NSCAI) Key Considerations for Responsible Development and
Fielding of Artificial Intelligence document. The report centres on the idea
that Responsible AI should be made the Norm rather than an Exception. It does
so by utilizing the guiding principles of: (1) alleviating friction in existing
workflows, (2) empowering stakeholders to get buy-in, and (3) conducting an
effective translation of abstract standards into actionable engineering
practices. After providing some overarching comments on the document from the
NSCAI, the report dives into the primary contribution of an actionable
framework to help operationalize the ideas presented in the document from the
NSCAI. The framework consists of: (1) a learning, knowledge, and information
exchange (LKIE), (2) the Three Ways of Responsible AI, (3) an
empirically-driven risk-prioritization matrix, and (4) achieving the right
level of complexity. All components reinforce each other to move from
principles to practice in service of making Responsible AI the norm rather than
the exception.

The ability to explain decisions to end-users is a necessity to deploy AI as
critical decision support. Yet making AI explainable to non-technical end-users
is a relatively ignored and challenging problem. To bridge the gap, we first
identify twelve end-user-friendly explanatory forms that do not require
technical knowledge to comprehend, including feature-, example-, and rule-based
explanations. We then instantiate the explanatory forms as prototyping cards in
four AI-assisted critical decision-making tasks, and conduct a user study to
co-design low-fidelity prototypes with 32 layperson participants. The results
confirm the relevance of using explanatory forms as building blocks of
explanations, and identify their proprieties - pros, cons, applicable
explanation goals, and design implications. The explanatory forms, their
proprieties, and prototyping supports (including a suggested prototyping
process, design templates and exemplars, and associated algorithms to actualize
explanatory forms) constitute the End-User-Centered explainable AI framework
EUCA, and is available at http://weinajin.github.io/end-user-xai . It serves as
a practical prototyping toolkit for HCI/AI practitioners and researchers to
understand user requirements and build end-user-centered explainable AI.

General AI system solves a wide range of tasks with high performance in an
automated fashion. The best general AI algorithm designed by one individual is
different from that devised by another. The best performance records achieved
by different users are also different. An inevitable component of general AI is
tacit knowledge that depends upon user-specific comprehension of task
information and individual model design preferences that are related to user
technical experiences. Tacit knowledge affects model performance but cannot be
automatically optimized in general AI algorithms. In this paper, we propose
User-Oriented Smart General AI System under Causal Inference, abbreviated as
UOGASuCI, where UOGAS represents User-Oriented General AI System and uCI means
under the framework of causal inference. User characteristics that have a
significant influence upon tacit knowledge can be extracted from observed model
training experiences of many users in external memory modules. Under the
framework of causal inference, we manage to identify the optimal value of user
characteristics that are connected with the best model performance designed by
users. We make suggestions to users about how different user characteristics
can improve the best model performance achieved by users. By recommending
updating user characteristics associated with individualized tacit knowledge
comprehension and technical preferences, UOGAS helps users design models with
better performance.

Labeling data is an important step in the supervised machine learning
lifecycle. It is a laborious human activity comprised of repeated decision
making: the human labeler decides which of several potential labels to apply to
each example. Prior work has shown that providing AI assistance can improve the
accuracy of binary decision tasks. However, the role of AI assistance in more
complex data-labeling scenarios with a larger set of labels has not yet been
explored. We designed an AI labeling assistant that uses a semi-supervised
learning algorithm to predict the most probable labels for each example. We
leverage these predictions to provide assistance in two ways: (i) providing a
label recommendation and (ii) reducing the labeler's decision space by focusing
their attention on only the most probable labels. We conducted a user study
(n=54) to evaluate an AI-assisted interface for data labeling in this context.
Our results highlight that the AI assistance improves both labeler accuracy and
speed, especially when the labeler finds the correct label in the reduced label
space. We discuss findings related to the presentation of AI assistance and
design implications for intelligent labeling interfaces.

Activists, journalists, and scholars have long raised critical questions
about the relationship between diversity, representation, and structural
exclusions in data-intensive tools and services. We build on work mapping the
emergent landscape of corporate AI ethics to center one outcome of these
conversations: the incorporation of diversity and inclusion in corporate AI
ethics activities. Using interpretive document analysis and analytic tools from
the values in design field, we examine how diversity and inclusion work is
articulated in public-facing AI ethics documentation produced by three
companies that create application and services layer AI infrastructure: Google,
Microsoft, and Salesforce.
  We find that as these documents make diversity and inclusion more tractable
to engineers and technical clients, they reveal a drift away from civil rights
justifications that resonates with the managerialization of diversity by
corporations in the mid-1980s. The focus on technical artifacts, such as
diverse and inclusive datasets, and the replacement of equity with fairness
make ethical work more actionable for everyday practitioners. Yet, they appear
divorced from broader DEI initiatives and other subject matter experts that
could provide needed context to nuanced decisions around how to operationalize
these values. Finally, diversity and inclusion, as configured by engineering
logic, positions firms not as ethics owners but as ethics allocators; while
these companies claim expertise on AI ethics, the responsibility of defining
who diversity and inclusion are meant to protect and where it is relevant is
pushed downstream to their customers.

Despite the immense societal importance of ethically designing artificial
intelligence (AI), little research on the public perceptions of ethical AI
principles exists. This becomes even more striking when considering that
ethical AI development has the aim to be human-centric and of benefit for the
whole society. In this study, we investigate how ethical principles
(explainability, fairness, security, accountability, accuracy, privacy, machine
autonomy) are weighted in comparison to each other. This is especially
important, since simultaneously considering ethical principles is not only
costly, but sometimes even impossible, as developers must make specific
trade-off decisions. In this paper, we give first answers on the relative
importance of ethical principles given a specific use case - the use of AI in
tax fraud detection. The results of a large conjoint survey (n=1099) suggest
that, by and large, German respondents found the ethical principles equally
important. However, subsequent cluster analysis shows that different preference
models for ethically designed systems exist among the German population. These
clusters substantially differ not only in the preferred attributes, but also in
the importance level of the attributes themselves. We further describe how
these groups are constituted in terms of sociodemographics as well as opinions
on AI. Societal implications as well as design challenges are discussed.

As AI systems are integrated into high stakes social domains, researchers now
examine how to design and operate them in a safe and ethical manner. However,
the criteria for identifying and diagnosing safety risks in complex social
contexts remain unclear and contested. In this paper, we examine the vagueness
in debates about the safety and ethical behavior of AI systems. We show how
this vagueness cannot be resolved through mathematical formalism alone, instead
requiring deliberation about the politics of development as well as the context
of deployment. Drawing from a new sociotechnical lexicon, we redefine vagueness
in terms of distinct design challenges at key stages in AI system development.
The resulting framework of Hard Choices in Artificial Intelligence (HCAI)
empowers developers by 1) identifying points of overlap between design
decisions and major sociotechnical challenges; 2) motivating the creation of
stakeholder feedback channels so that safety issues can be exhaustively
addressed. As such, HCAI contributes to a timely debate about the status of AI
development in democratic societies, arguing that deliberation should be the
goal of AI Safety, not just the procedure by which it is ensured.

The beginning of 2020 has seen the emergence of coronavirus outbreak caused
by a novel virus called SARS-CoV-2. The sudden explosion and uncontrolled
worldwide spread of COVID-19 show the limitations of existing healthcare
systems in timely handling public health emergencies. In such contexts,
innovative technologies such as blockchain and Artificial Intelligence (AI)
have emerged as promising solutions for fighting coronavirus epidemic. In
particular, blockchain can combat pandemics by enabling early detection of
outbreaks, ensuring the ordering of medical data, and ensuring reliable medical
supply chain during the outbreak tracing. Moreover, AI provides intelligent
solutions for identifying symptoms caused by coronavirus for treatments and
supporting drug manufacturing. Therefore, we present an extensive survey on the
use of blockchain and AI for combating COVID-19 epidemics. First, we introduce
a new conceptual architecture which integrates blockchain and AI for fighting
COVID-19. Then, we survey the latest research efforts on the use of blockchain
and AI for fighting COVID-19 in various applications. The newly emerging
projects and use cases enabled by these technologies to deal with coronavirus
pandemic are also presented. A case study is also provided using federated AI
for COVID-19 detection. Finally, we point out challenges and future directions
that motivate more research efforts to deal with future coronavirus-like
epidemics.

A total of 34% of AI research and development projects fails or are
abandoned, according to a recent survey by Rackspace Technology of 1,870
companies. We propose a new strategic framework, aiSTROM, that empowers
managers to create a successful AI strategy based on a thorough literature
review. This provides a unique and integrated approach that guides managers and
lead developers through the various challenges in the implementation process.
In the aiSTROM framework, we start by identifying the top n potential projects
(typically 3-5). For each of those, seven areas of focus are thoroughly
analysed. These areas include creating a data strategy that takes into account
unique cross-departmental machine learning data requirements, security, and
legal requirements. aiSTROM then guides managers to think about how to put
together an interdisciplinary artificial intelligence (AI) implementation team
given the scarcity of AI talent. Once an AI team strategy has been established,
it needs to be positioned within the organization, either cross-departmental or
as a separate division. Other considerations include AI as a service (AIaas),
or outsourcing development. Looking at new technologies, we have to consider
challenges such as bias, legality of black-box-models, and keeping humans in
the loop. Next, like any project, we need value-based key performance
indicators (KPIs) to track and validate the progress. Depending on the
company's risk-strategy, a SWOT analysis (strengths, weaknesses, opportunities,
and threats) can help further classify the shortlisted projects. Finally, we
should make sure that our strategy includes continuous education of employees
to enable a culture of adoption. This unique and comprehensive framework offers
a valuable, literature supported, tool for managers and lead developers.

We study the dynamics of the three-dimensional quadratic diffeomorphism using
a concept first introduced thirty years ago for the Frenkel-Kontorova model of
condensed matter physics: the anti-integrable (AI) limit. At the traditional AI
limit, orbits of a map degenerate to sequences of symbols and the dynamics is
reduced to the shift operator, a pure form of chaos. Under nondegeneracy
conditions, a contraction mapping argument can show that infinitely many AI
states continue to orbits of the deterministic map. For the 3D quadratic map,
the AI limit that we study is a quadratic correspondence whose branches, a pair
of one-dimensional maps, introduce symbolic dynamics on two symbols. The AI
states, however, are nontrivial orbits of this correspondence. The character of
these orbits depends on whether the quadratic takes the form of an ellipse, a
hyperbola, or a pair of lines. Using contraction arguments, we find parameter
domains for each case such that each symbol sequence corresponds to a unique AI
state. In some parameter domains, sufficient conditions are then found for each
such AI state to continue away from the limit to become an orbit of the
original 3D map. Numerical continuation methods extend these results, allowing
computation of bifurcations and obtaining orbits with horseshoe-like structures
and intriguing self-similarity. We conjecture that pairs of periodic orbits in
saddle-node or period doubling bifurcations have symbol sequences that differ
in exactly one position.

The recent rapid advancements in artificial intelligence research and
deployment have sparked more discussion about the potential ramifications of
socially- and emotionally-intelligent AI. The question is not if research can
produce such affectively-aware AI, but when it will. What will it mean for
society when machines -- and the corporations and governments they serve -- can
"read" people's minds and emotions? What should developers and operators of
such AI do, and what should they not do? The goal of this article is to
pre-empt some of the potential implications of these developments, and propose
a set of guidelines for evaluating the (moral and) ethical consequences of
affectively-aware AI, in order to guide researchers, industry professionals,
and policy-makers. We propose a multi-stakeholder analysis framework that
separates the ethical responsibilities of AI Developers vis-\`a-vis the
entities that deploy such AI -- which we term Operators. Our analysis produces
two pillars that clarify the responsibilities of each of these stakeholders:
Provable Beneficence, which rests on proving the effectiveness of the AI, and
Responsible Stewardship, which governs responsible collection, use, and storage
of data and the decisions made from such data. We end with recommendations for
researchers, developers, operators, as well as regulators and law-makers.

The field of artificial intelligence (AI), regarded as one of the most
enigmatic areas of science, has witnessed exponential growth in the past decade
including a remarkably wide array of applications, having already impacted our
everyday lives. Advances in computing power and the design of sophisticated AI
algorithms have enabled computers to outperform humans in a variety of tasks,
especially in the areas of computer vision and speech recognition. Yet, AI's
path has never been smooth, having essentially fallen apart twice in its
lifetime ('winters' of AI), both after periods of popular success ('summers' of
AI). We provide a brief rundown of AI's evolution over the course of decades,
highlighting its crucial moments and major turning points from inception to the
present. In doing so, we attempt to learn, anticipate the future, and discuss
what steps may be taken to prevent another 'winter'.

Signal capture stands in the forefront to perceive and understand the
environment and thus imaging plays the pivotal role in mobile vision. Recent
explosive progresses in Artificial Intelligence (AI) have shown great potential
to develop advanced mobile platforms with new imaging devices. Traditional
imaging systems based on the "capturing images first and processing afterwards"
mechanism cannot meet this unprecedented demand. Differently, Computational
Imaging (CI) systems are designed to capture high-dimensional data in an
encoded manner to provide more information for mobile vision systems.Thanks to
AI, CI can now be used in real systems by integrating deep learning algorithms
into the mobile vision platform to achieve the closed loop of intelligent
acquisition, processing and decision making, thus leading to the next
revolution of mobile vision.Starting from the history of mobile vision using
digital cameras, this work first introduces the advances of CI in diverse
applications and then conducts a comprehensive review of current research
topics combining CI and AI. Motivated by the fact that most existing studies
only loosely connect CI and AI (usually using AI to improve the performance of
CI and only limited works have deeply connected them), in this work, we propose
a framework to deeply integrate CI and AI by using the example of self-driving
vehicles with high-speed communication, edge computing and traffic planning.
Finally, we outlook the future of CI plus AI by investigating new materials,
brain science and new computing techniques to shed light on new directions of
mobile vision systems.

Large language models can be used for collaborative storytelling. In this
work we report on using GPT-3 \cite{brown2020language} to co-narrate stories.
The AI system must track plot progression and character arcs while the human
actors perform scenes. This event report details how a novel conversational
agent was employed as creative partner with a team of professional improvisers
to explore long-form spontaneous story narration in front of a live public
audience. We introduced novel constraints on our language model to produce
longer narrative text and tested the model in rehearsals with a team of
professional improvisers. We then field tested the model with two live
performances for public audiences as part of a live theatre festival in Europe.
We surveyed audience members after each performance as well as performers to
evaluate how well the AI performed in its role as narrator. Audiences and
performers responded positively to AI narration and indicated preference for AI
narration over AI characters within a scene. Performers also responded
positively to AI narration and expressed enthusiasm for the creative and
meaningful novel narrative directions introduced to the scenes. Our findings
support improvisational theatre as a useful test-bed to explore how different
language models can collaborate with humans in a variety of social contexts.

Recent developments in AI have made it ubiquitous, every industry is trying
to adopt some form of intelligent processing of their data. Despite so many
advances in the field, AIs full capability is yet to be exploited by the
industry. Industries that involve some risk factors still remain cautious about
the usage of AI due to the lack of trust in such autonomous systems.
Present-day AI might be very good in a lot of things but it is very bad in
reasoning and this behavior of AI can lead to catastrophic results. Autonomous
cars crashing into a person or a drone getting stuck in a tree are a few
examples where AI decisions lead to catastrophic results. To develop insight
and generate an explanation about the learning capability of AI, we will try to
analyze the working of loss functions. For our case, we will use two sets of
loss functions, generalized loss functions like Binary cross-entropy or BCE and
specialized loss functions like Dice loss or focal loss. Through a series of
experiments, we will establish whether combining different loss functions is
better than using a single loss function and if yes, then what is the reason
behind it. In order to establish the difference between generalized loss and
specialized losses, we will train several models using the above-mentioned
losses and then compare their robustness on adversarial examples. In
particular, we will look at how fast the accuracy of different models decreases
when we change the pixels corresponding to the most salient gradients.

Conceptual modeling (CM) applies abstraction to reduce the complexity of a
system under study (e.g., an excerpt of reality). As a result of the conceptual
modeling process a human interpretable, formalized representation (i.e., a
conceptual model) is derived which enables understanding and communication
among humans, and processing by machines. Artificial Intelligence (AI)
algorithms are also applied to complex realities (regularly represented by vast
amounts of data) to identify patterns or to classify entities in the data.
Aside from the commonalities of both approaches, a significant difference can
be observed by looking at the results. While conceptual models are
comprehensible, reproducible, and explicit knowledge representations, AI
techniques are capable of efficiently deriving an output from a given input
while acting as a black box. AI solutions often lack comprehensiveness and
reproducibility. Even the developers of AI systems can't explain why a certain
output is derived. In the Conceptual Modeling meets Artificial Intelligence
(CMAI) workshop, we are interested in tackling the intersection of the two,
thus far, mostly isolated approached disciplines of CM and AI. The workshop
embraces the assumption, that manifold mutual benefits can be realized by i)
investigating what Conceptual Modeling (CM) can contribute to AI, and ii) the
other way around, what Artificial Intelligence (AI) can contribute to CM.

Artificial intelligence (AI) techniques have significant potential to enable
effective, robust and automated image phenotyping including identification of
subtle patterns. AI-based detection searches the image space to find the
regions of interest based on patterns and features. There is a spectrum of
tumor histologies from benign to malignant that can be identified by AI-based
classification approaches using image features. The extraction of minable
information from images gives way to the field of radiomics and can be explored
via explicit (handcrafted/engineered) and deep radiomics frameworks. Radiomics
analysis has the potential to be utilized as a noninvasive technique for the
accurate characterization of tumors to improve diagnosis and treatment
monitoring. This work reviews AI-based techniques, with a special focus on
oncological PET and PET/CT imaging, for different detection, classification,
and prediction/prognosis tasks. We also discuss needed efforts to enable the
translation of AI techniques to routine clinical workflows, and potential
improvements and complementary techniques such as the use of natural language
processing on electronic health records and neuro-symbolic AI techniques.

This paper explores the environmental impact of the super-linear growth
trends for AI from a holistic perspective, spanning Data, Algorithms, and
System Hardware. We characterize the carbon footprint of AI computing by
examining the model development cycle across industry-scale machine learning
use cases and, at the same time, considering the life cycle of system hardware.
Taking a step further, we capture the operational and manufacturing carbon
footprint of AI computing and present an end-to-end analysis for what and how
hardware-software design and at-scale optimization can help reduce the overall
carbon footprint of AI. Based on the industry experience and lessons learned,
we share the key challenges and chart out important development directions
across the many dimensions of AI. We hope the key messages and insights
presented in this paper can inspire the community to advance the field of AI in
an environmentally-responsible manner.

In this work, we explain the setup for a technical, graduate-level course on
Fairness, Accountability, Confidentiality, and Transparency in Artificial
Intelligence (FACT-AI) at the University of Amsterdam, which teaches FACT-AI
concepts through the lens of reproducibility. The focal point of the course is
a group project based on reproducing existing FACT-AI algorithms from top AI
conferences and writing a corresponding report. In the first iteration of the
course, we created an open source repository with the code implementations from
the group projects. In the second iteration, we encouraged students to submit
their group projects to the Machine Learning Reproducibility Challenge,
resulting in 9 reports from our course being accepted for publication in the
ReScience journal. We reflect on our experience teaching the course over two
years, where one year coincided with a global pandemic, and propose guidelines
for teaching FACT-AI through reproducibility in graduate-level AI study
programs. We hope this can be a useful resource for instructors who want to set
up similar courses in the future.

Artificial Intelligence (AI) techniques continue to broaden across
governmental and public sectors, such as power and energy - which serve as
critical infrastructures for most societal operations. However, due to the
requirements of reliability, accountability, and explainability, it is risky to
directly apply AI-based methods to power systems because society cannot afford
cascading failures and large-scale blackouts, which easily cost billions of
dollars. To meet society requirements, this paper proposes a methodology to
develop, deploy, and evaluate AI systems in the energy sector by: (1)
understanding the power system measurements with physics, (2) designing AI
algorithms to forecast the need, (3) developing robust and accountable AI
methods, and (4) creating reliable measures to evaluate the performance of the
AI model. The goal is to provide a high level of confidence to energy utility
users. For illustration purposes, the paper uses power system event forecasting
(PEF) as an example, which carefully analyzes synchrophasor patterns measured
by the Phasor Measurement Units (PMUs). Such a physical understanding leads to
a data-driven framework that reduces the dimensionality with physics and
forecasts the event with high credibility. Specifically, for dimensionality
reduction, machine learning arranges physical information from different
dimensions, resulting inefficient information extraction. For event
forecasting, the supervised learning model fuses the results of different
models to increase the confidence. Finally, comprehensive experiments
demonstrate the high accuracy, efficiency, and reliability as compared to other
state-of-the-art machine learning methods.

In healthcare, the role of AI is continually evolving, and understanding the
challenges its introduction poses on relationships between healthcare providers
and patients will require a regulatory and behavioral approach that can provide
a guiding base for all users involved. In this paper, we present IAC
(Informing, Assessment, and Consent), a framework for evaluating patient
response to the introduction of AI-enabled digital technologies in healthcare
settings. We justify the need for IAC with a general introduction of the
challenges with and perceived relevance of AI in human-welfare-centered fields,
with an emphasis on the provision of healthcare. The framework is composed of
three core principles that guide how healthcare practitioners can inform
patients about the use of AI in their healthcare, how practitioners can assess
patients' acceptability and comfortability with the use of AI, and how patient
consent can be gained after this process. We propose that the principles
composing this framework can be translated into guidelines that improve
practitioner-patient relationships and, concurrently, patient agency regarding
the use of AI in healthcare while broadening the discourse on this topic.

Developing and implementing AI-based solutions help state and federal
government agencies, research institutions, and commercial companies enhance
decision-making processes, automate chain operations, and reduce the
consumption of natural and human resources. At the same time, most AI
approaches used in practice can only be represented as "black boxes" and suffer
from the lack of transparency. This can eventually lead to unexpected outcomes
and undermine trust in such systems. Therefore, it is crucial not only to
develop effective and robust AI systems, but to make sure their internal
processes are explainable and fair. Our goal in this chapter is to introduce
the topic of designing assurance methods for AI systems with high-impact
decisions using the example of the technology sector of the US economy. We
explain how these fields would benefit from revealing cause-effect
relationships between key metrics in the dataset by providing the causal
experiment on technology economics dataset. Several causal inference approaches
and AI assurance techniques are reviewed and the transformation of the data
into a graph-structured dataset is demonstrated.

We use artificial intelligence (AI) to learn and infer the physics of higher
order gravitational wave modes of quasi-circular, spinning, non precessing
binary black hole mergers. We trained AI models using 14 million waveforms,
produced with the surrogate model NRHybSur3dq8, that include modes up to $\ell
\leq 4$ and $(5,5)$, except for $(4,0)$ and $(4,1)$, that describe binaries
with mass-ratios $q\leq8$, individual spins $s^z_{\{1,2\}}\in[-0.8, 0.8]$, and
inclination angle $\theta\in[0,\pi]$.Our probabilistic AI surrogates can
accurately constrain the mass-ratio, individual spins, effective spin, and
inclination angle of numerical relativity waveforms that describe such signal
manifold. We compared the predictions of our AI models with Gaussian process
regression, random forest, k-nearest neighbors, and linear regression, and with
traditional Bayesian inference methods through the PyCBC Inference toolkit,
finding that AI outperforms all these approaches in terms of accuracy, and are
between three to four orders of magnitude faster than traditional Bayesian
inference methods. Our AI surrogates were trained within 3.4 hours using
distributed training on 1,536 NVIDIA V100 GPUs in the Summit supercomputer.

Artificial Intelligence (AI) is making a significant impact in multiple areas
like medical, military, industrial, domestic, law, arts as AI is capable to
perform several roles such as managing smart factories, driving autonomous
vehicles, creating accurate weather forecasts, detecting cancer and personal
assistants, etc. Software testing is the process of putting the software to
test for some abnormal behaviour of the software. Software testing is a
tedious, laborious and most time-consuming process. Automation tools have been
developed that help to automate some activities of the testing process to
enhance quality and timely delivery. Over time with the inclusion of continuous
integration and continuous delivery (CI/CD) pipeline, automation tools are
becoming less effective. The testing community is turning to AI to fill the gap
as AI is able to check the code for bugs and errors without any human
intervention and in a much faster way than humans. In this study, we aim to
recognize the impact of AI technologies on various software testing activities
or facets in the STLC. Further, the study aims to recognize and explain some of
the biggest challenges software testers face while applying AI to testing. The
paper also proposes some key contributions of AI in the future to the domain of
software testing.

In the precision medicine era, there is a growing need for precision
radiotherapy where the planned radiation dose needs to be optimally determined
by considering a myriad of patient-specific information in order to ensure
treatment efficacy. Existing artificial-intelligence (AI) methods can recommend
radiation dose prescriptions within the scope of this available information.
However, treating physicians may not fully entrust the AI's recommended
prescriptions due to known limitations or when the AI recommendation may go
beyond physicians' current knowledge. This paper lays out a systematic method
to integrate expert human knowledge with AI recommendations for optimizing
clinical decision making. Towards this goal, Gaussian process (GP) models are
integrated with deep neural networks (DNNs) to quantify the uncertainty of the
treatment outcomes given by physicians and AI recommendations, respectively,
which are further used as a guideline to educate clinical physicians and
improve AI models performance. The proposed method is demonstrated in a
comprehensive dataset where patient-specific information and treatment outcomes
are prospectively collected during radiotherapy of $67$ non-small cell lung
cancer patients and retrospectively analyzed.

Along with the massive growth of the Internet from the 1990s until now,
various innovative technologies have been created to bring users breathtaking
experiences with more virtual interactions in cyberspace. Many virtual
environments with thousands of services and applications, from social networks
to virtual gaming worlds, have been developed with immersive experience and
digital transformation, but most are incoherent instead of being integrated
into a platform. In this context, metaverse, a term formed by combining meta
and universe, has been introduced as a shared virtual world that is fueled by
many emerging technologies, such as fifth-generation networks and beyond,
virtual reality, and artificial intelligence (AI). Among such technologies, AI
has shown the great importance of processing big data to enhance immersive
experience and enable human-like intelligence of virtual agents. In this
survey, we make a beneficial effort to explore the role of AI in the foundation
and development of the metaverse. We first deliver a preliminary of AI,
including machine learning algorithms and deep learning architectures, and its
role in the metaverse. We then convey a comprehensive investigation of AI-based
methods concerning six technical aspects that have potentials for the
metaverse: natural language processing, machine vision, blockchain, networking,
digital twin, and neural interface, and being potential for the metaverse.
Subsequently, several AI-aided applications, such as healthcare, manufacturing,
smart cities, and gaming, are studied to be deployed in the virtual worlds.
Finally, we conclude the key contribution of this survey and open some future
research directions in AI for the metaverse.

This paper presents a new competition -- at the 2022 IEEE Conference on Games
(CoG) -- called DareFightingICE Competition. The competition has two tracks: a
sound design track and an AI track. The game platform for this competition is
also called DareFightingICE, a fighting game platform. DareFightingICE is a
sound-design-enhanced version of FightingICE, used earlier in a competition at
CoG until 2021 to promote artificial intelligence (AI) research in fighting
games. In the sound design track, participants compete for the best sound
design, given the default sound design of DareFightingICE as a sample, where we
define a sound design as a set of sound effects combined with the source code
that implements their timing-control algorithm. Participants of the AI track
are asked to develop their AI algorithm that controls a character given only
sound as the input (blind AI) to fight against their opponent; a sample
deep-learning blind AI will be provided by us. Our means to maximize the
synergy between the two tracks are also described. This competition serves to
come up with effective sound designs for visually impaired players, a group in
the gaming community which has been mostly ignored. To the best of our
knowledge, DareFightingICE Competition is the first of its kind within and
outside of CoG.

In the future, most companies will be confronted with the topic of Artificial
Intelligence (AI) and will have to decide on their strategy in this regards.
Currently, a lot of companies are thinking about whether and how AI and the
usage of data will impact their business model and what potential use cases
could look like. One of the biggest challenges lies in coming up with
innovative solution ideas with a clear business value. This requires business
competencies on the one hand and technical competencies in AI and data
analytics on the other hand. In this article, we present the concept of AI
innovation labs and demonstrate a comprehensive framework, from coming up with
the right ideas to incrementally implementing and evaluating them regarding
their business value and their feasibility based on a company's capabilities.
The concept is the result of nine years of working on data-driven innovations
with companies from various domains. Furthermore, we share some lessons learned
from its practical applications. Even though a lot of technical publications
can be found in the literature regarding the development of AI models and many
consultancy companies provide corresponding services for building AI
innovations, we found very few publications sharing details about what an
end-to-end framework could look like.

What explains the dramatic progress from 20th-century to 21st-century AI, and
how can the remaining limitations of current AI be overcome? The widely
accepted narrative attributes this progress to massive increases in the
quantity of computational and data resources available to support statistical
learning in deep artificial neural networks. We show that an additional crucial
factor is the development of a new type of computation. Neurocompositional
computing adopts two principles that must be simultaneously respected to enable
human-level cognition: the principles of Compositionality and Continuity. These
have seemed irreconcilable until the recent mathematical discovery that
compositionality can be realized not only through discrete methods of symbolic
computing, but also through novel forms of continuous neural computing. The
revolutionary recent progress in AI has resulted from the use of limited forms
of neurocompositional computing. New, deeper forms of neurocompositional
computing create AI systems that are more robust, accurate, and comprehensible.

The pervasiveness of Artificial Intelligence (AI) is unquestionable in our
society. Even in the arts, AI is present. A notorious case is the song "Hey
Ya!" of the OutKast group, successful in the 2000s. At this time, the music
industry began to make decisions based on data to strategize based on
predictions of listeners' habits. This case is just one of the countless
examples of AI applications in the arts. The advent of deep learning made it
possible to build systems capable of accurately recognizing artistic style in
paintings. Content generation is also possible; for example, Deepart customizes
images from two \textit{inputs}: 1) an image to be customized; 2) a style of
painting. The generation of songs according to specific styles from AI-based
systems is also possible. Such possibilities raise questions about the
intellectual property of such works. On this occasion, who owns the copyright
of a work produced from a system based on Artificial Intelligence? To the
creator of the AI? The company/corporation that subsidized the development of
this system? Or AI itself as a creator? This essay aims to contribute with a
technicist view on the discussion of copyright applicability from works
produced by AI.

Organisations can use artificial intelligence to make decisions about people
for a variety of reasons, for instance, to select the best candidates from many
job applications. However, AI systems can have discriminatory effects when used
for decision-making. To illustrate, an AI system could reject applications of
people with a certain ethnicity, while the organisation did not plan such
ethnicity discrimination. But in Europe, an organisation runs into a problem
when it wants to assess whether its AI system accidentally discriminates based
on ethnicity: the organisation may not know the applicants' ethnicity. In
principle, the GDPR bans the use of certain 'special categories of data'
(sometimes called 'sensitive data'), which include data on ethnicity, religion,
and sexual preference. The proposal for an AI Act of the European Commission
includes a provision that would enable organisations to use special categories
of data for auditing their AI systems. This paper asks whether the GDPR's rules
on special categories of personal data hinder the prevention of AI-driven
discrimination. We argue that the GDPR does prohibit such use of special
category data in many circumstances. We also map out the arguments for and
against creating an exception to the GDPR's ban on using special categories of
personal data, to enable preventing discrimination by AI systems. The paper
discusses European law, but the paper can be relevant outside Europe too, as
many policymakers in the world grapple with the tension between privacy and
non-discrimination policy.

The cloud-based solutions are becoming inefficient due to considerably large
time delays, high power consumption, security and privacy concerns caused by
billions of connected wireless devices and typically zillions bytes of data
they produce at the network edge. A blend of edge computing and Artificial
Intelligence (AI) techniques could optimally shift the resourceful computation
servers closer to the network edge, which provides the support for advanced AI
applications (e.g., video/audio surveillance and personal recommendation
system) by enabling intelligent decision making on computing at the point of
data generation as and when it is needed, and distributed Machine Learning (ML)
with its potential to avoid the transmission of large dataset and possible
compromise of privacy that may exist in cloud-based centralized learning.
Therefore, AI is envisioned to become native and ubiquitous in future
communication and networking systems. In this paper, we conduct a comprehensive
overview of recent advances in distributed intelligence in wireless networks
under the umbrella of native-AI wireless networks, with a focus on the basic
concepts of native-AI wireless networks, on the AI-enabled edge computing, on
the design of distributed learning architectures for heterogeneous networks, on
the communication-efficient technologies to support distributed learning, and
on the AI-empowered end-to-end communications. We highlight the advantages of
hybrid distributed learning architectures compared to the state-of-art
distributed learning techniques. We summarize the challenges of existing
research contributions in distributed intelligence in wireless networks and
identify the potential future opportunities.

The data revolution has generated a huge demand for data-driven solutions.
This demand propels a growing number of easy-to-use tools and training for
aspiring data scientists that enable the rapid building of predictive models.
Today, weapons of math destruction can be easily built and deployed without
detailed planning and validation. This rapidly extends the list of AI failures,
i.e. deployments that lead to financial losses or even violate democratic
values such as equality, freedom and justice. The lack of planning, rules and
standards around the model development leads to the ,,anarchisation of AI".
This problem is reported under different names such as validation debt,
reproducibility crisis, and lack of explainability. Post-mortem analysis of AI
failures often reveals mistakes made in the early phase of model development or
data acquisition. Thus, instead of curing the consequences of deploying harmful
models, we shall prevent them as early as possible by putting more attention to
the initial planning stage.
  In this paper, we propose a quick and simple framework to support planning of
AI solutions. The POCA framework is based on four pillars: Performance,
Opaqueness, Consequences, and Assumptions. It helps to set the expectations and
plan the constraints for the AI solution before any model is built and any data
is collected. With the help of the POCA method, preliminary requirements can be
defined for the model-building process, so that costly model misspecification
errors can be identified as soon as possible or even avoided. AI researchers,
product owners and business analysts can use this framework in the initial
stages of building AI solutions.

In this work, we study the effects of feature-based explanations on
distributive fairness of AI-assisted decisions, specifically focusing on the
task of predicting occupations from short textual bios. We also investigate how
any effects are mediated by humans' fairness perceptions and their reliance on
AI recommendations. Our findings show that explanations influence fairness
perceptions, which, in turn, relate to humans' tendency to adhere to AI
recommendations. However, we see that such explanations do not enable humans to
discern correct and incorrect AI recommendations. Instead, we show that they
may affect reliance irrespective of the correctness of AI recommendations.
Depending on which features an explanation highlights, this can foster or
hinder distributive fairness: when explanations highlight features that are
task-irrelevant and evidently associated with the sensitive attribute, this
prompts overrides that counter AI recommendations that align with gender
stereotypes. Meanwhile, if explanations appear task-relevant, this induces
reliance behavior that reinforces stereotype-aligned errors. These results
imply that feature-based explanations are not a reliable mechanism to improve
distributive fairness.

The evolution of network virtualization and native artificial intelligence
(AI) paradigms have conceptualized the vision of future wireless networks as a
comprehensive entity operating in whole over a digital platform, with smart
interaction with the physical domain, paving the way for the blooming of the
Digital Twin (DT) concept. The recent interest in the DT networks is fueled by
the emergence of novel wireless technologies and use-cases, that exacerbate the
level of complexity to orchestrate the network and to manage its resources.
Driven by AI, the key principle of the DT is to create a virtual twin for the
physical entities and network dynamics, where the virtual twin will be
leveraged to generate synthetic data and offer an on-demand platform for AI
model training. Despite the common understanding that AI is the seed for DT, we
anticipate that the DT and AI will be enablers for each other, in a way that
overcome their limitations and complement each other benefits. In this article,
we dig into the fundamentals of DT, where we reveal the role of DT in unifying
model-driven and data-driven approaches, and explore the opportunities offered
by DT in order to achieve the optimistic vision of 6G networks. We further
unfold the essential role of the theoretical underpinnings in unlocking further
opportunities by AI, and hence, we unveil their pivotal impact on the
realization of reliable, efficient, and low-latency DT.

As AI becomes more prevalent throughout society, effective methods of
integrating humans and AI systems that leverage their respective strengths and
mitigate risk have become an important priority. In this paper, we introduce
the paradigm of super reinforcement learning that takes advantage of Human-AI
interaction for data driven sequential decision making. This approach utilizes
the observed action, either from AI or humans, as input for achieving a
stronger oracle in policy learning for the decision maker (humans or AI). In
the decision process with unmeasured confounding, the actions taken by past
agents can offer valuable insights into undisclosed information. By including
this information for the policy search in a novel and legitimate manner, the
proposed super reinforcement learning will yield a super-policy that is
guaranteed to outperform both the standard optimal policy and the behavior one
(e.g., past agents' actions). We call this stronger oracle a blessing from
human-AI interaction. Furthermore, to address the issue of unmeasured
confounding in finding super-policies using the batch data, a number of
nonparametric and causal identifications are established. Building upon on
these novel identification results, we develop several super-policy learning
algorithms and systematically study their theoretical properties such as
finite-sample regret guarantee. Finally, we illustrate the effectiveness of our
proposal through extensive simulations and real-world applications.

A tool that could suggest new personalized research directions and ideas by
taking insights from the scientific literature could significantly accelerate
the progress of science. A field that might benefit from such an approach is
artificial intelligence (AI) research, where the number of scientific
publications has been growing exponentially over the last years, making it
challenging for human researchers to keep track of the progress. Here, we use
AI techniques to predict the future research directions of AI itself. We
develop a new graph-based benchmark based on real-world data -- the
Science4Cast benchmark, which aims to predict the future state of an evolving
semantic network of AI. For that, we use more than 100,000 research papers and
build up a knowledge network with more than 64,000 concept nodes. We then
present ten diverse methods to tackle this task, ranging from pure statistical
to pure learning methods. Surprisingly, the most powerful methods use a
carefully curated set of network features, rather than an end-to-end AI
approach. It indicates a great potential that can be unleashed for purely ML
approaches without human knowledge. Ultimately, better predictions of new
future research directions will be a crucial component of more advanced
research suggestion tools.

It is ten years since neural networks made their spectacular comeback.
Prompted by this anniversary, we take a holistic perspective on Artificial
Intelligence (AI). Supervised Learning for cognitive tasks is effectively
solved - provided we have enough high-quality labeled data. However, deep
neural network models are not easily interpretable, and thus the debate between
blackbox and whitebox modeling has come to the fore. The rise of attention
networks, self-supervised learning, generative modeling, and graph neural
networks has widened the application space of AI. Deep Learning has also
propelled the return of reinforcement learning as a core building block of
autonomous decision making systems. The possible harms made possible by new AI
technologies have raised socio-technical issues such as transparency, fairness,
and accountability. The dominance of AI by Big-Tech who control talent,
computing resources, and most importantly, data may lead to an extreme AI
divide. Failure to meet high expectations in high profile, and much heralded
flagship projects like self-driving vehicles could trigger another AI winter.

The shift from symbolic AI systems to black-box, sub-symbolic, and
statistical ones has motivated a rapid increase in the interest toward
explainable AI (XAI), i.e. approaches to make black-box AI systems explainable
to human decision makers with the aim of making these systems more acceptable
and more usable tools and supports. However, we make the point that, rather
than always making black boxes transparent, these approaches are at risk of
\emph{painting the black boxes white}, thus failing to provide a level of
transparency that would increase the system's usability and comprehensibility;
or, even, at risk of generating new errors, in what we termed the
\emph{white-box paradox}. To address these usability-related issues, in this
work we focus on the cognitive dimension of users' perception of explanations
and XAI systems. To this aim, we designed and conducted a questionnaire-based
experiment by which we involved 44 cardiology residents and specialists in an
AI-supported ECG reading task. In doing so, we investigated different research
questions concerning the relationship between users' characteristics (e.g.
expertise) and their perception of AI and XAI systems, including their trust,
the perceived explanations' quality and their tendency to defer the decision
process to automation (i.e. technology dominance), as well as the mutual
relationships among these different dimensions. Our findings provide a
contribution to the evaluation of AI-based support systems from a Human-AI
interaction-oriented perspective and lay the ground for further investigation
of XAI and its effects on decision making and user experience.

Card game AI has always been a hot topic in the research of artificial
intelligence. In recent years, complex card games such as Mahjong, DouDizhu and
Texas Hold'em have been solved and the corresponding AI programs have reached
the level of human experts. In this paper, we are devoted to developing an AI
program for a more complex card game, GuanDan, whose rules are similar to
DouDizhu but much more complicated. To be specific, the characteristics of
large state and action space, long length of one episode and the unsure number
of players in the GuanDan pose great challenges for the development of the AI
program. To address these issues, we propose the first AI program DanZero for
GuanDan using reinforcement learning technique. Specifically, we utilize a
distributed framework to train our AI system. In the actor processes, we
carefully design the state features and agents generate samples by self-play.
In the learner process, the model is updated by Deep Monte-Carlo Method. After
training for 30 days using 160 CPUs and 1 GPU, we get our DanZero bot. We
compare it with 8 baseline AI programs which are based on heuristic rules and
the results reveal the outstanding performance of DanZero. We also test DanZero
with human players and demonstrate its human-level performance.

Data wrangling tasks such as obtaining and linking data from various sources,
transforming data formats, and correcting erroneous records, can constitute up
to 80% of typical data engineering work. Despite the rise of machine learning
and artificial intelligence, data wrangling remains a tedious and manual task.
We introduce AI assistants, a class of semi-automatic interactive tools to
streamline data wrangling. An AI assistant guides the analyst through a
specific data wrangling task by recommending a suitable data transformation
that respects the constraints obtained through interaction with the analyst.
  We formally define the structure of AI assistants and describe how existing
tools that treat data cleaning as an optimization problem fit the definition.
We implement AI assistants for four common data wrangling tasks and make AI
assistants easily accessible to data analysts in an open-source notebook
environment for data science, by leveraging the common structure they follow.
We evaluate our AI assistants both quantitatively and qualitatively through
three example scenarios. We show that the unified and interactive design makes
it easy to perform tasks that would be difficult to do manually or with a fully
automatic tool.

As AI algorithms increasingly participate in daily activities that used to be
the sole province of humans, we are inevitably called upon to consider how much
machines are really like us. To address this question, we turn to the Turing
test and systematically benchmark current AIs in their abilities to imitate
humans. We establish a methodology to evaluate humans versus machines in
Turing-like tests and systematically evaluate a representative set of selected
domains, parameters, and variables. The experiments involved testing 769 human
agents, 24 state-of-the-art AI agents, 896 human judges, and 8 AI judges, in
21,570 Turing tests across 6 tasks encompassing vision and language modalities.
Surprisingly, the results reveal that current AIs are not far from being able
to impersonate human judges across different ages, genders, and educational
levels in complex visual and language challenges. In contrast, simple AI judges
outperform human judges in distinguishing human answers versus machine answers.
The curated large-scale Turing test datasets introduced here and their
evaluation metrics provide valuable insights to assess whether an agent is
human or not. The proposed formulation to benchmark human imitation ability in
current AIs paves a way for the research community to expand Turing tests to
other research areas and conditions. All of source code and data are publicly
available at https://tinyurl.com/8x8nha7p

Machine learning as a service (MLaaS) framework provides intelligent services
or well-trained artificial intelligence (AI) models for local devices. However,
in the process of model transmission and deployment, there are security issues,
i.e. AI model leakage due to the unreliable transmission environments and
illegal abuse at local devices without permission. Although existing works
study the intellectual property (IP) protection of AI models, they mainly focus
on the watermark-based and encryption-based methods and have the following
problems: (i) The watermark-based methods only provide passive verification
afterward rather than active protection. (ii) Encryption-based methods are low
efficiency in computation and low security in key storage. (iii) The existing
methods are not device-bind without the ability to avoid illegal abuse of AI
models. To deal with these problems, we propose a device-bind and
key-storageless hardware AI model IP protection mechanism. First, a physical
unclonable function (PUF) and permute-diffusion encryption-based AI model
protection framework is proposed, including the PUF-based secret key generation
and the geometric-value transformation-based weights encryption. Second, we
design a PUF-based key generation protocol, where delay-based Anderson PUF is
adopted to generate the derive-bind secret key. Besides, convolutional coding
and convolutional interleaving technologies are combined to improve the
stability of PUF-based key generation and reconstruction. Third, a permute and
diffusion-based intelligent model weights encryption/decryption method is
proposed to achieve effective IP protection, where chaos theory is utilized to
convert the PUF-based secret key to encryption/decryption keys. Finally,
experimental evaluation demonstrates the effectiveness of the proposed
intelligent model IP protection mechanism.

Machine learning is the science of credit assignment: finding patterns in
observations that predict the consequences of actions and help to improve
future performance. Credit assignment is also required for human understanding
of how the world works, not only for individuals navigating daily life, but
also for academic professionals like historians who interpret the present in
light of past events. Here I focus on the history of modern artificial
intelligence (AI) which is dominated by artificial neural networks (NNs) and
deep learning, both conceptually closer to the old field of cybernetics than to
what's been called AI since 1956 (e.g., expert systems and logic programming).
A modern history of AI will emphasize breakthroughs outside of the focus of
traditional AI text books, in particular, mathematical foundations of today's
NNs such as the chain rule (1676), the first NNs (linear regression, circa
1800), and the first working deep learners (1965-). From the perspective of
2022, I provide a timeline of the -- in hindsight -- most important relevant
events in the history of NNs, deep learning, AI, computer science, and
mathematics in general, crediting those who laid foundations of the field. The
text contains numerous hyperlinks to relevant overview sites from my AI Blog.
It supplements my previous deep learning survey (2015) which provides hundreds
of additional references. Finally, to round it off, I'll put things in a
broader historic context spanning the time since the Big Bang until when the
universe will be many times older than it is now.

Artificial Intelligence is one of the most significant and prominent
technological innovations which has reshaped all aspects of human life on the
lines of ease from magnitudes like shopping, data collection, driving, everyday
life, medical approach and many more. On the contrary, although recent
developments in both subjects that are backed by technology, progress on AI
alongside CE must have mostly been undertaken in isolation, providing little
understanding into how the two areas intersect. Artificial intelligence is now
widely used in services, from back-office tasks to front-line interactions with
customers. This trend has accelerated in recent years. Artificial intelligence
(AI)-based virtual assistants are changing successful engagement away from
being dominated by humans and toward being dominated by technologies. As a
result, people are expected to solve their own problems before calling customer
care representatives, eventually emerging as a crucial component of providing
services as value co-creators. AI-powered chats may potentially go awry, which
could enrage, perplex, and anger customers. Considering all these, the main
objectives of this study will engage the following
  1. To identify the alterations in the scope of human searches for information
offered by the application of AI?
  2. To analyse how AI helps in the way someone drives the car
  3. To evaluate how AI has changed the way customer interact with the
customers

The highest grossing media franchise of all times, with over \$90 billion in
total revenue, is Pokemon. The video games belong to the class of Japanese Role
Playing Games (J-RPG). Developing a powerful AI agent for these games is very
hard because they present big challenges to MinMax, Monte Carlo Tree Search and
statistical Machine Learning, as they are vastly different from the well
explored in AI literature games. An AI agent for one of these games means
significant progress in AI agents for the entire class. Further, the key
principles of such work can hopefully inspire approaches to several domains
that require excellent teamwork under conditions of extreme uncertainty,
including managing a team of doctors, robots or employees in an ever changing
environment, like a pandemic stricken region or a war-zone. In this paper we
first explain the mechanics of the game and we perform a game analysis. We
continue by proposing unique AI algorithms based on our understanding that the
two biggest challenges in the game are keeping a balanced team and dealing with
three sources of uncertainty. Later on, we describe why evaluating the
performance of such agents is challenging and we present the results of our
approach. Our AI agent performed significantly better than all previous
attempts and peaked at the 33rd place in the world, in one of the most popular
battle formats, while running on only 4 single socket servers.

Building large AI fleets to support the rapidly growing DL workloads is an
active research topic for modern cloud providers. Generating accurate
benchmarks plays an essential role in designing the fast-paced software and
hardware solutions in this space. Two fundamental challenges to make this
scalable are (i) workload representativeness and (ii) the ability to quickly
incorporate changes to the fleet into the benchmarks.
  To overcome these issues, we propose Mystique, an accurate and scalable
framework for production AI benchmark generation. It leverages the PyTorch
execution trace (ET), a new feature that captures the runtime information of AI
models at the granularity of operators, in a graph format, together with their
metadata. By sourcing fleet ETs, we can build AI benchmarks that are portable
and representative. Mystique is scalable, due to its lightweight data
collection, in terms of runtime overhead and instrumentation effort. It is also
adaptive because ET composability allows flexible control on benchmark
creation.
  We evaluate our methodology on several production AI models, and show that
benchmarks generated with Mystique closely resemble original AI models, both in
execution time and system-level metrics. We also showcase the portability of
the generated benchmarks across platforms, and demonstrate several use cases
enabled by the fine-grained composability of the execution trace.

In order to develop trustworthy healthcare artificial intelligence (AI)
prospective and ergonomics studies that consider the complexity and reality of
real-world applications of AI systems are needed. To achieve this, technology
developers and deploying organisations need to form collaborative partnerships.
This entails access to healthcare data, which frequently might also include
potentially identifiable data such as audio recordings of calls made to an
ambulance service call centre. Information Governance (IG) processes have been
put in place to govern the use of personal confidential data. However,
navigating IG processes in the formative stages of AI development and
pre-deployment can be challenging, because the legal basis for data sharing is
explicit only for the purpose of delivering patient care, i.e., once a system
is put into service. In this paper we describe our experiences of managing IG
for the assurance of healthcare AI, using the example of an
out-of-hospital-cardiac-arrest recognition software within the context of the
Welsh Ambulance Service. We frame IG as a socio-technical process. IG processes
for the development of trustworthy healthcare AI rely on information governance
work, which entails dialogue, negotiation, and trade-offs around the legal
basis for data sharing, data requirements and data control. Information
governance work should start early in the design life cycle and will likely
continue throughout. This includes a focus on establishing and building
relationships, as well as a focus on organisational readiness deeper
understanding of both AI technologies as well as their safety assurance
requirements.

The adoption of artificial intelligence (AI) in healthcare is growing
rapidly. Remote patient monitoring (RPM) is one of the common healthcare
applications that assist doctors to monitor patients with chronic or acute
illness at remote locations, elderly people in-home care, and even hospitalized
patients. The reliability of manual patient monitoring systems depends on staff
time management which is dependent on their workload. Conventional patient
monitoring involves invasive approaches which require skin contact to monitor
health status. This study aims to do a comprehensive review of RPM systems
including adopted advanced technologies, AI impact on RPM, challenges and
trends in AI-enabled RPM. This review explores the benefits and challenges of
patient-centric RPM architectures enabled with Internet of Things wearable
devices and sensors using the cloud, fog, edge, and blockchain technologies.
The role of AI in RPM ranges from physical activity classification to chronic
disease monitoring and vital signs monitoring in emergency settings. This
review results show that AI-enabled RPM architectures have transformed
healthcare monitoring applications because of their ability to detect early
deterioration in patients' health, personalize individual patient health
parameter monitoring using federated learning, and learn human behavior
patterns using techniques such as reinforcement learning. This review discusses
the challenges and trends to adopt AI to RPM systems and implementation issues.
The future directions of AI in RPM applications are analyzed based on the
challenges and trends

As artificial intelligence research advances, the platforms used to evaluate
AI agents need to adapt and grow to continue to challenge them. We present the
Polycraft World AI Lab (PAL), a task simulator with an API based on the
Minecraft mod Polycraft World. Our platform is built to allow AI agents with
different architectures to easily interact with the Minecraft world, train and
be evaluated in multiple tasks. PAL enables the creation of tasks in a flexible
manner as well as having the capability to manipulate any aspect of the task
during an evaluation. All actions taken by AI agents and external actors
(non-player-characters, NPCs) in the open-world environment are logged to
streamline evaluation. Here we present two custom tasks on the PAL platform,
one focused on multi-step planning and one focused on navigation, and
evaluations of agents solving them. In summary, we report a versatile and
extensible AI evaluation platform with a low barrier to entry for AI
researchers to utilize.

AMD Xilinx's new Versal Adaptive Compute Acceleration Platform (ACAP) is an
FPGA architecture combining reconfigurable fabric with other on-chip hardened
compute resources. AI engines are one of these and, by operating in a highly
vectorized manner, they provide significant raw compute that is potentially
beneficial for a range of workloads including HPC simulation. However, this
technology is still early-on, and as yet unproven for accelerating HPC codes,
with a lack of benchmarking and best practice.
  This paper presents an experience report, exploring porting of the Piacsek
and Williams (PW) advection scheme onto the Versal ACAP, using the chip's AI
engines to accelerate the compute. A stencil-based algorithm, advection is
commonplace in atmospheric modelling, including several Met Office codes who
initially developed this scheme. Using this algorithm as a vehicle, we explore
optimal approaches for structuring AI engine compute kernels and how best to
interface the AI engines with programmable logic. Evaluating performance using
a VCK5000 against non-AI engine FPGA configurations on the VCK5000 and Alveo
U280, as well as a 24-core Xeon Platinum Cascade Lake CPU and Nvidia V100 GPU,
we found that whilst the number of channels between the fabric and AI engines
are a limitation, by leveraging the ACAP we can double performance compared to
an Alveo U280.

Challenging the Nvidia monopoly, dedicated AI-accelerator chips have begun
emerging for tackling the computational challenge that the inference and,
especially, the training of modern deep neural networks (DNNs) poses to modern
computers. The field has been ridden with studies assessing the performance of
these contestants across various DNN model types. However, AI-experts are aware
of the limitations of current DNNs and have been working towards the fourth AI
wave which will, arguably, rely on more biologically inspired models,
predominantly on spiking neural networks (SNNs). At the same time, GPUs have
been heavily used for simulating such models in the field of computational
neuroscience, yet AI-chips have not been tested on such workloads. The current
paper aims at filling this important gap by evaluating multiple, cutting-edge
AI-chips (Graphcore IPU, GroqChip, Nvidia GPU with Tensor Cores and Google TPU)
on simulating a highly biologically detailed model of a brain region, the
inferior olive (IO). This IO application stress-tests the different
AI-platforms for highlighting architectural tradeoffs by varying its compute
density, memory requirements and floating-point numerical accuracy. Our
performance analysis reveals that the simulation problem maps extremely well
onto the GPU and TPU architectures, which for networks of 125,000 cells leads
to a 28x respectively 1,208x speedup over CPU runtimes. At this speed, the TPU
sets a new record for largest real-time IO simulation. The GroqChip outperforms
both platforms for small networks but, due to implementing some floating-point
operations at reduced accuracy, is found not yet usable for brain simulation.

Non-Terrestrial Networks (NTN) are expected to be a critical component of 6th
Generation (6G) networks, providing ubiquitous, continuous, and scalable
services. Satellites emerge as the primary enabler for NTN, leveraging their
extensive coverage, stable orbits, scalability, and adherence to international
regulations. However, satellite-based NTN presents unique challenges, including
long propagation delay, high Doppler shift, frequent handovers, spectrum
sharing complexities, and intricate beam and resource allocation, among others.
The integration of NTNs into existing terrestrial networks in 6G introduces a
range of novel challenges, including task offloading, network routing, network
slicing, and many more. To tackle all these obstacles, this paper proposes
Artificial Intelligence (AI) as a promising solution, harnessing its ability to
capture intricate correlations among diverse network parameters. We begin by
providing a comprehensive background on NTN and AI, highlighting the potential
of AI techniques in addressing various NTN challenges. Next, we present an
overview of existing works, emphasizing AI as an enabling tool for
satellite-based NTN, and explore potential research directions. Furthermore, we
discuss ongoing research efforts that aim to enable AI in satellite-based NTN
through software-defined implementations, while also discussing the associated
challenges. Finally, we conclude by providing insights and recommendations for
enabling AI-driven satellite-based NTN in future 6G networks.

How will superhuman artificial intelligence (AI) affect human decision
making? And what will be the mechanisms behind this effect? We address these
questions in a domain where AI already exceeds human performance, analyzing
more than 5.8 million move decisions made by professional Go players over the
past 71 years (1950-2021). To address the first question, we use a superhuman
AI program to estimate the quality of human decisions across time, generating
58 billion counterfactual game patterns and comparing the win rates of actual
human decisions with those of counterfactual AI decisions. We find that humans
began to make significantly better decisions following the advent of superhuman
AI. We then examine human players' strategies across time and find that novel
decisions (i.e., previously unobserved moves) occurred more frequently and
became associated with higher decision quality after the advent of superhuman
AI. Our findings suggest that the development of superhuman AI programs may
have prompted human players to break away from traditional strategies and
induced them to explore novel moves, which in turn may have improved their
decision-making.

Artificial intelligence (AI) systems will increasingly be used to cause harm
as they grow more capable. In fact, AI systems are already starting to be used
to automate fraudulent activities, violate human rights, create harmful fake
images, and identify dangerous toxins. To prevent some misuses of AI, we argue
that targeted interventions on certain capabilities will be warranted. These
restrictions may include controlling who can access certain types of AI models,
what they can be used for, whether outputs are filtered or can be traced back
to their user, and the resources needed to develop them. We also contend that
some restrictions on non-AI capabilities needed to cause harm will be required.
Though capability restrictions risk reducing use more than misuse (facing an
unfavorable Misuse-Use Tradeoff), we argue that interventions on capabilities
are warranted when other interventions are insufficient, the potential harm
from misuse is high, and there are targeted ways to intervene on capabilities.
We provide a taxonomy of interventions that can reduce AI misuse, focusing on
the specific steps required for a misuse to cause harm (the Misuse Chain), and
a framework to determine if an intervention is warranted. We apply this
reasoning to three examples: predicting novel toxins, creating harmful images,
and automating spear phishing campaigns.

This paper presents a new communication interface for the DareFightingICE
platform, a Java-based fighting game focused on implementing AI for controlling
a non-player character. The interface uses an open-source remote procedure
call, gRPC to improve the efficiency of data transfer between the game and the
AI, reducing the time spent on receiving information from the game server. This
is important because the main challenge of implementing AI in a fighting game
is the need for the AI to select an action to perform within a short response
time. The DareFightingICE platform has been integrated with Py4J, allowing
developers to create AIs using Python. However, Py4J is less efficient at
handling large amounts of data, resulting in excessive latency. In contrast,
gRPC is well-suited for transmitting large amounts of data. To evaluate the
effectiveness of the new communication interface, we conducted an experiment
comparing the latency of gRPC and Py4J, using a rule-based AI that sends a kick
command regardless of the information received from the game server. The
experiment results showed not only a 65\% reduction in latency but also
improved stability and eliminated missed frames compared to the current
interface.

Oversight is rightly recognised as vital within high-stakes public sector AI
applications, where decisions can have profound individual and collective
impacts. Much current thinking regarding forms of oversight mechanisms for AI
within the public sector revolves around the idea of human decision makers
being 'in-the-loop' and thus being able to intervene to prevent errors and
potential harm. However, in a number of high-stakes public sector contexts,
operational oversight of decisions is made by expert teams rather than
individuals. The ways in which deployed AI systems can be integrated into these
existing operational team oversight processes has yet to attract much
attention. We address this gap by exploring the impacts of AI upon pre-existing
oversight of clinical decision-making through institutional analysis. We find
that existing oversight is nested within professional training requirements and
relies heavily upon explanation and questioning to elicit vital information.
Professional bodies and liability mechanisms also act as additional levers of
oversight. These dimensions of oversight are impacted, and potentially
reconfigured, by AI systems. We therefore suggest a broader lens of
'team-in-the-loop' to conceptualise the system-level analysis required for
adoption of AI within high-stakes public sector deployment.

ChatGPT has enabled access to AI-generated writing for the masses, and within
just a few months, this product has disrupted the knowledge economy, initiating
a culture shift in the way people work, learn, and write. The need to
discriminate human writing from AI is now both critical and urgent,
particularly in domains like higher education and academic writing, where AI
had not been a significant threat or contributor to authorship. Addressing this
need, we developed a method for discriminating text generated by ChatGPT from
(human) academic scientists, relying on prevalent and accessible supervised
classification methods. We focused on how a particular group of humans,
academic scientists, write differently than ChatGPT, and this targeted approach
led to the discovery of new features for discriminating (these) humans from AI;
as examples, scientists write long paragraphs and have a penchant for equivocal
language, frequently using words like but, however, and although. With a set of
20 features, including the aforementioned ones and others, we built a model
that assigned the author, as human or AI, at well over 99% accuracy, resulting
in 20 times fewer misclassified documents compared to the field-leading
approach. This strategy for discriminating a particular set of humans writing
from AI could be further adapted and developed by others with basic skills in
supervised classification, enabling access to many highly accurate and targeted
models for detecting AI usage in academic writing and beyond.

The scale of the global edge AI market continues to grow. The current
technical challenges that hinder the large-scale replication of edge AI are
mainly small samples on the edge and heterogeneity of edge data. In addition,
edge AI customers often have requirements for data security compliance and
offline autonomy of edge AI services. Based on the lifelong learning method in
the academic world, we formally define the problem of edge-cloud collaborative
lifelong learning for the first time, and release the industry's first
open-source edge-cloud collaborative lifelong learning. Edge-cloud
collaborative lifelong learning adapts to data heterogeneity at different edge
locations through (1) multi-task transfer learning to achieve accurate
prediction of "thousands of people and thousands of faces"; (2) incremental
processing of unknown tasks, the more systems learn and the smarter systems are
with small samples, gradually realize AI engineering and automation; (3) Use
the cloud-side knowledge base to remember new situational knowledge to avoid
catastrophic forgetting; (4) The edge-cloud collaborative architecture enables
data security compliance and edge AI services to be offline autonomy while
applying cloud resources. This work hopes to help fundamentally solve the
above-mentioned challenges of edge-cloud collaborative machine learning.

Artificial intelligence (AI) methods hold immense potential to revolutionize
numerous medical care by enhancing the experience of medical experts and
patients. AI-based computer-assisted diagnosis and treatment tools can
democratize healthcare by matching the clinical level or surpassing clinical
experts. As a result, advanced healthcare services can be affordable to all
populations, irrespective of demographics, race, or socioeconomic background.
The democratization of such AI tools can reduce the cost of care, optimize
resource allocation, and improve the quality of care. In contrast to humans, AI
can uncover complex relations in the data from a large set of inputs and even
lead to new evidence-based knowledge in medicine. However, integrating AI into
healthcare raises several ethical and philosophical concerns, such as bias,
transparency, autonomy, responsibility, and accountability. Here, we emphasize
recent advances in AI-assisted medical image analysis, existing standards, and
the significance of comprehending ethical issues and best practices for
clinical settings. We cover the technical and ethical challenges and
implications of deploying AI in hospitals and public organizations. We also
discuss key measures and techniques to address ethical challenges, data
scarcity, racial bias, lack of transparency, and algorithmic bias and provide
recommendations and future directions.

This paper surveys the application and development of Artificial Intelligence
(AI) in Satellite Communication (SatCom) and Non-Terrestrial Networks (NTN). We
first present a comprehensive list of use cases, the relative challenges and
the main AI tools capable of addressing those challenges. For each use case, we
present the main motivation, a system description, the available non-AI
solutions and the potential benefits and available works using AI. We also
discuss the pros and cons of an on-board and on-ground AI-based architecture,
and we revise the current commercial and research activities relevant to this
topic. Next, we describe the state-of-the-art hardware solutions for developing
ML in real satellite systems. Finally, we discuss the long-term developments of
AI in the SatCom and NTN sectors and potential research directions. This paper
provides a comprehensive and up-to-date overview of the opportunities and
challenges offered by AI to improve the performance and efficiency of NTNs.

Photos serve as a way for humans to record what they experience in their
daily lives, and they are often regarded as trustworthy sources of information.
However, there is a growing concern that the advancement of artificial
intelligence (AI) technology may produce fake photos, which can create
confusion and diminish trust in photographs. This study aims to comprehensively
evaluate agents for distinguishing state-of-the-art AI-generated visual
content. Our study benchmarks both human capability and cutting-edge fake image
detection AI algorithms, using a newly collected large-scale fake image dataset
Fake2M. In our human perception evaluation, titled HPBench, we discovered that
humans struggle significantly to distinguish real photos from AI-generated
ones, with a misclassification rate of 38.7%. Along with this, we conduct the
model capability of AI-Generated images detection evaluation MPBench and the
top-performing model from MPBench achieves a 13% failure rate under the same
setting used in the human evaluation. We hope that our study can raise
awareness of the potential risks of AI-generated images and facilitate further
research to prevent the spread of false information. More information can refer
to https://github.com/Inf-imagine/Sentry.

Despite pronouncements about the inevitable diffusion of artificial
intelligence and autonomous technologies, in practice it is human behavior, not
technology in a vacuum, that dictates how technology seeps into -- and changes
-- societies. In order to better understand how human preferences shape
technological adoption and the spread of AI-enabled autonomous technologies, we
look at representative adult samples of US public opinion in 2018 and 2020 on
the use of four types of autonomous technologies: vehicles, surgery, weapons,
and cyber defense. By focusing on these four diverse uses of AI-enabled
autonomy that span transportation, medicine, and national security, we exploit
the inherent variation between these AI-enabled autonomous use cases. We find
that those with familiarity and expertise with AI and similar technologies were
more likely to support all of the autonomous applications we tested (except
weapons) than those with a limited understanding of the technology. Individuals
that had already delegated the act of driving by using ride-share apps were
also more positive about autonomous vehicles. However, familiarity cut both
ways; individuals are also less likely to support AI-enabled technologies when
applied directly to their life, especially if technology automates tasks they
are already familiar with operating. Finally, opposition to AI-enabled military
applications has slightly increased over time.

This perspective paper proposes a series of interactive scenarios that
utilize Artificial Intelligence (AI) to enhance classroom teaching, such as
dialogue auto-completion, knowledge and style transfer, and assessment of
AI-generated content. By leveraging recent developments in Large Language
Models (LLMs), we explore the potential of AI to augment and enrich
teacher-student dialogues and improve the quality of teaching. Our goal is to
produce innovative and meaningful conversations between teachers and students,
create standards for evaluation, and improve the efficacy of AI-for-Education
initiatives. In Section 3, we discuss the challenges of utilizing existing LLMs
to effectively complete the educated tasks and present a unified framework for
addressing diverse education dataset, processing lengthy conversations, and
condensing information to better accomplish more downstream tasks. In Section
4, we summarize the pivoting tasks including Teacher-Student Dialogue
Auto-Completion, Expert Teaching Knowledge and Style Transfer, and Assessment
of AI-Generated Content (AIGC), providing a clear path for future research. In
Section 5, we also explore the use of external and adjustable LLMs to improve
the generated content through human-in-the-loop supervision and reinforcement
learning. Ultimately, this paper seeks to highlight the potential for AI to aid
the field of education and promote its further exploration.

Technological advances in the context of digital transformation are the basis
for rapid developments in the field of artificial intelligence (AI). Although
AI is not a new topic in computer science (CS), recent developments are having
an immense impact on everyday life and society. In consequence, everyone needs
competencies to be able to adequately and competently analyze, discuss and help
shape the impact, opportunities, and limits of artificial intelligence on their
personal lives and our society. As a result, an increasing number of CS
curricula are being extended to include the topic of AI. However, in order to
integrate AI into existing CS curricula, what students can and should learn in
the context of AI needs to be clarified. This has proven to be particularly
difficult, considering that so far CS education research on central concepts
and principles of AI lacks sufficient elaboration. Therefore, in this paper, we
present a curriculum of learning objectives that addresses digital literacy and
the societal perspective in particular. The learning objectives can be used to
comprehensively design curricula, but also allow for analyzing current
curricula and teaching materials and provide insights into the central concepts
and corresponding competencies of AI.

Generative Artificial Intelligence systems have been developed for image,
code, story, and game generation with the goal of facilitating human
creativity. Recent work on neural generative systems has emphasized one
particular means of interacting with AI systems: the user provides a
specification, usually in the form of prompts, and the AI system generates the
content. However, there are other configurations of human and AI coordination,
such as co-creativity (CC) in which both human and AI systems can contribute to
content creation, and mixed-initiative (MI) in which both human and AI systems
can initiate content changes. In this paper, we define a hypothetical human-AI
configuration design space consisting of different means for humans and AI
systems to communicate creative intent to each other. We conduct a human
participant study with 185 participants to understand how users want to
interact with differently configured MI-CC systems. We find out that MI-CC
systems with more extensive coverage of the design space are rated higher or on
par on a variety of creative and goal-completion metrics, demonstrating that
wider coverage of the design space can improve user experience and achievement
when using the system; Preference varies greatly between expertise groups,
suggesting the development of adaptive, personalized MI-CC systems;
Participants identified new design space dimensions including scrutability --
the ability to poke and prod at models -- and explainability.

The deployment and use of AI systems should be both safe and broadly
ethically acceptable. The principles-based ethics assurance argument pattern is
one proposal in the AI ethics landscape that seeks to support and achieve that
aim. The purpose of this argument pattern or framework is to structure
reasoning about, and to communicate and foster confidence in, the ethical
acceptability of uses of specific real-world AI systems in complex
socio-technical contexts. This paper presents the interim findings of a case
study applying this ethics assurance framework to the use of Dora, an AI-based
telemedicine system, to assess its viability and usefulness as an approach. The
case study process to date has revealed some of the positive ethical impacts of
the Dora platform, as well as unexpected insights and areas to prioritise for
evaluation, such as risks to the frontline clinician, particularly in respect
of clinician autonomy. The ethics assurance argument pattern offers a practical
framework not just for identifying issues to be addressed, but also to start to
construct solutions in the form of adjustments to the distribution of benefits,
risks and constraints on human autonomy that could reduce ethical disparities
across affected stakeholders. Though many challenges remain, this research
represents a step in the direction towards the development and use of safe and
ethically acceptable AI systems and, ideally, a shift towards more
comprehensive and inclusive evaluations of AI systems in general.

The tremendous recent advances in generative artificial intelligence
techniques have led to significant successes and promise in a wide range of
different applications ranging from conversational agents and textual content
generation to voice and visual synthesis. Amid the rise in generative AI and
its increasing widespread adoption, there has been significant growing concern
over the use of generative AI for malicious purposes. In the realm of visual
content synthesis using generative AI, key areas of significant concern has
been image forgery (e.g., generation of images containing or derived from
copyright content), and data poisoning (i.e., generation of adversarially
contaminated images). Motivated to address these key concerns to encourage
responsible generative AI, we introduce the DeepfakeArt Challenge, a
large-scale challenge benchmark dataset designed specifically to aid in the
building of machine learning algorithms for generative AI art forgery and data
poisoning detection. Comprising of over 32,000 records across a variety of
generative forgery and data poisoning techniques, each entry consists of a pair
of images that are either forgeries / adversarially contaminated or not. Each
of the generated images in the DeepfakeArt Challenge benchmark dataset has been
quality checked in a comprehensive manner. The DeepfakeArt Challenge is a core
part of GenAI4Good, a global open source initiative for accelerating machine
learning for promoting responsible creation and deployment of generative AI for
good.

The technical progression of artificial intelligence (AI) research has been
built on breakthroughs in fields such as computer science, statistics, and
mathematics. However, in the past decade AI researchers have increasingly
looked to the social sciences, turning to human interactions to solve the
challenges of model development. Paying crowdsourcing workers to generate or
curate data, or data enrichment, has become indispensable for many areas of AI
research, from natural language processing to reinforcement learning from human
feedback (RLHF). Other fields that routinely interact with crowdsourcing
workers, such as Psychology, have developed common governance requirements and
norms to ensure research is undertaken ethically. This study explores how, and
to what extent, comparable research ethics requirements and norms have
developed for AI research and data enrichment. We focus on the approach taken
by two leading conferences: ICLR and NeurIPS, and journal publisher Springer.
In a longitudinal study of accepted papers, and via a comparison with
Psychology and CHI papers, this work finds that leading AI venues have begun to
establish protocols for human data collection, but these are are inconsistently
followed by authors. Whilst Psychology papers engaging with crowdsourcing
workers frequently disclose ethics reviews, payment data, demographic data and
other information, similar disclosures are far less common in leading AI venues
despite similar guidance. The work concludes with hypotheses to explain these
gaps in research ethics practices and considerations for its implications.

Researchers are constantly leveraging new forms of data with the goal of
understanding how people perceive the built environment and build the
collective place identity of cities. Latest advancements in generative
artificial intelligence (AI) models have enabled the production of realistic
representations learned from vast amounts of data. In this study, we aim to
test the potential of generative AI as the source of textual and visual
information in capturing the place identity of cities assessed by filtered
descriptions and images. We asked questions on the place identity of a set of
31 global cities to two generative AI models, ChatGPT and DALL-E2. Since
generative AI has raised ethical concerns regarding its trustworthiness, we
performed cross-validation to examine whether the results show similar patterns
to real urban settings. In particular, we compared the outputs with Wikipedia
data for text and images searched from Google for image. Our results indicate
that generative AI models have the potential to capture the collective image of
cities that can make them distinguishable. This study is among the first
attempts to explore the capabilities of generative AI in understanding human
perceptions of the built environment. It contributes to urban design literature
by discussing future research opportunities and potential limitations.

An emerging body of research indicates that ineffective cross-functional
collaboration -- the interdisciplinary work done by industry practitioners
across roles -- represents a major barrier to addressing issues of fairness in
AI design and development. In this research, we sought to better understand
practitioners' current practices and tactics to enact cross-functional
collaboration for AI fairness, in order to identify opportunities to support
more effective collaboration. We conducted a series of interviews and design
workshops with 23 industry practitioners spanning various roles from 17
companies. We found that practitioners engaged in bridging work to overcome
frictions in understanding, contextualization, and evaluation around AI
fairness across roles. In addition, in organizational contexts with a lack of
resources and incentives for fairness work, practitioners often piggybacked on
existing requirements (e.g., for privacy assessments) and AI development norms
(e.g., the use of quantitative evaluation metrics), although they worry that
these tactics may be fundamentally compromised. Finally, we draw attention to
the invisible labor that practitioners take on as part of this bridging and
piggybacking work to enact interdisciplinary collaboration for fairness. We
close by discussing opportunities for both FAccT researchers and AI
practitioners to better support cross-functional collaboration for fairness in
the design and development of AI systems.

The rise of Generative Artificial Intelligence systems ("AI systems") has
created unprecedented social engagement. AI code generation systems provide
responses (output) to questions or requests by accessing the vast library of
open-source code created by developers over the past few decades. However, they
do so by allegedly stealing the open-source code stored in virtual libraries,
known as repositories. This Article focuses on how this happens and whether
there is a solution that protects innovation and avoids years of litigation. We
also touch upon the array of issues raised by the relationship between AI and
copyright. Looking ahead, we propose the following: (a) immediate changes to
the licenses for open-source code created by developers that will limit access
and/or use of any open-source code to humans only; (b) we suggest revisions to
the Massachusetts Institute of Technology ("MIT") license so that AI systems
are required to procure appropriate licenses from open-source code developers,
which we believe will harmonize standards and build social consensus for the
benefit of all of humanity, rather than promote profit-driven centers of
innovation; (c) we call for urgent legislative action to protect the future of
AI systems while also promoting innovation; and (d) we propose a shift in the
burden of proof to AI systems in obfuscation cases.

The emergence of advanced Natural Language Processing (NLP) models like
ChatGPT has raised concerns among universities regarding AI-driven exam
completion. This paper provides a comprehensive evaluation of the proficiency
of GPT-4 and GPT-3.5 in answering a set of 42 exam papers derived from 10
distinct physics courses, administered at Durham University over the span of
2018 to 2022, totalling 593 questions and 2504 available marks. These exams,
spanning both undergraduate and postgraduate levels, include traditional
pre-COVID and adaptive COVID-era formats. Questions from the years 2018-2020
were designed for pre-COVID in person adjudicated examinations whereas the
2021-2022 exams were set for varying COVID-adapted conditions including
open-book conditions. To ensure a fair evaluation of AI performances, the exams
completed by AI were assessed by the original exam markers. However, due to
staffing constraints, only the aforementioned 593 out of the total 1280
questions were marked. GPT-4 and GPT-3.5 scored an average of 49.4\% and
38.6\%, respectively, suggesting only the weaker students would potential
improve their marks if using AI. For exams from the pre-COVID era, the average
scores for GPT-4 and GPT-3.5 were 50.8\% and 41.6\%, respectively. However,
post-COVID, these dropped to 47.5\% and 33.6\%. Thus contrary to expectations,
the change to less fact-based questions in the COVID era did not significantly
impact AI performance for the state-of-the-art models such as GPT-4. These
findings suggest that while current AI models struggle with university-level
Physics questions, an improving trend is observable. The code used for
automated AI completion is made publicly available for further research.

The evolution of wireless networks gravitates towards connected intelligence,
a concept that envisions seamless interconnectivity among humans, objects, and
intelligence in a hyper-connected cyber-physical world. Edge artificial
intelligence (Edge AI) is a promising solution to achieve connected
intelligence by delivering high-quality, low-latency, and privacy-preserving AI
services at the network edge. This article presents a vision of autonomous edge
AI systems that automatically organize, adapt, and optimize themselves to meet
users' diverse requirements, leveraging the power of large language models
(LLMs), i.e., Generative Pretrained Transformer (GPT). By exploiting the
powerful abilities of GPT in language understanding, planning, and code
generation, as well as incorporating classic wisdom such as task-oriented
communication and edge federated learning, we present a versatile framework
that efficiently coordinates edge AI models to cater to users' personal demands
while automatically generating code to train new models in a privacy-preserving
manner. Experimental results demonstrate the system's remarkable ability to
accurately comprehend user demands, efficiently execute AI models with minimal
cost, and effectively create high-performance AI models at edge servers.

Article 5 of the European Union's Artificial Intelligence Act is intended to
regulate AI use to prevent potentially harmful consequences. Nevertheless,
applying this legislation practically is likely to be challenging because of
ambiguously used terminologies and because it fails to specify which
manipulation techniques may be invoked by AI, potentially leading to
significant harm. This paper aims to bridge this gap by defining key terms and
demonstrating how AI may invoke these techniques, drawing from insights in
psychology and behavioural economics. First, this paper provides definitions of
the terms "subliminal techniques", "manipulative techniques" and "deceptive
techniques". Secondly, we identified from the literature in cognitive
psychology and behavioural economics three subliminal and five manipulative
techniques and exemplify how AI might implement these techniques to manipulate
users in real-world case scenarios. These illustrations may serve as a
practical guide for stakeholders to detect cases of AI manipulation and
consequently devise preventive measures. Article 5 has also been criticised for
offering inadequate protection. We critically assess the protection offered by
Article 5, proposing specific revisions to paragraph 1, points (a) and (b) of
Article 5 to increase its protective effectiveness.

In this position paper, we advocate for the idea that courses and exams in
the AI era have to be designed based on two factors: (1) the strengths and
limitations of AI, and (2) the pedagogical educational objectives. Based on
insights from the Delors report on education [1], we first address the role of
education and recall the main objectives that educational institutes must
strive to achieve independently of any technology. We then explore the
strengths and limitations of AI, based on current advances in AI. We explain
how courses and exams can be designed based on these strengths and limitations
of AI, providing different examples in the IT, English, and Art domains. We
show how we adopted a pedagogical approach that is inspired from the Socratic
teaching method from January 2023 to May 2023. Then, we present the data
analysis results of seven ChatGPT-authorized exams conducted between December
2022 and March 2023. Our exam data results show that there is no correlation
between students' grades and whether or not they use ChatGPT to answer their
exam questions. Finally, we present a new exam system that allows us to apply
our pedagogical approach in the AI era.

In 2020, the U.S. Department of Defense officially disclosed a set of ethical
principles to guide the use of Artificial Intelligence (AI) technologies on
future battlefields. Despite stark differences, there are core similarities
between the military and medical service. Warriors on battlefields often face
life-altering circumstances that require quick decision-making. Medical
providers experience similar challenges in a rapidly changing healthcare
environment, such as in the emergency department or during surgery treating a
life-threatening condition. Generative AI, an emerging technology designed to
efficiently generate valuable information, holds great promise. As computing
power becomes more accessible and the abundance of health data, such as
electronic health records, electrocardiograms, and medical images, increases,
it is inevitable that healthcare will be revolutionized by this technology.
Recently, generative AI has captivated the research community, leading to
debates about its application in healthcare, mainly due to concerns about
transparency and related issues. Meanwhile, concerns about the potential
exacerbation of health disparities due to modeling biases have raised notable
ethical concerns regarding the use of this technology in healthcare. However,
the ethical principles for generative AI in healthcare have been understudied,
and decision-makers often fail to consider the significance of generative AI.
In this paper, we propose GREAT PLEA ethical principles, encompassing
governance, reliability, equity, accountability, traceability, privacy,
lawfulness, empathy, and autonomy, for generative AI in healthcare. We aim to
proactively address the ethical dilemmas and challenges posed by the
integration of generative AI in healthcare.

Ocean modeling is a powerful tool for simulating the physical, chemical, and
biological processes of the ocean, which is the foundation for marine science
research and operational oceanography. Modern numerical ocean modeling mainly
consists of governing equations and numerical algorithms. Nonlinear
instability, computational expense, low reusability efficiency and high
coupling costs have gradually become the main bottlenecks for the further
development of numerical ocean modeling. Recently, artificial
intelligence-based modeling in scientific computing has shown revolutionary
potential for digital twins and scientific simulations, but the bottlenecks of
numerical ocean modeling have not been further solved. Here, we present
AI-GOMS, a large AI-driven global ocean modeling system, for accurate and
efficient global ocean daily prediction. AI-GOMS consists of a backbone model
with the Fourier-based Masked Autoencoder structure for basic ocean variable
prediction and lightweight fine-tuning models incorporating regional
downscaling, wave decoding, and biochemistry coupling modules. AI-GOMS has
achieved the best performance in 30 days of prediction for the global ocean
basic variables with 15 depth layers at 1/4{\deg} spatial resolution. Beyond
the good performance in statistical metrics, AI-GOMS realizes the simulation of
mesoscale eddies in the Kuroshio region at 1/12{\deg} spatial resolution and
ocean stratification in the tropical Pacific Ocean. AI-GOMS provides a new
backbone-downstream paradigm for Earth system modeling, which makes the system
transferable, scalable and reusable.

Generative Artificial Intelligence (AI) has seen mainstream adoption lately,
especially in the form of consumer-facing, open-ended, text and image
generating models. However, the use of such systems raises significant ethical
and safety concerns, including privacy violations, misinformation and
intellectual property theft. The potential for generative AI to displace human
creativity and livelihoods has also been under intense scrutiny. To mitigate
these risks, there is an urgent need of policies and regulations responsible
and ethical development in the field of generative AI. Existing and proposed
centralized regulations by governments to rein in AI face criticisms such as
not having sufficient clarity or uniformity, lack of interoperability across
lines of jurisdictions, restricting innovation, and hindering free market
competition. Decentralized protections via crowdsourced safety tools and
mechanisms are a potential alternative. However, they have clear deficiencies
in terms of lack of adequacy of oversight and difficulty of enforcement of
ethical and safety standards, and are thus not enough by themselves as a
regulation mechanism. We propose a marriage of these two strategies via a
framework we call Dual Governance. This framework proposes a cooperative
synergy between centralized government regulations in a U.S. specific context
and safety mechanisms developed by the community to protect stakeholders from
the harms of generative AI. By implementing the Dual Governance framework, we
posit that innovation and creativity can be promoted while ensuring safe and
ethical deployment of generative AI.

Whether current or near-term AI systems could be conscious is a topic of
scientific interest and increasing public concern. This report argues for, and
exemplifies, a rigorous and empirically grounded approach to AI consciousness:
assessing existing AI systems in detail, in light of our best-supported
neuroscientific theories of consciousness. We survey several prominent
scientific theories of consciousness, including recurrent processing theory,
global workspace theory, higher-order theories, predictive processing, and
attention schema theory. From these theories we derive "indicator properties"
of consciousness, elucidated in computational terms that allow us to assess AI
systems for these properties. We use these indicator properties to assess
several recent AI systems, and we discuss how future systems might implement
them. Our analysis suggests that no current AI systems are conscious, but also
suggests that there are no obvious technical barriers to building AI systems
which satisfy these indicators.

Over the last year, the ascent of Generative AI (GenAI) has raised concerns
about its impact on core skill development, such as problem-solving and
algorithmic thinking, in Computer Science students. Preliminary anonymous
surveys show that at least 48.5% of our students use GenAI for homework. With
the proliferation of these tools, the academic community must contemplate the
appropriate role of these tools in education. Neglecting this might culminate
in a phenomenon we term the "Junior-Year Wall," where students struggle in
advanced courses due to prior over-dependence on GenAI. Instead of discouraging
GenAI use, which may unintentionally foster covert usage, our research seeks to
answer: "How can educators guide students' interactions with GenAI to preserve
core skill development during their foundational academic years?"
  We introduce "AI-Lab," a pedagogical framework for guiding students in
effectively leveraging GenAI within core collegiate programming courses. This
framework accentuates GenAI's benefits and potential as a pedagogical
instrument. By identifying and rectifying GenAI's errors, students enrich their
learning process. Moreover, AI-Lab presents opportunities to use GenAI for
tailored support such as topic introductions, detailed examples, corner case
identification, rephrased explanations, and debugging assistance. Importantly,
the framework highlights the risks of GenAI over-dependence, aiming to
intrinsically motivate students towards balanced usage. This approach is
premised on the idea that mere warnings of GenAI's potential failures may be
misconstrued as instructional shortcomings rather than genuine tool
limitations.
  Additionally, AI-Lab offers strategies for formulating prompts to elicit
high-quality GenAI responses. For educators, AI-Lab provides mechanisms to
explore students' perceptions of GenAI's role in their learning experience.

In this paper, we present "1001 Nights", an AI-native game that allows
players lead in-game reality through co-created storytelling with the character
driven by large language model. The concept is inspired by Wittgenstein's idea
of the limits of one's world being determined by the bounds of their language.
Using advanced AI tools like GPT-4 and Stable Diffusion, the second iteration
of the game enables the protagonist, Shahrzad, to realize words and stories in
her world. The player can steer the conversation with the AI King towards
specific keywords, which then become battle equipment in the game. This blend
of interactive narrative and text-to-image transformation challenges the
conventional border between the game world and reality through a dual
perspective. We focus on Shahrzad, who seeks to alter her fate compared to the
original folklore, and the player, who collaborates with AI to craft narratives
and shape the game world. We explore the technical and design elements of
implementing such a game with an objective to enhance the narrative game genre
with AI-generated content and to delve into AI-native gameplay possibilities.

Over the past decades, cognitive neuroscientists and behavioral economists
have recognized the value of describing the process of decision making in
detail and modeling the emergence of decisions over time. For example, the time
it takes to decide can reveal more about an agent's true hidden preferences
than only the decision itself. Similarly, data that track the ongoing decision
process such as eye movements or neural recordings contain critical information
that can be exploited, even if no decision is made. Here, we argue that
artificial intelligence (AI) research would benefit from a stronger focus on
insights about how decisions emerge over time and incorporate related process
data to improve AI predictions in general and human-AI interactions in
particular. First, we introduce a highly established computational framework
that assumes decisions to emerge from the noisy accumulation of evidence, and
we present related empirical work in psychology, neuroscience, and economics.
Next, we discuss to what extent current approaches in multi-agent AI do or do
not incorporate process data and models of decision making. Finally, we outline
how a more principled inclusion of the evidence-accumulation framework into the
training and use of AI can help to improve human-AI interactions in the future.

This report describes trade-offs in the design of international governance
arrangements for civilian artificial intelligence (AI) and presents one
approach in detail. This approach represents the extension of a standards,
licensing, and liability regime to the global level. We propose that states
establish an International AI Organization (IAIO) to certify state
jurisdictions (not firms or AI projects) for compliance with international
oversight standards. States can give force to these international standards by
adopting regulations prohibiting the import of goods whose supply chains embody
AI from non-IAIO-certified jurisdictions. This borrows attributes from models
of existing international organizations, such as the International Civilian
Aviation Organization (ICAO), the International Maritime Organization (IMO),
and the Financial Action Task Force (FATF). States can also adopt multilateral
controls on the export of AI product inputs, such as specialized hardware, to
non-certified jurisdictions. Indeed, both the import and export standards could
be required for certification. As international actors reach consensus on risks
of and minimum standards for advanced AI, a jurisdictional certification regime
could mitigate a broad range of potential harms, including threats to public
safety.

While we do not always use words, communicating what we want to an AI is a
conversation -- with ourselves as well as with it, a recurring loop with
optional steps depending on the complexity of the situation and our request.
Any given conversation of this type may include: (a) the human forming an
intent, (b) the human expressing that intent as a command or utterance, (c) the
AI performing one or more rounds of inference on that command to resolve
ambiguities and/or requesting clarifications from the human, (d) the AI showing
the inferred meaning of the command and/or its execution on current and future
situations or data, (e) the human hopefully correctly recognizing whether the
AI's interpretation actually aligns with their intent. In the process, they may
(f) update their model of the AI's capabilities and characteristics, (g) update
their model of the situations in which the AI is executing its interpretation
of their intent, (h) confirm or refine their intent, and (i) revise their
expression of their intent to the AI, where the loop repeats until the human is
satisfied. With these critical cognitive and computational steps within this
back-and-forth laid out as a framework, it is easier to anticipate where
communication can fail, and design algorithms and interfaces that ameliorate
those failure points.

Generative AIs produce creative outputs in the style of human expression. We
argue that encounters with the outputs of modern generative AI models are
mediated by the same kinds of aesthetic judgments that organize our
interactions with artwork. The interpretation procedure we use on art we find
in museums is not an innate human faculty, but one developed over history by
disciplines such as art history and art criticism to fulfill certain social
functions. This gives us pause when considering our reactions to generative AI,
how we should approach this new medium, and why generative AI seems to incite
so much fear about the future. We naturally inherit a conundrum of causal
inference from the history of art: a work can be read as a symptom of the
cultural conditions that influenced its creation while simultaneously being
framed as a timeless, seemingly acausal distillation of an eternal human
condition. In this essay, we focus on an unresolved tension when we bring this
dilemma to bear in the context of generative AI: are we looking for proof that
generated media reflects something about the conditions that created it or some
eternal human essence? Are current modes of interpretation sufficient for this
task? Historically, new forms of art have changed how art is interpreted, with
such influence used as evidence that a work of art has touched some essential
human truth. As generative AI influences contemporary aesthetic judgment we
outline some of the pitfalls and traps in attempting to scrutinize what AI
generated media means.

Recent advancements in artificial intelligence (AI) systems, including large
language models like ChatGPT, offer promise and peril for scholarly peer
review. On the one hand, AI can enhance efficiency by addressing issues like
long publication delays. On the other hand, it brings ethical and social
concerns that could compromise the integrity of the peer review process and
outcomes. However, human peer review systems are also fraught with related
problems, such as biases, abuses, and a lack of transparency, which already
diminish credibility. While there is increasing attention to the use of AI in
peer review, discussions revolve mainly around plagiarism and authorship in
academic journal publishing, ignoring the broader epistemic, social, cultural,
and societal epistemic in which peer review is positioned. The legitimacy of
AI-driven peer review hinges on the alignment with the scientific ethos,
encompassing moral and epistemic norms that define appropriate conduct in the
scholarly community. In this regard, there is a "norm-counternorm continuum,"
where the acceptability of AI in peer review is shaped by institutional logics,
ethical practices, and internal regulatory mechanisms. The discussion here
emphasizes the need to critically assess the legitimacy of AI-driven peer
review, addressing the benefits and downsides relative to the broader
epistemic, social, ethical, and regulatory factors that sculpt its
implementation and impact.

Despite the basic premise that next-generation wireless networks (e.g., 6G)
will be artificial intelligence (AI)-native, to date, most existing efforts
remain either qualitative or incremental extensions to existing "AI for
wireless" paradigms. Indeed, creating AI-native wireless networks faces
significant technical challenges due to the limitations of data-driven,
training-intensive AI. These limitations include the black-box nature of the AI
models, their curve-fitting nature, which can limit their ability to reason and
adapt, their reliance on large amounts of training data, and the energy
inefficiency of large neural networks. In response to these limitations, this
article presents a comprehensive, forward-looking vision that addresses these
shortcomings by introducing a novel framework for building AI-native wireless
networks; grounded in the emerging field of causal reasoning. Causal reasoning,
founded on causal discovery, causal representation learning, and causal
inference, can help build explainable, reasoning-aware, and sustainable
wireless networks. Towards fulfilling this vision, we first highlight several
wireless networking challenges that can be addressed by causal discovery and
representation, including ultra-reliable beamforming for terahertz (THz)
systems, near-accurate physical twin modeling for digital twins, training data
augmentation, and semantic communication. We showcase how incorporating causal
discovery can assist in achieving dynamic adaptability, resilience, and
cognition in addressing these challenges. Furthermore, we outline potential
frameworks that leverage causal inference to achieve the overarching objectives
of future-generation networks, including intent management, dynamic
adaptability, human-level cognition, reasoning, and the critical element of
time sensitivity.

The advancement of visual intelligence is intrinsically tethered to the
availability of large-scale data. In parallel, generative Artificial
Intelligence (AI) has unlocked the potential to create synthetic images that
closely resemble real-world photographs. This prompts a compelling inquiry: how
much visual intelligence could benefit from the advance of generative AI? This
paper explores the innovative concept of harnessing these AI-generated images
as new data sources, reshaping traditional modeling paradigms in visual
intelligence. In contrast to real data, AI-generated data exhibit remarkable
advantages, including unmatched abundance and scalability, the rapid generation
of vast datasets, and the effortless simulation of edge cases. Built on the
success of generative AI models, we examine the potential of their generated
data in a range of applications, from training machine learning models to
simulating scenarios for computational modeling, testing, and validation. We
probe the technological foundations that support this groundbreaking use of
generative AI, engaging in an in-depth discussion on the ethical, legal, and
practical considerations that accompany this transformative paradigm shift.
Through an exhaustive survey of current technologies and applications, this
paper presents a comprehensive view of the synthetic era in visual
intelligence. A project associated with this paper can be found at
https://github.com/mwxely/AIGS .

Artificial intelligence (AI) methods have become critical in scientific
applications to help accelerate scientific discovery. Large language models
(LLMs) are being considered as a promising approach to address some of the
challenging problems because of their superior generalization capabilities
across domains. The effectiveness of the models and the accuracy of the
applications is contingent upon their efficient execution on the underlying
hardware infrastructure. Specialized AI accelerator hardware systems have
recently become available for accelerating AI applications. However, the
comparative performance of these AI accelerators on large language models has
not been previously studied. In this paper, we systematically study LLMs on
multiple AI accelerators and GPUs and evaluate their performance
characteristics for these models. We evaluate these systems with (i) a
micro-benchmark using a core transformer block, (ii) a GPT- 2 model, and (iii)
an LLM-driven science use case, GenSLM. We present our findings and analyses of
the models' performance to better understand the intrinsic capabilities of AI
accelerators. Furthermore, our analysis takes into account key factors such as
sequence lengths, scaling behavior, sparsity, and sensitivity to gradient
accumulation steps.

This study is focused on the ethics of Artificial Intelligence and its
application in the United States, the paper highlights the impact AI has in
every sector of the US economy and multiple facets of the technological space
and the resultant effect on entities spanning businesses, government, academia,
and civil society. There is a need for ethical considerations as these entities
are beginning to depend on AI for delivering various crucial tasks, which
immensely influence their operations, decision-making, and interactions with
each other. The adoption of ethical principles, guidelines, and standards of
work is therefore required throughout the entire process of AI development,
deployment, and usage to ensure responsible and ethical AI practices. Our
discussion explores eleven fundamental 'ethical principles' structured as
overarching themes. These encompass Transparency, Justice, Fairness, Equity,
Non- Maleficence, Responsibility, Accountability, Privacy, Beneficence,
Freedom, Autonomy, Trust, Dignity, Sustainability, and Solidarity. These
principles collectively serve as a guiding framework, directing the ethical
path for the responsible development, deployment, and utilization of artificial
intelligence (AI) technologies across diverse sectors and entities within the
United States. The paper also discusses the revolutionary impact of AI
applications, such as Machine Learning, and explores various approaches used to
implement AI ethics. This examination is crucial to address the growing
concerns surrounding the inherent risks associated with the widespread use of
artificial intelligence.

AI companies are attempting to create AI systems that outperform humans at
most economically valuable work. Current AI models are already automating away
the livelihoods of some artists, actors, and writers. But there is infighting
between those who prioritize current harms and future harms. We construct a
game-theoretic model of conflict to study the causes and consequences of this
disunity. Our model also helps explain why throughout history, stakeholders
sharing a common threat have found it advantageous to unite against it, and why
the common threat has in turn found it advantageous to divide and conquer.
  Under realistic parameter assumptions, our model makes several predictions
that find preliminary corroboration in the historical-empirical record. First,
current victims of AI-driven disempowerment need the future victims to realize
that their interests are also under serious and imminent threat, so that future
victims are incentivized to support current victims in solidarity. Second, the
movement against AI-driven disempowerment can become more united, and thereby
more likely to prevail, if members believe that their efforts will be
successful as opposed to futile. Finally, the movement can better unite and
prevail if its members are less myopic. Myopic members prioritize their future
well-being less than their present well-being, and are thus disinclined to
solidarily support current victims today at personal cost, even if this is
necessary to counter the shared threat of AI-driven disempowerment.

In recent years, artificial intelligence (AI) and machine learning (ML) are
reshaping society's production methods and productivity, and also changing the
paradigm of scientific research. Among them, the AI language model represented
by ChatGPT has made great progress. Such large language models (LLMs) serve
people in the form of AI-generated content (AIGC) and are widely used in
consulting, healthcare, and education. However, it is difficult to guarantee
the authenticity and reliability of AIGC learning data. In addition, there are
also hidden dangers of privacy disclosure in distributed AI training. Moreover,
the content generated by LLMs is difficult to identify and trace, and it is
difficult to cross-platform mutual recognition. The above information security
issues in the coming era of AI powered by LLMs will be infinitely amplified and
affect everyone's life. Therefore, we consider empowering LLMs using blockchain
technology with superior security features to propose a vision for trusted AI.
This paper mainly introduces the motivation and technical route of blockchain
for LLM (BC4LLM), including reliable learning corpus, secure training process,
and identifiable generated content. Meanwhile, this paper also reviews the
potential applications and future challenges, especially in the frontier
communication networks field, including network resource allocation, dynamic
spectrum sharing, and semantic communication. Based on the above work combined
and the prospect of blockchain and LLMs, it is expected to help the early
realization of trusted AI and provide guidance for the academic community.

In the rapid development of artificial intelligence, solving complex AI tasks
is a crucial technology in intelligent mobile networks. Despite the good
performance of specialized AI models in intelligent mobile networks, they are
unable to handle complicated AI tasks. To address this challenge, we propose
Systematic Artificial Intelligence (SAI), which is a framework designed to
solve AI tasks by leveraging Large Language Models (LLMs) and JSON-format
intent-based input to connect self-designed model library and database.
Specifically, we first design a multi-input component, which simultaneously
integrates Large Language Models (LLMs) and JSON-format intent-based inputs to
fulfill the diverse intent requirements of different users. In addition, we
introduce a model library module based on model cards which employ model cards
to pairwise match between different modules for model composition. Model cards
contain the corresponding model's name and the required performance metrics.
Then when receiving user network requirements, we execute each subtask for
multiple selected model combinations and provide output based on the execution
results and LLM feedback. By leveraging the language capabilities of LLMs and
the abundant AI models in the model library, SAI can complete numerous complex
AI tasks in the communication network, achieving impressive results in network
optimization, resource allocation, and other challenging tasks.

Large Language Models (LLMs) have revolutionized the domain of natural
language processing (NLP) with remarkable capabilities of generating human-like
text responses. However, despite these advancements, several works in the
existing literature have raised serious concerns about the potential misuse of
LLMs such as spreading misinformation, generating fake news, plagiarism in
academia, and contaminating the web. To address these concerns, a consensus
among the research community is to develop algorithmic solutions to detect
AI-generated text. The basic idea is that whenever we can tell if the given
text is either written by a human or an AI, we can utilize this information to
address the above-mentioned concerns. To that end, a plethora of detection
frameworks have been proposed, highlighting the possibilities of AI-generated
text detection. But in parallel to the development of detection frameworks,
researchers have also concentrated on designing strategies to elude detection,
i.e., focusing on the impossibilities of AI-generated text detection. This is a
crucial step in order to make sure the detection frameworks are robust enough
and it is not too easy to fool a detector. Despite the huge interest and the
flurry of research in this domain, the community currently lacks a
comprehensive analysis of recent developments. In this survey, we aim to
provide a concise categorization and overview of current work encompassing both
the prospects and the limitations of AI-generated text detection. To enrich the
collective knowledge, we engage in an exhaustive discussion on critical and
challenging open questions related to ongoing research on AI-generated text
detection.

Artificial Intelligence (AI) systems have made remarkable progress, attaining
super-human performance across various domains. This presents us with an
opportunity to further human knowledge and improve human expert performance by
leveraging the hidden knowledge encoded within these highly performant AI
systems. Yet, this knowledge is often hard to extract, and may be hard to
understand or learn from. Here, we show that this is possible by proposing a
new method that allows us to extract new chess concepts in AlphaZero, an AI
system that mastered the game of chess via self-play without human supervision.
Our analysis indicates that AlphaZero may encode knowledge that extends beyond
the existing human knowledge, but knowledge that is ultimately not beyond human
grasp, and can be successfully learned from. In a human study, we show that
these concepts are learnable by top human experts, as four top chess
grandmasters show improvements in solving the presented concept prototype
positions. This marks an important first milestone in advancing the frontier of
human knowledge by leveraging AI; a development that could bear profound
implications and help us shape how we interact with AI systems across many AI
applications.

Authorship Analysis, also known as stylometry, has been an essential aspect
of Natural Language Processing (NLP) for a long time. Likewise, the recent
advancement of Large Language Models (LLMs) has made authorship analysis
increasingly crucial for distinguishing between human-written and AI-generated
texts. However, these authorship analysis tasks have primarily been focused on
written texts, not considering spoken texts. Thus, we introduce the largest
benchmark for spoken texts - HANSEN (Human ANd ai Spoken tExt beNchmark).
HANSEN encompasses meticulous curation of existing speech datasets accompanied
by transcripts, alongside the creation of novel AI-generated spoken text
datasets. Together, it comprises 17 human datasets, and AI-generated spoken
texts created using 3 prominent LLMs: ChatGPT, PaLM2, and Vicuna13B. To
evaluate and demonstrate the utility of HANSEN, we perform Authorship
Attribution (AA) & Author Verification (AV) on human-spoken datasets and
conducted Human vs. AI spoken text detection using state-of-the-art (SOTA)
models. While SOTA methods, such as, character ngram or Transformer-based
model, exhibit similar AA & AV performance in human-spoken datasets compared to
written ones, there is much room for improvement in AI-generated spoken text
detection. The HANSEN benchmark is available at:
https://huggingface.co/datasets/HANSEN-REPO/HANSEN.

This manuscript presents a novel state-of-the-art cyber-physical water
testbed, namely: The AI and Cyber for Water and Agriculture testbed (ACWA).
ACWA is motivated by the need to advance water supply management using AI and
Cybersecurity experimentation. The main goal of ACWA is to address pressing
challenges in the water and agricultural domains by utilising cutting-edge AI
and data-driven technologies. These challenges include Cyberbiosecurity,
resources management, access to water, sustainability, and data-driven
decision-making, among others. To address such issues, ACWA consists of
multiple topologies, sensors, computational nodes, pumps, tanks, smart water
devices, as well as databases and AI models that control the system. Moreover,
we present ACWA simulator, which is a software-based water digital twin. The
simulator runs on fluid and constituent transport principles that produce
theoretical time series of a water distribution system. This creates a good
validation point for comparing the theoretical approach with real-life results
via the physical ACWA testbed. ACWA data are available to AI and water domain
researchers and are hosted in an online public repository. In this paper, the
system is introduced in detail and compared with existing water testbeds;
additionally, example use-cases are described along with novel outcomes such as
datasets, software, and AI-related scenarios.

The relentless advancement of artificial intelligence (AI) and machine
learning (ML) applications necessitates the development of specialized hardware
accelerators capable of handling the increasing complexity and computational
demands. Traditional computing architectures, based on the von Neumann model,
are being outstripped by the requirements of contemporary AI/ML algorithms,
leading to a surge in the creation of accelerators like the Graphcore
Intelligence Processing Unit (IPU), Sambanova Reconfigurable Dataflow Unit
(RDU), and enhanced GPU platforms. These hardware accelerators are
characterized by their innovative data-flow architectures and other design
optimizations that promise to deliver superior performance and energy
efficiency for AI/ML tasks.
  This research provides a preliminary evaluation and comparison of these
commercial AI/ML accelerators, delving into their hardware and software design
features to discern their strengths and unique capabilities. By conducting a
series of benchmark evaluations on common DNN operators and other AI/ML
workloads, we aim to illuminate the advantages of data-flow architectures over
conventional processor designs and offer insights into the performance
trade-offs of each platform. The findings from our study will serve as a
valuable reference for the design and performance expectations of research
prototypes, thereby facilitating the development of next-generation hardware
accelerators tailored for the ever-evolving landscape of AI/ML applications.
Through this analysis, we aspire to contribute to the broader understanding of
current accelerator technologies and to provide guidance for future innovations
in the field.

The adoption of generative AI technologies is swiftly expanding. Services
employing both linguistic and mul-timodal models are evolving, offering users
increasingly precise responses. Consequently, human reliance on these
technologies is expected to grow rapidly. With the premise that people will be
impacted by the output of AI, we explored approaches to help AI output produce
better results. Initially, we evaluated how contemporary AI services
competitively meet user needs, then examined society's depiction as mirrored by
Large Language Models (LLMs). We did a query experiment, querying about social
conventions in various countries and eliciting a one-word response. We compared
the LLMs' value judgments with public data and suggested an model of
decision-making in value-conflicting scenarios which could be adopted for
future machine value judgments. This paper advocates for a practical approach
to using AI as a tool for investigating other remote worlds. This re-search has
significance in implicitly rejecting the notion of AI making value judgments
and instead arguing a more critical perspective on the environment that defers
judgmental capabilities to individuals. We anticipate this study will empower
anyone, regardless of their capacity, to receive safe and accurate value
judgment-based out-puts effectively.

Many environmental remediation and energy applications (conversion and
storage) for sustainability need design and development of green novel
materials. Discovery processes of such novel materials are time taking and
cumbersome due to large number of possible combinations and permutations of
materials structures. Often theoretical studies based on Density Functional
Theory (DFT) and other theories, coupled with Simulations are conducted to
narrow down sample space of candidate materials, before conducting
laboratory-based synthesis and analytical process. With the emergence of
artificial intelligence (AI), AI techniques are being tried in this process too
to ease out simulation time and cost. However tremendous values of previously
published research from various parts of the world are still left as
labor-intensive manual effort and discretion of individual researcher and prone
to human omissions. AIMS-EREA is our novel framework to blend best of breed of
Material Science theory with power of Generative AI to give best impact and
smooth and quickest discovery of material for sustainability. This also helps
to eliminate the possibility of production of hazardous residues and
bye-products of the reactions. AIMS-EREA uses all available resources --
Predictive and Analytical AI on large collection of chemical databases along
with automated intelligent assimilation of deep materials knowledge from
previously published research works through Generative AI. We demonstrate use
of our own novel framework with an example, how this framework can be
successfully applied to achieve desired success in development of
thermoelectric material for waste heat conversion.

Research within sociotechnical domains, such as Software Engineering,
fundamentally requires a thorough consideration of the human perspective.
However, traditional qualitative data collection methods suffer from challenges
related to scale, labor intensity, and the increasing difficulty of participant
recruitment. This vision paper proposes a novel approach to qualitative data
collection in software engineering research by harnessing the capabilities of
artificial intelligence (AI), especially large language models (LLMs) like
ChatGPT. We explore the potential of AI-generated synthetic text as an
alternative source of qualitative data, by discussing how LLMs can replicate
human responses and behaviors in research settings. We examine the application
of AI in automating data collection across various methodologies, including
persona-based prompting for interviews, multi-persona dialogue for focus
groups, and mega-persona responses for surveys. Additionally, we discuss the
prospective development of new foundation models aimed at emulating human
behavior in observational studies and user evaluations. By simulating human
interaction and feedback, these AI models could offer scalable and efficient
means of data generation, while providing insights into human attitudes,
experiences, and performance. We discuss several open problems and research
opportunities to implement this vision and conclude that while AI could augment
aspects of data gathering in software engineering research, it cannot replace
the nuanced, empathetic understanding inherent in human subjects in some cases,
and an integrated approach where both AI and human-generated data coexist will
likely yield the most effective outcomes.

Generative Artificial Intelligence (AI), particularly tools like OpenAI's
popular ChatGPT, is reshaping the landscape of computer science research. Used
wisely, these tools can boost the productivity of a computer research
scientist. This paper provides an exploration of the diverse applications of
ChatGPT and other generative AI technologies in computer science academic
research, making recommendations about the use of Generative AI to make more
productive the role of the computer research scientist, with the focus of
writing new research papers. We highlight innovative uses such as brainstorming
research ideas, aiding in the drafting and styling of academic papers and
assisting in the synthesis of state-of-the-art section. Further, we delve into
using these technologies in understanding interdisciplinary approaches, making
complex texts simpler, and recommending suitable academic journals for
publication. Significant focus is placed on generative AI's contributions to
synthetic data creation, research methodology, and mentorship, as well as in
task organization and article quality assessment. The paper also addresses the
utility of AI in article review, adapting texts to length constraints,
constructing counterarguments, and survey development. Moreover, we explore the
capabilities of these tools in disseminating ideas, generating images and
audio, text transcription, and engaging with editors. We also describe some
non-recommended uses of generative AI for computer science research, mainly
because of the limitations of this technology.

The development and regulation of multi-purpose, large "foundation models" of
AI seems to have reached a critical stage, with major investments and new
applications announced every other day. Some experts are calling for a
moratorium on the training of AI systems more powerful than GPT-4. Legislators
globally compete to set the blueprint for a new regulatory regime. This paper
analyses the most advanced legal proposal, the European Union's AI Act
currently in the stage of final "trilogue" negotiations between the EU
institutions. This legislation will likely have extra-territorial implications,
sometimes called "the Brussels effect". It also constitutes a radical departure
from conventional information and communications technology policy by
regulating AI ex-ante through a risk-based approach that seeks to prevent
certain harmful outcomes based on product safety principles. We offer a review
and critique, specifically discussing the AI Act's problematic obligations
regarding data quality and human oversight. Our proposal is to take liability
seriously as the key regulatory mechanism. This signals to industry that if a
breach of law occurs, firms are required to know in particular what their
inputs were and how to retrain the system to remedy the breach. Moreover, we
suggest differentiating between endogenous and exogenous sources of potential
harm, which can be mitigated by carefully allocating liability between
developers and deployers of AI technology.

In the pharmaceutical industry, the use of artificial intelligence (AI) has
seen consistent growth over the past decade. This rise is attributed to major
advancements in statistical machine learning methodologies, computational
capabilities and the increased availability of large datasets. AI techniques
are applied throughout different stages of drug development, ranging from drug
discovery to post-marketing benefit-risk assessment. Kolluri et al. provided a
review of several case studies that span these stages, featuring key
applications such as protein structure prediction, success probability
estimation, subgroup identification, and AI-assisted clinical trial monitoring.
From a regulatory standpoint, there was a notable uptick in submissions
incorporating AI components in 2021. The most prevalent therapeutic areas
leveraging AI were oncology (27%), psychiatry (15%), gastroenterology (12%),
and neurology (11%). The paradigm of personalized or precision medicine has
gained significant traction in recent research, partly due to advancements in
AI techniques \cite{hamburg2010path}. This shift has had a transformative
impact on the pharmaceutical industry. Departing from the traditional
"one-size-fits-all" model, personalized medicine incorporates various
individual factors, such as environmental conditions, lifestyle choices, and
health histories, to formulate customized treatment plans. By utilizing
sophisticated machine learning algorithms, clinicians and researchers are
better equipped to make informed decisions in areas such as disease prevention,
diagnosis, and treatment selection, thereby optimizing health outcomes for each
individual.

There is no denying that the use of Information Technology (IT) is undergoing
exponential growth in today's world. This digital transformation has also given
rise to a multitude of security challenges, notably in the realm of cybercrime.
In response to these growing threats, public and private sectors have
prioritized the strengthening of IT security measures. In light of the growing
security concern, Artificial Intelligence (AI) has gained prominence within the
cybersecurity landscape. This paper presents a comprehensive survey of recent
advancements in AI-driven threat response systems. To the best of our
knowledge, the most recent survey covering the AI reaction domain was conducted
in 2017. Since then, considerable literature has been published and therefore
it is worth reviewing it. By means of several shared features, each of the
studies is compared on a common ground. Through an analysis of the research
papers conducted on a standardized basis, this survey aims to unravel the
complexities and opportunities of integrating AI into cyber defense. The
conclusions drawn from this collective analysis provide a comprehensive
snapshot of the evolving landscape at the intersection of AI and cybersecurity.
This landscape underscores the growing significance of not only anticipating
and detecting threats but also responding to them effectively. Additionally,
from these reviews, various research challenges for the future are presented.
These challenges serve as a roadmap for researchers and practitioners in the
field of AI-integrated reactive strategies.

This study explores the capabilities of multimodal large language models
(LLMs) in handling challenging multistep tasks that integrate language and
vision, focusing on model steerability, composability, and the application of
long-term memory and context understanding. The problem addressed is the LLM's
ability (Nov 2023 GPT-4 Vision Preview) to manage tasks that require
synthesizing visual and textual information, especially where stepwise
instructions and sequential logic are paramount. The research presents a series
of 14 creatively and constructively diverse tasks, ranging from AI Lego
Designing to AI Satellite Image Analysis, designed to test the limits of
current LLMs in contexts that previously proved difficult without extensive
memory and contextual understanding. Key findings from evaluating 800 guided
dialogs include notable disparities in task completion difficulty. For
instance, 'Image to Ingredient AI Bartender' (Low difficulty) contrasted
sharply with 'AI Game Self-Player' (High difficulty), highlighting the LLM's
varying proficiency in processing complex visual data and generating coherent
instructions. Tasks such as 'AI Genetic Programmer' and 'AI Negotiator' showed
high completion difficulty, emphasizing challenges in maintaining context over
multiple steps. The results underscore the importance of developing LLMs that
combine long-term memory and contextual awareness to mimic human-like thought
processes in complex problem-solving scenarios.

The rapid advance of generative AI is reshaping the strategic vision for R&D
across industries. The unique challenges of pharmaceutical R&D will see
applications of generative AI deliver value along the entire value chain from
early discovery to regulatory approval. This perspective reviews these
challenges and takes a three-horizon approach to explore the generative AI
applications already delivering impact, the disruptive opportunities which are
just around the corner, and the longer-term transformation which will shape the
future of the industry. Selected applications are reviewed for their potential
to drive increase productivity, accelerate timelines, improve the quality of
research, data and decision making, and support a sustainable future for the
industry. Recommendations are given for Pharma R&D leaders developing a
generative AI strategy today which will lay the groundwork for getting real
value from the technology and safeguarding future growth. Generative AI is
today providing new, efficient routes to accessing and combining organisational
data to drive productivity. Next, this impact will reach clinical development,
enhancing the patient experience, driving operational efficiency, and unlocking
digital innovation to better tackle the future burden of disease. Looking to
the furthest horizon, rapid acquisition of rich multi-omics data, which capture
the 'language of life', in combination with next generation AI technologies
will allow organisations to close the loop around phases of the pipeline
through rapid, automated generation and testing of hypotheses from bench to
bedside. This provides a vision for the future of R&D with sustainability at
the core, with reduced timescales and reduced dependency on resources, while
offering new hope to patients to treat the untreatable and ultimately cure
diseases.

The author's goal in this paper is to explore how artificial intelligence
(AI) has been utilised to inform our understanding of and ability to estimate
at scale a critical aspect of musical creativity - musical tempo. The central
importance of tempo to musical creativity can be seen in how it is used to
express specific emotions (Eerola and Vuoskoski 2013), suggest particular
musical styles (Li and Chan 2011), influence perception of expression (Webster
and Weir 2005) and mediate the urge to move one's body in time to the music
(Burger et al. 2014). Traditional tempo estimation methods typically detect
signal periodicities that reflect the underlying rhythmic structure of the
music, often using some form of autocorrelation of the amplitude envelope
(Lartillot and Toiviainen 2007). Recently, AI-based methods utilising
convolutional or recurrent neural networks (CNNs, RNNs) on spectral
representations of the audio signal have enjoyed significant improvements in
accuracy (Aarabi and Peeters 2022). Common AI-based techniques include those
based on probability (e.g., Bayesian approaches, hidden Markov models (HMM)),
classification and statistical learning (e.g., support vector machines (SVM)),
and artificial neural networks (ANNs) (e.g., self-organising maps (SOMs), CNNs,
RNNs, deep learning (DL)). The aim here is to provide an overview of some of
the more common AI-based tempo estimation algorithms and to shine a light on
notable benefits and potential drawbacks of each. Limitations of AI in this
field in general are also considered, as is the capacity for such methods to
account for idiosyncrasies inherent in tempo perception, i.e., how well
AI-based approaches are able to think and act like humans.

AI and generative AI tools, including chatbots like ChatGPT that rely on
large language models (LLMs), have burst onto the scene this year, creating
incredible opportunities to increase work productivity and improve our lives.
Statisticians and data scientists have begun experiencing the benefits from the
availability of these tools in numerous ways, such as the generation of
programming code from text prompts to analyze data or fit statistical models.
One area that these tools can make a substantial impact is in research
discovery and summarization. Standalone tools and plugins to chatbots are being
developed that allow researchers to more quickly find relevant literature than
pre-2023 search tools. Furthermore, generative AI tools have improved to the
point where they can summarize and extract the key points from research
articles in succinct language. Finally, chatbots based on highly parameterized
LLMs can be used to simulate abductive reasoning, which provides researchers
the ability to make connections among related technical topics, which can also
be used for research discovery. We review the developments in AI and generative
AI for research discovery and summarization, and propose directions where these
types of tools are likely to head in the future that may be of interest to
statistician and data scientists.

This survey paper explores the transformative influence of frontier AI,
foundation models, and Large Language Models (LLMs) in the realm of Intelligent
Transportation Systems (ITS), emphasizing their integral role in advancing
transportation intelligence, optimizing traffic management, and contributing to
the realization of smart cities. Frontier AI refers to the forefront of AI
technology, encompassing the latest advancements, innovations, and experimental
techniques in the field, especially AI foundation models and LLMs. Foundation
models, like GPT-4, are large, general-purpose AI models that provide a base
for a wide range of applications. They are characterized by their versatility
and scalability. LLMs are obtained from finetuning foundation models with a
specific focus on processing and generating natural language. They excel in
tasks like language understanding, text generation, translation, and
summarization. By leveraging vast textual data, including traffic reports and
social media interactions, LLMs extract critical insights, fostering the
evolution of ITS. The survey navigates the dynamic synergy between LLMs and
ITS, delving into applications in traffic management, integration into
autonomous vehicles, and their role in shaping smart cities. It provides
insights into ongoing research, innovations, and emerging trends, aiming to
inspire collaboration at the intersection of language, intelligence, and
mobility for safer, more efficient, and sustainable transportation systems. The
paper further surveys interactions between LLMs and various aspects of ITS,
exploring roles in traffic management, facilitating autonomous vehicles, and
contributing to smart city development, while addressing challenges brought by
frontier AI and foundation models. This paper offers valuable inspiration for
future research and innovation in the transformative domain of intelligent
transportation.

In an era characterized by the pervasive integration of artificial
intelligence into decision-making processes across diverse industries, the
demand for trust has never been more pronounced. This thesis embarks on a
comprehensive exploration of bias and fairness, with a particular emphasis on
their ramifications within the banking sector, where AI-driven decisions bear
substantial societal consequences. In this context, the seamless integration of
fairness, explainability, and human oversight is of utmost importance,
culminating in the establishment of what is commonly referred to as
"Responsible AI". This emphasizes the critical nature of addressing biases
within the development of a corporate culture that aligns seamlessly with both
AI regulations and universal human rights standards, particularly in the realm
of automated decision-making systems. Nowadays, embedding ethical principles
into the development, training, and deployment of AI models is crucial for
compliance with forthcoming European regulations and for promoting societal
good. This thesis is structured around three fundamental pillars: understanding
bias, mitigating bias, and accounting for bias. These contributions are
validated through their practical application in real-world scenarios, in
collaboration with Intesa Sanpaolo. This collaborative effort not only
contributes to our understanding of fairness but also provides practical tools
for the responsible implementation of AI-based decision-making systems. In line
with open-source principles, we have released Bias On Demand and FairView as
accessible Python packages, further promoting progress in the field of AI
fairness.

Humans frequently make decisions with the aid of artificially intelligent
(AI) systems. A common pattern is for the AI to recommend an action to the
human who retains control over the final decision. Researchers have identified
ensuring that a human has appropriate reliance on an AI as a critical component
of achieving complementary performance. We argue that the current definition of
appropriate reliance used in such research lacks formal statistical grounding
and can lead to contradictions. We propose a formal definition of reliance,
based on statistical decision theory, which separates the concepts of reliance
as the probability the decision-maker follows the AI's recommendation from
challenges a human may face in differentiating the signals and forming accurate
beliefs about the situation. Our definition gives rise to a framework that can
be used to guide the design and interpretation of studies on human-AI
complementarity and reliance. Using recent AI-advised decision making studies
from literature, we demonstrate how our framework can be used to separate the
loss due to mis-reliance from the loss due to not accurately differentiating
the signals. We evaluate these losses by comparing to a baseline and a
benchmark for complementary performance defined by the expected payoff achieved
by a rational decision-maker facing the same decision task as the behavioral
decision-makers.

Growing awareness of the environmental impact of digital technologies has led
to several isolated initiatives to promote sustainable practices. However,
despite these efforts, the environmental footprint of generative AI,
particularly in terms of greenhouse gas emissions and water consumption,
remains considerable. This contribution first presents the components of this
environmental footprint, highlighting the massive CO2 emissions and water
consumption associated with training large language models, thus underlining
the need to rethink learning and inference methods. The paper also explores the
factors and characteristics of models that have an influence on their
environmental footprint and demonstrates the existence of solutions to reduce
it, such as using more efficient processors or optimising the energy
performance of data centres. The potentially harmful effects of AI on the
planet and its ecosystem have made environmental protection one of the founding
principles of AI ethics at international and European levels. However, this
recognition has not yet translated into concrete measures to address it.To
address this issue, our contribution puts forward twelve pragmatic
recommendations for public action to promote sustainable generative AI, in
particular by building a long-term strategy to achieve carbon neutrality for AI
models, encouraging international cooperation to set common standards,
supporting scientific research and developing appropriate legal and regulatory
frameworks.This paper seeks to inform the members of the Interministerial
Committee on Generative AI about the environmental challenges of this
technology by providing a brief review of the scientific literature on the
subject and proposing concrete recommendations of public policy actions to
reconcile technological innovation with the need to protect our environment.

Artificial intelligence (AI) has the potential to transform education with
its power of uncovering insights from massive data about student learning
patterns. However, ethical and trustworthy concerns of AI have been raised but
are unsolved. Prominent ethical issues in high school AI education include data
privacy, information leakage, abusive language, and fairness. This paper
describes technological components that were built to address ethical and
trustworthy concerns in a multi-modal collaborative platform (called ALLURE
chatbot) for high school students to collaborate with AI to solve the Rubik's
cube. In data privacy, we want to ensure that the informed consent of children,
parents, and teachers, is at the center of any data that is managed. Since
children are involved, language, whether textual, audio, or visual, is
acceptable both from users and AI and the system can steer interaction away
from dangerous situations. In information management, we also want to ensure
that the system, while learning to improve over time, does not leak information
about users from one group to another.

As humans advance toward a higher level of artificial intelligence, it is
always at the cost of escalating computational resource consumption, which
requires developing novel solutions to meet the exponential growth of AI
computing demand. Neuromorphic hardware takes inspiration from how the brain
processes information and promises energy-efficient computing of AI workloads.
Despite its potential, neuromorphic hardware has not found its way into
commercial AI data centers. In this article, we try to analyze the underlying
reasons for this and derive requirements and guidelines to promote neuromorphic
systems for efficient and sustainable cloud computing: We first review
currently available neuromorphic hardware systems and collect examples where
neuromorphic solutions excel conventional AI processing on CPUs and GPUs. Next,
we identify applications, models and algorithms which are commonly deployed in
AI data centers as further directions for neuromorphic algorithms research.
Last, we derive requirements and best practices for the hardware and software
integration of neuromorphic systems into data centers. With this article, we
hope to increase awareness of the challenges of integrating neuromorphic
hardware into data centers and to guide the community to enable sustainable and
energy-efficient AI at scale.

One of today's most significant societal challenges is building AI systems
whose behaviour, or the behaviour it enables within communities of interacting
agents (human and artificial), aligns with human values. To address this
challenge, we detail a formal model of human values for their explicit
computational representation. To our knowledge, this has not been attempted as
yet, which is surprising given the growing volume of research integrating
values within AI. Taking as our starting point the wealth of research
investigating the nature of human values from social psychology over the last
few decades, we set out to provide such a formal model. We show how this model
can provide the foundational apparatus for AI-based reasoning over values, and
demonstrate its applicability in real-world use cases. We illustrate how our
model captures the key ideas from social psychology research and propose a
roadmap for future integrated, and interdisciplinary, research into human
values in AI. The ability to automatically reason over values not only helps
address the value alignment problem but also facilitates the design of AI
systems that can support individuals and communities in making more informed,
value-aligned decisions. More and more, individuals and organisations are
motivated to understand their values more explicitly and explore whether their
behaviours and attitudes properly reflect them. Our work on modelling human
values will enable AI systems to be designed and deployed to meet this growing
need.

The construction industry is a vital sector of the global economy, but it
faces many productivity challenges in various processes, such as design,
planning, procurement, inspection, and maintenance. Generative artificial
intelligence (AI), which can create novel and realistic data or content, such
as text, image, video, or code, based on some input or prior knowledge, offers
innovative and disruptive solutions to address these challenges. However, there
is a gap in the literature on the current state, opportunities, and challenges
of generative AI in the construction industry. This study aims to fill this gap
by providing a state-of-the-art analysis of generative AI in construction, with
three objectives: (1) to review and categorize the existing and emerging
generative AI opportunities and challenges in the construction industry; (2) to
propose a framework for construction firms to build customized generative AI
solutions using their own data, comprising steps such as data collection,
dataset curation, training custom large language model (LLM), model evaluation,
and deployment; and (3) to demonstrate the framework via a case study of
developing a generative model for querying contract documents. The results show
that retrieval augmented generation (RAG) improves the baseline LLM by 5.2,
9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study
provides academics and construction professionals with a comprehensive analysis
and practical framework to guide the adoption of generative AI techniques to
enhance productivity, quality, safety, and sustainability across the
construction industry.

Advances in artificial intelligence (AI) have enabled unprecedented
capabilities, yet innovation teams struggle when envisioning AI concepts. Data
science teams think of innovations users do not want, while domain experts
think of innovations that cannot be built. A lack of effective ideation seems
to be a breakdown point. How might multidisciplinary teams identify buildable
and desirable use cases? This paper presents a first hand account of ideating
AI concepts to improve critical care medicine. As a team of data scientists,
clinicians, and HCI researchers, we conducted a series of design workshops to
explore more effective approaches to AI concept ideation and problem
formulation. We detail our process, the challenges we encountered, and
practices and artifacts that proved effective. We discuss the research
implications for improved collaboration and stakeholder engagement, and discuss
the role HCI might play in reducing the high failure rate experienced in AI
innovation.

The advent of autonomous vehicles has heralded a transformative era in
transportation, reshaping the landscape of mobility through cutting-edge
technologies. Central to this evolution is the integration of Artificial
Intelligence (AI) and learning algorithms, propelling vehicles into realms of
unprecedented autonomy. This paper provides a comprehensive exploration of the
evolutionary trajectory of AI within autonomous vehicles, tracing the journey
from foundational principles to the most recent advancements. Commencing with a
current landscape overview, the paper delves into the fundamental role of AI in
shaping the autonomous decision-making capabilities of vehicles. It elucidates
the steps involved in the AI-powered development life cycle in vehicles,
addressing ethical considerations and bias in AI-driven software development
for autonomous vehicles. The study presents statistical insights into the usage
and types of AI/learning algorithms over the years, showcasing the evolving
research landscape within the automotive industry. Furthermore, the paper
highlights the pivotal role of parameters in refining algorithms for both
trucks and cars, facilitating vehicles to adapt, learn, and improve performance
over time. It concludes by outlining different levels of autonomy, elucidating
the nuanced usage of AI and learning algorithms, and automating key tasks at
each level. Additionally, the document discusses the variation in software
package sizes across different autonomy levels

Recently, there has been a huge effort focused on developing highly efficient
open source libraries to perform Artificial Intelligence (AI) related
computations on different computer architectures (for example, CPUs, GPUs and
new AI processors). This has not only made the algorithms based on these
libraries highly efficient and portable between different architectures, but
also has substantially simplified the entry barrier to develop methods using
AI. Here, we present a novel methodology to bring the power of both AI software
and hardware into the field of numerical modelling by repurposing AI methods,
such as Convolutional Neural Networks (CNNs), for the standard operations
required in the field of the numerical solution of Partial Differential
Equations (PDEs). The aim of this work is to bring the high performance,
architecture agnosticism and ease of use into the field of the numerical
solution of PDEs. We use the proposed methodology to solve the
advection-diffusion equation, the non-linear Burgers equation and
incompressible flow past a bluff body. For the latter, a convolutional neural
network is used as a multigrid solver in order to enforce the incompressibility
constraint. We show that the presented methodology can solve all these problems
using repurposed AI libraries in an efficient way, and presents a new avenue to
explore in the development of methods to solve PDEs and Computational Fluid
Dynamics problems with implicit methods.

Large Language Models (LLMs), such as GPT-3 and BERT, reshape how textual
content is written and communicated. These models have the potential to
generate scientific content that is indistinguishable from that written by
humans. Hence, LLMs carry severe consequences for the scientific community,
which relies on the integrity and reliability of publications. This research
paper presents a novel ChatGPT-generated scientific text detection method,
AI-Catcher. AI-Catcher integrates two deep learning models, multilayer
perceptron (MLP) and convolutional neural networks (CNN). The MLP learns the
feature representations of the linguistic and statistical features. The CNN
extracts high-level representations of the sequential patterns from the textual
content. AI-Catcher is a multimodal model that fuses hidden patterns derived
from MLP and CNN. In addition, a new ChatGPT-Generated scientific text dataset
is collected to enhance AI-generated text detection tools, AIGTxt. AIGTxt
contains 3000 records collected from published academic articles across ten
domains and divided into three classes: Human-written, ChatGPT-generated, and
Mixed text. Several experiments are conducted to evaluate the performance of
AI-Catcher. The comparative results demonstrate the capability of AI-Catcher to
distinguish between human-written and ChatGPT-generated scientific text more
accurately than alternative methods. On average, AI-Catcher improved accuracy
by 37.4%.

Recent advancements in large foundation models have remarkably enhanced our
understanding of sensory information in open-world environments. In leveraging
the power of foundation models, it is crucial for AI research to pivot away
from excessive reductionism and toward an emphasis on systems that function as
cohesive wholes. Specifically, we emphasize developing Agent AI -- an embodied
system that integrates large foundation models into agent actions. The emerging
field of Agent AI spans a wide range of existing embodied and agent-based
multimodal interactions, including robotics, gaming, and healthcare systems,
etc. In this paper, we propose a novel large action model to achieve embodied
intelligent behavior, the Agent Foundation Model. On top of this idea, we
discuss how agent AI exhibits remarkable capabilities across a variety of
domains and tasks, challenging our understanding of learning and cognition.
Furthermore, we discuss the potential of Agent AI from an interdisciplinary
perspective, underscoring AI cognition and consciousness within scientific
discourse. We believe that those discussions serve as a basis for future
research directions and encourage broader societal engagement.

Email continues to serve as a central medium for managing collaborations.
While unstructured email messaging is lightweight and conducive to
coordination, it is easy to overlook commitments and requests for
collaborations that are embedded in the text of free-flowing communications.
Twenty-one years ago, Bellotti et al. proposed TaskMaster with the goal of
redesigning the email interface to have explicit task management capabilities.
Recently, AI-based task recognition and reminder services have been introduced
in major email systems as one approach to managing asynchronous collaborations.
While these services have been provided to millions of people around the world,
there is little understanding of how people interact with and benefit from
them. We explore knowledge workers' experiences with Microsoft's Viva Daily
Briefing Email to better understand how AI-powered reminders can support
asynchronous collaborations. Through semi-structured interviews and surveys, we
shed light on how AI-powered reminders are incorporated into workflows to
support asynchronous collaborations. We identify what knowledge workers prefer
AI-powered reminders to remind them about and how they would like to interact
with these reminders. Using mixed methods and a self-assessment methodology, we
investigate the relationship between information workers' work styles and the
perceived value of the Viva Daily Briefing Email to identify users who are more
likely to benefit from AI-powered reminders for asynchronous collaborations. We
conclude by discussing the experiences and futures of AI-powered reminders for
collaborative tasks and asynchronous collaborations.

The human-centered artificial intelligence (HCAI) design approach, the
user-centered design (UCD) version in the intelligence era, has been promoted
to address potential negative issues caused by AI technology; user experience
design (UXD) is specifically called out to facilitate the design and
development of human-centered AI systems. Over the last three decades, user
experience (UX) practice can be divided into three stages in terms of
technology platform, user needs, design philosophy, ecosystem, scope, focus,
and methodology of UX practice. UX practice is moving towards the intelligence
era. Still, the existing UX paradigm mainly aims at non-intelligent systems and
lacks a systematic approach to address UX for designing and developing
human-centered AI products and systems. The intelligence era has put forward
new demands on the UX paradigm. This paper proposes a "UX 3.0" paradigm
framework and the corresponding UX methodology for UX practice in the
intelligence era. The "UX 3.0" paradigm framework includes four categories of
emerging experiences in the intelligence era: ecosystem-based experience,
innovation-enabled experience, AI-enabled experience, and human-AI
interaction-based experience, each compelling us to enhance current UX practice
in terms of design philosophy, scope, focus, and methodology. We believe that
the "UX 3.0" paradigm helps enhance existing UX practice and provides
methodological support for the research and applications of UX in developing
human-centered AI systems. Finally, this paper looks forward to future work
implementing the "UX 3.0" paradigm.

Integration of artificial intelligence (AI) and machine learning (ML) into
the air interface has been envisioned as a key technology for next-generation
(NextG) cellular networks. At the air interface, multiple-input multiple-output
(MIMO) and its variants such as multi-user MIMO (MU-MIMO) and
massive/full-dimension MIMO have been key enablers across successive
generations of cellular networks with evolving complexity and design
challenges. Initiating active investigation into leveraging AI/ML tools to
address these challenges for MIMO becomes a critical step towards an AI-enabled
NextG air interface. At the NextG air interface, the underlying wireless
environment will be extremely dynamic with operation adaptations performed on a
sub-millisecond basis by MIMO operations such as MU-MIMO scheduling and
rank/link adaptation. Given the enormously large number of operation adaptation
possibilities, we contend that online real-time AI/ML-based approaches
constitute a promising paradigm. To this end, we outline the inherent
challenges and offer insights into the design of such online real-time
AI/ML-based solutions for MIMO operations. An online real-time AI/ML-based
method for MIMO-OFDM channel estimation is then presented, serving as a
potential roadmap for developing similar techniques across various MIMO
operations in NextG.

The increasing use of information technology has led to a significant share
of energy consumption and carbon emissions from data centers. These
contributions are expected to rise with the growing demand for big data
analytics, increasing digitization, and the development of large artificial
intelligence (AI) models. The need to address the environmental impact of
software development has led to increased interest in green (sustainable)
coding and claims that the use of AI models can lead to energy efficiency
gains. Here, we provide an empirical study on green code and an overview of
green coding practices, as well as metrics used to quantify the sustainability
awareness of AI models. In this framework, we evaluate the sustainability of
auto-generated code. The auto-generate codes considered in this study are
produced by generative commercial AI language models, GitHub Copilot, OpenAI
ChatGPT-3, and Amazon CodeWhisperer. Within our methodology, in order to
quantify the sustainability awareness of these AI models, we propose a
definition of the code's "green capacity", based on certain sustainability
metrics. We compare the performance and green capacity of human-generated code
and code generated by the three AI language models in response to easy-to-hard
problem statements. Our findings shed light on the current capacity of AI
models to contribute to sustainable software development.

This study evaluates $n = 300$ short-form physics essay submissions, equally
divided between student work submitted before the introduction of ChatGPT and
those generated by OpenAI's GPT-4. In blinded evaluations conducted by five
independent markers who were unaware of the origin of the essays, we observed
no statistically significant differences in scores between essays authored by
humans and those produced by AI (p-value $= 0.107$, $\alpha$ = 0.05).
Additionally, when the markers subsequently attempted to identify the
authorship of the essays on a 4-point Likert scale - from `Definitely AI' to
`Definitely Human' - their performance was only marginally better than random
chance. This outcome not only underscores the convergence of AI and human
authorship quality but also highlights the difficulty of discerning
AI-generated content solely through human judgment. Furthermore, the
effectiveness of five commercially available software tools for identifying
essay authorship was evaluated. Among these, ZeroGPT was the most accurate,
achieving a 98% accuracy rate and a precision score of 1.0 when its
classifications were reduced to binary outcomes. This result is a source of
potential optimism for maintaining assessment integrity. Finally, we propose
that texts with $\leq 50\%$ AI-generated content should be considered the upper
limit for classification as human-authored, a boundary inclusive of a future
with ubiquitous AI assistance whilst also respecting human-authorship.

Deep Learning is advancing medical imaging Research and Development (R&D),
leading to the frequent clinical use of Artificial Intelligence/Machine
Learning (AI/ML)-based medical devices. However, to advance AI R&D, two
challenges arise: 1) significant data imbalance, with most data from
Europe/America and under 10% from Asia, despite its 60% global population
share; and 2) hefty time and investment needed to curate proprietary datasets
for commercial use. In response, we established the first commercial medical
imaging platform, encompassing steps like: 1) data collection, 2) data
selection, 3) annotation, and 4) pre-processing. Moreover, we focus on
harnessing under-represented data from Japan and broader Asia, including
Computed Tomography, Magnetic Resonance Imaging, and Whole Slide Imaging scans.
Using the collected data, we are preparing/providing ready-to-use datasets for
medical AI R&D by 1) offering these datasets to AI firms, biopharma, and
medical device makers and 2) using them as training/test data to develop
tailored AI solutions for such entities. We also aim to merge Blockchain for
data security and plan to synthesize rare disease data via generative AI.
DataHub Website: https://medical-datahub.ai/

The European legislature has proposed the Digital Services Act (DSA) and
Artificial Intelligence Act (AIA) to regulate platforms and Artificial
Intelligence (AI) products. We review to what extent third-party audits are
part of both laws and to what extent access to models and data is provided. By
considering the value of third-party audits and third-party data access in an
audit ecosystem, we identify a regulatory gap in that the Artificial
Intelligence Act does not provide access to data for researchers and civil
society. Our contributions to the literature include: (1) Defining an AI audit
ecosystem that incorporates compliance and oversight. (2) Highlighting a
regulatory gap within the DSA and AIA regulatory framework, preventing the
establishment of an AI audit ecosystem. (3) Emphasizing that third-party audits
by research and civil society must be part of that ecosystem and demand that
the AIA include data and model access for certain AI products. We call for the
DSA to provide NGOs and investigative journalists with data access to platforms
by delegated acts and for adaptions and amendments of the AIA to provide
third-party audits and data and model access at least for high-risk systems to
close the regulatory gap. Regulations modeled after European Union AI
regulations should enable data access and third-party audits, fostering an AI
audit ecosystem that promotes compliance and oversight mechanisms.

Energy Internet (EI) is emerging as new share economy platform for flexible
local energy supplies in smart cities. Empowered by the Internet-of-Things
(IoT) and Artificial Intelligence (AI), EI aims to unlock peer-to-peer energy
trading and sharing among prosumers, who can adeptly switch roles between
providers and consumers in localized energy markets with rooftop photovoltaic
panels, vehicle-to-everything technologies, packetized energy management, etc.
The integration of prosumers in EI, however, will encounter many challenges in
modelling, analyzing, and designing an efficient, economic, and social-optimal
platform for energy sharing, calling for advanced AI/IoT-based solutions to
resource optimization, information exchange, and interaction protocols in the
context of the share economy. In this study, we aim to introduce a recently
emerged paradigm, Machina Economicus, to investigate the economic rationality
in modelling, analysis, and optimization of AI/IoT-based EI prosumer behaviors.
The new paradigm, built upon the theory of machine learning and mechanism
design, will offer new angles to investigate the selfishness of AI through a
game-theoretic perspective, revealing potential competition and collaborations
resulting from the self-adaptive learning and decision-making capacity. This
study will focus on how the introduction of AI will reshape prosumer behaviors
on the EI, and how this paradigm will reveal new research questions and
directions when AI meets the share economy. With an extensive case analysis in
the literature, we will also shed light on potential solutions for advancements
of AI in future smart cities.

The rapid advancement of Generative AI (Gen-AI) is transforming
Human-Computer Interaction (HCI), with significant implications across various
sectors. This study investigates the public's perception of Sora OpenAI, a
pioneering Gen-AI video generation tool, via social media discussions on Reddit
before its release. It centers on two main questions: the envisioned
applications and the concerns related to Sora's integration. The analysis
forecasts positive shifts in content creation, predicting that Sora will
democratize video marketing and innovate game development by making video
production more accessible and economical. Conversely, there are concerns about
deepfakes and the potential for disinformation, underscoring the need for
strategies to address disinformation and bias. This paper contributes to the
Gen-AI discourse by fostering discussion on current and future capabilities,
enriching the understanding of public expectations, and establishing a temporal
benchmark for user anticipation. This research underscores the necessity for
informed, ethical approaches to AI development and integration, ensuring that
technological advancements align with societal values and user needs.

With the release of Generative AI systems such as ChatGPT, an increasing
interest in using Artificial Intelligence (AI) has been observed across
domains, including higher education. While emerging statistics show the
popularity of using AI amongst undergraduate students, little is yet known
about students' perceptions regarding AI including self-reported benefits and
concerns from their actual usage, in particular in distance learning contexts.
Using a two-step, mixed-methods approach, we examined the perceptions of ten
online and distance learning students from diverse disciplines regarding the
design of a hypothetical AI Digital Assistant (AIDA). In the first step, we
captured students' perceptions via interviews, while the second step supported
the triangulation of data by enabling students to share, compare, and contrast
perceptions with those of peers. All participants agreed on the usefulness of
such an AI tool while studying and reported benefits from using it for
real-time assistance and query resolution, support for academic tasks,
personalisation and accessibility, together with emotional and social support.
Students' concerns related to the ethical and social implications of
implementing AIDA, data privacy and data use, operational challenges, academic
integrity and misuse, and the future of education. Implications for the design
of AI-tailored systems are also discussed.

The sustainability of AI systems depends on the capacity of project teams to
proceed with a continuous sensitivity to their potential real-world impacts and
transformative effects. Stakeholder Impact Assessments (SIAs) are governance
mechanisms that enable this kind of responsiveness. They are tools that create
a procedure for, and a means of documenting, the collaborative evaluation and
reflective anticipation of the possible harms and benefits of AI innovation
projects. SIAs are not one-off governance actions. They require project teams
to pay continuous attention to the dynamic and changing character of AI
production and use and to the shifting conditions of the real-world
environments in which AI technologies are embedded. This workbook is part two
of two workbooks on AI Sustainability. It provides a template of the SIA and
activities that allow a deeper dive into crucial parts of it. It discusses
methods for weighing values and considering trade-offs during the SIA. And, it
highlights the need to treat the SIA as an end-to-end process of responsive
evaluation and re-assessment.

A clinical artificial intelligence (AI) system is often validated on a
held-out set of data which it has not been exposed to before (e.g., data from a
different hospital with a distinct electronic health record system). This
evaluation process is meant to mimic the deployment of an AI system on data in
the wild; those which are currently unseen by the system yet are expected to be
encountered in a clinical setting. However, when data in the wild differ from
the held-out set of data, a phenomenon referred to as distribution shift, and
lack ground-truth annotations, it becomes unclear the extent to which AI-based
findings can be trusted on data in the wild. Here, we introduce SUDO, a
framework for evaluating AI systems without ground-truth annotations. SUDO
assigns temporary labels to data points in the wild and directly uses them to
train distinct models, with the highest performing model indicative of the most
likely label. Through experiments with AI systems developed for dermatology
images, histopathology patches, and clinical reports, we show that SUDO can be
a reliable proxy for model performance and thus identify unreliable
predictions. We also demonstrate that SUDO informs the selection of models and
allows for the previously out-of-reach assessment of algorithmic bias for data
in the wild without ground-truth annotations. The ability to triage unreliable
predictions for further inspection and assess the algorithmic bias of AI
systems can improve the integrity of research findings and contribute to the
deployment of ethical AI systems in medicine.

Artificial Intelligence (AI) models are now being utilized in all facets of
our lives such as healthcare, education and employment. Since they are used in
numerous sensitive environments and make decisions that can be life altering,
potential biased outcomes are a pressing matter. Developers should ensure that
such models don't manifest any unexpected discriminatory practices like
partiality for certain genders, ethnicities or disabled people. With the
ubiquitous dissemination of AI systems, researchers and practitioners are
becoming more aware of unfair models and are bound to mitigate bias in them.
Significant research has been conducted in addressing such issues to ensure
models don't intentionally or unintentionally perpetuate bias. This survey
offers a synopsis of the different ways researchers have promoted fairness in
AI systems. We explore the different definitions of fairness existing in the
current literature. We create a comprehensive taxonomy by categorizing
different types of bias and investigate cases of biased AI in different
application domains. A thorough study is conducted of the approaches and
techniques employed by researchers to mitigate bias in AI models. Moreover, we
also delve into the impact of biased models on user experience and the ethical
considerations to contemplate when developing and deploying such models. We
hope this survey helps researchers and practitioners understand the intricate
details of fairness and bias in AI systems. By sharing this thorough survey, we
aim to promote additional discourse in the domain of equitable and responsible
AI.

Artificial Intelligence has outperformed human experts in functional tasks
such as chess and baduk. How about creative tasks? This paper evaluates AI's
capability in the creative domain compared to human experts, which little
research has been conducted so far. We propose a novel Prompt-for-Prompt to
generate social media creatives via prompt augmentation by Large Language
Models. We take the most popular Instagram posts (with the biggest number of
like clicks) in top brands' Instagram accounts to create social media
creatives. We give GPT 4 several prompt instructions with text descriptions to
generate the most effective prompts for cutting-edge text-to-image generators:
Midjourney, DALL E 3, and Stable Diffusion. LLM-augmented prompts can boost
AI's abilities by adding objectives, engagement strategy, lighting and brand
consistency for social media image creation. We conduct an extensive human
evaluation experiment, and find that AI excels human experts, and Midjourney is
better than the other text-to-image generators. Surprisingly, unlike
conventional wisdom in the social media industry, prompt instruction including
eye-catching shows much poorer performance than those including natural.
Regarding the type of creatives, AI improves creatives with animals or products
but less with real people. Also, AI improves creatives with short text
descriptions more than with long text descriptions, because there is more room
for AI to augment prompts with shorter descriptions.

Artificial intelligence (AI) can improve human decision-making in various
application areas. Ideally, collaboration between humans and AI should lead to
complementary team performance (CTP) -- a level of performance that neither of
them can attain individually. So far, however, CTP has rarely been observed,
suggesting an insufficient understanding of the complementary constituents in
human-AI collaboration that can contribute to CTP in decision-making. This work
establishes a holistic theoretical foundation for understanding and developing
human-AI complementarity. We conceptualize complementarity by introducing and
formalizing the notion of complementarity potential and its realization.
Moreover, we identify and outline sources that explain CTP. We illustrate our
conceptualization by applying it in two empirical studies exploring two
different sources of complementarity potential. In the first study, we focus on
information asymmetry as a source and, in a real estate appraisal use case,
demonstrate that humans can leverage unique contextual information to achieve
CTP. In the second study, we focus on capability asymmetry as an alternative
source, demonstrating how heterogeneous capabilities can help achieve CTP. Our
work provides researchers with a theoretical foundation of complementarity in
human-AI decision-making and demonstrates that leveraging sources of
complementarity potential constitutes a viable pathway toward effective
human-AI collaboration.

The Deep learning (DL) models for diagnosing breast cancer from mammographic
images often operate as "black boxes", making it difficult for healthcare
professionals to trust and understand their decision-making processes. The
study presents an integrated framework combining Convolutional Neural Networks
(CNNs) and Explainable Artificial Intelligence (XAI) for the enhanced diagnosis
of breast cancer using the CBIS-DDSM dataset. The methodology encompasses an
elaborate data preprocessing pipeline and advanced data augmentation techniques
to counteract dataset limitations and transfer learning using pre-trained
networks such as VGG-16, Inception-V3 and ResNet was employed. A focal point of
our study is the evaluation of XAI's effectiveness in interpreting model
predictions, highlighted by utilizing the Hausdorff measure to assess the
alignment between AI-generated explanations and expert annotations
quantitatively. This approach is critical for XAI in promoting trustworthiness
and ethical fairness in AI-assisted diagnostics. The findings from our research
illustrate the effective collaboration between CNNs and XAI in advancing
diagnostic methods for breast cancer, thereby facilitating a more seamless
integration of advanced AI technologies within clinical settings. By enhancing
the interpretability of AI driven decisions, this work lays the groundwork for
improved collaboration between AI systems and medical practitioners, ultimately
enriching patient care. Furthermore, the implications of our research extended
well beyond the current methodologies. It encourages further research into how
to combine multimodal data and improve AI explanations to meet the needs of
clinical practice.

Human-created works represent critical data inputs to artificial intelligence
(AI). Strategic behavior can play a major role for AI training datasets, be it
in limiting access to existing works or in deciding which types of new works to
create or whether to create new works at all. We examine creators' behavioral
change when their works become training data for AI. Specifically, we focus on
contributors on Unsplash, a popular stock image platform with about 6 million
high-quality photos and illustrations. In the summer of 2020, Unsplash launched
an AI research program by releasing a dataset of 25,000 images for commercial
use. We study contributors' reactions, comparing contributors whose works were
included in this dataset to contributors whose works were not included. Our
results suggest that treated contributors left the platform at a
higher-than-usual rate and substantially slowed down the rate of new uploads.
Professional and more successful photographers react stronger than amateurs and
less successful photographers. We also show that affected users changed the
variety and novelty of contributions to the platform, with long-run
implications for the stock of works potentially available for AI training.
Taken together, our findings highlight the trade-off between interests of
rightsholders and promoting innovation at the technological frontier. We
discuss implications for copyright and AI policy.

Non-cognitive skills are crucial for personal and social life well-being, and
such skill development can be supported by narrative-based (e.g., storytelling)
technologies. While generative AI enables interactive and role-playing
storytelling, little is known about how users engage with and perceive the use
of AI in social life simulation for non-cognitive skills learning. To this end,
we introduced SimuLife++, an interactive platform enabled by a large language
model (LLM). The system allows users to act as protagonists, creating stories
with one or multiple AI-based characters in diverse social scenarios. In
particular, we expanded the Human-AI interaction to a Human-AI-AI collaboration
by including a sage agent, who acts as a bystander to provide users with more
insightful perspectives on their choices and conversations. Through a
within-subject user study, we found that the inclusion of the sage agent
significantly enhanced narrative immersion, according to the narrative
transportation scale, leading to more messages, particularly in group chats.
Participants' interactions with the sage agent were also associated with
significantly higher scores in their perceived motivation, self-perceptions,
and resilience and coping, indicating positive impacts on non-cognitive skills
reflection. Participants' interview results further explained the sage agent's
aid in decision-making, solving ethical dilemmas, and problem-solving; on the
other hand, they suggested improvements in user control and balanced responses
from multiple characters. We provide design implications on the application of
generative AI in narrative solutions for non-cognitive skill development in
broader social contexts.

Powerful artificial intelligence systems are often used in settings where
they must interact with agents that are computationally much weaker, for
example when they work alongside humans or operate in complex environments
where some tasks are handled by algorithms, heuristics, or other entities of
varying computational power. For AI agents to successfully interact in these
settings, however, achieving superhuman performance alone is not sufficient;
they also need to account for suboptimal actions or idiosyncratic style from
their less-skilled counterparts. We propose a formal evaluation framework for
assessing the compatibility of near-optimal AI with interaction partners who
may have much lower levels of skill; we use popular collaborative chess
variants as model systems to study and develop AI agents that can successfully
interact with lower-skill entities. Traditional chess engines designed to
output near-optimal moves prove to be inadequate partners when paired with
engines of various lower skill levels in this domain, as they are not designed
to consider the presence of other agents. We contribute three methodologies to
explicitly create skill-compatible AI agents in complex decision-making
settings, and two chess game frameworks designed to foster collaboration
between powerful AI agents and less-skilled partners. On these frameworks, our
agents outperform state-of-the-art chess AI (based on AlphaZero) despite being
weaker in conventional chess, demonstrating that skill-compatibility is a
tangible trait that is qualitatively and measurably distinct from raw
performance. Our evaluations further explore and clarify the mechanisms by
which our agents achieve skill-compatibility.

Generative AI (GAI) is impacting teaching and learning directly or indirectly
across a range of subjects and disciplines. As educators, we need to understand
the potential and limitations of AI in HCI education and ensure our graduating
HCI students are aware of the potential and limitations of AI in HCI. In this
paper, we report on the main pedagogical insights gained from the inclusion of
generative AI into a 10 week undergraduate module. We designed the module to
encourage student experimentation with GAI models as part of the design brief
requirement and planned practical sessions and discussions. Our insights are
based on replies to a survey sent out to the students after completing the
module. Our key findings, for HCI educators, report on the use of AI as a
persona for developing project ideas and creating resources for design, and AI
as a mirror for reflecting students' understanding of key concepts and ideas
and highlighting knowledge gaps. We also discuss potential pitfalls that should
be considered and the need to assess students' literacies and assumptions of
GAIs as pedagogical tools. Finally, we put forward the case for educators to
take the opportunities GAI presents as an educational tool and be experimental,
creative, and courageous in their practice. We end with a discussion of our
findings in relation to the TPACK framework in HCI.

"Human-aware" has become a popular keyword used to describe a particular
class of AI systems that are designed to work and interact with humans. While
there exists a surprising level of consistency among the works that use the
label human-aware, the term itself mostly remains poorly understood. In this
work, we retroactively try to provide an account of what constitutes a
human-aware AI system. We see that human-aware AI is a design-oriented
paradigm, one that focuses on the need for modeling the humans it may interact
with. Additionally, we see that this paradigm offers us intuitive dimensions to
understand and categorize the kinds of interactions these systems might have
with humans. We show the pedagogical value of these dimensions by using them as
a tool to understand and review the current landscape of work related to
human-AI systems that purport some form of human modeling. To fit the scope of
a workshop paper, we specifically narrowed our review to papers that deal with
sequential decision-making and were published in a major AI conference in the
last three years. Our analysis helps identify the space of potential research
problems that are currently being overlooked. We perform additional analysis on
the degree to which these works make explicit reference to results from social
science and whether they actually perform user-studies to validate their
systems. We also provide an accounting of the various AI methods used by these
works.

Deep neural network (DNN) architectures, such as convolutional neural
networks (CNN), involve heavy computation and require hardware, such as CPU,
GPU, and AI accelerators, to provide the massive computing power. With the many
varieties of AI hardware prevailing on the market, it is often hard to decide
which one is the best to use. Thus, benchmarking AI hardware effectively
becomes important and is of great help to select and optimize AI hardware.
Unfortunately, there are few AI benchmarks available in both academia and
industry. Examples are BenchNN[1], DeepBench[2], and Dawn Bench[3], which are
usually a collection of typical real DNN applications. While these benchmarks
provide performance comparison across different AI hardware, they suffer from a
number of drawbacks. First, they cannot adapt to the emerging changes of DNN
algorithms and are fixed once selected. Second, they contain tens to hundreds
of applications and take very long time to finish running. Third, they are
mainly selected from open sources, which are restricted by copyright and are
not representable to proprietary applications. In this work, a synthetic
benchmarks framework is firstly proposed to address the above drawbacks of AI
benchmarks. Instead of pre-selecting a set of open-sourced benchmarks and
running all of them, the synthetic approach generates only a one or few
benchmarks that best represent a broad range of applications using profiled
workload characteristics data of these applications. Thus, it can adapt to
emerging changes of new DNN algorithms by re-profiling new applications and
updating itself, greatly reduce benchmark count and running time, and strongly
represent DNN applications of interests. The generated benchmarks are called AI
Matrix, serving as a performance benchmarks matching the statistical workload
characteristics of a combination of applications of interests.

There's a long tradition of research using computational intelligence
(methods from artificial intelligence (AI) and machine learning (ML)), to
automatically discover, implement, and fine-tune strategies for autonomous
adaptive automated trading in financial markets, with a sequence of research
papers on this topic published at AI conferences such as IJCAI and in journals
such as Artificial Intelligence: we show here that this strand of research has
taken a number of methodological mis-steps and that actually some of the
reportedly best-performing public-domain AI/ML trading strategies can routinely
be out-performed by extremely simple trading strategies that involve no AI or
ML at all. The results that we highlight here could easily have been revealed
at the time that the relevant key papers were published, more than a decade
ago, but the accepted methodology at the time of those publications involved a
somewhat minimal approach to experimental evaluation of trader-agents, making
claims on the basis of a few thousand test-sessions of the trader-agent in a
small number of market scenarios. In this paper we present results from
exhaustive testing over wide ranges of parameter values, using parallel
cloud-computing facilities, where we conduct millions of tests and thereby
create much richer data from which firmer conclusions can be drawn. We show
that the best public-domain AI/ML traders in the published literature can be
routinely outperformed by a "sub-zero-intelligence" trading strategy that at
face value appears to be so simple as to be financially ruinous, but which
interacts with the market in such a way that in practice it is more profitable
than the well-known AI/ML strategies from the research literature. That such a
simple strategy can outperform established AI/ML-based strategies is a sign
that perhaps the AI/ML trading strategies were good answers to the wrong
question.

Objective: Breast cancer screening is of great significance in contemporary
women's health prevention. The existing machines embedded in the AI system do
not reach the accuracy that clinicians hope. How to make intelligent systems
more reliable is a common problem. Methods: 1) Ultrasound image
super-resolution: the SRGAN super-resolution network reduces the unclearness of
ultrasound images caused by the device itself and improves the accuracy and
generalization of the detection model. 2) In response to the needs of medical
images, we have improved the YOLOv4 and the CenterNet models. 3) Multi-AI
model: based on the respective advantages of different AI models, we employ two
AI models to determine clinical resuls cross validation. And we accept the same
results and refuses others. Results: 1) With the help of the super-resolution
model, the YOLOv4 model and the CenterNet model both increased the mAP score by
9.6% and 13.8%. 2) Two methods for transforming the target model into a
classification model are proposed. And the unified output is in a specified
format to facilitate the call of the molti-AI model. 3) In the classification
evaluation experiment, concatenated by the YOLOv4 model (sensitivity 57.73%,
specificity 90.08%) and the CenterNet model (sensitivity 62.64%, specificity
92.54%), the multi-AI model will refuse to make judgments on 23.55% of the
input data. Correspondingly, the performance has been greatly improved to
95.91% for the sensitivity and 96.02% for the specificity. Conclusion: Our work
makes the AI model more reliable in medical image diagnosis. Significance: 1)
The proposed method makes the target detection model more suitable for
diagnosing breast ultrasound images. 2) It provides a new idea for artificial
intelligence in medical diagnosis, which can more conveniently introduce target
detection models from other fields to serve medical lesion screening.

Responsible AI is becoming critical as AI is widely used in our everyday
lives. Many companies that deploy AI publicly state that when training a model,
we not only need to improve its accuracy, but also need to guarantee that the
model does not discriminate against users (fairness), is resilient to noisy or
poisoned data (robustness), is explainable, and more. In addition, these
objectives are not only relevant to model training, but to all steps of
end-to-end machine learning, which include data collection, data cleaning and
validation, model training, model evaluation, and model management and serving.
Finally, responsible AI is conceptually challenging, and supporting all the
objectives must be as easy as possible. We thus propose three key research
directions towards this vision - depth, breadth, and usability - to measure
progress and introduce our ongoing research. First, responsible AI must be
deeply supported where multiple objectives like fairness and robust must be
handled together. To this end, we propose FR-Train, a holistic framework for
fair and robust model training in the presence of data bias and poisoning.
Second, responsible AI must be broadly supported, preferably in all steps of
machine learning. Currently we focus on the data pre-processing steps and
propose Slice Tuner, a selective data acquisition framework for training fair
and accurate models, and MLClean, a data cleaning framework that also improves
fairness and robustness. Finally, responsible AI must be usable where the
techniques must be easy to deploy and actionable. We propose FairBatch, a batch
selection approach for fairness that is effective and simple to use, and Slice
Finder, a model evaluation tool that automatically finds problematic slices. We
believe we scratched the surface of responsible AI for end-to-end machine
learning and suggest research challenges moving forward.

Human resource management technologies have moved from biometric surveillance
to emotional artificial intelligence (AI) that monitor employees' engagement
and productivity, analyze video interviews and CVs of job applicants. The rise
of the US$20 billion emotional AI industry will transform the future workplace.
Yet, besides no international consensus on the principles or standards for such
technologies, there is a lack of cross-cultural research on future job seekers'
attitude toward such use of AI technologies. This study collects a
cross-sectional dataset of 1,015 survey responses of international students
from 48 countries and 8 regions worldwide. A majority of the respondents (52%)
are concerned about being managed by AI. Following the hypothetico-deductivist
philosophy of science, we use the MCMC Hamiltonian approach and conduct a
detailed comparison of 10 Bayesian network models with the PSIS-LOO method. We
consistently find having a higher income, being male, majoring in business,
and/or self-rated familiarity with AI correlate with a more positive view of
emotional AI in the workplace. There is also a stark cross-cultural and
cross-regional difference. Our analysis shows people from economically less
developed regions (Africa, Oceania, Central Asia) tend to exhibit less concern
for AI managers. And for East Asian countries, 64% of the Japanese, 56% of the
South Korean, and 42% of the Chinese professed the trusting attitude. In
contrast, an overwhelming majority of 75% of the European and Northern American
possesses the worrying/neutral attitude toward being managed by AI. Regarding
religion, Muslim students correlate with the most concern toward emotional AI
in the workplace. When religiosity is higher, the correlation becomes stronger
for Muslim and Buddhist students.

The field of artificial intelligence (AI) is witnessing a recent upsurge in
research, tools development, and deployment of applications. Multiple software
companies are shifting their focus to developing intelligent systems; and many
others are deploying AI paradigms to their existing processes. In parallel, the
academic research community is injecting AI paradigms to provide solutions to
traditional engineering problems. Similarly, AI has evidently been proved
useful to software engineering (SE). When one observes the SE phases
(requirements, design, development, testing, release, and maintenance), it
becomes clear that multiple AI paradigms (such as neural networks, machine
learning, knowledge-based systems, natural language processing) could be
applied to improve the process and eliminate many of the major challenges that
the SE field has been facing. This survey chapter is a review of the most
commonplace methods of AI applied to SE. The review covers methods between
years 1975-2017, for the requirements phase, 46 major AI-driven methods are
found, 19 for design, 15 for development, 68 for testing, and 15 for release
and maintenance. Furthermore, the purpose of this chapter is threefold;
firstly, to answer the following questions: is there sufficient intelligence in
the SE lifecycle? What does applying AI to SE entail? Secondly, to measure,
formulize, and evaluate the overlap of SE phases and AI disciplines. Lastly,
this chapter aims to provide serious questions to challenging the current
conventional wisdom (i.e., status quo) of the state-of-the-art, craft a call
for action, and to redefine the path forward.

The recent advancements in artificial intelligence (AI) combined with the
extensive amount of data generated by today's clinical systems, has led to the
development of imaging AI solutions across the whole value chain of medical
imaging, including image reconstruction, medical image segmentation,
image-based diagnosis and treatment planning. Notwithstanding the successes and
future potential of AI in medical imaging, many stakeholders are concerned of
the potential risks and ethical implications of imaging AI solutions, which are
perceived as complex, opaque, and difficult to comprehend, utilise, and trust
in critical clinical applications. Despite these concerns and risks, there are
currently no concrete guidelines and best practices for guiding future AI
developments in medical imaging towards increased trust, safety and adoption.
To bridge this gap, this paper introduces a careful selection of guiding
principles drawn from the accumulated experiences, consensus, and best
practices from five large European projects on AI in Health Imaging. These
guiding principles are named FUTURE-AI and its building blocks consist of (i)
Fairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness
and (vi) Explainability. In a step-by-step approach, these guidelines are
further translated into a framework of concrete recommendations for specifying,
developing, evaluating, and deploying technically, clinically and ethically
trustworthy AI solutions into clinical practice.

The proposed European Artificial Intelligence Act (AIA) is the first attempt
to elaborate a general legal framework for AI carried out by any major global
economy. As such, the AIA is likely to become a point of reference in the
larger discourse on how AI systems can (and should) be regulated. In this
article, we describe and discuss the two primary enforcement mechanisms
proposed in the AIA: the conformity assessments that providers of high-risk AI
systems are expected to conduct, and the post-market monitoring plans that
providers must establish to document the performance of high-risk AI systems
throughout their lifetimes. We argue that AIA can be interpreted as a proposal
to establish a Europe-wide ecosystem for conducting AI auditing, albeit in
other words. Our analysis offers two main contributions. First, by describing
the enforcement mechanisms included in the AIA in terminology borrowed from
existing literature on AI auditing, we help providers of AI systems understand
how they can prove adherence to the requirements set out in the AIA in
practice. Second, by examining the AIA from an auditing perspective, we seek to
provide transferable lessons from previous research about how to refine further
the regulatory approach outlined in the AIA. We conclude by highlighting seven
aspects of the AIA where amendments (or simply clarifications) would be
helpful. These include, above all, the need to translate vague concepts into
verifiable criteria and to strengthen the institutional safeguards concerning
conformity assessments based on internal checks.

The Observation--Hypothesis--Prediction--Experimentation loop paradigm for
scientific research has been practiced by researchers for years towards
scientific discoveries. However, with data explosion in both mega-scale and
milli-scale scientific research, it has been sometimes very difficult to
manually analyze the data and propose new hypotheses to drive the cycle for
scientific discovery. In this paper, we discuss the role of Explainable AI in
scientific discovery process by demonstrating an Explainable AI-based paradigm
for science discovery. The key is to use Explainable AI to help derive data or
model interpretations, hypotheses, as well as scientific discoveries or
insights. We show how computational and data-intensive methodology -- together
with experimental and theoretical methodology -- can be seamlessly integrated
for scientific research. To demonstrate the AI-based science discovery process,
and to pay our respect to some of the greatest minds in human history, we show
how Kepler's laws of planetary motion and Newton's law of universal gravitation
can be rediscovered by (Explainable) AI based on Tycho Brahe's astronomical
observation data, whose works were leading the scientific revolution in the
16-17th century. This work also highlights the important role of Explainable AI
(as compared to Blackbox AI) in science discovery to help humans prevent or
better prepare for the possible technological singularity that may happen in
the future, since science is not only about the know how, but also the know
why. Presentation of the work is available at
https://slideslive.com/38986142/from-kepler-to-newton-explainable-ai-for-science-discovery.

When people receive advice while making difficult decisions, they often make
better decisions in the moment and also increase their knowledge in the
process. However, such incidental learning can only occur when people
cognitively engage with the information they receive and process this
information thoughtfully. How do people process the information and advice they
receive from AI, and do they engage with it deeply enough to enable learning?
To answer these questions, we conducted three experiments in which individuals
were asked to make nutritional decisions and received simulated AI
recommendations and explanations. In the first experiment, we found that when
people were presented with both a recommendation and an explanation before
making their choice, they made better decisions than they did when they
received no such help, but they did not learn. In the second experiment,
participants first made their own choice, and only then saw a recommendation
and an explanation from AI; this condition also resulted in improved decisions,
but no learning. However, in our third experiment, participants were presented
with just an AI explanation but no recommendation and had to arrive at their
own decision. This condition led to both more accurate decisions and learning
gains. We hypothesize that learning gains in this condition were due to deeper
engagement with explanations needed to arrive at the decisions. This work
provides some of the most direct evidence to date that it may not be sufficient
to include explanations together with AI-generated recommendation to ensure
that people engage carefully with the AI-provided information. This work also
presents one technique that enables incidental learning and, by implication,
can help people process AI recommendations and explanations more carefully.

Deep learning models for medical image segmentation can fail unexpectedly and
spectacularly for pathological cases and images acquired at different centers
than training images, with labeling errors that violate expert knowledge. Such
errors undermine the trustworthiness of deep learning models for medical image
segmentation. Mechanisms for detecting and correcting such failures are
essential for safely translating this technology into clinics and are likely to
be a requirement of future regulations on artificial intelligence (AI). In this
work, we propose a trustworthy AI theoretical framework and a practical system
that can augment any backbone AI system using a fallback method and a fail-safe
mechanism based on Dempster-Shafer theory. Our approach relies on an actionable
definition of trustworthy AI. Our method automatically discards the voxel-level
labeling predicted by the backbone AI that violate expert knowledge and relies
on a fallback for those voxels. We demonstrate the effectiveness of the
proposed trustworthy AI approach on the largest reported annotated dataset of
fetal MRI consisting of 540 manually annotated fetal brain 3D T2w MRIs from 13
centers. Our trustworthy AI method improves the robustness of a
state-of-the-art backbone AI for fetal brain MRIs acquired across various
centers and for fetuses with various brain abnormalities.

Objectives: To compare artificial intelligence (AI) as a second reader in
detecting lung nodules on chest X-rays (CXR) versus radiologists of two
binational institutions, and to evaluate AI performance when using two
different modes: automated versus assisted (additional remote radiologist
review).
  Methods: The CXR public database (n = 247) of the Japanese Society of
Radiological Technology with various types and sizes of lung nodules was
analyzed. Eight radiologists evaluated the CXR images with regard to the
presence of lung nodules and nodule conspicuity. After radiologist review, the
AI software processed and flagged the CXR with the highest probability of
missed nodules. The calculated accuracy metrics were the area under the curve
(AUC), sensitivity, specificity, F1 score, false negative case number (FN), and
the effect of different AI modes (automated/assisted) on the accuracy of nodule
detection.
  Results: For radiologists, the average AUC value was 0.77 $\pm$ 0.07, while
the average FN was 52.63 $\pm$ 17.53 (all studies) and 32 $\pm$ 11.59 (studies
containing a nodule of malignant etiology = 32% rate of missed malignant
nodules). Both AI modes -- automated and assisted -- produced an average
increase in sensitivity (by 14% and 12%) and of F1-score (5% and 6%) and a
decrease in specificity (by 10% and 3%, respectively).
  Conclusions: Both AI modes flagged the pulmonary nodules missed by
radiologists in a significant number of cases. AI as a second reader has a high
potential to improve diagnostic accuracy and radiology workflow. AI might
detect certain pulmonary nodules earlier than radiologists, with a potentially
significant impact on patient outcomes.

Purpose: Artificial intelligence (AI) solutions for medical diagnosis require
thorough evaluation to demonstrate that performance is maintained for all
patient sub-groups and to ensure that proposed improvements in care will be
delivered equitably. This study evaluates the robustness of an AI solution for
the diagnosis of normal chest X-rays (CXRs) by comparing performance across
multiple patient and environmental subgroups, as well as comparing AI errors
with those made by human experts.
  Methods: A total of 4,060 CXRs were sampled to represent a diverse dataset of
NHS patients and care settings. Ground-truth labels were assigned by a
3-radiologist panel. AI performance was evaluated against assigned labels and
sub-groups analysis was conducted against patient age and sex, as well as CXR
view, modality, device manufacturer and hospital site.
  Results: The AI solution was able to remove 18.5% of the dataset by
classification as High Confidence Normal (HCN). This was associated with a
negative predictive value (NPV) of 96.0%, compared to 89.1% for diagnosis of
normal scans by radiologists. In all AI false negative (FN) cases, a
radiologist was found to have also made the same error when compared to final
ground-truth labels. Subgroup analysis showed no statistically significant
variations in AI performance, whilst reduced normal classification was observed
in data from some hospital sites.
  Conclusion: We show the AI solution could provide meaningful workload savings
by diagnosis of 18.5% of scans as HCN with a superior NPV to human readers. The
AI solution is shown to perform well across patient subgroups and error cases
were shown to be subjective or subtle in nature.

As artificial intelligence (AI) becomes a prominent part of modern life, AI
literacy is becoming important for all citizens, not just those in technology
careers. Previous research in AI education materials has largely focused on the
introduction of terminology as well as AI use cases and ethics, but few allow
students to learn by creating their own machine learning models. Therefore,
there is a need for enriching AI educational tools with more adaptable and
flexible platforms for interested educators with any level of technical
experience to utilize within their teaching material. As such, we propose the
development of an open-source tool (Build-a-Bot) for students and teachers to
not only create their own transformer-based chatbots based on their own course
material, but also learn the fundamentals of AI through the model creation
process. The primary concern of this paper is the creation of an interface for
students to learn the principles of artificial intelligence by using a natural
language pipeline to train a customized model to answer questions based on
their own school curriculums. The model uses contexts given by their
instructor, such as chapters of a textbook, to answer questions and is deployed
on an interactive chatbot/voice agent. The pipeline teaches students data
collection, data augmentation, intent recognition, and question answering by
having them work through each of these processes while creating their AI agent,
diverging from previous chatbot work where students and teachers use the bots
as black-boxes with no abilities for customization or the bots lack AI
capabilities, with the majority of dialogue scripts being rule-based. In
addition, our tool is designed to make each step of this pipeline intuitive for
students at a middle-school level. Further work primarily lies in providing our
tool to schools and seeking student and teacher evaluations.

Large AI models, or foundation models, are models recently emerging with
massive scales both parameter-wise and data-wise, the magnitudes of which can
reach beyond billions. Once pretrained, large AI models demonstrate impressive
performance in various downstream tasks. A prime example is ChatGPT, whose
capability has compelled people's imagination about the far-reaching influence
that large AI models can have and their potential to transform different
domains of our lives. In health informatics, the advent of large AI models has
brought new paradigms for the design of methodologies. The scale of multi-modal
data in the biomedical and health domain has been ever-expanding especially
since the community embraced the era of deep learning, which provides the
ground to develop, validate, and advance large AI models for breakthroughs in
health-related areas. This article presents a comprehensive review of large AI
models, from background to their applications. We identify seven key sectors in
which large AI models are applicable and might have substantial influence,
including 1) bioinformatics; 2) medical diagnosis; 3) medical imaging; 4)
medical informatics; 5) medical education; 6) public health; and 7) medical
robotics. We examine their challenges, followed by a critical discussion about
potential future directions and pitfalls of large AI models in transforming the
field of health informatics.

With the Proposal for a Regulation laying down harmonised rules on Artificial
Intelligence (AI Act) the European Union provides the first regulatory document
that applies to the entire complex of AI systems. While some fear that the
regulation leaves too much room for interpretation and thus bring little
benefit to society, others expect that the regulation is too restrictive and,
thus, blocks progress and innovation, as well as hinders the economic success
of companies within the EU. Without a systematic approach, it is difficult to
assess how it will actually impact the AI landscape. In this paper, we suggest
a systematic approach that we applied on the initial draft of the AI Act that
has been released in April 2021. We went through several iterations of
compiling the list of AI products and projects in and from Germany, which the
Lernende Systeme platform lists, and then classified them according to the AI
Act together with experts from the fields of computer science and law. Our
study shows a need for more concrete formulation, since for some provisions it
is often unclear whether they are applicable in a specific case or not. Apart
from that, it turns out that only about 30\% of the AI systems considered would
be regulated by the AI Act, the rest would be classified as low-risk. However,
as the database is not representative, the results only provide a first
assessment. The process presented can be applied to any collections, and also
repeated when regulations are about to change. This allows fears of over- or
under-regulation to be investigated before the regulations comes into effect.

In this paper we, an epistemologist and a machine learning scientist, argue
that we need to pursue a novel area of philosophical research in AI - the
ethics of belief for AI. Here we take the ethics of belief to refer to a field
at the intersection of epistemology and ethics concerned with possible moral,
practical, and other non-truth-related dimensions of belief. In this paper we
will primarily be concerned with the normative question within the ethics of
belief regarding what agents - both human and artificial - ought to believe,
rather than with questions concerning whether beliefs meet certain evaluative
standards such as being true, being justified, constituting knowledge, etc. We
suggest four topics in extant work in the ethics of (human) belief that can be
applied to an ethics of AI belief: doxastic wronging by AI (morally wronging
someone in virtue of beliefs held about them); morally owed beliefs (beliefs
that agents are morally obligated to hold); pragmatic and moral encroachment
(cases where the practical or moral features of a belief is relevant to its
epistemic status, and in our case specifically to whether an agent ought to
hold the belief); and moral responsibility for AI beliefs. We also indicate two
relatively nascent areas of philosophical research that haven't yet been
generally recognized as ethics of AI belief research, but that do fall within
this field of research in virtue of investigating various moral and practical
dimensions of belief: the epistemic and ethical decolonization of AI; and
epistemic injustice in AI.

Current ethical debates on the use of artificial intelligence (AI) in health
care treat AI as a product of technology in three ways: First, by assessing
risks and potential benefits of currently developed AI-enabled products with
ethical checklists; second, by proposing ex ante lists of ethical values seen
as relevant for the design and development of assisting technology, and third,
by promoting AI technology to use moral reasoning as part of the automation
process. Subsequently, we propose a fourth approach to AI, namely as a
methodological tool to assist ethical reflection. We provide a concept of an
AI-simulation informed by three separate elements: 1) stochastic human behavior
models based on behavioral data for simulating realistic settings, 2)
qualitative empirical data on value statements regarding internal policy, and
3) visualization components that aid in understanding the impact of changes in
these variables. The potential of this approach is to inform an
interdisciplinary field about anticipated ethical challenges or ethical
trade-offs in concrete settings and, hence, to spark a re-evaluation of design
and implementation plans. This may be particularly useful for applications that
deal with extremely complex values and behavior or with limitations on the
communication resources of affected persons (e.g., persons with dementia care
or for care of persons with cognitive impairment). Simulation does not replace
ethical reflection but does allow for detailed, context-sensitive analysis
during the design process and prior to implementation. Finally, we discuss the
inherently quantitative methods of analysis afforded by stochastic simulations
as well as the potential for ethical discussions and how simulations with AI
can improve traditional forms of thought experiments and future-oriented
technology assessment.

The Multisource AI Scorecard Table (MAST) is a checklist tool based on
analytic tradecraft standards to inform the design and evaluation of
trustworthy AI systems. In this study, we evaluate whether MAST is associated
with people's trust perceptions in AI-enabled decision support systems
(AI-DSSs). Evaluating trust in AI-DSSs poses challenges to researchers and
practitioners. These challenges include identifying the components,
capabilities, and potential of these systems, many of which are based on the
complex deep learning algorithms that drive DSS performance and preclude
complete manual inspection. We developed two interactive, AI-DSS test
environments using the MAST criteria. One emulated an identity verification
task in security screening, and another emulated a text summarization system to
aid in an investigative reporting task. Each test environment had one version
designed to match low-MAST ratings, and another designed to match high-MAST
ratings, with the hypothesis that MAST ratings would be positively related to
the trust ratings of these systems. A total of 177 subject matter experts were
recruited to interact with and evaluate these systems. Results generally show
higher MAST ratings for the high-MAST conditions compared to the low-MAST
groups, and that measures of trust perception are highly correlated with the
MAST ratings. We conclude that MAST can be a useful tool for designing and
evaluating systems that will engender high trust perceptions, including AI-DSS
that may be used to support visual screening and text summarization tasks.
However, higher MAST ratings may not translate to higher joint performance.

Although data-driven artificial intelligence (AI) in medical image diagnosis
has shown impressive performance in silico, the lack of interpretability makes
it difficult to incorporate the "black box" into clinicians' workflows. To make
the diagnostic patterns learned from data understandable by clinicians, we
develop an interpretable model, knowledge-guided diagnosis model (KGDM), that
provides a visualized reasoning process containing AI-based biomarkers and
retrieved cases that with the same diagnostic patterns. It embraces clinicians'
prompts into the interpreted reasoning through human-AI interaction, leading to
potentially enhanced safety and more accurate predictions. This study
investigates the performance, interpretability, and clinical utility of KGDM in
the diagnosis of infectious keratitis (IK), which is the leading cause of
corneal blindness. The classification performance of KGDM is evaluated on a
prospective validation dataset, an external testing dataset, and an publicly
available testing dataset. The diagnostic odds ratios (DOR) of the interpreted
AI-based biomarkers are effective, ranging from 3.011 to 35.233 and exhibit
consistent diagnostic patterns with clinic experience. Moreover, a human-AI
collaborative diagnosis test is conducted and the participants with
collaboration achieved a performance exceeding that of both humans and AI. By
synergistically integrating interpretability and interaction, this study
facilitates the convergence of clinicians' expertise and data-driven
intelligence. The promotion of inexperienced ophthalmologists with the aid of
AI-based biomarkers, as well as increased AI prediction by intervention from
experienced ones, demonstrate a promising diagnostic paradigm for infectious
keratitis using KGDM, which holds the potential for extension to other diseases
where experienced medical practitioners are limited and the safety of AI is
concerned.

Over the past decade, artificial intelligence (AI) methods in pathology have
advanced substantially. However, integration into routine clinical practice has
been slow due to numerous challenges, including technical and regulatory
hurdles in translating research results into clinical diagnostic products and
the lack of standardized interfaces. The open and vendor-neutral EMPAIA
initiative addresses these challenges. Here, we provide an overview of EMPAIA's
achievements and lessons learned. EMPAIA integrates various stakeholders of the
pathology AI ecosystem, i.e., pathologists, computer scientists, and industry.
In close collaboration, we developed technical interoperability standards,
recommendations for AI testing and product development, and explainability
methods. We implemented the modular and open-source EMPAIA platform and
successfully integrated 14 AI-based image analysis apps from 8 different
vendors, demonstrating how different apps can use a single standardized
interface. We prioritized requirements and evaluated the use of AI in real
clinical settings with 14 different pathology laboratories in Europe and Asia.
In addition to technical developments, we created a forum for all stakeholders
to share information and experiences on digital pathology and AI. Commercial,
clinical, and academic stakeholders can now adopt EMPAIA's common open-source
interfaces, providing a unique opportunity for large-scale standardization and
streamlining of processes. Further efforts are needed to effectively and
broadly establish AI assistance in routine laboratory use. To this end, a
sustainable infrastructure, the non-profit association EMPAIA International,
has been established to continue standardization and support broad
implementation and advocacy for an AI-assisted digital pathology future.

Recent progress in artificial intelligence (AI) marks a pivotal moment in
human history. It presents the opportunity for machines to learn, adapt, and
perform tasks that have the potential to assist people, from everyday
activities to their most creative and ambitious projects. It also has the
potential to help businesses and organizations harness knowledge, increase
productivity, innovate, transform, and power shared prosperity. This tremendous
potential raises two fundamental questions: (1) Will AI actually advance
national and global economic transformation to benefit society at large? and
(2) What issues must we get right to fully realize AI's economic value, expand
prosperity and improve lives everywhere? We explore these questions by
considering the recent history of technology and innovation as a guide for the
likely impact of AI and what we must do to realize its economic potential to
benefit society. While we do not presume the future will be entirely like that
past, for reasons we will discuss, we do believe prior experience with
technological change offers many useful lessons. We conclude that while
progress in AI presents a historic opportunity to advance our economic
prosperity and future wellbeing, its economic benefits will not come
automatically and that AI risks exacerbating existing economic challenges
unless we collectively and purposefully act to enable its potential and address
its challenges. We suggest a collective policy agenda - involving developers,
deployers and users of AI, infrastructure providers, policymakers, and those
involved in workforce training - that may help both realize and harness AI's
economic potential and address its risks to our shared prosperity.

Computing power, or "compute," is crucial for the development and deployment
of artificial intelligence (AI) capabilities. As a result, governments and
companies have started to leverage compute as a means to govern AI. For
example, governments are investing in domestic compute capacity, controlling
the flow of compute to competing countries, and subsidizing compute access to
certain sectors. However, these efforts only scratch the surface of how compute
can be used to govern AI development and deployment. Relative to other key
inputs to AI (data and algorithms), AI-relevant compute is a particularly
effective point of intervention: it is detectable, excludable, and
quantifiable, and is produced via an extremely concentrated supply chain. These
characteristics, alongside the singular importance of compute for cutting-edge
AI models, suggest that governing compute can contribute to achieving common
policy objectives, such as ensuring the safety and beneficial use of AI. More
precisely, policymakers could use compute to facilitate regulatory visibility
of AI, allocate resources to promote beneficial outcomes, and enforce
restrictions against irresponsible or malicious AI development and usage.
However, while compute-based policies and technologies have the potential to
assist in these areas, there is significant variation in their readiness for
implementation. Some ideas are currently being piloted, while others are
hindered by the need for fundamental research. Furthermore, naive or poorly
scoped approaches to compute governance carry significant risks in areas like
privacy, economic impacts, and centralization of power. We end by suggesting
guardrails to minimize these risks from compute governance.

The question "Can machines think?" and the Turing Test to assess whether
machines could achieve human-level intelligence is one of the roots of AI. With
the philosophical argument "I think, therefore I am", this paper challenge the
idea of a "thinking machine" supported by current AIs since there is no sense
of self in them. Current artificial intelligence is only seemingly intelligent
information processing and does not truly understand or be subjectively aware
of oneself and perceive the world with the self as human intelligence does. In
this paper, we introduce a Brain-inspired and Self-based Artificial
Intelligence (BriSe AI) paradigm. This BriSe AI paradigm is dedicated to
coordinating various cognitive functions and learning strategies in a
self-organized manner to build human-level AI models and robotic applications.
Specifically, BriSe AI emphasizes the crucial role of the Self in shaping the
future AI, rooted with a practical hierarchical Self framework, including
Perception and Learning, Bodily Self, Autonomous Self, Social Self, and
Conceptual Self. The hierarchical framework of the Self highlights self-based
environment perception, self-bodily modeling, autonomous interaction with the
environment, social interaction and collaboration with others, and even more
abstract understanding of the Self. Furthermore, the positive mutual promotion
and support among multiple levels of Self, as well as between Self and
learning, enhance the BriSe AI's conscious understanding of information and
flexible adaptation to complex environments, serving as a driving force
propelling BriSe AI towards real Artificial General Intelligence.

This study explores the challenge of sentence-level AI-generated text
detection within human-AI collaborative hybrid texts. Existing studies of
AI-generated text detection for hybrid texts often rely on synthetic datasets.
These typically involve hybrid texts with a limited number of boundaries. We
contend that studies of detecting AI-generated content within hybrid texts
should cover different types of hybrid texts generated in realistic settings to
better inform real-world applications. Therefore, our study utilizes the
CoAuthor dataset, which includes diverse, realistic hybrid texts generated
through the collaboration between human writers and an intelligent writing
system in multi-turn interactions. We adopt a two-step, segmentation-based
pipeline: (i) detect segments within a given hybrid text where each segment
contains sentences of consistent authorship, and (ii) classify the authorship
of each identified segment. Our empirical findings highlight (1) detecting
AI-generated sentences in hybrid texts is overall a challenging task because
(1.1) human writers' selecting and even editing AI-generated sentences based on
personal preferences adds difficulty in identifying the authorship of segments;
(1.2) the frequent change of authorship between neighboring sentences within
the hybrid text creates difficulties for segment detectors in identifying
authorship-consistent segments; (1.3) the short length of text segments within
hybrid texts provides limited stylistic cues for reliable authorship
determination; (2) before embarking on the detection process, it is beneficial
to assess the average length of segments within the hybrid text. This
assessment aids in deciding whether (2.1) to employ a text segmentation-based
strategy for hybrid texts with longer segments, or (2.2) to adopt a direct
sentence-by-sentence classification strategy for those with shorter segments.

As jurisdictions around the world take their first steps toward regulating
the most powerful AI systems, such as the EU AI Act and the US Executive Order
14110, there is a growing need for effective enforcement mechanisms that can
verify compliance and respond to violations. We argue that compute providers
should have legal obligations and ethical responsibilities associated with AI
development and deployment, both to provide secure infrastructure and to serve
as intermediaries for AI regulation. Compute providers can play an essential
role in a regulatory ecosystem via four key capacities: as securers,
safeguarding AI systems and critical infrastructure; as record keepers,
enhancing visibility for policymakers; as verifiers of customer activities,
ensuring oversight; and as enforcers, taking actions against rule violations.
We analyze the technical feasibility of performing these functions in a
targeted and privacy-conscious manner and present a range of technical
instruments. In particular, we describe how non-confidential information, to
which compute providers largely already have access, can provide two key
governance-relevant properties of a computational workload: its type-e.g.,
large-scale training or inference-and the amount of compute it has consumed.
Using AI Executive Order 14110 as a case study, we outline how the US is
beginning to implement record keeping requirements for compute providers. We
also explore how verification and enforcement roles could be added to establish
a comprehensive AI compute oversight scheme. We argue that internationalization
will be key to effective implementation, and highlight the critical challenge
of balancing confidentiality and privacy with risk mitigation as the role of
compute providers in AI regulation expands.

Aims. To develop a deep-learning based system for recognition of subclinical
atherosclerosis on a plain frontal chest x-ray. Methods and Results. A
deep-learning algorithm to predict coronary artery calcium (CAC) score (the
AI-CAC model) was developed on 460 chest x-ray (80% training cohort, 20%
internal validation cohort) of primary prevention patients (58.4% male, median
age 63 [51-74] years) with available paired chest x-ray and chest computed
tomography (CT) indicated for any clinical reason and performed within 3
months. The CAC score calculated on chest CT was used as ground truth. The
model was validated on an temporally-independent cohort of 90 patients from the
same institution (external validation). The diagnostic accuracy of the AI-CAC
model assessed by the area under the curve (AUC) was the primary outcome.
Overall, median AI-CAC score was 35 (0-388) and 28.9% patients had no AI-CAC.
AUC of the AI-CAC model to identify a CAC>0 was 0.90 in the internal validation
cohort and 0.77 in the external validation cohort. Sensitivity was consistently
above 92% in both cohorts. In the overall cohort (n=540), among patients with
AI-CAC=0, a single ASCVD event occurred, after 4.3 years. Patients with
AI-CAC>0 had significantly higher Kaplan Meier estimates for ASCVD events
(13.5% vs. 3.4%, log-rank=0.013). Conclusion. The AI-CAC model seems to
accurately detect subclinical atherosclerosis on chest x-ray with elevated
sensitivity, and to predict ASCVD events with elevated negative predictive
value. Adoption of the AI-CAC model to refine CV risk stratification or as an
opportunistic screening tool requires prospective evaluation.

As Artificial Intelligence (AI) making advancements in medical
decision-making, there is a growing need to ensure doctors develop appropriate
reliance on AI to avoid adverse outcomes. However, existing methods in enabling
appropriate AI reliance might encounter challenges while being applied in the
medical domain. With this regard, this work employs and provides the validation
of an alternative approach -- majority voting -- to facilitate appropriate
reliance on AI in medical decision-making. This is achieved by a
multi-institutional user study involving 32 medical professionals with various
backgrounds, focusing on the pathology task of visually detecting a pattern,
mitoses, in tumor images. Here, the majority voting process was conducted by
synthesizing decisions under AI assistance from a group of pathology doctors
(pathologists). Two metrics were used to evaluate the appropriateness of AI
reliance: Relative AI Reliance (RAIR) and Relative Self-Reliance (RSR). Results
showed that even with groups of three pathologists, majority-voted decisions
significantly increased both RAIR and RSR -- by approximately 9% and 31%,
respectively -- compared to decisions made by one pathologist collaborating
with AI. This increased appropriateness resulted in better precision and recall
in the detection of mitoses. While our study is centered on pathology, we
believe these insights can be extended to general high-stakes decision-making
processes involving similar visual tasks.

Advances in artificial intelligence (AI) will transform many aspects of our
lives and society, bringing immense opportunities but also posing significant
risks and challenges. The next several decades may well be a turning point for
humanity, comparable to the industrial revolution. We write to share a set of
recommendations for moving forward from the perspective of the founder and
leaders of the One Hundred Year Study on AI. Launched a decade ago, the project
is committed to a perpetual series of studies by multidisciplinary experts to
evaluate the immediate, longer-term, and far-reaching effects of AI on people
and society, and to make recommendations about AI research, policy, and
practice. As we witness new capabilities emerging from neural models, it is
crucial that we engage in efforts to advance our scientific understanding of
these models and their behaviors. We must address the impact of AI on people
and society through technical, social, and sociotechnical lenses, incorporating
insights from a diverse range of experts including voices from engineering,
social, behavioral, and economic disciplines. By fostering dialogue,
collaboration, and action among various stakeholders, we can strategically
guide the development and deployment of AI in ways that maximize its potential
for contributing to human flourishing. Despite the growing divide in the field
between focusing on short-term versus long-term implications, we think both are
of critical importance. As Alan Turing, one of the pioneers of AI, wrote in
1950, "We can only see a short distance ahead, but we can see plenty there that
needs to be done." We offer ten recommendations for action that collectively
address both the short- and long-term potential impacts of AI technologies.

Over the past decade, AI research has focused heavily on building ever-larger
deep learning models. This approach has simultaneously unlocked incredible
achievements in science and technology, and hindered AI from overcoming
long-standing limitations with respect to explainability, ethical harms, and
environmental efficiency. Drawing on qualitative interviews and computational
analyses, our three-part history of AI research traces the creation of this
"epistemic monoculture" back to a radical reconceptualization of scientific
progress that began in the late 1980s. In the first era of AI research
(1950s-late 1980s), researchers and patrons approached AI as a "basic" science
that would advance through autonomous exploration and organic assessments of
progress (e.g., peer-review, theoretical consensus). The failure of this
approach led to a retrenchment of funding in the 1980s. Amid this "AI Winter,"
an intervention by the U.S. government reoriented the field towards measurable
progress on tasks of military and commercial interest. A new evaluation system
called "benchmarking" provided an objective way to quantify progress on tasks
by focusing exclusively on increasing predictive accuracy on example datasets.
Distilling science down to verifiable metrics clarified the roles of
scientists, allowed the field to rapidly integrate talent, and provided clear
signals of significance and progress. But history has also revealed a tradeoff
to this streamlined approach to science: the consolidation around external
interests and inherent conservatism of benchmarking has disincentivized
exploration beyond scaling monoculture. In the discussion, we explain how AI's
monoculture offers a compelling challenge to the belief that basic,
exploration-driven research is needed for scientific progress. Implications for
the spread of AI monoculture to other sciences in the era of generative AI are
also discussed.

Regions in the genome that affect complex traits, quantitative trait loci
(QTL), can be identified using statistical analysis of genetic and phenotypic
data. When restricted maximum-likelihood (REML) models are used, the mapping
procedure is normally computationally demanding. We develop a new efficient
computational scheme for QTL mapping using variance component analysis and the
AI-REML algorithm. The algorithm uses an exact or approximative low-rank
representation of the identity-by-descent matrix, which combined with the
Woodbury formula for matrix inversion results in that the computations in the
AI-REML iteration body can be performed more efficiently. For cases where an
exact low-rank representation of the IBD matrix is available a-priori, the
improved AI-REML algorithm normally runs almost twice as fast compared to the
standard version. When an exact low-rank representation is not available, a
truncated spectral decomposition is used to determine a low-rank approximation.
We show that also in this case, the computational efficiency of the AI-REML
scheme can often be significantly improved.

Lagrangian Floer homology in a general case has been constructed by Fukaya,
Oh, Ohta and Ono, where they construct an $\AI$-algebra or an $\AI$-bimodule
from Lagrangian submanifolds, and studied the obstructions and deformation
theories. But for obstructed Lagrangian submanifolds, standard Lagrangian Floer
homology can not be defined.
  We explore several well-known cohomology theories on these $\AI$-objects and
explore their properties, which are well-defined and invariant even in the
obstructed cases. These are Hochschild and cyclic homology of an $\AI$-objects
and Chevalley-Eilenberg or cyclic Chevalley-Eilenberg homology of their
underlying $\LI$ objects. We explain how the existence of $m_0$ effects the
usual homological algebra of these homology theories. We also provide some
computations. We show that for an obstructed $\AI$-algebra with a non-trivial
primary obstruction, Chevalley-Eilenberg Floer homology vanishes, whose proof
is inspired by the comparison with cluster homology theory of Lagrangian
submanifolds by Cornea and Lalonde.
  In contrast, we also provide an example of an obstructed case whose cyclic
Floer homology is non-vanishing.

A definition of Artificial Intelligence was proposed in [1] but this
definition was not absolutely formal at least because the word "Human" was
used. In this paper we will formalize the definition from [1]. The biggest
problem in this definition was that the level of intelligence of AI is compared
to the intelligence of a human being. In order to change this we will introduce
some parameters to which AI will depend. One of this parameters will be the
level of intelligence and we will define one AI to each level of intelligence.
We assume that for some level of intelligence the respective AI will be more
intelligent than a human being. Nevertheless, we cannot say which is this level
because we cannot calculate its exact value.

We present two sampling algorithms for probabilistic confidence inference in
Bayesian networks. These two algorithms (we call them AIS-BN-mu and
AIS-BN-sigma algorithms) guarantee that estimates of posterior probabilities
are with a given probability within a desired precision bound. Our algorithms
are based on recent advances in sampling algorithms for (1) estimating the mean
of bounded random variables and (2) adaptive importance sampling in Bayesian
networks. In addition to a simple stopping rule for sampling that they provide,
the AIS-BN-mu and AIS-BN-sigma algorithms are capable of guiding the learning
process in the AIS-BN algorithm. An empirical evaluation of the proposed
algorithms shows excellent performance, even for very unlikely evidence.

Markov random fields (MRFs) are difficult to evaluate as generative models
because computing the test log-probabilities requires the intractable partition
function. Annealed importance sampling (AIS) is widely used to estimate MRF
partition functions, and often yields quite accurate results. However, AIS is
prone to overestimate the log-likelihood with little indication that anything
is wrong. We present the Reverse AIS Estimator (RAISE), a stochastic lower
bound on the log-likelihood of an approximation to the original MRF model.
RAISE requires only the same MCMC transition operators as standard AIS.
Experimental results indicate that RAISE agrees closely with AIS
log-probability estimates for RBMs, DBMs, and DBNs, but typically errs on the
side of underestimating, rather than overestimating, the log-likelihood.

High sensitivity differential atom interferometers are promising for
precision measurements in science frontiers in space, including gravity field
mapping for Earth science studies and gravitational wave detection. We propose
a new configuration of twin atom interferometers connected by a laser ranging
interferometer (LRI-AI) to provide precise information of the displacements
between the two AI reference mirrors and a means to phase-lock the two
independent interferometer lasers over long distances, thereby further
enhancing the feasibility of long baseline differential atom interferometers.
We show that a properly implemented LRI-AI can achieve equivalent functionality
to the conventional differential atom interferometer measurement system. LRI-AI
isolates the laser requirements for atom interferometers and for optical phase
readout between distant locations, thus enabling optimized allocation of
available laser power within a limited physical size and resource budget. A
unique aspect of LRI-AI also enables extended dynamic range of differential
signals and the highest possible effective data rate.

The compositional dependence of thermal expansion behaviour in 19 different
perovskite-like metal-organic frameworks (MOFs) of composition [AI][MII(HCOO)3]
(A = alkylammonium cation; M = octahedrally-coordinated divalent metal) is
studied using variable-temperature X-ray powder diffraction measurements. While
all systems show essentially the same type of thermomechanical
response-irrespective of their particular structural details-the magnitude of
this response is shown to be a function of AI and MII cation radii, as well as
the molecular anisotropy of AI. Flexibility is maximised for large MII and
small AI, while the shape of AI has implications for the direction of framework
hingeing.

We consider the problem of efficient "on the fly" tuning of existing, or {\it
legacy}, Artificial Intelligence (AI) systems. The legacy AI systems are
allowed to be of arbitrary class, albeit the data they are using for computing
interim or final decision responses should posses an underlying structure of a
high-dimensional topological real vector space. The tuning method that we
propose enables dealing with errors without the need to re-train the system.
Instead of re-training a simple cascade of perceptron nodes is added to the
legacy system. The added cascade modulates the AI legacy system's decisions. If
applied repeatedly, the process results in a network of modulating rules
"dressing up" and improving performance of existing AI systems. Mathematical
rationale behind the method is based on the fundamental property of measure
concentration in high dimensional spaces. The method is illustrated with an
example of fine-tuning a deep convolutional network that has been pre-trained
to detect pedestrians in images.

If you are an artificial intelligence researcher, you should look to video
games as ideal testbeds for the work you do. If you are a video game developer,
you should look to AI for the technology that makes completely new types of
games possible. This chapter lays out the case for both of these propositions.
It asks the question "what can video games do for AI", and discusses how in
particular general video game playing is the ideal testbed for artificial
general intelligence research. It then asks the question "what can AI do for
video games", and lays out a vision for what video games might look like if we
had significantly more advanced AI at our disposal. The chapter is based on my
keynote at IJCCI 2015, and is written in an attempt to be accessible to a broad
audience.

In this paper, we present the first-of-its-kind machine learning (ML) system,
called AI Programmer, that can automatically generate full software programs
requiring only minimal human guidance. At its core, AI Programmer uses genetic
algorithms (GA) coupled with a tightly constrained programming language that
minimizes the overhead of its ML search space. Part of AI Programmer's novelty
stems from (i) its unique system design, including an embedded, hand-crafted
interpreter for efficiency and security and (ii) its augmentation of GAs to
include instruction-gene randomization bindings and programming
language-specific genome construction and elimination techniques. We provide a
detailed examination of AI Programmer's system design, several examples
detailing how the system works, and experimental data demonstrating its
software generation capabilities and performance using only mainstream CPUs.

Artificial intelligence (AI) has achieved superhuman performance in a growing
number of tasks, but understanding and explaining AI remain challenging. This
paper clarifies the connections between machine-learning algorithms to develop
AIs and the econometrics of dynamic structural models through the case studies
of three famous game AIs. Chess-playing Deep Blue is a calibrated value
function, whereas shogi-playing Bonanza is an estimated value function via
Rust's (1987) nested fixed-point method. AlphaGo's "supervised-learning policy
network" is a deep neural network implementation of Hotz and Miller's (1993)
conditional choice probability estimation; its "reinforcement-learning value
network" is equivalent to Hotz, Miller, Sanders, and Smith's (1994) conditional
choice simulation method. Relaxing these AIs' implicit econometric assumptions
would improve their structural interpretability.

Research in Artificial Intelligence is breaking technology barriers every
day. New algorithms and high performance computing are making things possible
which we could only have imagined earlier. Though the enhancements in AI are
making life easier for human beings day by day, there is constant fear that AI
based systems will pose a threat to humanity. People in AI community have
diverse set of opinions regarding the pros and cons of AI mimicking human
behavior. Instead of worrying about AI advancements, we propose a novel idea of
cognitive agents, including both human and machines, living together in a
complex adaptive ecosystem, collaborating on human computation for producing
essential social goods while promoting sustenance, survival and evolution of
the agents' life cycle. We highlight several research challenges and technology
barriers in achieving this goal. We propose a governance mechanism around this
ecosystem to ensure ethical behaviors of all cognitive agents. Along with a
novel set of use-cases of Cogniculture, we discuss the road map ahead for this
journey.

Artificial intelligence (AI) is an extensive scientific discipline which
enables computer systems to solve problems by emulating complex biological
processes such as learning, reasoning and self-correction. This paper presents
a comprehensive review of the application of AI techniques for improving
performance of optical communication systems and networks. The use of AI-based
techniques is first studied in applications related to optical transmission,
ranging from the characterization and operation of network components to
performance monitoring, mitigation of nonlinearities, and quality of
transmission estimation. Then, applications related to optical network control
and management are also reviewed, including topics like optical network
planning and operation in both transport and access networks. Finally, the
paper also presents a summary of opportunities and challenges in optical
networking where AI is expected to play a key role in the near future.

We consider a simplified version of the problem of insider trading in a
financial market. We approach it by means of anticipating stochastic calculus
and compare the use of the Hitsuda-Skorokhod, the Ayed-Kuo, and the
Russo-Vallois forward integrals within this context. Our results give some
indication that, while the forward integral yields results with a suitable
financial meaning, the Hitsuda-Skorokhod and the Ayed-Kuo integrals do not
provide an appropriate formulation of this problem. Further results regarding
the use of the Ayed-Kuo integral in this context are also provided, including
the proof of the fact that the expectation of a Russo-Vallois solution is
strictly greater than that of an Ayed-Kuo solution. Finally, we conjecture the
explicit solution of an Ayed-Kuo stochastic differential equation that
possesses discontinuous sample paths with finite probability.

We describe a set of techniques to generate queries automatically based on
one or more ingested, input corpuses. These queries require no a priori domain
knowledge, and hence no human domain experts. Thus, these auto-generated
queries help address the epistemological question of how we know what we know,
or more precisely in this case, how an AI system with ingested data knows what
it knows. These auto-generated queries can also be used to identify and remedy
problem areas in ingested material -- areas for which the knowledge of the AI
system is incomplete or even erroneous. Similarly, the proposed techniques
facilitate tests of AI capability -- both in terms of coverage and accuracy. By
removing humans from the main learning loop, our approach also allows more
effective scaling of AI and cognitive capabilities to provide (1) broader
coverage in a single domain such as health or geology; and (2) more rapid
deployment to new domains. The proposed techniques also allow ingested
knowledge to be extended naturally. Our investigations are early, and this
paper provides a description of the techniques. Assessment of their efficacy is
our next step for future work.

In this study, we use our recently prepared graphene oxide (GO) with an
almost intact {\sigma}-framework of C-atoms (ai-GO) to probe the thermal
stability of the carbon framework for the first time. Ai-GO exhibits few
defects only by preventing CO2 formation during synthesis. Ai-GO was thermally
treated before chemical reduction and subsequently the resulting defect density
in graphene was determined by statistical Raman microscopy. Surprisingly, the
carbon framework of ai-GO is stable in thin films up to 100 {\deg}C.
Furthermore, we find evidence for an increasing quality of ai-GO upon annealing
at 50 {\deg}C before reduction. The carbon framework of GO prepared according
to the popular Hummers method (GO-c)appears to be less stable and decomposition
starts at 50 {\deg}C what is qualitatively indicated by CO2-trapping
experiments in {\mu}m-thin films. Information about the stability of GO is
important for storing, processing and applying GO in applications.

Adaptive importance sampling (AIS) uses past samples to update the
\textit{sampling policy} $q_t$ at each stage $t$. Each stage $t$ is formed with
two steps : (i) to explore the space with $n_t$ points according to $q_t$ and
(ii) to exploit the current amount of information to update the sampling
policy. The very fundamental question raised in this paper concerns the
behavior of empirical sums based on AIS. Without making any assumption on the
allocation policy $n_t$, the theory developed involves no restriction on the
split of computational resources between the explore (i) and the exploit (ii)
step. It is shown that AIS is asymptotically optimal : the asymptotic behavior
of AIS is the same as some "oracle" strategy that knows the targeted sampling
policy from the beginning. From a practical perspective, weighted AIS is
introduced, a new method that allows to forget poor samples from early stages.

In a world of global trading, maritime safety, security and efficiency are
crucial issues. We propose a multi-task deep learning framework for vessel
monitoring using Automatic Identification System (AIS) data streams. We combine
recurrent neural networks with latent variable modeling and an embedding of AIS
messages to a new representation space to jointly address key issues to be
dealt with when considering AIS data streams: massive amount of streaming data,
noisy data and irregular timesampling. We demonstrate the relevance of the
proposed deep learning framework on real AIS datasets for a three-task setting,
namely trajectory reconstruction, anomaly detection and vessel type
identification.

Focusing on Business AI, this article introduces the AIQ quadrant that
enables us to measure AI for business applications in a relative comparative
manner, i.e. to judge that software A has more or less intelligence than
software B. Recognizing that the goal of Business software is to maximize value
in terms of business results, the dimensions of the quadrant are the key
factors that determine the business value of AI software: Level of Output
Quality (Smartness) and Level of Automation. The use of the quadrant is
illustrated by several software solutions to support the real life business
challenge of field service scheduling. The role of machine learning and
conversational digital assistants in increasing the business value are also
discussed and illustrated with a recent integration of existing intelligent
digital assistants for factory floor decision making with the new version of
Google Glass. Such hands free AI solutions elevate the AIQ level to its
ultimate position.

Public sector organisations are increasingly interested in using data science
and artificial intelligence capabilities to deliver policy and generate
efficiencies in high uncertainty environments. The long-term success of data
science and AI in the public sector relies on effectively embedding it into
delivery solutions for policy implementation. However, governments cannot do
this integration of AI into public service delivery on their own. The UK
Government Industrial Strategy is clear that delivering on the AI grand
challenge requires collaboration between universities and public and private
sectors. This cross-sectoral collaborative approach is the norm in applied AI
centres of excellence around the world. Despite their popularity, cross-sector
collaborations entail serious management challenges that hinder their success.
In this article we discuss the opportunities and challenges from AI for public
sector. Finally, we propose a series of strategies to successfully manage these
cross-sectoral collaborations.

In order to engender trust in AI, humans must understand what an AI system is
trying to achieve, and why. To overcome this problem, the underlying AI process
must produce justifications and explanations that are both transparent and
comprehensible to the user. AI Planning is well placed to be able to address
this challenge. In this paper we present a methodology to provide initial
explanations for the decisions made by the planner. Explanations are created by
allowing the user to suggest alternative actions in plans and then compare the
resulting plans with the one found by the planner. The methodology is
implemented in the new XAI-Plan framework.

In recent years, artificial intelligence (AI) decision-making and autonomous
systems became an integrated part of the economy, industry, and society. The
evolving economy of the human-AI ecosystem raising concerns regarding the risks
and values inherited in AI systems. This paper investigates the dynamics of
creation and exchange of values and points out gaps in perception of
cost-value, knowledge, space and time dimensions. It shows aspects of value
bias in human perception of achievements and costs that encoded in AI systems.
It also proposes rethinking hard goals definitions and cost-optimal
problem-solving principles in the lens of effectiveness and efficiency in the
development of trusted machines. The paper suggests a value-driven with cost
awareness strategy and principles for problem-solving and planning of effective
research progress to address real-world problems that involve diverse forms of
achievements, investments, and survival scenarios.

As artificial intelligence (AI) systems become increasingly complex and
ubiquitous, these systems will be responsible for making decisions that
directly affect individuals and society as a whole. Such decisions will need to
be justified due to ethical concerns as well as trust, but achieving this has
become difficult due to the `black-box' nature many AI models have adopted.
Explainable AI (XAI) can potentially address this problem by explaining its
actions, decisions and behaviours of the system to users. However, much
research in XAI is done in a vacuum using only the researchers' intuition of
what constitutes a `good' explanation while ignoring the interaction and the
human aspect. This workshop invites researchers in the HCI community and
related fields to have a discourse about human-centred approaches to XAI rooted
in interaction and to shed light and spark discussion on interaction design
challenges in XAI.

The rise of Artificial intelligence (AI) has the potential to significantly
transform the practice of project management. Project management has a large
socio-technical element with many uncertainties arising from variability in
human aspects e.g., customers' needs, developers' performance and team
dynamics. AI can assist project managers and team members by automating
repetitive, high-volume tasks to enable project analytics for estimation and
risk prediction, providing actionable recommendations, and even making
decisions. AI is potentially a game changer for project management in helping
to accelerate productivity and increase project success rates. In this paper,
we propose a framework where AI technologies can be leveraged to offer support
for managing agile projects, which have become increasingly popular in the
industry.

This is an integrative review that address the question, "What makes for a
good explanation?" with reference to AI systems. Pertinent literatures are
vast. Thus, this review is necessarily selective. That said, most of the key
concepts and issues are expressed in this Report. The Report encapsulates the
history of computer science efforts to create systems that explain and instruct
(intelligent tutoring systems and expert systems). The Report expresses the
explainability issues and challenges in modern AI, and presents capsule views
of the leading psychological theories of explanation. Certain articles stand
out by virtue of their particular relevance to XAI, and their methods, results,
and key points are highlighted. It is recommended that AI/XAI researchers be
encouraged to include in their research reports fuller details on their
empirical or experimental methods, in the fashion of experimental psychology
research reports: details on Participants, Instructions, Procedures, Tasks,
Dependent Variables (operational definitions of the measures and metrics),
Independent Variables (conditions), and Control Conditions.

Of primary importance in formulating a response to the increasing prevalence
and power of artificial intelligence (AI) applications in society are questions
of ontology. Questions such as: What "are" these systems? How are they to be
regarded? How does an algorithm come to be regarded as an agent? We discuss
three factors which hinder discussion and obscure attempts to form a clear
ontology of AI: (1) the various and evolving definitions of AI, (2) the
tendency for pre-existing technologies to be assimilated and regarded as
"normal," and (3) the tendency of human beings to anthropomorphize. This list
is not intended as exhaustive, nor is it seen to preclude entirely a clear
ontology, however, these challenges are a necessary set of topics for
consideration. Each of these factors is seen to present a 'moving target' for
discussion, which poses a challenge for both technical specialists and
non-practitioners of AI systems development (e.g., philosophers and
theologians) to speak meaningfully given that the corpus of AI structures and
capabilities evolves at a rapid pace. Finally, we present avenues for moving
forward, including opportunities for collaborative synthesis for scholars in
philosophy and science.

We propose AI-CARGO, a revenue management system for air-cargo that combines
machine learning prediction with decision-making using mathematical
optimization methods. AI-CARGO addresses a problem that is unique to the
air-cargo business, namely the wide discrepancy between the quantity (weight or
volume) that a shipper will book and the actual received amount at departure
time by the airline. The discrepancy results in sub-optimal and inefficient
behavior by both the shipper and the airline resulting in the overall loss of
potential revenue for the airline. AI-CARGO also includes a data cleaning
component to deal with the heterogeneous forms in which booking data is
transmitted to the airline cargo system. AI-CARGO is deployed in the production
environment of a large commercial airline company. We have validated the
benefits of AI-CARGO using real and synthetic datasets. Especially, we have
carried out simulations using dynamic programming techniques to elicit the
impact on offloading costs and revenue generation of our proposed system. Our
results suggest that combining prediction within a decision-making framework
can help dramatically to reduce offloading costs and optimize revenue
generation.

The idea of Artificial Intelligence (AI) has a long history. It turned out,
however, that reaching intelligence at human levels is more complicated than
originally anticipated. Currently we are experiencing a renewed interest in AI,
fueled by an enormous increase in computing power and an even larger increase
in data, in combination with improved AI technologies like deep learning.
Healthcare is considered the next domain to be revolutionized by Artificial
Intelligence. While AI approaches are excellently suited to develop certain
algorithms, for biomedical applications there are specific challenges. We
propose recommendations to improve AI projects in the biomedical space and
especially clinical healthcare.

Financial decisions impact our lives, and thus everyone from the regulator to
the consumer is interested in fair, sound, and explainable decisions. There is
increasing competitive desire and regulatory incentive to deploy AI mindfully
within financial services. An important mechanism towards that end is to
explain AI decisions to various stakeholders. State-of-the-art explainable AI
systems mostly serve AI engineers and offer little to no value to business
decision makers, customers, and other stakeholders. Towards addressing this
gap, in this work we consider the scenario of explaining loan denials. We build
the first-of-its-kind dataset that is representative of loan-applicant friendly
explanations. We design a novel Generative Adversarial Network (GAN) that can
accommodate smaller datasets, to generate user-friendly textual explanations.
We demonstrate how our system can also generate explanations serving different
purposes: those that help educate the loan applicants, or help them take
appropriate action towards a future approval.

Being a complex subject of major importance in AI Safety research, value
alignment has been studied from various perspectives in the last years.
However, no final consensus on the design of ethical utility functions
facilitating AI value alignment has been achieved yet. Given the urgency to
identify systematic solutions, we postulate that it might be useful to start
with the simple fact that for the utility function of an AI not to violate
human ethical intuitions, it trivially has to be a model of these intuitions
and reflect their variety $ - $ whereby the most accurate models pertaining to
human entities being biological organisms equipped with a brain constructing
concepts like moral judgements, are scientific models. Thus, in order to better
assess the variety of human morality, we perform a transdisciplinary analysis
applying a security mindset to the issue and summarizing variety-relevant
background knowledge from neuroscience and psychology. We complement this
information by linking it to augmented utilitarianism as a suitable ethical
framework. Based on that, we propose first practical guidelines for the design
of approximate ethical goal functions that might better capture the variety of
human moral judgements. Finally, we conclude and address future possible
challenges.

The claims that big data holds the key to enterprise successes and that
Artificial Intelligence is going to replace humanity have become increasingly
more popular over the past few years, both in academia and in the industry.
However, while these claims may indeed capture some truth, they have also been
massively oversold, or so we contend here. The goal of this paper is two-fold.
First, we provide a qualified defence of the value of less data within the
context of AI. This is done by carefully reviewing two distinct problems for
big data driven AI, namely a) the limited track record of Deep Learning in key
areas such as Natural Language Processing, b) the regulatory and business
significance of being able to learn from few data points. Second, we briefly
sketch what we refer to as a case of AI with humans and for humans, namely an
AI paradigm whereby the systems we build are privacy-oriented and focused on
human-machine collaboration, not competition. Combining our claims above, we
conclude that when seen through the lens of cognitively inspired AI, the bright
future of the discipline is about less data, not more, and more humans, not
fewer.

Work integrating conversations around AI and Disability is vital and valued,
particularly when done through a lens of fairness. Yet at the same time,
analyzing the ethical implications of AI for disabled people solely through the
lens of a singular idea of "fairness" risks reinforcing existing power
dynamics, either through reinforcing the position of existing medical
gatekeepers, or promoting tools and techniques that benefit
otherwise-privileged disabled people while harming those who are rendered
outliers in multiple ways. In this paper we present two case studies from
within computer vision - a subdiscipline of AI focused on training algorithms
that can "see" - of technologies putatively intended to help disabled people
but, through failures to consider structural injustices in their design, are
likely to result in harms not addressed by a "fairness" framing of ethics.
Drawing on disability studies and critical data science, we call on researchers
into AI ethics and disability to move beyond simplistic notions of fairness,
and towards notions of justice.

Artificial Intelligence (AI) is a cognitive science to enables human to
explore many intelligent ways to model our sensing and reasoning processes.
Industrial AI is a systematic discipline to enable engineers to systematically
develop and deploy AI algorithms with repeating and consistent successes. In
this paper, the key enablers for this transformative technology along with
their significant advantages are discussed. In addition, this research explains
Lighthouse Factories as an emerging status applying to the top manufacturers
that have implemented Industrial AI in their manufacturing ecosystem and gained
significant financial benefits. It is believed that this research will work as
a guideline and roadmap for researchers and industries towards the real-world
implementation of Industrial AI.

Systems that augment sensory abilities are increasingly employing AI and
machine learning (ML) approaches, with applications ranging from object
recognition and scene description tools for blind users to sound awareness
tools for d/Deaf users. However, unlike many other AI-enabled technologies,
these systems provide information that is already available to non-disabled
people. In this paper, we discuss unique AI fairness challenges that arise in
this context, including accessibility issues with data and models, ethical
implications in deciding what sensory information to convey to the user, and
privacy concerns both for the primary user and for others.

Machine Learning (ML) and Artificial Intelligence(AI) have become alternative
approaches in wireless networksbeside conventional approaches such as model
based solutionconcepts. Whereas traditional design concepts include the
mod-elling of the behaviour of the underlying processes, AI basedapproaches
allow to design network functions by learning frominput data which is supposed
to get mapped to specific outputs(training). Additionally, new input/output
relations can be learntduring the deployement phase of the function (online
learning)and make AI based solutions flexible, in order to react to
newsituations. Especially, new introduced use cases such as UltraReliable Low
Latency Communication (URLLC) and MassiveMachine Type Communications (MMTC) in
5G make this ap-proach necessary, as the network complexity is further
enhancedcompared to networks mainly designed for human driven traffic(4G, 5G
xMBB). The focus of this paper is to illustrate exemplaryapplications of AI
techniques at the Physical Layer (PHY) offuture wireless systems and therfore
they can be seen as candidatetechnologies for e.g. 6G systems.

It is a long-standing goal of artificial intelligence (AI) to be superior to
human beings in decision making. Games are suitable for testing AI capabilities
of making good decisions in non-numerical tasks. In this paper, we develop a
new AI algorithm to play the penny-matching game considered in Shannon's
"mind-reading machine" (1953) against human players. In particular, we exploit
cognitive hierarchy theory and Bayesian learning techniques to continually
evolve a model for predicting human player decisions, and let the AI player
make decisions according to the model predictions to pursue the best chance of
winning. Experimental results show that our AI algorithm beats 27 out of 30
volunteer human players.

The performance of mobile AI accelerators has been evolving rapidly in the
past two years, nearly doubling with each new generation of SoCs. The current
4th generation of mobile NPUs is already approaching the results of
CUDA-compatible Nvidia graphics cards presented not long ago, which together
with the increased capabilities of mobile deep learning frameworks makes it
possible to run complex and deep AI models on mobile devices. In this paper, we
evaluate the performance and compare the results of all chipsets from Qualcomm,
HiSilicon, Samsung, MediaTek and Unisoc that are providing hardware
acceleration for AI inference. We also discuss the recent changes in the
Android ML pipeline and provide an overview of the deployment of deep learning
models on mobile devices. All numerical results provided in this paper can be
found and are regularly updated on the official project website:
http://ai-benchmark.com.

Deploying successful software-reliant systems that address their mission
goals and user needs within cost, resource, and expected quality constraints
require design trade-offs. These trade-offs dictate how systems are structured
and how they behave and consequently can effectively be evolved and sustained.
Software engineering practices address this challenge by centering system
design and evolution around delivering key quality attributes, such as
security, privacy, data centricity, sustainability, and explainability. These
concerns are more urgent requirements for software-reliant systems that also
include AI components due to the uncertainty introduced by data elements.
Moreover, systems employed by the public sector exhibit unique design time and
runtime challenges due to the regulatory nature of the domains. We assert that
the quality attributes of security, privacy, data centricity, sustainability,
and explainability pose new challenges to AI engineering and will drive the
success of AI-enabled systems in the public sector. In this position paper, we
enumerate with examples from healthcare domain concerns related to these
requirements to mitigate barriers to architecting and fielding AI-enabled
systems in the public sector.

AI models and services are used in a growing number of highstakes areas,
resulting in a need for increased transparency. Consistent with this, several
proposals for higher quality and more consistent documentation of AI data,
models, and systems have emerged. Little is known, however, about the needs of
those who would produce or consume these new forms of documentation. Through
semi-structured developer interviews, and two document creation exercises, we
have assembled a clearer picture of these needs and the various challenges
faced in creating accurate and useful AI documentation. Based on the
observations from this work, supplemented by feedback received during multiple
design explorations and stakeholder conversations, we make recommendations for
easing the collection and flexible presentation of AI facts to promote
transparency.

Representing maritime traffic patterns and detecting anomalies from them are
key to vessel monitoring and maritime situational awareness. We propose a novel
approach -- referred to as GeoTrackNet -- for maritime anomaly detection from
AIS data streams. Our model exploits state-of-the-art neural network schemes to
learn a probabilistic representation of AIS tracks and a contrario detection to
detect abnormal events. The neural network provides a new means to capture
complex and heterogeneous patterns in vessels' behaviours, while the \textit{a
contrario} detector takes into account the fact that the learnt distribution
may be location-dependent. Experiments on a real AIS dataset comprising more
than 4.2 million AIS messages demonstrate the relevance of the proposed method
compared with state-of-the-art schemes.

To build AI-based systems that users and the public can justifiably trust one
needs to understand how machine learning technologies impact trust put in these
services. To guide technology developments, this paper provides a systematic
approach to relate social science concepts of trust with the technologies used
in AI-based services and products. We conceive trust as discussed in the ABI
(Ability, Benevolence, Integrity) framework and use a recently proposed mapping
of ABI on qualities of technologies. We consider four categories of machine
learning technologies, namely these for Fairness, Explainability, Auditability
and Safety (FEAS) and discuss if and how these possess the required qualities.
Trust can be impacted throughout the life cycle of AI-based systems, and we
introduce the concept of Chain of Trust to discuss technological needs for
trust in different stages of the life cycle. FEAS has obvious relations with
known frameworks and therefore we relate FEAS to a variety of international
Principled AI policy and technology frameworks that have emerged in recent
years.

Humans increasingly interact with Artificial intelligence(AI) systems. AI
systems are optimized for objectives such as minimum computation or minimum
error rate in recognizing and interpreting inputs from humans. In contrast,
inputs created by humans are often treated as a given. We investigate how
inputs of humans can be altered to reduce misinterpretation by the AI system
and to improve efficiency of input generation for the human while altered
inputs should remain as similar as possible to the original inputs. These
objectives result in trade-offs that are analyzed for a deep learning system
classifying handwritten digits. To create examples that serve as demonstrations
for humans to improve, we develop a model based on a conditional convolutional
autoencoder (CCAE). Our quantitative and qualitative evaluation shows that in
many occasions the generated proposals lead to lower error rates, require less
effort to create and differ only modestly from the original samples.

There is growing concern over the potential misuse of artificial intelligence
(AI) research. Publishing scientific research can facilitate misuse of the
technology, but the research can also contribute to protections against misuse.
This paper addresses the balance between these two effects. Our theoretical
framework elucidates the factors governing whether the published research will
be more useful for attackers or defenders, such as the possibility for adequate
defensive measures, or the independent discovery of the knowledge outside of
the scientific community. The balance will vary across scientific fields.
However, we show that the existing conversation within AI has imported concepts
and conclusions from prior debates within computer security over the disclosure
of software vulnerabilities. While disclosure of software vulnerabilities often
favours defence, this cannot be assumed for AI research. The AI research
community should consider concepts and policies from a broad set of adjacent
fields, and ultimately needs to craft policy well-suited to its particular
challenges.

Many researchers work on improving the data efficiency of machine learning.
What would happen if they succeed? This paper explores the social-economic
impact of increased data efficiency. Specifically, we examine the intuition
that data efficiency will erode the barriers to entry protecting incumbent
data-rich AI firms, exposing them to more competition from data-poor firms. We
find that this intuition is only partially correct: data efficiency makes it
easier to create ML applications, but large AI firms may have more to gain from
higher performing AI systems. Further, we find that the effect on privacy, data
markets, robustness, and misuse are complex. For example, while it seems
intuitive that misuse risk would increase along with data efficiency -- as more
actors gain access to any level of capability -- the net effect crucially
depends on how much defensive measures are improved. More investigation into
data efficiency, as well as research into the "AI production function", will be
key to understanding the development of the AI industry and its societal
impacts.

The AI revolution is data driven. AI "data wrangling" is the process by which
unusable data is transformed to support AI algorithm development (training) and
deployment (inference). Significant time is devoted to translating diverse data
representations supporting the many query and analysis steps found in an AI
pipeline. Rigorous mathematical representations of these data enables data
translation and analysis optimization within and across steps. Associative
array algebra provides a mathematical foundation that naturally describes the
tabular structures and set mathematics that are the basis of databases.
Likewise, the matrix operations and corresponding inference/training
calculations used by neural networks are also well described by associative
arrays. More surprisingly, a general denormalized form of hierarchical formats,
such as XML and JSON, can be readily constructed. Finally, pivot tables, which
are among the most widely used data analysis tools, naturally emerge from
associative array constructors. A common foundation in associative arrays
provides interoperability guarantees, proving that their operations are linear
systems with rigorous mathematical properties, such as, associativity,
commutativity, and distributivity that are critical to reordering
optimizations.

The fifth generation (5G) wireless communication networks are currently being
deployed, and beyond 5G (B5G) networks are expected to be developed over the
next decade. Artificial intelligence (AI) technologies and, in particular,
machine learning (ML) have the potential to efficiently solve the unstructured
and seemingly intractable problems by involving large amounts of data that need
to be dealt with in B5G. This article studies how AI and ML can be leveraged
for the design and operation of B5G networks. We first provide a comprehensive
survey of recent advances and future challenges that result from bringing AI/ML
technologies into B5G wireless networks. Our survey touches different aspects
of wireless network design and optimization, including channel measurements,
modeling, and estimation, physical-layer research, and network management and
optimization. Then, ML algorithms and applications to B5G networks are
reviewed, followed by an overview of standard developments of applying AI/ML
algorithms to B5G networks. We conclude this study by the future challenges on
applying AI/ML to B5G networks.

This whitepaper reports on Project CLAI (Command Line AI), which aims to
bring the power of AI to the command line interface (CLI). The CLAI platform
sets up the CLI as a new environment for AI researchers to conquer by surfacing
the command line as a generic environment that researchers can interface to
using a simple sense-act API, much like the traditional AI agent architecture.
In this paper, we discuss the design and implementation of the platform in
detail, through illustrative use cases of new end user interaction patterns
enabled by this design, and through quantitative evaluation of the system
footprint of a CLAI-enabled terminal. We also report on some early user
feedback on CLAI's features from an internal survey.

The recent wave of AI and automation has been argued to differ from previous
General Purpose Technologies (GPTs), in that it may lead to rapid change in
occupations' underlying task requirements and persistent technological
unemployment. In this paper, we apply a novel methodology of dynamic task
shares to a large dataset of online job postings to explore how exactly
occupational task demands have changed over the past decade of AI innovation,
especially across high, mid and low wage occupations. Notably, big data and AI
have risen significantly among high wage occupations since 2012 and 2016,
respectively. We built an ARIMA model to predict future occupational task
demands and showcase several relevant examples in Healthcare, Administration,
and IT. Such task demands predictions across occupations will play a pivotal
role in retraining the workforce of the future.

Explainability and interpretability of AI models is an essential factor
affecting the safety of AI. While various explainable AI (XAI) approaches aim
at mitigating the lack of transparency in deep networks, the evidence of the
effectiveness of these approaches in improving usability, trust, and
understanding of AI systems are still missing. We evaluate multimodal
explanations in the setting of a Visual Question Answering (VQA) task, by
asking users to predict the response accuracy of a VQA agent with and without
explanations. We use between-subjects and within-subjects experiments to probe
explanation effectiveness in terms of improving user prediction accuracy,
confidence, and reliance, among other factors. The results indicate that the
explanations help improve human prediction accuracy, especially in trials when
the VQA system's answer is inaccurate. Furthermore, we introduce active
attention, a novel method for evaluating causal attentional effects through
intervention by editing attention maps. User explanation ratings are strongly
correlated with human prediction accuracy and suggest the efficacy of these
explanations in human-machine AI collaboration tasks.

Artificial intelligence shows promise for solving many practical societal
problems in areas such as healthcare and transportation. However, the current
mechanisms for AI model diffusion such as Github code repositories, academic
project webpages, and commercial AI marketplaces have some limitations; for
example, a lack of monetization methods, model traceability, and model
auditabilty. In this work, we sketch guidelines for a new AI diffusion method
based on a decentralized online marketplace. We consider the technical,
economic, and regulatory aspects of such a marketplace including a discussion
of solutions for problems in these areas. Finally, we include a comparative
analysis of several current AI marketplaces that are already available or in
development. We find that most of these marketplaces are centralized commercial
marketplaces with relatively few models.

Recently there has been an ever-increasing trend in the use of machine
learning (ML) and artificial intelligence (AI) methods by the materials
science, condensed matter physics, and chemistry communities. This perspective
article identifies key scientific, technical, and social opportunities that the
materials community must prioritize to consistently develop and leverage
Scientific AI to provide a credible path towards the advancement of current
materials-limited technologies. Here we highlight the intersections of these
opportunities with a series of proposed paths forward. The opportunities are
roughly sorted from scientific/technical (e.g., development of robust,
physically meaningful multiscale material representations) to social (e.g.,
promoting an AI-ready workforce). The proposed paths forward range from
developing new infrastructure and capabilities to deploying them in industry
and academia. We provide a brief introduction to AI in materials science and
engineering, followed by detailed discussions of each of the opportunities and
paths forward.

Blockchain technology and artificial intelligence (AI) are current hot topics
in research and practice. However, the potentials of their combination have
been studied just recently to a larger extend. While different use cases for
combining AI and blockchain have been discussed, the idea of enabling
blockchain-based smart contracts to perform "smarter" decisions by using AI or
machine learning (ML) models has only been considered on the conceptual level
so far. It remained open, how such AI-enabled smart contracts could be
implemented in a robust way for real-world applications. Therefore, in this
paper a new, enterprise-class implementation of AI-enabled smart contracts is
presented and first insights regarding its feasibility are discussed.

Modern high performance computing clusters heavily rely on accelerators to
overcome the limited compute power of CPUs. These supercomputers run various
applications from different domains such as simulations, numerical applications
or artificial intelligence (AI). As a result, vendors need to be able to
efficiently run a wide variety of workloads on their hardware. In the AI domain
this is in particular exacerbated by the existence of a number of popular
frameworks (e.g, PyTorch, TensorFlow, etc.) that have no common code base, and
can vary in functionality. The code of these frameworks evolves quickly, making
it expensive to keep up with all changes and potentially forcing developers to
go through constant rounds of upstreaming. In this paper we explore how to
provide hardware support in AI frameworks without changing the framework's
source code in order to minimize maintenance overhead. We introduce SOL, an AI
acceleration middleware that provides a hardware abstraction layer that allows
us to transparently support heterogeneous hardware. As a proof of concept, we
implemented SOL for PyTorch with three backends: CPUs, GPUs and vector
processors.

Modern Unmanned Aerial Vehicles equipped with state of the art artificial
intelligence (AI) technologies are opening to a wide plethora of novel and
interesting applications. While this field received a strong impact from the
recent AI breakthroughs, most of the provided solutions either entirely rely on
commercial software or provide a weak integration interface which denies the
development of additional techniques. This leads us to propose a novel and
efficient framework for the UAV-AI joint technology. Intelligent UAV systems
encounter complex challenges to be tackled without human control. One of these
complex challenges is to be able to carry out computer vision tasks in
real-time use cases. In this paper we focus on this challenge and introduce a
multi-layer AI (MLAI) framework to allow easy integration of ad-hoc
visual-based AI applications. To show its features and its advantages, we
implemented and evaluated different modern visual-based deep learning models
for object detection, target tracking and target handover.

With the increased expectation of artificial intelligence, academic research
face complex questions of human-centred, responsible and trustworthy technology
embedded into society and culture. Several academic debates, social
consultations and impact studies are available to reveal the key aspects of the
changing human-machine ecosystem. To contribute to these studies, hundreds of
related academic sources are summarized below regarding AI-driven decisions and
valuable AI. In details, sociocultural filters, taxonomy of human-machine
decisions and perspectives of value-based AI are in the focus of this
literature review. For better understanding, it is proposed to invite
stakeholders in the prepared large-scale survey about the next generation AI
that investigates issues that go beyond the technology.

Despite significant progress in general AI planning, certain domains remain
out of reach of current AI planning systems. Sokoban is a PSPACE-complete
planning task and represents one of the hardest domains for current AI
planners. Even domain-specific specialized search methods fail quickly due to
the exponential search complexity on hard instances. Our approach based on deep
reinforcement learning augmented with a curriculum-driven method is the first
one to solve hard instances within one day of training while other modern
solvers cannot solve these instances within any reasonable time limit. In
contrast to prior efforts, which use carefully handcrafted pruning techniques,
our approach automatically uncovers domain structure. Our results reveal that
deep RL provides a promising framework for solving previously unsolved AI
planning problems, provided a proper training curriculum can be devised.

Framed in positive terms, this report examines how technical AI research
might be steered in a manner that is more attentive to humanity's long-term
prospects for survival as a species. In negative terms, we ask what existential
risks humanity might face from AI development in the next century, and by what
principles contemporary technical research might be directed to address those
risks.
  A key property of hypothetical AI technologies is introduced, called
\emph{prepotence}, which is useful for delineating a variety of potential
existential risks from artificial intelligence, even as AI paradigms might
shift. A set of \auxref{dirtot} contemporary research \directions are then
examined for their potential benefit to existential safety. Each research
direction is explained with a scenario-driven motivation, and examples of
existing work from which to build. The research directions present their own
risks and benefits to society that could occur at various scales of impact, and
in particular are not guaranteed to benefit existential safety if major
developments in them are deployed without adequate forethought and oversight.
As such, each direction is accompanied by a consideration of potentially
negative side effects.

The demand for smartness in embedded systems has been mounting up drastically
in the past few years. Embedded system today must address the fundamental
challenges introduced by cloud computing and artificial intelligence. KubeEdge
[1] is an edge computing framework build on top of Kubernetes [2]. It provides
compute resource management, deployment, runtime and operation capabilities on
geo-located edge computing resources, from the cloud, which is a natural fit
for embedded systems. Here we propose KubeEdge.AI, an edge AI framework on top
of KubeEdge. It provides a set of key modules and interfaces: a data handling
and processing engine, a concise AI runtime, a decision engine, and a
distributed data query interface. KubeEdge.AI will help reduce the burdens for
developing specific edge/embedded AI systems and promote edge-cloud
coordination and synergy.

Recent research by legal scholars suggests that the law might inevitably be
transformed into legal micro-directives consisting of legal rules that are
derived from legal standards or that are otherwise produced automatically or
via the consequent derivations of legal goals and then propagated via
automation for everyday use as readily accessible lawful directives throughout
society. This paper examines and extends the legal micro-directives theories in
three crucial respects: (1) By indicating that legal micro-directives are
likely to be AI-enabled and evolve over time in scope and velocity across the
autonomous levels of AI Legal Reasoning, (2) By exploring the trade-offs
between legal standards and legal rules as the imprinters of the
micro-directives, and (3) By illuminating a set of brittleness exposures that
can undermine legal micro-directives and proffering potential mitigating
remedies to seek greater robustness in the instantiation and promulgation of
such AI-powered lawful directives.

The advent of the wireless communications systems augurs new cutting-edge
technologies, including self-driving vehicles, unmanned aerial systems,
autonomous robots, Internet-of-Things, and virtual reality. These technologies
require high data rates, ultra-low latency, and high reliability, all of which
are promised by the fifth generation of wireless communication systems (5G).
Many research groups state that 5G cannot meet its demands without artificial
intelligence (AI) integration as 5G wireless networks are expected to generate
unprecedented traffic giving wireless research designers access to big data
that can help in predicting the demands and adjust cell designs to meet the
users requirements. Subsequently, many researchers applied AI in many aspects
of 5G wireless communication design including radio resource allocation,
network management, and cyber-security. In this paper, we provide an in-depth
review of AI for 5G wireless communication systems. In this respect, the aim of
this paper is to survey AI in 5G wireless communication systems by discussing
many case studies and the associated challenges, and shedding new light on
future research directions for leveraging AI in 5G wireless communications.

With children talking to smart-speakers, smart-phones and even
smart-microwaves daily, it is increasingly important to educate students on how
these agents work-from underlying mechanisms to societal implications.
Researchers are developing tools and curriculum to teach K-12 students broadly
about artificial intelligence (AI); however, few studies have evaluated these
tools with respect to AI-specific learning outcomes, and even fewer have
addressed student learning about AI-based conversational agents. We evaluate
our Conversational Agent Interface for MIT App Inventor and workshop curriculum
with respect to eight AI competencies from the literature. Furthermore, we
analyze teacher (n=9) and student (n=47) feedback from workshops with the
interface and recommend that future work leverages design considerations from
the literature to optimize engagement, collaborates with teachers, and
addresses a range of student abilities through pacing and opportunities for
extension. We found students struggled most with the concepts of AI ethics and
learning, and recommend emphasizing these topics when teaching.
  The appendix, including a demo video, can be found here:
https://gist.github.com/jessvb/1cd959e32415a6ad4389761c49b54bbf

An Intelligent Tutoring System (ITS) has been shown to improve students'
learning outcomes by providing a personalized curriculum that addresses
individual needs of every student. However, despite the effectiveness and
efficiency that ITS brings to students' learning process, most of the studies
in ITS research have conducted less effort to design the interface of ITS that
promotes students' interest in learning, motivation and engagement by making
better use of AI features. In this paper, we explore AI-driven design for the
interface of ITS describing diagnostic feedback for students' problem-solving
process and investigate its impacts on their engagement. We propose several
interface designs powered by different AI components and empirically evaluate
their impacts on student engagement through Santa, an active mobile ITS.
Controlled A/B tests conducted on more than 20K students in the wild show that
AI-driven interface design improves the factors of engagement by up to 25.13%.

Humans rely more and more on systems with AI components. The AI community
typically treats human inputs as a given and optimizes AI models only. This
thinking is one-sided and it neglects the fact that humans can learn, too. In
this work, human inputs are optimized for better interaction with an AI model
while keeping the model fixed. The optimized inputs are accompanied by
instructions on how to create them. They allow humans to save time and cut on
errors, while keeping required changes to original inputs limited. We propose
continuous and discrete optimization methods modifying samples in an iterative
fashion. Our quantitative and qualitative evaluation including a human study on
different hand-generated inputs shows that the generated proposals lead to
lower error rates, require less effort to create and differ only modestly from
the original samples.

Artificial Intelligence (AI) and Machine Learning have enormous potential to
transform businesses and disrupt entire industry sectors. However, companies
wishing to integrate algorithmic decisions into their face multiple challenges:
They have to identify use-cases in which artificial intelligence can create
value, as well as decisions that can be supported or executed automatically.
Furthermore, the organization will need to be transformed to be able to
integrate AI based systems into their human work-force. Furthermore, the more
technical aspects of the underlying machine learning model have to be discussed
in terms of how they impact the various units of a business: Where do the
relevant data come from, which constraints have to be considered, how is the
quality of the data and the prediction evaluated?
  The Enterprise AI canvas is designed to bring Data Scientist and business
expert together to discuss and define all relevant aspects which need to be
clarified in order to integrate AI based systems into a digital enterprise. It
consists of two parts where part one focuses on the business view and
organizational aspects, whereas part two focuses on the underlying machine
learning model and the data it uses.

Procedural Content Generation via Machine Learning (PCGML) refers to a group
of methods for creating game content (e.g. platformer levels, game maps, etc.)
using machine learning models. PCGML approaches rely on black box models, which
can be difficult to understand and debug by human designers who do not have
expert knowledge about machine learning. This can be even more tricky in
co-creative systems where human designers must interact with AI agents to
generate game content. In this paper we present an approach to explainable
artificial intelligence in which certain training instances are offered to
human users as an explanation for the AI agent's actions during a co-creation
process. We evaluate this approach by approximating its ability to provide
human users with the explanations of AI agent's actions and helping them to
more efficiently cooperate with the AI agent.

Over the next few years, society as a whole will need to address what core
values it wishes to protect when dealing with technology. Anthropology, a field
dedicated to the very notion of what it means to be human, can provide some
interesting insights into how to cope and tackle these changes in our Western
society and other areas of the world. It can be challenging for social science
practitioners to grasp and keep up with the pace of technological innovation,
with many being unfamiliar with the jargon of AI. This short guide serves as
both an introduction to AI ethics and social science and anthropological
perspectives on the development of AI. It intends to provide those unfamiliar
with the field with an insight into the societal impact of AI systems and how,
in turn, these systems can lead us to rethink how our world operates.

This paper proposes a research direction to advance AI which draws
inspiration from cognitive theories of human decision making. The premise is
that if we gain insights about the causes of some human capabilities that are
still lacking in AI (for instance, adaptability, generalizability, common
sense, and causal reasoning), we may obtain similar capabilities in an AI
system by embedding these causal components. We hope that the high-level
description of our vision included in this paper, as well as the several
research questions that we propose to consider, can stimulate the AI research
community to define, try and evaluate new methodologies, frameworks, and
evaluation metrics, in the spirit of achieving a better understanding of both
human and machine intelligence.

In this paper, we introduce Watch-And-Help (WAH), a challenge for testing
social intelligence in agents. In WAH, an AI agent needs to help a human-like
agent perform a complex household task efficiently. To succeed, the AI agent
needs to i) understand the underlying goal of the task by watching a single
demonstration of the human-like agent performing the same task (social
perception), and ii) coordinate with the human-like agent to solve the task in
an unseen environment as fast as possible (human-AI collaboration). For this
challenge, we build VirtualHome-Social, a multi-agent household environment,
and provide a benchmark including both planning and learning based baselines.
We evaluate the performance of AI agents with the human-like agent as well as
with real humans using objective metrics and subjective user ratings.
Experimental results demonstrate that the proposed challenge and virtual
environment enable a systematic evaluation on the important aspects of machine
social intelligence at scale.

With the recent release of AI interaction guidelines from Apple, Google, and
Microsoft, there is clearly interest in understanding the best practices in
human-AI interaction. However, industry standards are not determined by a
single company, but rather by the synthesis of knowledge from the whole
community. We have surveyed all of the design guidelines from each of these
major companies and developed a single, unified structure of guidelines, giving
developers a centralized reference. We have then used this framework to compare
each of the surveyed companies to find differences in areas of emphasis.
Finally, we encourage people to contribute additional guidelines from other
companies, academia, or individuals, to provide an open and extensible
reference of AI design guidelines at
https://ai-open-guidelines.readthedocs.io/.

With rapid progress in artificial intelligence (AI), popularity of generative
art has grown substantially. From creating paintings to generating novel art
styles, AI based generative art has showcased a variety of applications.
However, there has been little focus concerning the ethical impacts of AI based
generative art. In this work, we investigate biases in the generative art AI
pipeline right from those that can originate due to improper problem
formulation to those related to algorithm design. Viewing from the lens of art
history, we discuss the socio-cultural impacts of these biases. Leveraging
causal models, we highlight how current methods fall short in modeling the
process of art creation and thus contribute to various types of biases. We
illustrate the same through case studies, in particular those related to style
transfer. To the best of our knowledge, this is the first extensive analysis
that investigates biases in the generative art AI pipeline from the perspective
of art history. We hope our work sparks interdisciplinary discussions related
to accountability of generative art.

Modern AI systems are reaping the advantage of novel learning methods. With
their increasing usage, we are realizing the limitations and shortfalls of
these systems. Brittleness to minor adversarial changes in the input data,
ability to explain the decisions, address the bias in their training data, high
opacity in terms of revealing the lineage of the system, how they were trained
and tested, and under which parameters and conditions they can reliably
guarantee a certain level of performance, are some of the most prominent
limitations. Ensuring the privacy and security of the data, assigning
appropriate credits to data sources, and delivering decent outputs are also
required features of an AI system. We propose the tutorial on Trustworthy AI to
address six critical issues in enhancing user and public trust in AI systems,
namely: (i) bias and fairness, (ii) explainability, (iii) robust mitigation of
adversarial attacks, (iv) improved privacy and security in model building, (v)
being decent, and (vi) model attribution, including the right level of credit
assignment to the data sources, model architectures, and transparency in
lineage.

With the growth of the internet, it is becoming hard to manage, configure and
monitor networks. Recent trends to control and operate them is artificial
intelligence based automation to minimize human intervention. Albeit this
concept has been introduced since a decade with several different names, but
the underlying goal remains the same, which is to make network intelligent
enough to assemble, reassemble if configuration changes, and detect a problem
on its own and fix it. As a result, in addition to Data Plane, Control Plane
and Management Plane, a new plane called Artificial Intelligence (AI) Plane is
introduced. Our main objective is to analyze all major AI plane techniques,
frameworks and algorithms proposed in various types of networks. We propose a
comprehensive and network independent framework to cover all aspects of AI
plane, in particular we provide a systematically means of comparison. In
conjunction to make AI plane understand simpler, this framework highlights
relevant challenges and design considerations for future research. To the best
of our knowledge this is the first survey report which represents a complete
comparison of AI planes with their investigation issues in several types of
networks.

In this paper, we present a theoretical discussion on AI deep learning neural
network uncertainty investigation based on the classical Rademacher complexity
and Shannon entropy. First it is shown that the classical Rademacher complexity
and Shannon entropy is closely related by quantity by definitions. Secondly
based on the Shannon mathematical theory on communication [3], we derive a
criteria to ensure AI correctness and accuracy in classifications problems.
Last but not the least based on Peter Barlette's work, we show both a relaxing
condition and a stricter condition to guarantee the correctness and accuracy in
AI classification . By elucidating in this paper criteria condition in terms of
Shannon entropy based on Shannon theory, it becomes easier to explore other
criteria in terms of other complexity measurements such as Vapnik-Cheronenkis,
Gaussian complexity by taking advantage of the relations studies results in
other references. A close to 0.5 criteria on Shannon entropy is derived in this
paper for the theoretical investigation of AI accuracy and correctness for
classification problems.

The diffusion of artificial intelligence (AI) applications in organizations
and society has fueled research on explaining AI decisions. The explainable AI
(xAI) field is rapidly expanding with numerous ways of extracting information
and visualizing the output of AI technologies (e.g. deep neural networks). Yet,
we have a limited understanding of how xAI research addresses the need for
explainable AI. We conduct a systematic review of xAI literature on the topic
and identify four thematic debates central to how xAI addresses the black-box
problem. Based on this critical analysis of the xAI scholarship we synthesize
the findings into a future research agenda to further the xAI body of
knowledge.

In recent years, data-intensive AI, particularly the domain of natural
language processing and understanding, has seen significant progress driven by
the advent of large datasets and deep neural networks that have sidelined more
classic AI approaches to the field. These systems can apparently demonstrate
sophisticated linguistic understanding or generation capabilities, but often
fail to transfer their skills to situations they have not encountered before.
We argue that computational situated grounding provides a solution to some of
these learning challenges by creating situational representations that both
serve as a formal model of the salient phenomena, and contain rich amounts of
exploitable, task-appropriate data for training new, flexible computational
models. Our model reincorporates some ideas of classic AI into a framework of
neurosymbolic intelligence, using multimodal contextual modeling of interactive
situations, events, and object properties. We discuss how situated grounding
provides diverse data and multiple levels of modeling for a variety of AI
learning challenges, including learning how to interact with object
affordances, learning semantics for novel structures and configurations, and
transferring such learned knowledge to new objects and situations.

ColorShapeLinks is an AI board game competition framework specially designed
for students and educators in videogame development, with openness and
accessibility in mind. The competition is based on an arbitrarily-sized version
of the Simplexity board game, the motto of which, "simple to learn, complex to
master", is curiously also applicable to AI agents. ColorShapeLinks offers
graphical and text-based frontends and a completely open and documented
development framework built using industry standard tools and following
software engineering best practices. ColorShapeLinks is not only a competition,
but both a game and a framework which educators and students can extend and use
to host their own competitions. It has been successfully used for running
internal competitions in AI classes, as well as for hosting an international AI
competition at the IEEE Conference on Games.

Abolition is the process of destroying and then rebuilding the structures
that impede liberation. This paper addresses the particular case of Black folk
in the United States, but is relevant to the global decolonization movement.
Using notions of abolition and infrastructures of feeling developed by Ruth
Wilson Gilmore, I view Historically Black Colleges and Universities ( HBCUs )
as a particular kind of abolitionist project, created for the explicit purpose
of nurturing and sustaining Black excellence particularly within the sciences.
I then examine how artificial intelligence (AI) in particular and computing in
general have contributed to racial oppression and the further confinement and
diminishing of Black existence. I conclude by examining how the space held by
HBCUs in computing might contribute to a re-imagining of AI as a technology
that enhances the possibility and actualization of Black life.

We study whether receiving advice from either a human or algorithmic advisor,
accompanied by five types of Local and Global explanation labelings, has an
effect on the readiness to adopt, willingness to pay, and trust in a financial
AI consultant. We compare the differences over time and in various key
situations using a unique experimental framework where participants play a
web-based game with real monetary consequences. We observed that accuracy-based
explanations of the model in initial phases leads to higher adoption rates.
When the performance of the model is immaculate, there is less importance
associated with the kind of explanation for adoption. Using more elaborate
feature-based or accuracy-based explanations helps substantially in reducing
the adoption drop upon model failure. Furthermore, using an autopilot increases
adoption significantly. Participants assigned to the AI-labeled advice with
explanations were willing to pay more for the advice than the AI-labeled advice
with a No-explanation alternative. These results add to the literature on the
importance of XAI for algorithmic adoption and trust.

Recently, consumer-facing health technologies such as Artificial Intelligence
(AI)-based symptom checkers (AISCs) have sprung up in everyday healthcare
practice. AISCs solicit symptom information from users and provide medical
suggestions and possible diagnoses, a responsibility that people usually
entrust with real-person authorities such as physicians and expert patients.
Thus, the advent of AISCs begs a question of whether and how they transform the
notion of medical authority in everyday healthcare practice. To answer this
question, we conducted an interview study with thirty AISC users. We found that
users assess the medical authority of AISCs using various factors including
automated decisions and interaction design patterns of AISC apps, associations
with established medical authorities like hospitals, and comparisons with other
health technologies. We reveal how AISCs are used in healthcare delivery,
discuss how AI transforms conventional understandings of medical authority, and
derive implications for designing AI-enabled health technology.

The lack of explainability of a decision from an Artificial Intelligence (AI)
based "black box" system/model, despite its superiority in many real-world
applications, is a key stumbling block for adopting AI in many high stakes
applications of different domain or industry. While many popular Explainable
Artificial Intelligence (XAI) methods or approaches are available to facilitate
a human-friendly explanation of the decision, each has its own merits and
demerits, with a plethora of open challenges. We demonstrate popular XAI
methods with a mutual case study/task (i.e., credit default prediction),
analyze for competitive advantages from multiple perspectives (e.g., local,
global), provide meaningful insight on quantifying explainability, and
recommend paths towards responsible or human-centered AI using XAI as a medium.
Practitioners can use this work as a catalog to understand, compare, and
correlate competitive advantages of popular XAI methods. In addition, this
survey elicits future research directions towards responsible or human-centric
AI systems, which is crucial to adopt AI in high stakes applications.

Despite interest in communicating ethical problems and social contexts within
the undergraduate curriculum to advance Public Interest Technology (PIT) goals,
interventions at the graduate level remain largely unexplored. This may be due
to the conflicting ways through which distinct Artificial Intelligence (AI)
research tracks conceive of their interface with social contexts. In this paper
we track the historical emergence of sociotechnical inquiry in three distinct
subfields of AI research: AI Safety, Fair Machine Learning (Fair ML) and
Human-in-the-Loop (HIL) Autonomy. We show that for each subfield, perceptions
of PIT stem from the particular dangers faced by past integration of technical
systems within a normative social order. We further interrogate how these
histories dictate the response of each subfield to conceptual traps, as defined
in the Science and Technology Studies literature. Finally, through a
comparative analysis of these currently siloed fields, we present a roadmap for
a unified approach to sociotechnical graduate pedagogy in AI.

Artificial intelligence (AI) is now widely used to facilitate social
interaction, but its impact on social relationships and communication is not
well understood. We study the social consequences of one of the most pervasive
AI applications: algorithmic response suggestions ("smart replies"). Two
randomized experiments (n = 1036) provide evidence that a commercially-deployed
AI changes how people interact with and perceive one another in pro-social and
anti-social ways. We find that using algorithmic responses increases
communication efficiency, use of positive emotional language, and positive
evaluations by communication partners. However, consistent with common
assumptions about the negative implications of AI, people are evaluated more
negatively if they are suspected to be using algorithmic responses. Thus, even
though AI can increase communication efficiency and improve interpersonal
perceptions, it risks changing users' language production and continues to be
viewed negatively.

In recent years, AI generated art has become very popular. From generating
art works in the style of famous artists like Paul Cezanne and Claude Monet to
simulating styles of art movements like Ukiyo-e, a variety of creative
applications have been explored using AI. Looking from an art historical
perspective, these applications raise some ethical questions. Can AI model
artists' styles without stereotyping them? Does AI do justice to the
socio-cultural nuances of art movements? In this work, we take a first step
towards analyzing these issues. Leveraging directed acyclic graphs to represent
potential process of art creation, we propose a simple metric to quantify
confounding bias due to the lack of modeling the influence of art movements in
learning artists' styles. As a case study, we consider the popular cycleGAN
model and analyze confounding bias across various genres. The proposed metric
is more effective than state-of-the-art outlier detection method in
understanding the influence of art movements in artworks. We hope our work will
elucidate important shortcomings of computationally modeling artists' styles
and trigger discussions related to accountability of AI generated art.

A plethora of demanding services and use cases mandate a revolutionary shift
in the management of future wireless network resources. Indeed, when tight
quality of service demands of applications are combined with increased
complexity of the network, legacy network management routines will become
unfeasible in 6G. Artificial Intelligence (AI) is emerging as a fundamental
enabler to orchestrate the network resources from bottom to top. AI-enabled
radio access and AI-enabled core will open up new opportunities for automated
configuration of 6G. On the other hand, there are many challenges in AI-enabled
networks that need to be addressed. Long convergence time, memory complexity,
and complex behaviour of machine learning algorithms under uncertainty as well
as highly dynamic channel, traffic and mobility conditions of the network
contribute to the challenges. In this paper, we survey the state-of-art
research in utilizing machine learning techniques in improving the performance
of wireless networks. In addition, we identify challenges and open issues to
provide a roadmap for the researchers.

Advances in reinforcement learning (RL) have resulted in recent breakthroughs
in the application of artificial intelligence (AI) across many different
domains. An emerging landscape of development environments is making powerful
RL techniques more accessible for a growing community of researchers. However,
most existing frameworks do not directly address the problem of learning in
complex operating environments, such as dense urban settings or defense-related
scenarios, that incorporate distributed, heterogeneous teams of agents. To help
enable AI research for this important class of applications, we introduce the
AI Arena: a scalable framework with flexible abstractions for distributed
multi-agent reinforcement learning. The AI Arena extends the OpenAI Gym
interface to allow greater flexibility in learning control policies across
multiple agents with heterogeneous learning strategies and localized views of
the environment. To illustrate the utility of our framework, we present
experimental results that demonstrate performance gains due to a distributed
multi-agent learning approach over commonly-used RL techniques in several
different learning environments.

Welcome to the fourth edition of the AI Index Report. This year we
significantly expanded the amount of data available in the report, worked with
a broader set of external organizations to calibrate our data, and deepened our
connections with the Stanford Institute for Human-Centered Artificial
Intelligence (HAI). The AI Index Report tracks, collates, distills, and
visualizes data related to artificial intelligence. Its mission is to provide
unbiased, rigorously vetted, and globally sourced data for policymakers,
researchers, executives, journalists, and the general public to develop
intuitions about the complex field of AI. The report aims to be the most
credible and authoritative source for data and insights about AI in the world.

This paper outlines a perspective on the future of AI, discussing directions
for machines models of human-like intelligence. We explain how developmental
and evolutionary theories of human cognition should further inform artificial
intelligence. We emphasize the role of ecological niches in sculpting
intelligent behavior, and in particular that human intelligence was
fundamentally shaped to adapt to a constantly changing socio-cultural
environment. We argue that a major limit of current work in AI is that it is
missing this perspective, both theoretically and experimentally. Finally, we
discuss the promising approach of developmental artificial intelligence,
modeling infant development through multi-scale interaction between
intrinsically motivated learning, embodiment and a fastly changing
socio-cultural environment. This paper takes the form of an interview of
Pierre-Yves Oudeyer by Mandred Eppe, organized within the context of a KI -
K{\"{u}}nstliche Intelligenz special issue in developmental robotics.

A local surrogate for an AI-model correcting a simpler 'base' model is
introduced representing an analytical method to yield explanations of
AI-predictions. The approach is studied here in the context of the base model
being linear regression. The AI-model approximates the residual error of the
linear model and the explanations are formulated in terms of the change of the
interpretable base model's parameters. Criteria are formulated for the precise
relation between lost accuracy of the surrogate, the accuracy of the AI-model,
and the surrogate fidelity. It is shown that, assuming a certain maximal amount
of noise in the observed data, these criteria induce neighborhoods of the
instances to be explained which have an ideal size in terms of maximal accuracy
and fidelity.

The emerging age of connected, digital world means that there are tons of
data, distributed to various organizations and their databases. Since this data
can be confidential in nature, it cannot always be openly shared in seek of
artificial intelligence (AI) and machine learning (ML) solutions. Instead, we
need integration mechanisms, analogous to integration patterns in information
systems, to create multi-organization AI/ML systems. In this paper, we present
two real-world cases. First, we study integration between two organizations in
detail. Second, we address scaling of AI/ML to multi-organization context. The
setup we assume is that of continuous deployment, often referred to DevOps in
software development. When also ML components are deployed in a similar
fashion, term MLOps is used. Towards the end of the paper, we list the main
observations and draw some final conclusions. Finally, we propose some
directions for future work.

This work provides a starting point for researchers interested in gaining a
deeper understanding of the big picture of artificial intelligence (AI). To
this end, a narrative is conveyed that allows the reader to develop an
objective view on current developments that is free from false promises that
dominate public communication. An essential takeaway for the reader is that AI
must be understood as an umbrella term encompassing a plethora of different
methods, schools of thought, and their respective historical movements.
Consequently, a bottom-up strategy is pursued in which the field of AI is
introduced by presenting various aspects that are characteristic of the
subject. This paper is structured in three parts: (i) Discussion of current
trends revealing false public narratives, (ii) an introduction to the history
of AI focusing on recurring patterns and main characteristics, and (iii) a
critical discussion on the limitations of current methods in the context of the
potential emergence of a strong(er) AI. It should be noted that this work does
not cover any of these aspects holistically; rather, the content addressed is a
selection made by the author and subject to a didactic strategy.

Medical imaging is widely used in cancer diagnosis and treatment, and
artificial intelligence (AI) has achieved tremendous success in various tasks
of medical image analysis. This paper reviews AI-based tumor subregion analysis
in medical imaging. We summarize the latest AI-based methods for tumor
subregion analysis and their applications. Specifically, we categorize the
AI-based methods by training strategy: supervised and unsupervised. A detailed
review of each category is presented, highlighting important contributions and
achievements. Specific challenges and potential AI applications in tumor
subregion analysis are discussed.

With there being many technical and ethical issues with artificial
intelligence (AI) that involve marginalized communities, there is a growing
interest for design methods used with marginalized people that may be
transferable to the design of AI technologies. Participatory design (PD) is a
design method that is often used with marginalized communities for the design
of social development, policy, IT and other matters and solutions. However,
there are issues with the current PD, raising concerns when it is applied to
the design of technologies, including AI technologies. This paper argues for
the use of PD for the design of AI technologies, and introduces and proposes a
new PD, which we call agile participatory design, that not only can could be
used for the design of AI and data-driven technologies, but also overcomes
issues surrounding current PD and its use in the design of such technologies.

Artificial Intelligence (AI) has a tremendous impact on the unexpected growth
of technology in almost every aspect. AI-powered systems are monitoring and
deciding about sensitive economic and societal issues. The future is towards
automation, and it must not be prevented. However, this is a conflicting
viewpoint for a lot of people, due to the fear of uncontrollable AI systems.
This concern could be reasonable if it was originating from considerations
associated with social issues, like gender-biased, or obscure decision-making
systems. Explainable AI (XAI) is recently treated as a huge step towards
reliable systems, enhancing the trust of people to AI. Interpretable machine
learning (IML), a subfield of XAI, is also an urgent topic of research. This
paper presents a small but significant contribution to the IML community,
focusing on a local-based, neural-specific interpretation process applied to
textual and time-series data. The proposed methodology introduces new
approaches to the presentation of feature importance based interpretations, as
well as the production of counterfactual words on textual datasets. Eventually,
an improved evaluation metric is introduced for the assessment of
interpretation techniques, which supports an extensive set of qualitative and
quantitative experiments.

Artificial intelligence (AI)-based models used to improve different fields
including healthcare, and finance. One of the field that receive advantages of
AI is automation. However, it is important to consider human factors in
application of AI in automation. This paper reports on a systematic review of
the published studies used to investigate the application of AI in PM. This
comprehensive systematic review used ScienceDirect to identify relevant
articles. Of the 422 articles found, 40 met the inclusion and exclusion
criteria and were used in the review. Selected articles were classified based
on categories of human factors and areas of application. The results indicated
that application of AI in automation with respect to human factors could be
divided into three areas of physical ergonomics, cognitive ergonomic and
organizational ergonomics. The main areas of application in physical and
cognitive ergonomics are including transportation, User experience, and
human-machine interactions.

Since its beginning in the 1950s, the field of artificial intelligence has
cycled several times between periods of optimistic predictions and massive
investment ("AI spring") and periods of disappointment, loss of confidence, and
reduced funding ("AI winter"). Even with today's seemingly fast pace of AI
breakthroughs, the development of long-promised technologies such as
self-driving cars, housekeeping robots, and conversational companions has
turned out to be much harder than many people expected. One reason for these
repeating cycles is our limited understanding of the nature and complexity of
intelligence itself. In this paper I describe four fallacies in common
assumptions made by AI researchers, which can lead to overconfident predictions
about the field. I conclude by discussing the open questions spurred by these
fallacies, including the age-old challenge of imbuing machines with humanlike
common sense.

This article reviews the "Once learning" mechanism that was proposed 23 years
ago and the subsequent successes of "One-shot learning" in image classification
and "You Only Look Once - YOLO" in objective detection. Analyzing the current
development of Artificial Intelligence (AI), the proposal is that AI should be
clearly divided into the following categories: Artificial Human Intelligence
(AHI), Artificial Machine Intelligence (AMI), and Artificial Biological
Intelligence (ABI), which will also be the main directions of theory and
application development for AI. As a watershed for the branches of AI, some
classification standards and methods are discussed: 1) Human-oriented,
machine-oriented, and biological-oriented AI R&D; 2) Information input
processed by Dimensionality-up or Dimensionality-reduction; 3) The use of
one/few or large samples for knowledge learning.

Recent advances in natural language processing and computer vision have led
to AI models that interpret simple scenes at human levels. Yet, we do not have
a complete understanding of how humans and AI models differ in their
interpretation of more complex scenes. We created a dataset of complex scenes
that contained human behaviors and social interactions. AI and humans had to
describe the scenes with a sentence. We used a quantitative metric of
similarity between scene descriptions of the AI/human and ground truth of five
other human descriptions of each scene. Results show that the machine/human
agreement scene descriptions are much lower than human/human agreement for our
complex scenes. Using an experimental manipulation that occludes different
spatial regions of the scenes, we assessed how machines and humans vary in
utilizing regions of images to understand the scenes. Together, our results are
a first step toward understanding how machines fall short of human visual
reasoning with complex scenes depicting human behaviors.

In this independent report fAshIon after fashion, we examine the development
of fAshIon (artificial intelligence (AI) in fashion) and explore its
potentiality to become a major disruptor of the fashion industry in the near
future. To do this, we investigate AI technologies used in the fashion industry
through several lenses. We summarise fAshIon studies conducted over the past
decade and categorise them into seven groups: Overview, Evaluation, Basic Tech,
Selling, Styling, Design, and Buying. The datasets mentioned in fAshIon
research have been consolidated on one GitHub page for ease of use. We analyse
the authors' backgrounds and the geographic regions treated in these studies to
determine the landscape of fAshIon research. The results of our analysis are
presented with an aim to provide researchers with a holistic view of research
in fAshIon. As part of our primary research, we also review a wide range of
cases of applied fAshIon in the fashion industry and analyse their impact on
the industry, markets and individuals. We also identify the challenges
presented by fAshIon and suggest that these may form the basis for future
research. We finally exhibit that many potential opportunities exist for the
use of AI in fashion which can transform the fashion industry embedded with AI
technologies and boost profits.

Equity of educational outcome and fairness of AI with respect to race have
been topics of increasing importance in education. In this work, we address
both with empirical evaluations of grade prediction in higher education, an
important task to improve curriculum design, plan interventions for academic
support, and offer course guidance to students. With fairness as the aim, we
trial several strategies for both label and instance balancing to attempt to
minimize differences in algorithm performance with respect to race. We find
that an adversarial learning approach, combined with grade label balancing,
achieved by far the fairest results. With equity of educational outcome as the
aim, we trial strategies for boosting predictive performance on historically
underserved groups and find success in sampling those groups in inverse
proportion to their historic outcomes. With AI-infused technology supports
increasingly prevalent on campuses, our methodologies fill a need for
frameworks to consider performance trade-offs with respect to sensitive student
attributes and allow institutions to instrument their AI resources in ways that
are attentive to equity and fairness.

Verification of AI is a challenge that has engineering, algorithmic and
programming language components. For example, AI planners are deployed to model
actions of autonomous agents. They comprise a number of searching algorithms
that, given a set of specified properties, find a sequence of actions that
satisfy these properties. Although AI planners are mature tools from the
algorithmic and engineering points of view, they have limitations as
programming languages. Decidable and efficient automated search entails
restrictions on the syntax of the language, prohibiting use of higher-order
properties or recursion. This paper proposes a methodology for embedding plans
produced by AI planners into dependently-typed language Agda, which enables
users to reason about and verify more general and abstract properties of plans,
and also provides a more holistic programming language infrastructure for
modelling plan execution.

In April 2021, the European Commission published a proposed regulation on AI.
It intends to create a uniform legal framework for AI within the European Union
(EU). In this chapter, we analyze and assess the proposal. We show that the
proposed regulation is actually not needed due to existing regulations. We also
argue that the proposal clearly poses the risk of overregulation. As a
consequence, this would make the use or development of AI applications in
safety-critical application areas, such as in healthcare, almost impossible in
the EU. This would also likely further strengthen Chinese and US corporations
in their technology leadership. Our assessment is based on the oral evidence we
gave in May 2021 to the joint session of the European Union affairs committees
of the German federal parliament and the French National Assembly.

Resource management plays a pivotal role in wireless networks, which,
unfortunately, leads to challenging NP-hard problems. Artificial Intelligence
(AI), especially deep learning techniques, has recently emerged as a disruptive
technology to solve such challenging problems in a real-time manner. However,
although promising results have been reported, practical design guidelines and
performance guarantees of AI-based approaches are still missing. In this paper,
we endeavor to address two fundamental questions: 1) What are the main
advantages of AI-based methods compared with classical techniques; and 2) Which
neural network should we choose for a given resource management task. For the
first question, four advantages are identified and discussed. For the second
question, \emph{optimality gap}, i.e., the gap to the optimal performance, is
proposed as a measure for selecting model architectures, as well as, for
enabling a theoretical comparison between different AI-based approaches.
Specifically, for $K$-user interference management problem, we theoretically
show that graph neural networks (GNNs) are superior to multi-layer perceptrons
(MLPs), and the performance gap between these two methods grows with
$\sqrt{K}$.

Ethical AI spans a gamut of considerations. Among these, the most popular
ones, fairness and interpretability, have remained largely distinct in
technical pursuits. We discuss and elucidate the differences between fairness
and interpretability across a variety of dimensions. Further, we develop two
principles-based frameworks towards developing ethical AI for the future that
embrace aspects of both fairness and interpretability. First, interpretability
for fairness proposes instantiating interpretability within the realm of
fairness to develop a new breed of ethical AI. Second, fairness and
interpretability initiates deliberations on bringing the best aspects of both
together. We hope that these two frameworks will contribute to intensifying
scholarly discussions on new frontiers of ethical AI that brings together
fairness and interpretability.

Web search engines influence perception of social reality by filtering and
ranking information. However, their outputs are often subjected to bias that
can lead to skewed representation of subjects such as professional occupations
or gender. In our paper, we use a mixed-method approach to investigate presence
of race and gender bias in representation of artificial intelligence (AI) in
image search results coming from six different search engines. Our findings
show that search engines prioritize anthropomorphic images of AI that portray
it as white, whereas non-white images of AI are present only in non-Western
search engines. By contrast, gender representation of AI is more diverse and
less skewed towards a specific gender that can be attributed to higher
awareness about gender bias in search outputs. Our observations indicate both
the the need and the possibility for addressing bias in representation of
societally relevant subjects, such as technological innovation, and emphasize
the importance of designing new approaches for detecting bias in information
retrieval systems.

Current research on Explainable AI (XAI) heavily targets on expert users
(data scientists or AI developers). However, increasing importance has been
argued for making AI more understandable to nonexperts, who are expected to
leverage AI techniques, but have limited knowledge about AI. We present a
mobile application to support nonexperts to interactively make sense of
Convolutional Neural Networks (CNN); it allows users to play with a pretrained
CNN by taking pictures of their surrounding objects. We use an up-to-date XAI
technique (Class Activation Map) to intuitively visualize the model's decision
(the most important image regions that lead to a certain result). Deployed in a
university course, this playful learning tool was found to support design
students to gain vivid understandings about the capabilities and limitations of
pretrained CNNs in real-world environments. Concrete examples of students'
playful explorations are reported to characterize their sensemaking processes
reflecting different depths of thought.

AI for supporting designers needs to be rethought. It should aim to
cooperate, not automate, by supporting and leveraging the creativity and
problem-solving of designers. The challenge for such AI is how to infer
designers' goals and then help them without being needlessly disruptive. We
present AI-assisted design: a framework for creating such AI, built around
generative user models which enable reasoning about designers' goals,
reasoning, and capabilities.

Artificial intelligence (AI) techniques for image-based segmentation have
garnered much attention in recent years. Convolutional neural networks (CNNs)
have shown impressive results and potential towards fully automated
segmentation in medical imaging, and particularly PET imaging. To cope with the
limited access to annotated data needed in supervised AI methods, given tedious
and prone-to-error manual delineations, semi-supervised and unsupervised AI
techniques have also been explored for segmentation of tumors or normal organs
in single and bi-modality scans. This work provides a review of existing AI
techniques for segmentation tasks and the evaluation criteria for translational
AI-based segmentation efforts towards routine adoption in clinical workflows.

Several approaches have been developed for answering users' specific
questions about AI behavior and for assessing their core functionality in terms
of primitive executable actions. However, the problem of summarizing an AI
agent's broad capabilities for a user is comparatively new. This paper presents
an algorithm for discovering from scratch the suite of high-level
"capabilities" that an AI system with arbitrary internal planning
algorithms/policies can perform. It computes conditions describing the
applicability and effects of these capabilities in user-interpretable terms.
Starting from a set of user-interpretable state properties, an AI agent, and a
simulator that the agent can interact with, our algorithm returns a set of
high-level capabilities with their parameterized descriptions. Empirical
evaluation on several game-based scenarios shows that this approach efficiently
learns descriptions of various types of AI agents in deterministic, fully
observable settings. User studies show that such descriptions are easier to
understand and reason with than the agent's primitive actions.

We review practical challenges in building and deploying ethical AI at the
scale of contemporary industrial and societal uses. Apart from the purely
technical concerns that are the usual focus of academic research, the
operational challenges of inconsistent regulatory pressures, conflicting
business goals, data quality issues, development processes, systems integration
practices, and the scale of deployment all conspire to create new ethical
risks. Such ethical concerns arising from these practical considerations are
not adequately addressed by existing research results. We argue that a holistic
consideration of ethics in the development and deployment of AI systems is
necessary for building ethical AI in practice, and exhort researchers to
consider the full operational contexts of AI systems when assessing ethical
risks.

We describe ACE0, a lightweight platform for evaluating the suitability and
viability of AI methods for behaviour discovery in multiagent simulations.
Specifically, ACE0 was designed to explore AI methods for multi-agent
simulations used in operations research studies related to new technologies
such as autonomous aircraft. Simulation environments used in production are
often high-fidelity, complex, require significant domain knowledge and as a
result have high R&D costs. Minimal and lightweight simulation environments can
help researchers and engineers evaluate the viability of new AI technologies
for behaviour discovery in a more agile and potentially cost effective manner.
In this paper we describe the motivation for the development of ACE0.We provide
a technical overview of the system architecture, describe a case study of
behaviour discovery in the aerospace domain, and provide a qualitative
evaluation of the system. The evaluation includes a brief description of
collaborative research projects with academic partners, exploring different AI
behaviour discovery methods.

Analytical quality assurance, especially testing, is an integral part of
software-intensive system development. With the increased usage of Artificial
Intelligence (AI) and Machine Learning (ML) as part of such systems, this
becomes more difficult as well-understood software testing approaches cannot be
applied directly to the AI-enabled parts of the system. The required adaptation
of classical testing approaches and the development of new concepts for AI
would benefit from a deeper understanding and exchange between AI and software
engineering experts. We see the different terminologies used in the two
communities as a major obstacle on this way. As we consider a mutual
understanding of the testing terminology a key, this paper contributes a
mapping between the most important concepts from classical software testing and
AI testing. In the mapping, we highlight differences in the relevance and
naming of the mapped concepts.

This paper quantitatively reveals the state-of-the-art and
state-of-the-practice AI systems only achieve acceptable performance on the
stringent conditions that all categories of subjects are known, which we call
closed clinical settings, but fail to work in real-world clinical settings.
Compared to the diagnosis task in the closed setting, real-world clinical
settings pose severe challenges, and we must treat them differently. We build a
clinical AI benchmark named Clinical AIBench to set up real-world clinical
settings to facilitate researches. We propose an open, dynamic machine learning
framework and develop an AI system named OpenClinicalAI to diagnose diseases in
real-world clinical settings. The first versions of Clinical AIBench and
OpenClinicalAI target Alzheimer's disease. In the real-world clinical setting,
OpenClinicalAI significantly outperforms the state-of-the-art AI system. In
addition, OpenClinicalAI develops personalized diagnosis strategies to avoid
unnecessary testing and seamlessly collaborates with clinicians. It is
promising to be embedded in the current medical systems to improve medical
services.

Human-technology interaction deals with trust as an inevitable requirement
for user acceptance. As the applications of artificial intelligence (AI) and
robotics emerge and with their ever-growing socio-economic influence in various
fields of research and practice, there is an imminent need to study trust in
such systems. With the opaque work mechanism of AI-based systems and the
prospect of intelligent robots as workers' companions, context-specific
interdisciplinary studies on trust are key in increasing their adoption.
Through a thorough systematic literature review on (1) trust in AI and robotics
(AIR) and (2) AIR applications in the architecture, engineering, and
construction (AEC) industry, this study identifies common trust dimensions in
the literature and uses them to organize the paper. Furthermore, the
connections of the identified dimensions to the existing and potential AEC
applications are determined and discussed. Finally, major future directions on
trustworthy AI and robotics in AEC research and practice are outlined.

The premise of this paper is that compliance with Trustworthy AI governance
best practices and regulatory frameworks is an inherently fragmented process
spanning across diverse organizational units, external stakeholders, and
systems of record, resulting in process uncertainties and in compliance gaps
that may expose organizations to reputational and regulatory risks. Moreover,
there are complexities associated with meeting the specific dimensions of
Trustworthy AI best practices such as data governance, conformance testing,
quality assurance of AI model behaviors, transparency, accountability, and
confidentiality requirements. These processes involve multiple steps,
hand-offs, re-works, and human-in-the-loop oversight. In this paper, we
demonstrate that process mining can provide a useful framework for gaining
fact-based visibility to AI compliance process execution, surfacing compliance
bottlenecks, and providing for an automated approach to analyze, remediate and
monitor uncertainty in AI regulatory compliance processes.

This paper describes a research study that aims to investigate changes in
effective communication during human-AI collaboration with special attention to
the perception of competence among team members and varying levels of task load
placed on the team. We will also investigate differences between human-human
teamwork and human-agent teamwork. Our project will measure differences in the
communication quality, team perception and performance of a human actor playing
a Commercial Off - The Shelf game (COTS) with either a human teammate or a
simulated AI teammate under varying task load. We argue that the increased
cognitive workload associated with increases task load will be negatively
associated with team performance and have a negative impact on communication
quality. In addition, we argue that positive team perceptions will have a
positive impact on the communication quality between a user and teammate in
both the human and AI teammate conditions. This project will offer more refined
insights on Human - AI relationship dynamics in collaborative tasks by
considering communication quality, team perception, and performance under
increasing cognitive workload.

Principles of fairness and solidarity in AI ethics regularly overlap,
creating obscurity in practice: acting in accordance with one can appear
indistinguishable from deciding according to the rules of the other. However,
there exist irregular cases where the two concepts split, and so reveal their
disparate meanings and uses. This paper explores two cases in AI medical
ethics, one that is irregular and the other more conventional, to fully
distinguish fairness and solidarity. Then the distinction is applied to the
frequently cited COMPAS versus ProPublica dispute in judicial ethics. The
application provides a broader model for settling contemporary and topical
debates about fairness and solidarity. It also implies a deeper and
disorienting truth about AI ethics principles and their justification.

Governance efforts for artificial intelligence (AI) are taking on
increasingly more concrete forms, drawing on a variety of approaches and
instruments from hard regulation to standardisation efforts, aimed at
mitigating challenges from high-risk AI systems. To implement these and other
efforts, new institutions will need to be established on a national and
international level. This paper sketches a blueprint of such institutions, and
conducts in-depth investigations of three key components of any future AI
governance institutions, exploring benefits and associated drawbacks: (1)
purpose, relating to the institution's overall goals and scope of work or
mandate; (2) geography, relating to questions of participation and the reach of
jurisdiction; and (3) capacity, the infrastructural and human make-up of the
institution. Subsequently, the paper highlights noteworthy aspects of various
institutional roles specifically around questions of institutional purpose, and
frames what these could look like in practice, by placing these debates in a
European context and proposing different iterations of a European AI Agency.
Finally, conclusions and future research directions are proposed.

This study discusses the interplay between metrics used to measure the
explainability of the AI systems and the proposed EU Artificial Intelligence
Act. A standardisation process is ongoing: several entities (e.g. ISO) and
scholars are discussing how to design systems that are compliant with the
forthcoming Act and explainability metrics play a significant role. This study
identifies the requirements that such a metric should possess to ease
compliance with the AI Act. It does so according to an interdisciplinary
approach, i.e. by departing from the philosophical concept of explainability
and discussing some metrics proposed by scholars and standardisation entities
through the lenses of the explainability obligations set by the proposed AI
Act. Our analysis proposes that metrics to measure the kind of explainability
endorsed by the proposed AI Act shall be risk-focused, model-agnostic,
goal-aware, intelligible & accessible. This is why we discuss the extent to
which these requirements are met by the metrics currently under discussion.

Explainable artificial intelligence (xAI) is seen as a solution to making AI
systems less of a black box. It is essential to ensure transparency, fairness,
and accountability, which are especially paramount in the financial sector. The
aim of this study was a preliminary investigation of the perspectives of
supervisory authorities and regulated entities regarding the application of xAI
in the fi-nancial sector. Three use cases (consumer credit, credit risk, and
anti-money laundering) were examined using semi-structured interviews at three
banks and two supervisory authorities in the Netherlands. We found that for the
investigated use cases a disparity exists between supervisory authorities and
banks regarding the desired scope of explainability of AI systems. We argue
that the financial sector could benefit from clear differentiation between
technical AI (model) ex-plainability requirements and explainability
requirements of the broader AI system in relation to applicable laws and
regulations.

AI modeling for source code understanding tasks has been making significant
progress, and is being adopted in production development pipelines. However,
reliability concerns, especially whether the models are actually learning
task-related aspects of source code, are being raised. While recent
model-probing approaches have observed a lack of signal awareness in many
AI-for-code models, i.e. models not capturing task-relevant signals, they do
not offer solutions to rectify this problem. In this paper, we explore
data-driven approaches to enhance models' signal-awareness: 1) we combine the
SE concept of code complexity with the AI technique of curriculum learning; 2)
we incorporate SE assistance into AI models by customizing Delta Debugging to
generate simplified signal-preserving programs, augmenting them to the training
dataset. With our techniques, we achieve up to 4.8x improvement in model signal
awareness. Using the notion of code complexity, we further present a novel
model learning introspection approach from the perspective of the dataset.

Since the noun phrase `artificial intelligence' (AI) was coined, it has been
debated whether humans are able to create intelligence using technology. We
shed new light on this question from the point of view of themodynamics and
mathematics. First, we define what it is to be an agent (device) that could be
the bearer of AI. Then we show that the mainstream definitions of
`intelligence' proposed by Hutter and others and still accepted by the AI
community are too weak even to capture what is involved when we ascribe
intelligence to an insect. We then summarise the highly useful definition of
basic (arthropod) intelligence proposed by Rodney Brooks, and we identify the
properties that an AI agent would need to possess in order to be the bearer of
intelligence by this definition. Finally, we show that, from the perspective of
the disciplines needed to create such an agent, namely mathematics and physics,
these properties are realisable by neither implicit nor explicit mathematical
design nor by setting up an environment in which an AI could evolve
spontaneously.

Many educational technologies use artificial intelligence (AI) that presents
generated or produced language to the learner. We contend that all language,
including all AI communication, encodes information about the identity of the
human or humans who contributed to crafting the language. With AI
communication, however, the user may index identity information that does not
match the source. This can lead to representational harms if language
associated with one cultural group is presented as "standard" or "neutral", if
the language advantages one group over another, or if the language reinforces
negative stereotypes. In this work, we discuss a case study using a Visual
Question Generation (VQG) task involving gathering crowdsourced data from
targeted demographic groups. Generated questions will be presented to human
evaluators to understand how they index the identity behind the language,
whether and how they perceive any representational harms, and how they would
ideally address any such harms caused by AI communication. We reflect on the
educational applications of this work as well as the implications for equality,
diversity, and inclusion (EDI).

There has been recent and growing interest in the development and deployment
of autonomous vehicles, encouraged by the empirical successes of powerful
artificial intelligence techniques (AI), especially in the applications of deep
learning and reinforcement learning. However, as demonstrated by recent traffic
accidents, autonomous driving technology is not fully reliable for safe
deployment. As AI is the main technology behind the intelligent navigation
systems of self-driving vehicles, both the stakeholders and transportation
regulators require their AI-driven software architecture to be safe,
explainable, and regulatory compliant. In this paper, we propose a design
framework that integrates autonomous control, explainable AI (XAI), and
regulatory compliance to address this issue, and then provide an initial
validation of the framework with a critical analysis in a case study. Moreover,
we describe relevant XAI approaches that can help achieve the goals of the
framework.

Cellular networks, such as 5G systems, are becoming increasingly complex for
supporting various deployment scenarios and applications. Embracing artificial
intelligence (AI) in 5G evolution is critical to managing the complexity and
fueling the next quantum leap in 6G cellular networks. In this article, we
share our experience and best practices in applying AI in cellular networks. We
first present a primer on the state of the art of AI in cellular networks,
including basic concepts and recent key advances. Then we discuss 3GPP
standardization aspects and share various design rationales influencing
standardization. We also present case studies with real network data to
showcase how AI can improve network performance and enable network automation.

Single-agent reinforcement learning algorithms in a multi-agent environment
are inadequate for fostering cooperation. If intelligent agents are to interact
and work together to solve complex problems, methods that counter
non-cooperative behavior are needed to facilitate the training of multiple
agents. This is the goal of cooperative AI. Recent research in adversarial
machine learning, however, shows that models (e.g., image classifiers) can be
easily deceived into making inferior decisions. Meanwhile, an important line of
research in cooperative AI has focused on introducing algorithmic improvements
that accelerate learning of optimally cooperative behavior. We argue that
prominent methods of cooperative AI are exposed to weaknesses analogous to
those studied in prior machine learning research. More specifically, we show
that three algorithms inspired by human-like social intelligence are, in
principle, vulnerable to attacks that exploit weaknesses introduced by
cooperative AI's algorithmic improvements and report experimental findings that
illustrate how these vulnerabilities can be exploited in practice.

Non-orthogonal multiple access (NOMA) enabled fog radio access networks
(NOMA-F-RANs) have been taken as a promising enabler to release network
congestion, reduce delivery latency, and improve fog user equipments' (F-UEs')
quality of services (QoS). Nevertheless, the effectiveness of NOMA-F-RANs
highly relies on the charted feature information (preference distribution,
positions, mobilities, etc.) of F-UEs as well as the effective caching,
computing, and resource allocation strategies. In this article, we explore how
artificial intelligence (AI) techniques are utilized to solve foregoing
tremendous challenges. Specifically, we first elaborate on the NOMA-F-RANs
architecture, shedding light on the key modules, namely, cooperative caching
and cache-aided mobile edge computing (MEC). Then, the potentially applicable
AI-driven techniques in solving the principal issues of NOMA-F-RANs are
reviewed. Through case studies, we show the efficacy of AI-enabled methods in
terms of F-UEs' latent feature extraction and cooperative caching. Finally,
future trends of AI-driven NOMA-F-RANs, including open research issues and
challenges, are identified.

When 5G began its commercialisation journey around 2020, the discussion on
the vision of 6G also surfaced. Researchers expect 6G to have higher bandwidth,
coverage, reliability, energy efficiency, lower latency, and an integrated
"human-centric" network system powered by artificial intelligence (AI). Such a
6G network will lead to an excessive number of automated decisions made in
real-time. These decisions can range widely, from network resource allocation
to collision avoidance for self-driving cars. However, the risk of losing
control over decision-making may increase due to high-speed, data-intensive AI
decision-making beyond designers' and users' comprehension. The promising
explainable AI (XAI) methods can mitigate such risks by enhancing the
transparency of the black-box AI decision-making process. This paper surveys
the application of XAI towards the upcoming 6G age in every aspect, including
6G technologies (e.g., intelligent radio, zero-touch network management) and 6G
use cases (e.g., industry 5.0). Moreover, we summarised the lessons learned
from the recent attempts and outlined important research challenges in applying
XAI for 6G in the near future.

Today's battlefield environment is complex, dynamic and uncertain, and
requires efficient support to ensure mission success. This relies on a proper
support strategy to provide supported equipment able to fulfill the mission. In
the context of defense where both systems and organization are complex, having
a holistic approach is challenging by nature, forces and support agencies need
to rely on an efficient decision support system. Logistics, readiness and
sustainability are critical factors for asset management, which can benefit
from AI to reach "Smart In Service" level relying especially on predictive and
prescriptive approaches and on effective management of operational re-sources.
Smart Support capacities can be then monitored by appropriate metrics and
improved by multi-criteria decision support and knowledge management system.
Depending on the operational context in terms of information and the objective,
different AI paradigms (data-driven AI, knowledge-based AI) are suitable even a
combination through hybrid AI.

Modern deep learning frameworks provide imperative, eager execution
programming interfaces embedded in Python to provide a productive development
experience. However, deep learning practitioners sometimes need to capture and
transform program structure for performance optimization, visualization,
analysis, and hardware integration. We study the different designs for program
capture and transformation used in deep learning. By designing for typical deep
learning use cases rather than long tail ones, it is possible to create a
simpler framework for program capture and transformation. We apply this
principle in torch.fx, a program capture and transformation library for PyTorch
written entirely in Python and optimized for high developer productivity by ML
practitioners. We present case studies showing how torch.fx enables workflows
previously inaccessible in the PyTorch ecosystem.

Artificial intelligence (AI) enables machines to learn from human experience,
adjust to new inputs, and perform human-like tasks. AI is progressing rapidly
and is transforming the way businesses operate, from process automation to
cognitive augmentation of tasks and intelligent process/data analytics.
However, the main challenge for human users would be to understand and
appropriately trust the result of AI algorithms and methods. In this paper, to
address this challenge, we study and analyze the recent work done in
Explainable Artificial Intelligence (XAI) methods and tools. We introduce a
novel XAI process, which facilitates producing explainable models while
maintaining a high level of learning performance. We present an interactive
evidence-based approach to assist human users in comprehending and trusting the
results and output created by AI-enabled algorithms. We adopt a typical
scenario in the Banking domain for analyzing customer transactions. We develop
a digital dashboard to facilitate interacting with the algorithm results and
discuss how the proposed XAI method can significantly improve the confidence of
data scientists in understanding the result of AI-enabled algorithms.

Fully automatic semantic segmentation of highly specific semantic classes and
complex shapes may not meet the accuracy standards demanded by scientists. In
such cases, human-centered AI solutions, able to assist operators while
preserving human control over complex tasks, are a good trade-off to speed up
image labeling while maintaining high accuracy levels. TagLab is an open-source
AI-assisted software for annotating large orthoimages which takes advantage of
different degrees of automation; it speeds up image annotation from scratch
through assisted tools, creates custom fully automatic semantic segmentation
models, and, finally, allows the quick edits of automatic predictions. Since
the orthoimages analysis applies to several scientific disciplines, TagLab has
been designed with a flexible labeling pipeline. We report our results in two
different scenarios, marine ecology, and architectural heritage.

The field of mimicking the structure of the brain on a chip is experiencing
interest driven by the demand for machine intelligent applications. However,
the power consumption and available performance of machine-learning (ML)
accelerating hardware still leave much desire for improvement. In this letter,
we share viewpoints, challenges, and prospects of electronic-photonic neural
network (NN) accelerators. Combining electronics with photonics offers
synergistic co-design strategies for high-performance AI Application-specific
integrated circuits (ASICs) and systems. Taking advantages of photonic signal
processing capabilities and combining them with electronic logic control and
data storage is an emerging prospect. However, the optical component library
leaves much to be desired and is challenged by the enormous size of photonic
devices. Within this context, we will review the emerging electro-optic
materials, functional devices, and systems packaging strategies that, when
realized, provide significant performance gains and fuel the ongoing AI
revolution, leading to a stand-alone photonics-inside AI ASIC 'black-box' for
streamlined plug-and-play board integration in future AI processors.

The last two decades have seen tremendous advances in Artificial
Intelligence. The exponential growth in terms of computation capabilities has
given us hope of developing humans like robots. The question is: are we there
yet? Maybe not. With the integration of cognitive science, the 'artificial'
characteristic of Artificial Intelligence (AI) might soon be replaced with
'smart'. This will help develop more powerful AI systems and simultaneously
gives us a better understanding of how the human brain works. We discuss the
various possibilities and challenges of bridging these two fields and how they
can benefit each other. We argue that the possibility of AI taking over human
civilization is low as developing such an advanced system requires a better
understanding of the human brain first.

When a student is asked to perform a given task, her subjective estimate of
the difficulty of that task has a strong influence on her performance. There
exists a rich literature on the impact of perceived task difficulty on
performance and motivation. Yet, there is another topic that is closely related
to the subject of the influence of perceived task difficulty that did not
receive any attention in previous research - the influence of revealing the
true difficulty of a task to the student. This paper investigates the impact of
revealing the task difficulty on the student's performance, motivation,
self-efficacy and subjective task value via an experiment in which workers are
asked to solve matchstick riddles. Furthermore, we discuss how the experiment
results might be relevant for AI-aided education. Specifically, we elaborate on
the question of how a student's learning experience might be improved by
supporting her with two types of AI systems: an AI system that predicts task
difficulty and an AI system that determines when task difficulty should be
revealed and when not.

An imperfect-information game is a type of game with asymmetric information.
It is more common in life than perfect-information game. Artificial
intelligence (AI) in imperfect-information games, such like poker, has made
considerable progress and success in recent years. The great success of
superhuman poker AI, such as Libratus and Deepstack, attracts researchers to
pay attention to poker research. However, the lack of open-source code limits
the development of Texas hold'em AI to some extent. This article introduces
DecisionHoldem, a high-level AI for heads-up no-limit Texas hold'em with safe
depth-limited subgame solving by considering possible ranges of opponent's
private hands to reduce the exploitability of the strategy. Experimental
results show that DecisionHoldem defeats the strongest openly available agent
in heads-up no-limit Texas hold'em poker, namely Slumbot, and a high-level
reproduction of Deepstack, viz, Openstack, by more than 730 mbb/h
(one-thousandth big blind per round) and 700 mbb/h. Moreover, we release the
source codes and tools of DecisionHoldem to promote AI development in
imperfect-information games.

Mentorship in the AI community is crucial to maintaining and increasing
diversity, especially with respect to fostering the academic growth of
underserved students. While the research process itself is important, there is
not sufficient emphasis on the submission, presentation, and publication
process, which is a cause for concern given the meteoric rise of predatory
scientific conferences, which are based on profit only and have little to no
peer review. These conferences are a direct threat to integrity in science by
promoting work with little to no scientific merit. However, they also threaten
diversity in the AI community by marginalizing underrepresented groups away
from legitimate conferences due to convenience and targeting mechanisms like
e-mail invitations. Due to the importance of conference presentation in AI
research, this very specific problem must be addressed through direct
mentorship. In this work, we propose PreDefense, a mentorship program that
seeks to guide underrepresented students through the scientific conference and
workshop process, with an emphasis on choosing legitimate venues that align
with the specific work that the students are focused in and preparing students
of all backgrounds for future successful, integrous AI research careers.

What does it mean for a generative AI model to be explainable? The emergent
discipline of explainable AI (XAI) has made great strides in helping people
understand discriminative models. Less attention has been paid to generative
models that produce artifacts, rather than decisions, as output. Meanwhile,
generative AI (GenAI) technologies are maturing and being applied to
application domains such as software engineering. Using scenario-based design
and question-driven XAI design approaches, we explore users' explainability
needs for GenAI in three software engineering use cases: natural language to
code, code translation, and code auto-completion. We conducted 9 workshops with
43 software engineers in which real examples from state-of-the-art generative
AI models were used to elicit users' explainability needs. Drawing from prior
work, we also propose 4 types of XAI features for GenAI for code and gathered
additional design ideas from participants. Our work explores explainability
needs for GenAI for code and demonstrates how human-centered approaches can
drive the technical development of XAI in novel domains.

We conducted a lab-based eye-tracking study to investigate how the
interactivity of an AI-powered fact-checking system affects user interactions,
such as dwell time, attention, and mental resources involved in using the
system. A within-subject experiment was conducted, where participants used an
interactive and a non-interactive version of a mock AI fact-checking system and
rated their perceived correctness of COVID-19 related claims. We collected
web-page interactions, eye-tracking data, and mental workload using NASA-TLX.
We found that the presence of the affordance of interactively manipulating the
AI system's prediction parameters affected users' dwell times, and
eye-fixations on AOIs, but not mental workload. In the interactive system,
participants spent the most time evaluating claims' correctness, followed by
reading news. This promising result shows a positive role of interactivity in a
mixed-initiative AI-powered system.

Mahjong is a popular multi-player imperfect-information game developed in
China in the late 19th-century, with some very challenging features for AI
research. Sanma, being a 3-player variant of the Japanese Riichi Mahjong,
possesses unique characteristics including fewer tiles and, consequently, a
more aggressive playing style. It is thus challenging and of great research
interest in its own right, but has not yet been explored. In this paper, we
present Meowjong, an AI for Sanma using deep reinforcement learning. We define
an informative and compact 2-dimensional data structure for encoding the
observable information in a Sanma game. We pre-train 5 convolutional neural
networks (CNNs) for Sanma's 5 actions -- discard, Pon, Kan, Kita and Riichi,
and enhance the major action's model, namely the discard model, via self-play
reinforcement learning using the Monte Carlo policy gradient method. Meowjong's
models achieve test accuracies comparable with AIs for 4-player Mahjong through
supervised learning, and gain a significant further enhancement from
reinforcement learning. Being the first ever AI in Sanma, we claim that
Meowjong stands as a state-of-the-art in this game.

In many applications of forensic image analysis, state-of-the-art results are
nowadays achieved with machine learning methods. However, concerns about their
reliability and opaqueness raise the question whether such methods can be used
in criminal investigations. So far, this question of legal compliance has
hardly been discussed, also because legal regulations for machine learning
methods were not defined explicitly. To this end, the European Commission
recently proposed the artificial intelligence (AI) act, a regulatory framework
for the trustworthy use of AI. Under the draft AI act, high-risk AI systems for
use in law enforcement are permitted but subject to compliance with mandatory
requirements. In this paper, we review why the use of machine learning in
forensic image analysis is classified as high-risk. We then summarize the
mandatory requirements for high-risk AI systems and discuss these requirements
in light of two forensic applications, license plate recognition and deep fake
detection. The goal of this paper is to raise awareness of the upcoming legal
requirements and to point out avenues for future research.

Artificial intelligence (AI) techniques have emerged as a powerful approach
to make wireless networks more efficient and adaptable. In this paper we
present an ns-3 simulation framework, able to implement AI algorithms for the
optimization of wireless networks. Our pipeline consists of: (i) a new
geometry-based mobility-dependent channel model for V2X; (ii) all the layers of
a 5G-NR-compliant protocol stack, based on the ns3-mmwave module; (iii) a new
application to simulate V2X data transmission, and (iv) a new intelligent
entity for the control of the network via AI. Thanks to its flexible and
modular design, researchers can use this tool to implement, train, and evaluate
their own algorithms in a realistic and controlled environment. We test the
behavior of our framework in a Predictive Quality of Service (PQoS) scenario,
where AI functionalities are implemented using Reinforcement Learning (RL), and
demonstrate that it promotes better network optimization compared to baseline
solutions that do not implement AI.

Training an AI/ML system on simulated data while using that system to infer
on data from real detectors introduces a systematic error which is difficult to
estimate and in many analyses is simply not confronted. It is crucial to
minimize and to quantitatively estimate the uncertainties in such analysis and
do so with a precision and accuracy that matches those that AI/ML techniques
bring. Here we highlight the need to confront this class of systematic error,
discuss conventional ways to estimate it and describe ways to quantify and to
minimize the uncertainty using methods which are themselves based on the power
of AI/ML. We also describe methods to introduce a simulation into an AI/ML
network to allow for training of its semantically meaningful parameters. This
whitepaper is a contribution to the Computational Frontier of Snowmass21.

Being able to explain the prediction to clinical end-users is a necessity to
leverage the power of artificial intelligence (AI) models for clinical decision
support. For medical images, a feature attribution map, or heatmap, is the most
common form of explanation that highlights important features for AI models'
prediction. However, it is unknown how well heatmaps perform on explaining
decisions on multi-modal medical images, where each image modality or channel
visualizes distinct clinical information of the same underlying biomedical
phenomenon. Understanding such modality-dependent features is essential for
clinical users' interpretation of AI decisions. To tackle this clinically
important but technically ignored problem, we propose the modality-specific
feature importance (MSFI) metric. It encodes clinical image and explanation
interpretation patterns of modality prioritization and modality-specific
feature localization. We conduct a clinical requirement-grounded, systematic
evaluation using computational methods and a clinician user study. Results show
that the examined 16 heatmap algorithms failed to fulfill clinical requirements
to correctly indicate AI model decision process or decision quality. The
evaluation and MSFI metric can guide the design and selection of XAI algorithms
to meet clinical requirements on multi-modal explanation.

As artificial intelligence becomes more powerful and a ubiquitous presence in
daily life, it is imperative to understand and manage the impact of AI systems
on our lives and decisions. Modern ML systems often change user behavior (e.g.
personalized recommender systems learn user preferences to deliver
recommendations that change online behavior). An externality of behavior change
is preference change. This article argues for the establishment of a
multidisciplinary endeavor focused on understanding how AI systems change
preference: Preference Science. We operationalize preference to incorporate
concepts from various disciplines, outlining the importance of meta-preferences
and preference-change preferences, and proposing a preliminary framework for
how preferences change. We draw a distinction between preference change,
permissible preference change, and outright preference manipulation. A
diversity of disciplines contribute unique insights to this framework.

In this position paper we draw attention to safety risks against youth and
young adults that originate through the combination of online and in-person
interaction, and opportunities for AI to address these risks. Our context of
study is social matching systems (e.g., Tinder, Bumble), which are used by
young adults for online-to-offline interaction with strangers, and which are
correlated with sexual violence both online and in-person. The paper presents
early insights from an ongoing participatory AI design study in which young
women build directly explainable models for detecting risk associated with
discovered social opportunities, and articulate what AI should do once risk has
been detected. We seek to advocate for participatory AI design as a way to
directly incorporate youth and young adults into the design of a safer
Internet. We also draw attention to challenges with the method.

In this position paper we intend to advocate for participatory design methods
and mobile social matching apps as ripe contexts for exploring novel human-AI
interactions that benefit marginalized groups. Mobile social matching apps like
Tinder and Bumble use AI to introduce users to each other for rapid
face-to-face meetings. These user discoveries and subsequent interactions pose
disproportionate risk of sexual violence and other harms to marginalized user
demographics, specifically women and the LGBTQIA+ community. We want to extend
the role of AI in these apps to keep users safe while they interact with
strangers across online and offline modalities. To do this, we are using
participatory design methods to empower women and LGBTQIA+ individuals to
envision future human-AI interactions that prioritize their safety during
social matching app-use. In one study, stakeholders identifying as LGBTQIA+ or
women are redesigning dating apps to mediate exchange of sexual consent and
therefore mitigate sexual violence. In the other study, women are designing
multi-purpose, opportunistic social matching apps that foreground women's
safety.

AI-based decision support tools (ADS) are increasingly used to augment human
decision-making in high-stakes, social contexts. As public sector agencies
begin to adopt ADS, it is critical that we understand workers' experiences with
these systems in practice. In this paper, we present findings from a series of
interviews and contextual inquiries at a child welfare agency, to understand
how they currently make AI-assisted child maltreatment screening decisions.
Overall, we observe how workers' reliance upon the ADS is guided by (1) their
knowledge of rich, contextual information beyond what the AI model captures,
(2) their beliefs about the ADS's capabilities and limitations relative to
their own, (3) organizational pressures and incentives around the use of the
ADS, and (4) awareness of misalignments between algorithmic predictions and
their own decision-making objectives. Drawing upon these findings, we discuss
design implications towards supporting more effective human-AI decision-making.

Recent years have witnessed the great breakthrough of deep reinforcement
learning (DRL) in various perfect and imperfect information games. Among these
games, DouDizhu, a popular card game in China, is very challenging due to the
imperfect information, large state space, elements of collaboration and a
massive number of possible moves from turn to turn. Recently, a DouDizhu AI
system called DouZero has been proposed. Trained using traditional Monte Carlo
method with deep neural networks and self-play procedure without the
abstraction of human prior knowledge, DouZero has outperformed all the existing
DouDizhu AI programs. In this work, we propose to enhance DouZero by
introducing opponent modeling into DouZero. Besides, we propose a novel coach
network to further boost the performance of DouZero and accelerate its training
process. With the integration of the above two techniques into DouZero, our
DouDizhu AI system achieves better performance and ranks top in the Botzone
leaderboard among more than 400 AI agents, including DouZero.

In this paper we presented the Software Testing, AI and Robotics (STAIR)
Learning Lab. STAIR is an initiative started at the University of Innsbruck to
bring robotics, Artificial Intelligence (AI) and software testing into schools.
In the lab physical and virtual learning units are developed in parallel and in
sync with each other. Its core learning approach is based the develop of both a
physical and simulated robotics environment. In both environments AI scenarios
(like traffic sign recognition) are deployed and tested. We present and focus
on our newly designed MiniBot that are both built on hardware which was
designed for educational and research purposes as well as the simulation
environment. Additionally, we describe first learning design concepts and a
showcase scenario (i.e., AI-based traffic sign recognition) with different
exercises which can easily be extended.

Artificial Intelligence (AI) is often defined as the next general purpose
technology (GPT) with profound economic and societal consequences. We examine
how strongly four patent AI classification methods reproduce the GPT-like
features of (1) intrinsic growth, (2) generality, and (3) innovation
complementarities. Studying US patents from 1990-2019, we find that the four
methods (keywords, scientific citations, WIPO, and USPTO approach) vary in
classifying between 3-17% of all patents as AI. The keyword-based approach
demonstrates the strongest intrinsic growth and generality despite identifying
the smallest set of AI patents. The WIPO and science approaches generate each
GPT characteristic less strikingly, whilst the USPTO set with the largest
number of patents produces the weakest features. The lack of overlap and
heterogeneity between all four approaches emphasises that the evaluation of AI
innovation policies may be sensitive to the choice of classification method.

Ensuring fairness in artificial intelligence (AI) is important to counteract
bias and discrimination in far-reaching applications. Recent work has started
to investigate how humans judge fairness and how to support machine learning
(ML) experts in making their AI models fairer. Drawing inspiration from an
Explainable AI (XAI) approach called \emph{explanatory debugging} used in
interactive machine learning, our work explores designing interpretable and
interactive human-in-the-loop interfaces that allow ordinary end-users without
any technical or domain background to identify potential fairness issues and
possibly fix them in the context of loan decisions. Through workshops with
end-users, we co-designed and implemented a prototype system that allowed
end-users to see why predictions were made, and then to change weights on
features to "debug" fairness issues. We evaluated the use of this prototype
system through an online study. To investigate the implications of diverse
human values about fairness around the globe, we also explored how cultural
dimensions might play a role in using this prototype. Our results contribute to
the design of interfaces to allow end-users to be involved in judging and
addressing AI fairness through a human-in-the-loop approach.

Despite impressive performance in many benchmark datasets, AI models can
still make mistakes, especially among out-of-distribution examples. It remains
an open question how such imperfect models can be used effectively in
collaboration with humans. Prior work has focused on AI assistance that helps
people make individual high-stakes decisions, which is not scalable for a large
amount of relatively low-stakes decisions, e.g., moderating social media
comments. Instead, we propose conditional delegation as an alternative paradigm
for human-AI collaboration where humans create rules to indicate trustworthy
regions of a model. Using content moderation as a testbed, we develop novel
interfaces to assist humans in creating conditional delegation rules and
conduct a randomized experiment with two datasets to simulate in-distribution
and out-of-distribution scenarios. Our study demonstrates the promise of
conditional delegation in improving model performance and provides insights
into design for this novel paradigm, including the effect of AI explanations.

Artificial Intelligence (AI) has found its applications in a variety of
environments ranging from data science to cybersecurity. AI helps break through
the limitations of traditional algorithms and provides more efficient and
flexible methods for solving problems. In this paper, we focus on the
applications of artificial intelligence in authentication, which is used in a
wide range of scenarios including facial recognition to access buildings,
keystroke dynamics to unlock smartphones. With the emerging AI-assisted
authentication schemes, our comprehensive survey provides an overall
understanding on a high level, which paves the way for future research in this
area. In contrast to other relevant surveys, our research is the first of its
kind to focus on the roles of AI in authentication.

Artificial intelligence is gaining momentum, ongoing pandemic is fuel to that
with more opportunities in every sector specially in health and education
sector. But with the growth in technology, challenges associated with ethics
also grow (Katharine Schwab, 2021). Whenever a new AI product is developed,
companies publicize that their systems are transparent, fair, and are in
accordance with the existing laws and regulations as the methods and procedures
followed by a big tech company for ensuring AI ethics, not only affect the
trust and perception of public, but it also challenges the capabilities of the
companies towards business strategies in different regions, and the kind of
brains it can attract for their projects. AI Big Tech companies have influence
over AI ethics as many influencing ethical-AI researchers have roots in Big
Tech or its associated labs.

Research on human-AI teams usually provides experts with a single label,
which ignores the uncertainty in a model's recommendation. Conformal prediction
(CP) is a well established line of research that focuses on building a
theoretically grounded, calibrated prediction set, which may contain multiple
labels. We explore how such prediction sets impact expert decision-making in
human-AI teams. Our evaluation on human subjects finds that set valued
predictions positively impact experts. However, we notice that the predictive
sets provided by CP can be very large, which leads to unhelpful AI assistants.
To mitigate this, we introduce D-CP, a method to perform CP on some examples
and defer to experts. We prove that D-CP can reduce the prediction set size of
non-deferred examples. We show how D-CP performs in quantitative and in human
subject experiments ($n=120$). Our results suggest that CP prediction sets
improve human-AI team performance over showing the top-1 prediction alone, and
that experts find D-CP prediction sets are more useful than CP prediction sets.

AI-synthesized faces are visually challenging to discern from real ones. They
have been used as profile images for fake social media accounts, which leads to
high negative social impacts. Although progress has been made in developing
automatic methods to detect AI-synthesized faces, there is no open platform to
study the human performance of AI-synthesized faces detection. In this work, we
develop an online platform called Open-eye to study the human performance of
AI-synthesized face detection. We describe the design and workflow of the
Open-eye in this paper.

The impact of Artificial Intelligence does not depend only on fundamental
research and technological developments, but for a large part on how these
systems are introduced into society and used in everyday situations. AI is
changing the way we work, live and solve challenges but concerns about
fairness, transparency or privacy are also growing. Ensuring responsible,
ethical AI is more than designing systems whose result can be trusted. It is
about the way we design them, why we design them, and who is involved in
designing them. In order to develop and use AI responsibly, we need to work
towards technical, societal, institutional and legal methods and tools which
provide concrete support to AI practitioners, as well as awareness and training
to enable participation of all, to ensure the alignment of AI systems with our
societies' principles and values.

